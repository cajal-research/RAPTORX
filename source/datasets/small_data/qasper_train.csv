title,abstract,full_text_0_section_name,full_text_0_paragraphs,full_text_1_section_name,full_text_1_paragraphs,full_text_2_section_name,full_text_2_paragraphs,full_text_3_section_name,full_text_3_paragraphs,full_text_4_section_name,full_text_4_paragraphs,full_text_5_section_name,full_text_5_paragraphs,full_text_6_section_name,full_text_6_paragraphs,full_text_7_section_name,full_text_7_paragraphs,full_text_8_section_name,full_text_8_paragraphs,full_text_9_section_name,full_text_9_paragraphs,full_text_10_section_name,full_text_10_paragraphs,full_text_11_section_name,full_text_11_paragraphs,full_text_12_section_name,full_text_12_paragraphs,full_text_13_section_name,full_text_13_paragraphs,full_text_14_section_name,full_text_14_paragraphs,full_text_15_section_name,full_text_15_paragraphs,full_text_16_section_name,full_text_16_paragraphs,full_text_17_section_name,full_text_17_paragraphs,full_text_18_section_name,full_text_18_paragraphs,full_text_19_section_name,full_text_19_paragraphs,full_text_20_section_name,full_text_20_paragraphs,qas_0_question,qas_0_question_id,qas_0_nlp_background,qas_0_topic_background,qas_0_paper_read,qas_0_search_query,qas_0_question_writer,qas_0_answers_0_answer_unanswerable,qas_0_answers_0_answer_yes_no,qas_0_answers_0_answer_free_form_answer,qas_0_answers_0_answer_evidence,qas_0_answers_0_answer_highlighted_evidence,qas_0_answers_0_annotation_id,qas_0_answers_0_worker_id,qas_0_answers_1_answer_unanswerable,qas_0_answers_1_answer_extractive_spans,qas_0_answers_1_answer_yes_no,qas_0_answers_1_answer_free_form_answer,qas_0_answers_1_answer_evidence,qas_0_answers_1_answer_highlighted_evidence,qas_0_answers_1_annotation_id,qas_0_answers_1_worker_id,qas_1_question,qas_1_question_id,qas_1_nlp_background,qas_1_topic_background,qas_1_paper_read,qas_1_search_query,qas_1_question_writer,qas_1_answers_0_answer_unanswerable,qas_1_answers_0_answer_yes_no,qas_1_answers_0_answer_free_form_answer,qas_1_answers_0_answer_evidence,qas_1_answers_0_answer_highlighted_evidence,qas_1_answers_0_annotation_id,qas_1_answers_0_worker_id,qas_2_question,qas_2_question_id,qas_2_nlp_background,qas_2_topic_background,qas_2_paper_read,qas_2_search_query,qas_2_question_writer,qas_2_answers_0_answer_unanswerable,qas_2_answers_0_answer_yes_no,qas_2_answers_0_answer_free_form_answer,qas_2_answers_0_answer_evidence,qas_2_answers_0_answer_highlighted_evidence,qas_2_answers_0_annotation_id,qas_2_answers_0_worker_id,qas_2_answers_1_answer_unanswerable,qas_2_answers_1_answer_yes_no,qas_2_answers_1_answer_free_form_answer,qas_2_answers_1_answer_evidence,qas_2_answers_1_answer_highlighted_evidence,qas_2_answers_1_annotation_id,qas_2_answers_1_worker_id,qas_3_question,qas_3_question_id,qas_3_nlp_background,qas_3_topic_background,qas_3_paper_read,qas_3_search_query,qas_3_question_writer,qas_3_answers_0_answer_unanswerable,qas_3_answers_0_answer_yes_no,qas_3_answers_0_answer_free_form_answer,qas_3_answers_0_answer_evidence,qas_3_answers_0_answer_highlighted_evidence,qas_3_answers_0_annotation_id,qas_3_answers_0_worker_id,qas_3_answers_1_answer_unanswerable,qas_3_answers_1_answer_yes_no,qas_3_answers_1_answer_free_form_answer,qas_3_answers_1_answer_evidence,qas_3_answers_1_answer_highlighted_evidence,qas_3_answers_1_annotation_id,qas_3_answers_1_worker_id,qas_4_question,qas_4_question_id,qas_4_nlp_background,qas_4_topic_background,qas_4_paper_read,qas_4_search_query,qas_4_question_writer,qas_4_answers_0_answer_unanswerable,qas_4_answers_0_answer_extractive_spans,qas_4_answers_0_answer_yes_no,qas_4_answers_0_answer_free_form_answer,qas_4_answers_0_answer_evidence,qas_4_answers_0_answer_highlighted_evidence,qas_4_answers_0_annotation_id,qas_4_answers_0_worker_id,qas_5_question,qas_5_question_id,qas_5_nlp_background,qas_5_topic_background,qas_5_paper_read,qas_5_search_query,qas_5_question_writer,qas_5_answers_0_answer_unanswerable,qas_5_answers_0_answer_yes_no,qas_5_answers_0_answer_free_form_answer,qas_5_answers_0_answer_evidence,qas_5_answers_0_answer_highlighted_evidence,qas_5_answers_0_annotation_id,qas_5_answers_0_worker_id,qas_6_question,qas_6_question_id,qas_6_nlp_background,qas_6_topic_background,qas_6_paper_read,qas_6_search_query,qas_6_question_writer,qas_6_answers_0_answer_unanswerable,qas_6_answers_0_answer_yes_no,qas_6_answers_0_answer_free_form_answer,qas_6_answers_0_answer_evidence,qas_6_answers_0_answer_highlighted_evidence,qas_6_answers_0_annotation_id,qas_6_answers_0_worker_id,qas_7_question,qas_7_question_id,qas_7_nlp_background,qas_7_topic_background,qas_7_paper_read,qas_7_search_query,qas_7_question_writer,qas_7_answers_0_answer_unanswerable,qas_7_answers_0_answer_yes_no,qas_7_answers_0_answer_free_form_answer,qas_7_answers_0_answer_evidence,qas_7_answers_0_answer_highlighted_evidence,qas_7_answers_0_annotation_id,qas_7_answers_0_worker_id,qas_8_question,qas_8_question_id,qas_8_nlp_background,qas_8_topic_background,qas_8_paper_read,qas_8_search_query,qas_8_question_writer,qas_8_answers_0_answer_unanswerable,qas_8_answers_0_answer_extractive_spans,qas_8_answers_0_answer_yes_no,qas_8_answers_0_answer_free_form_answer,qas_8_answers_0_answer_evidence,qas_8_answers_0_answer_highlighted_evidence,qas_8_answers_0_annotation_id,qas_8_answers_0_worker_id,figures_and_tables_0_file,figures_and_tables_0_caption,figures_and_tables_1_file,figures_and_tables_1_caption,figures_and_tables_2_file,figures_and_tables_2_caption,figures_and_tables_3_file,figures_and_tables_3_caption,figures_and_tables_4_file,figures_and_tables_4_caption,figures_and_tables_5_file,figures_and_tables_5_caption,full_text_21_section_name,full_text_21_paragraphs,full_text_22_section_name,full_text_22_paragraphs,full_text_23_section_name,full_text_23_paragraphs,full_text_24_section_name,full_text_24_paragraphs,qas_1_answers_0_answer_extractive_spans,qas_2_answers_0_answer_extractive_spans,figures_and_tables_6_file,figures_and_tables_6_caption,figures_and_tables_7_file,figures_and_tables_7_caption,figures_and_tables_8_file,figures_and_tables_8_caption,figures_and_tables_9_file,figures_and_tables_9_caption,figures_and_tables_10_file,figures_and_tables_10_caption,qas_5_answers_0_answer_extractive_spans,qas_0_answers_0_answer_extractive_spans,qas_1_answers_1_answer_unanswerable,qas_1_answers_1_answer_extractive_spans,qas_1_answers_1_answer_yes_no,qas_1_answers_1_answer_free_form_answer,qas_1_answers_1_answer_evidence,qas_1_answers_1_answer_highlighted_evidence,qas_1_answers_1_annotation_id,qas_1_answers_1_worker_id,qas_3_answers_0_answer_extractive_spans,qas_9_question,qas_9_question_id,qas_9_nlp_background,qas_9_topic_background,qas_9_paper_read,qas_9_search_query,qas_9_question_writer,qas_9_answers_0_answer_unanswerable,qas_9_answers_0_answer_extractive_spans,qas_9_answers_0_answer_yes_no,qas_9_answers_0_answer_free_form_answer,qas_9_answers_0_answer_evidence,qas_9_answers_0_answer_highlighted_evidence,qas_9_answers_0_annotation_id,qas_9_answers_0_worker_id,qas_10_question,qas_10_question_id,qas_10_nlp_background,qas_10_topic_background,qas_10_paper_read,qas_10_search_query,qas_10_question_writer,qas_10_answers_0_answer_unanswerable,qas_10_answers_0_answer_extractive_spans,qas_10_answers_0_answer_yes_no,qas_10_answers_0_answer_free_form_answer,qas_10_answers_0_answer_evidence,qas_10_answers_0_answer_highlighted_evidence,qas_10_answers_0_annotation_id,qas_10_answers_0_worker_id,qas_11_question,qas_11_question_id,qas_11_nlp_background,qas_11_topic_background,qas_11_paper_read,qas_11_search_query,qas_11_question_writer,qas_11_answers_0_answer_unanswerable,qas_11_answers_0_answer_extractive_spans,qas_11_answers_0_answer_yes_no,qas_11_answers_0_answer_free_form_answer,qas_11_answers_0_answer_evidence,qas_11_answers_0_answer_highlighted_evidence,qas_11_answers_0_annotation_id,qas_11_answers_0_worker_id,qas_2_answers_1_answer_extractive_spans,qas_3_answers_1_answer_extractive_spans,figures_and_tables_11_file,figures_and_tables_11_caption,figures_and_tables_12_file,figures_and_tables_12_caption,qas_4_answers_1_answer_unanswerable,qas_4_answers_1_answer_yes_no,qas_4_answers_1_answer_free_form_answer,qas_4_answers_1_answer_evidence,qas_4_answers_1_answer_highlighted_evidence,qas_4_answers_1_annotation_id,qas_4_answers_1_worker_id,figures_and_tables_13_file,figures_and_tables_13_caption,figures_and_tables_14_file,figures_and_tables_14_caption,figures_and_tables_15_file,figures_and_tables_15_caption,figures_and_tables_16_file,figures_and_tables_16_caption,qas_7_answers_0_answer_extractive_spans,qas_6_answers_0_answer_extractive_spans,full_text_25_section_name,full_text_25_paragraphs,full_text_26_section_name,full_text_26_paragraphs,full_text_27_section_name,full_text_27_paragraphs,full_text_28_section_name,full_text_28_paragraphs,full_text_29_section_name,full_text_29_paragraphs,full_text_30_section_name,full_text_30_paragraphs,full_text_31_section_name,full_text_31_paragraphs,full_text_32_section_name,full_text_32_paragraphs,full_text_33_section_name,full_text_33_paragraphs,full_text_34_section_name,full_text_34_paragraphs,full_text_35_section_name,full_text_35_paragraphs,figures_and_tables_17_file,figures_and_tables_17_caption,figures_and_tables_18_file,figures_and_tables_18_caption,figures_and_tables_19_file,figures_and_tables_19_caption,figures_and_tables_20_file,figures_and_tables_20_caption,figures_and_tables_21_file,figures_and_tables_21_caption,figures_and_tables_22_file,figures_and_tables_22_caption,figures_and_tables_23_file,figures_and_tables_23_caption,figures_and_tables_24_file,figures_and_tables_24_caption,figures_and_tables_25_file,figures_and_tables_25_caption,figures_and_tables_26_file,figures_and_tables_26_caption,figures_and_tables_27_file,figures_and_tables_27_caption,figures_and_tables_28_file,figures_and_tables_28_caption,full_text_36_section_name,full_text_36_paragraphs,full_text_37_section_name,full_text_37_paragraphs,full_text_38_section_name,full_text_38_paragraphs,full_text_39_section_name,full_text_39_paragraphs,full_text_40_section_name,full_text_40_paragraphs,full_text_41_section_name,full_text_41_paragraphs,full_text_42_section_name,full_text_42_paragraphs,full_text_43_section_name,full_text_43_paragraphs,figures_and_tables_29_file,figures_and_tables_29_caption,figures_and_tables_30_file,figures_and_tables_30_caption,figures_and_tables_31_file,figures_and_tables_31_caption,figures_and_tables_32_file,figures_and_tables_32_caption,full_text_44_section_name,full_text_44_paragraphs,full_text_45_section_name,full_text_45_paragraphs,full_text_46_section_name,full_text_46_paragraphs,full_text_47_section_name,full_text_47_paragraphs,full_text_48_section_name,full_text_48_paragraphs,figures_and_tables_33_file,figures_and_tables_33_caption,figures_and_tables_34_file,figures_and_tables_34_caption,figures_and_tables_35_file,figures_and_tables_35_caption,figures_and_tables_36_file,figures_and_tables_36_caption,figures_and_tables_37_file,figures_and_tables_37_caption,figures_and_tables_38_file,figures_and_tables_38_caption,figures_and_tables_39_file,figures_and_tables_39_caption,figures_and_tables_40_file,figures_and_tables_40_caption,figures_and_tables_41_file,figures_and_tables_41_caption,figures_and_tables_42_file,figures_and_tables_42_caption,figures_and_tables_43_file,figures_and_tables_43_caption,figures_and_tables_44_file,figures_and_tables_44_caption,figures_and_tables_45_file,figures_and_tables_45_caption,figures_and_tables_46_file,figures_and_tables_46_caption,figures_and_tables_47_file,figures_and_tables_47_caption,figures_and_tables_48_file,figures_and_tables_48_caption,figures_and_tables_49_file,figures_and_tables_49_caption,figures_and_tables_50_file,figures_and_tables_50_caption,figures_and_tables_51_file,figures_and_tables_51_caption,figures_and_tables_52_file,figures_and_tables_52_caption,figures_and_tables_53_file,figures_and_tables_53_caption,figures_and_tables_54_file,figures_and_tables_54_caption,figures_and_tables_55_file,figures_and_tables_55_caption,figures_and_tables_56_file,figures_and_tables_56_caption,figures_and_tables_57_file,figures_and_tables_57_caption,figures_and_tables_58_file,figures_and_tables_58_caption,figures_and_tables_59_file,figures_and_tables_59_caption,figures_and_tables_60_file,figures_and_tables_60_caption,figures_and_tables_61_file,figures_and_tables_61_caption,figures_and_tables_62_file,figures_and_tables_62_caption,figures_and_tables_63_file,figures_and_tables_63_caption,figures_and_tables_64_file,figures_and_tables_64_caption,figures_and_tables_65_file,figures_and_tables_65_caption,figures_and_tables_66_file,figures_and_tables_66_caption,figures_and_tables_67_file,figures_and_tables_67_caption,figures_and_tables_68_file,figures_and_tables_68_caption,figures_and_tables_69_file,figures_and_tables_69_caption,figures_and_tables_70_file,figures_and_tables_70_caption,figures_and_tables_71_file,figures_and_tables_71_caption,figures_and_tables_72_file,figures_and_tables_72_caption,figures_and_tables_73_file,figures_and_tables_73_caption,figures_and_tables_74_file,figures_and_tables_74_caption,figures_and_tables_75_file,figures_and_tables_75_caption,figures_and_tables_76_file,figures_and_tables_76_caption,figures_and_tables_77_file,figures_and_tables_77_caption
Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning,"Universal feature extractors, such as BERT for natural language processing and VGG for computer vision, have become effective methods for improving deep learning models without requiring more labeled data. A common paradigm is to pre-train a feature extractor on large amounts of data then fine-tune it as part of a deep learning model on some downstream task (i.e. transfer learning). While effective, feature extractors like BERT may be prohibitively large for some deployment scenarios. We explore weight pruning for BERT and ask: how does compression during pre-training affect transfer learning? We find that pruning affects transfer learning in three broad regimes. Low levels of pruning (30-40\%) do not affect pre-training loss or transfer to downstream tasks at all. Medium levels of pruning increase the pre-training loss and prevent useful pre-training information from being transferred to downstream tasks. High levels of pruning additionally prevent models from fitting downstream datasets, leading to further degradation. Finally, we observe that fine-tuning BERT on a specific task does not improve its prunability. We conclude that BERT can be pruned once during pre-training rather than separately for each task without affecting performance.",Introduction,"Pre-trained feature extractors, such as BERT BIBREF0 for natural language processing and VGG BIBREF1 for computer vision, have become effective methods for improving the performance of deep learning models. In the last year, models similar to BERT have become state-of-the-art in many NLP tasks, including natural language inference (NLI), named entity recognition (NER), sentiment analysis, etc. These models follow a pre-training paradigm: they are trained on a large amount of unlabeled text via a task that resembles language modeling BIBREF2, BIBREF3 and are then fine-tuned on a smaller amount of “downstream” data, which is labeled for a specific task. Pre-trained models usually achieve higher accuracy than any model trained on downstream data alone. The pre-training paradigm, while effective, still has some problems. While some claim that language model pre-training is a “universal language learning task"" BIBREF4, there is no theoretical justification for this, only empirical evidence. Second, due to the size of the pre-training dataset, BERT models tend to be slow and require impractically large amounts of GPU memory. BERT-Large can only be used with access to a Google TPU, and BERT-Base requires some optimization tricks such as gradient checkpointing or gradient accumulation to be trained effectively on consumer hardware BIBREF5. Training BERT-Base from scratch costs $\sim $$7k and emits $\sim $1438 pounds of CO$_2$ BIBREF6. Model compression BIBREF7, which attempts to shrink a model without losing accuracy, is a viable approach to decreasing GPU usage. It might also be used to trade accuracy for memory in some low-resource cases, such as deploying to smartphones for real-time prediction. The main questions this paper attempts to answer are: Does compressing BERT impede it's ability to transfer to new tasks? And does fine-tuning make BERT more or less compressible? To explore these questions, we compressed English BERT using magnitude weight pruning BIBREF8 and observed the results on transfer learning to the General Language Understanding Evaluation (GLUE) benchmark BIBREF9, a diverse set of natural language understanding tasks including sentiment analysis, NLI, and textual similarity evaluation. We chose magnitude weight pruning, which compresses models by removing weights close to 0, because it is one of the most fine-grained and effective compression methods and because there are many interesting ways to view pruning, which we explore in the next section. Our findings are as follows: Low levels of pruning (30-40%) do not increase pre-training loss or affect transfer to downstream tasks at all. Medium levels of pruning increase the pre-training loss and prevent useful pre-training information from being transferred to downstream tasks. This information is not equally useful to each task; tasks degrade linearly with pre-train loss, but at different rates. High levels of pruning, depending on the size of the downstream dataset, may additionally degrade performance by preventing models from fitting downstream datasets. Finally, we observe that fine-tuning BERT on a specific task does not improve its prunability or change the order of pruning by a meaningful amount. To our knowledge, prior work had not shown whether BERT could be compressed in a task-generic way, keeping the benefits of pre-training while avoiding costly experimentation associated with compressing and re-training BERT multiple times. Nor had it shown whether BERT could be over-pruned for a memory / accuracy trade-off for deployment to low-resource devices. In this work, we conclude that BERT can be pruned prior to distribution without affecting it's universality, and that BERT may be over-pruned during pre-training for a reasonable accuracy trade-off for certain tasks.","Pruning: Compression, Regularization, Architecture Search","Neural network pruning involves examining a trained network and removing parts deemed to be unnecessary by some heuristic saliency criterion. One might remove weights, neurons, layers, channels, attention heads, etc. depending on which heuristic is used. Below, we describe three different lenses through which we might interpret pruning. Compression Pruning a neural network decreases the number of parameters required to specify the model, which decreases the disk space required to store it. This allows large models to be deployed on edge computing devices like smartphones. Pruning can also increase inference speed if whole neurons or convolutional channels are pruned, which reduces GPU usage. Regularization Pruning a neural network also regularizes it. We might consider pruning to be a form of permanent dropout BIBREF11 or a heuristic-based L0 regularizer BIBREF12. Through this lens, pruning decreases the complexity of the network and therefore narrows the range of possible functions it can express. The main difference between L0 or L1 regularization and weight pruning is that the former induce sparsity via a penalty on the loss function, which is learned during gradient descent via stochastic relaxation. It's not clear which approach is more principled or preferred. BIBREF13 Interestingly, recent work used compression not to induce simplicity but to measure it BIBREF14. Sparse Architecture Search Finally, we can view neural network pruning as a type of sparse architecture search. BIBREF15 and BIBREF16 show that they can train carefully re-initialized pruned architectures to similar performance levels as dense networks. Under this lens, stochastic gradient descent (SGD) induces network sparsity, and pruning simply makes that sparsity explicit. These sparse architectures, along with the appropriate initializations, are sometimes referred to as “lottery tickets.” Sparse networks are difficult to train from scratch BIBREF17. However, BIBREF18 and BIBREF19 present methods to do this by allowing SGD to search over the space of possible subnetworks. Our findings suggest that these methods might be used to train sparse BERT from scratch.","Pruning: Compression, Regularization, Architecture Search ::: Magnitude Weight Pruning","In this work, we focus on weight magnitude pruning because it is one of the most fine-grained and effective pruning methods. It also has a compelling saliency criterion BIBREF8: if a weight is close to zero, then its input is effectively ignored, which means the weight can be pruned. Magnitude weight pruning itself is a simple procedure: 1. Pick a target percentage of weights to be pruned, say 50%. 2. Calculate a threshold such that 50% of weight magnitudes are under that threshold. 3. Remove those weights. 4. Continue training the network to recover any lost accuracy. 5. Optionally, return to step 1 and increase the percentage of weights pruned. This procedure is conveniently implemented in a Tensorflow BIBREF20 package, which we use BIBREF21. Calculating a threshold and pruning can be done for all network parameters holistically (global pruning) or for each weight matrix individually (matrix-local pruning). Both methods will prune to the same sparsity, but in global pruning the sparsity might be unevenly distributed across weight matrices. We use matrix-local pruning because it is more popular in the community. For information on other pruning techniques, we recommend BIBREF13 and BIBREF15.",Experimental Setup,"BERT is a large Transformer encoder; for background, we refer readers to BIBREF22 or one of these excellent tutorials BIBREF23, BIBREF24.",Experimental Setup ::: Implementing BERT Pruning,"BERT-Base consists of 12 encoder layers, each of which contains 6 prunable matrices: 4 for the multi-headed self-attention and 2 for the layer's output feed-forward network. Recall that self-attention first projects layer inputs into key, query, and value embeddings via linear projections. While there is a separate key, query, and value projection matrix for each attention head, implementations typically “stack” matrices from each attention head, resulting in only 3 parameter matrices: one for key projections, one for value projections, and one for query projections. We prune each of these matrices separately, calculating a threshold for each. We also prune the linear output projection, which combines outputs from each attention head into a single embedding. We prune word embeddings in the same way we prune feed-foward networks and self-attention parameters. The justification is similar: if a word embedding value is close to zero, we can assume it's zero and store the rest in a sparse matrix. This is useful because token / subword embeddings tend to account for a large portion of a natural language model's memory. In BERT-Base specifically, the embeddings account for $\sim $21% of the model's memory. Our experimental code for pruning BERT, based on the public BERT repository, is available here.",Experimental Setup ::: Pruning During Pre-Training,"We perform weight magnitude pruning on a pre-trained BERT-Base model. We select sparsities from 0% to 90% in increments of 10% and gradually prune BERT to this sparsity over the first 10k steps of training. We continue pre-training on English Wikipedia and BookCorpus for another 90k steps to regain any lost accuracy. The resulting pre-training losses are shown in Table TABREF27. We then fine-tune these pruned models on tasks from the General Language Understanding Evaluation (GLUE) benchmark, which is a standard set of 9 tasks that include sentiment analysis, natural language inference, etc. We avoid WNLI, which is known to be problematic. We also avoid tasks with less than 5k training examples because the results tend to be noisy (RTE, MRPC, STS-B). We fine-tune a separate model on each of the remaining 5 GLUE tasks for 3 epochs and try 4 learning rates: $[2, 3, 4, 5] \times 10^{-5}$. The best evaluation accuracies are averaged and plotted in Figure FIGREF15. Individual task results are in Table TABREF27. BERT can be used as a static feature-extractor or as a pre-trained model which is fine-tuned end-to-end. In all experiments, we fine-tune weights in all layers of BERT on downstream tasks.",Experimental Setup ::: Disentangling Complexity Restriction and Information Deletion,"Pruning involves two steps: it deletes the information stored in a weight by setting it to 0 and then regularizes the model by preventing that weight from changing during further training. To disentangle these two effects (model complexity restriction and information deletion), we repeat the experiments from Section SECREF9 with an identical pre-training setup, but instead of pruning we simply set the weights to 0 and allow them to vary during downstream training. This deletes the pre-training information associated with the weight but does not prevent the model from fitting downstream datasets by keeping the weight at zero during downstream training. We also fine-tune on downstream tasks until training loss becomes comparable to models with no pruning. We trained most models for 13 epochs rather than 3. Models with 70-90% information deletion required 15 epochs to fit the training data. The results are also included in Figure FIGREF15 and Table TABREF27.",Experimental Setup ::: Pruning After Downstream Fine-tuning,"We might expect that BERT would be more compressible after downstream fine-tuning. Intuitively, the information needed for downstream tasks is a subset of the information learned during pre-training; some tasks require more semantic information than syntactic, and vice-versa. We should be able to discard the “extra"" information and only keep what we need for, say, parsing BIBREF25. For magnitude weight pruning specifically, we might expect downstream training to change the distribution of weights in the parameter matrices. This, in turn, changes the sort-order of the absolute values of those weights, which changes the order that we prune them in. This new pruning order, hypothetically, would be less degrading to our specific downstream task. To test this, we fine-tuned pre-trained BERT-Base on downstream data for 3 epochs. We then pruned at various sparsity levels and continued training for 5 more epochs (7 for 80/90% sparsity), at which point the training losses became comparable to those of models pruned during pre-training. We repeat this for learning rates in $[2, 3, 4, 5] \times 10^{-5}$ and show the results with the best development accuracy in Figure FIGREF15 / Table TABREF27. We also measure the difference in which weights are selected for pruning during pre-training vs. downstream fine-tuning and plot the results in Figure FIGREF25.",Pruning Regimes ::: 30-40% of Weights Are Not Useful,"Figure FIGREF15 shows that the first 30-40% of weights pruned by magnitude weight pruning do not impact pre-training loss or inference on any downstream task. These weights can be pruned either before or after fine-tuning. This makes sense from the perspective of pruning as sparse architecture search: when we initialize BERT-Base, we initialize many possible subnetworks. SGD selects the best one for pre-training and pushes the rest of the weights to 0. We can then prune those weights without affecting the output of the network.",Pruning Regimes ::: Medium Pruning Levels Prevent Information Transfer,"Past 40% pruning, performance starts to degrade. Pre-training loss increases as we prune weights necessary for fitting the pre-training data (Table TABREF27). Feature activations of the hidden layers start to diverge from models with low levels of pruning (Figure FIGREF18). Downstream accuracy also begins to degrade at this point. We believe this observation may point towards a more principled stopping criterion for pruning. Currently, the only way to know how much to prune is by trial and (dev-set) error. Predictors of performance degradation while pruning might help us decide which level of sparsity is appropriate for a given trained network without trying many at once. Why does pruning at these levels hurt downstream performance? On one hand, pruning deletes pre-training information by setting weights to 0, preventing the transfer of the useful inductive biases learned during pre-training. On the other hand, pruning regularizes the model by keeping certain weights at zero, which might prevent fitting downstream datasets. Figure FIGREF15 and Table TABREF27 show information deletion is the main cause of performance degradation between 40 - 60% sparsity, since pruning and information deletion degrade models by the same amount. Information deletion would not be a problem if pre-training and downstream datasets contained similar information. However, pre-training is effective precisely because the pre-training dataset is much larger than the labeled downstream dataset, which allows learning of more robust representations. We see that the main obstacle to compressing pre-trained models is maintaining the inductive bias of the model learned during pre-training. Encoding this bias requires many more weights than fitting downstream datasets, and it cannot be recovered due to a fundamental information gap between pre-training and downstream datasets. The amount a model can be pruned is limited by the largest dataset the model has been trained on: in this case, the pre-training dataset. Practitioners should be aware of this; pruning may subtly harm downstream generalization without affecting training loss. We might consider finding a lottery ticket for BERT, which we would expect to fit the GLUE training data just as well as pre-trained BERT BIBREF27, BIBREF28. However, we predict that the lottery-ticket will not reach similar generalization levels unless the lottery ticket encodes enough information to close the information gap.",Pruning Regimes ::: High Pruning Levels Also Prevent Fitting Downstream Datasets,"At 70% sparsity and above, models with information deletion recover some accuracy w.r.t. pruned models, so complexity restriction is a secondary cause of performance degradation. However, these models do not recover all evaluation accuracy, despite matching un-pruned model's training loss. Table TABREF27 shows that on the MNLI and QQP tasks, which have the largest amount of training data, information deletion performs much better than pruning. In contrast, models do not recover as well on SST-2 and CoLA, which have less data. We believe this is because the larger datasets require larger models to fit, so complexity restriction becomes an issue earlier. We might be concerned that poorly performing models are over-fitting, since they have lower training losses than unpruned models. But the best performing information-deleted models have the lowest training error of all, so overfitting seems unlikely.",Pruning Regimes ::: How Much Is A Bit Of BERT Worth?,"We've seen that over-pruning BERT deletes information useful for downstream tasks. Is this information equally useful to all tasks? We might consider the pre-training loss as a proxy for how much pre-training information we've deleted in total. Similarly, the performance of information-deletion models is a proxy for how much of that information was useful for each task. Figure FIGREF18 shows that the pre-training loss linearly predicts the effects of information deletion on downstream accuracy. For every bit of information we delete from BERT, it appears only a fraction is useful for CoLA, and an even smaller fraction useful for QQP. This relationship should be taken into account when considering the memory / accuracy trade-off of over-pruning. Pruning an extra 30% of BERT's weights is worth only one accuracy point on QQP but 10 points on CoLA. It's unclear, however, whether this is because the pre-training task is less relevant to QQP or whether QQP simply has a bigger dataset with more information content.",Downstream Fine-tuning Does Not Improve Prunability,"Since pre-training information deletion plays a central role in performance degradation while over-pruning, we might expect that downstream fine-tuning would improve prunability by making important weights more salient (increasing their magnitude). However, Figure FIGREF15 shows that models pruned after downstream fine-tuning do not surpass the development accuracies of models pruned during pre-training, despite achieving similar training losses. Figure FIGREF25 shows fine-tuning changes which weights are pruned by less than 6%. Why doesn't fine-tuning change which weights are pruned much? Table TABREF30 shows that the magnitude sorting order of weights is mostly preserved; weights move on average 0-4% away from their starting positions in the sort order. We also see that high magnitude weights are more stable than lower ones (Figure FIGREF31). Our experiments suggest that training on downstream data before pruning is too blunt an instrument to improve prunability. Even so, we might consider simply training on the downstream tasks for much longer, which would increase the difference in weights pruned. However, Figure FIGREF26 shows that even after an epoch of downstream fine-tuning, weights quickly re-stabilize in a new sorting order, meaning longer downstream training will have only a marginal effect on which weights are pruned. Indeed, Figure FIGREF25 shows that the weights selected for 60% pruning quickly stabilize and evaluation accuracy does not improve with more training before pruning.",Related Work,"Compressing BERT for Specific Tasks Section SECREF5 showed that downstream fine-tuning does not increase prunability. However, several alternative compression approaches have been proposed to discard non-task-specific information. BIBREF25 used an information bottleneck to discard non-syntactic information. BIBREF31 used BERT as a knowledge distillation teacher to compress relevant information into smaller Bi-LSTMs, while BIBREF32 took a similar distillation approach. While fine-tuning does not increase prunability, task-specific knowledge might be extracted from BERT with other methods. Attention Head Pruning BIBREF33 previously showed redundancy in transformer models by pruning entire attention heads. BIBREF34 showed that after fine-tuning on MNLI, up to 40% of attention heads can be pruned from BERT without affecting test accuracy. They show redundancy in BERT after fine-tuning on a single downstream task; in contrast, our work emphasizes the interplay between compression and transfer learning to many tasks, pruning both before and after fine-tuning. Also, magnitude weight pruning allows us to additionally prune the feed-foward networks and sub-word embeddings in BERT (not just self-attention), which account for $\sim $72% of BERT's total memory usage. We suspect that attention head pruning and weight pruning remove different redundancies from BERT. Figure FIGREF26 shows that weight pruning does not prune any specific attention head much more than the pruning rate for the whole model. It is not clear, however, whether weight pruning and recovery training makes attention heads less prunable by distributing functionality to unused heads.",Conclusion And Future Work,"We've shown that encoding BERT's inductive bias requires many more weights than are required to fit downstream data. Future work on compressing pre-trained models should focus on maintaining that inductive bias and quantifying its relevance to various tasks during accuracy/memory trade-offs. For magnitude weight pruning, we've shown that 30-40% of the weights do not encode any useful inductive bias and can be discarded without affecting BERT's universality. The relevance of the rest of the weights vary from task to task, and fine-tuning on downstream tasks does not change the nature of this trade-off by changing which weights are pruned. In future work, we will investigate the factors that influence language modeling's relevance to downstream tasks and how to improve compression in a task-general way. It's reasonable to believe that these conclusions will generalize to other pre-trained language models such as Kermit BIBREF3, XLNet BIBREF2, GPT-2 BIBREF4, RoBERTa BIBREF35 or ELMO BIBREF36. All of these learn some variant of language modeling, and most use Transformer architectures. While it remains to be shown in future work, viewing pruning as architecture search implies these models will be prunable due to the training dynamics inherent to neural networks.",,,,,,,,,,,,,How they observe that fine-tuning BERT on a specific task does not improve its prunability?,dc4096b8bab0afcbbd4fbb015da2bea5d38251cd,zero,unfamiliar,no,computer vision,258ee4069f740c400c0049a2580945a1cc7f044c,False,,,"Model compression BIBREF7, which attempts to shrink a model without losing accuracy, is a viable approach to decreasing GPU usage. It might also be used to trade accuracy for memory in some low-resource cases, such as deploying to smartphones for real-time prediction. The main questions this paper attempts to answer are: Does compressing BERT impede it's ability to transfer to new tasks? And does fine-tuning make BERT more or less compressible? To explore these questions, we compressed English BERT using magnitude weight pruning BIBREF8 and observed the results on transfer learning to the General Language Understanding Evaluation (GLUE) benchmark BIBREF9, a diverse set of natural language understanding tasks including sentiment analysis, NLI, and textual similarity evaluation. We chose magnitude weight pruning, which compresses models by removing weights close to 0, because it is one of the most fine-grained and effective compression methods and because there are many interesting ways to view pruning, which we explore in the next section. Our findings are as follows: Low levels of pruning (30-40%) do not increase pre-training loss or affect transfer to downstream tasks at all. Medium levels of pruning increase the pre-training loss and prevent useful pre-training information from being transferred to downstream tasks. This information is not equally useful to each task; tasks degrade linearly with pre-train loss, but at different rates. High levels of pruning, depending on the size of the downstream dataset, may additionally degrade performance by preventing models from fitting downstream datasets. Finally, we observe that fine-tuning BERT on a specific task does not improve its prunability or change the order of pruning by a meaningful amount.","The main questions this paper attempts to answer are: Does compressing BERT impede it's ability to transfer to new tasks? And does fine-tuning make BERT more or less compressible?

To explore these questions, we compressed English BERT using magnitude weight pruning BIBREF8 and observed the results on transfer learning to the General Language Understanding Evaluation (GLUE) benchmark BIBREF9, a diverse set of natural language understanding tasks including sentiment analysis, NLI, and textual similarity evaluation. We chose magnitude weight pruning, which compresses models by removing weights close to 0, because it is one of the most fine-grained and effective compression methods and because there are many interesting ways to view pruning, which we explore in the next section. Our findings are as follows: Low levels of pruning (30-40%) do not increase pre-training loss or affect transfer to downstream tasks at all. Medium levels of pruning increase the pre-training loss and prevent useful pre-training information from being transferred to downstream tasks.",458ca650f9fcfbab21f6524e5cd7cceb82103d7c,a0b403873302db7cada39008f04d01155ef68f4f,,,,,,,,,How much is pre-training loss increased in Low/Medium/Hard level of pruning?,c4c9c7900a0480743acc7599efb359bc81cf3a4d,zero,unfamiliar,no,computer vision,258ee4069f740c400c0049a2580945a1cc7f044c,False,,"The increase is linearly from lowest on average 2.0 , medium around 3.5, and the largest is 6.0","We've seen that over-pruning BERT deletes information useful for downstream tasks. Is this information equally useful to all tasks? We might consider the pre-training loss as a proxy for how much pre-training information we've deleted in total. Similarly, the performance of information-deletion models is a proxy for how much of that information was useful for each task. Figure FIGREF18 shows that the pre-training loss linearly predicts the effects of information deletion on downstream accuracy. FLOAT SELECTED: Figure 2: (Left) Pre-training loss predicts information deletion GLUE accuracy linearly as sparsity increases. We believe the slope of each line tells us how much a bit of BERT is worth to each task. (CoLA at 90% is excluded from the line of best fit.) (Right) The cosine similarities of features extracted for a subset of the pre-training development data before and after pruning. Features are extracted from activations of all 12 layers of BERT and compared layer-wise to a model that has not been pruned. As performance degrades, cosine similarities of features decreases.","Figure FIGREF18 shows that the pre-training loss linearly predicts the effects of information deletion on downstream accuracy. FLOAT SELECTED: Figure 2: (Left) Pre-training loss predicts information deletion GLUE accuracy linearly as sparsity increases. We believe the slope of each line tells us how much a bit of BERT is worth to each task. (CoLA at 90% is excluded from the line of best fit.) (Right) The cosine similarities of features extracted for a subset of the pre-training development data before and after pruning. Features are extracted from activations of all 12 layers of BERT and compared layer-wise to a model that has not been pruned. As performance degrades, cosine similarities of features decreases.",d08049e419f2cc2e2e4bcb38fe702938087efeba,a0b403873302db7cada39008f04d01155ef68f4f,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5-Figure1-1.png,"Figure 1: (Blue) The best GLUE dev accuracy and training losses for models pruned during pretraining, averaged over 5 tasks. Also shown are models with information deletion during pre-training (orange), models pruned after downstream fine-tuning (green), and models pruned randomly during pre-training instead of by lowest magnitude (red). 30-40% of weights can be pruned using magnitude weight pruning without decreasing dowsntream accuracy. Notice that information deletion fits the training data better than un-pruned models at all sparsity levels but does not fully recover evaluation accuracy. Also, models pruned after downstream fine-tuning have the same or worse development accuracy, despite achieving lower training losses. Note: none of the pruned models are overfitting because un-pruned models have the lowest training loss and the highest development accuracy. While the results for individual tasks are in Table 1, each task does not vary much from the average trend, with an exception discussed in Section 4.3.",5-Figure2-1.png,"Figure 2: (Left) Pre-training loss predicts information deletion GLUE accuracy linearly as sparsity increases. We believe the slope of each line tells us how much a bit of BERT is worth to each task. (CoLA at 90% is excluded from the line of best fit.) (Right) The cosine similarities of features extracted for a subset of the pre-training development data before and after pruning. Features are extracted from activations of all 12 layers of BERT and compared layer-wise to a model that has not been pruned. As performance degrades, cosine similarities of features decreases.",7-Figure3-1.png,"Figure 3: (Left) The measured difference in pruning masks between models pruned during pretraining and models pruned during downstream fine-tuning. As predicted, the differences are less than 6%, since fine-tuning only changes the magnitude sorting order of weights locally, not globally. (Right) The average GLUE development accuracy and pruning mask difference for models trained on downstream datasets before pruning 60% at learning rate 5e-5. After pruning, models are trained for an additional 2 epochs to regain accuracy. We see that training between 3 and 12 epochs before pruning does not change which weights are pruned or improve performance.",8-Figure4-1.png,"Figure 4: (Left) The average, min, and max percentage of individual attention heads pruned at each sparsity level. We see at 60% sparsity, each attention head individually is pruned strictly between 55% and 65%. (Right) We compute the magnitude sorting order of each weight before and after downstream fine-tuning. If a weight’s original position is 59 / 100 before fine-tuning and 63 / 100 after fine-tuning, then that weight moved 4% in the sorting order. After even an epoch of downstream fine-tuning, weights quickly stabilize in a new sorting order which is not far from the original sorting order. Variances level out similarly.",12-Table1-1.png,"Table 1: Pre-training development losses and GLUE task development accuracies for various levels of pruning. Each development accuracy is accompanied on its right by the achieved training loss, evaluated on the entire training set. Averages are summarized in Figure 1. Pre-training losses are omitted for models pruned after downstream fine-tuning because it is not clear how to measure their performance on the pre-training task in a fair way.",13-Figure5-1.png,"Figure 5: The sum of weights pruned at each sparsity level for one shot pruning of BERT. Given the motivation for our saliency criterion, it seems strange that such a large magnitude of weights can be pruned without decreasing accuracy.",,,,,,,,,,,13-Table2-1.png,"Table 2: We compute the magnitude sorting order of each weight before and after downstream finetuning. If a weight’s original position is 59 / 100 before fine-tuning and 63 / 100 after fine-tuning, then that weight moved 4% in the sorting order. We then list the average movement of weights in each model, along with the standard deviation. Sorting order changes mostly locally across tasks: a weight moves, on average, 0-4% away from its starting position. As expected, larger datasets and larger learning rates have more movement (per epoch). We also see that higher magnitude weights are more stable than lower weights, see Figure 7.",13-Figure7-1.png,"Figure 7: We show how weight sort order movements are distributed during fine-tuning, given a weight’s starting magnitude. We see that higher magnitude weights are more stable than lower magnitude weights and do not move as much in the sort order. This plot is nearly identical for every model and learning rate, so we only show it once.",14-Figure8-1.png,"Figure 8: A heatmap of the weight magnitudes of the 12 horizontally stacked self-attention key projection matrices for layer 1. A banding pattern can be seen: the highest values of the matrix tend to cluster in certain attention heads. This pattern appears in most of the self-attention parameter matrices, but it does not cause pruning to prune one head more than another. However, it may prove to be a useful heuristic for attention head pruning, which would not require making many passes over the training data.",14-Figure9-1.png,"Figure 9: A heatmap of the weight magnitudes of BERT’s subword embeddings. Interestingly, pruning BERT embeddings are more interpretable; we can see shorter subwords (top rows) have smaller magnitude values and thus will be pruned earlier than other subword embeddings.",16-Table3-1.png,Table 3: The values of BERT’s weights are normally distributed in each weight matrix. The means and variances are listed for each.,,"we compressed English BERT using magnitude weight pruning BIBREF8 and observed the results on transfer learning to the General Language Understanding Evaluation (GLUE) benchmark BIBREF9, a diverse set of natural language understanding tasks including sentiment analysis, NLI, and textual similarity evaluation. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Quantifying Similarity between Relations with Fact Distribution,"We introduce a conceptually simple and effective method to quantify the similarity between relations in knowledge bases. Specifically, our approach is based on the divergence between the conditional probability distributions over entity pairs. In this paper, these distributions are parameterized by a very simple neural network. Although computing the exact similarity is in-tractable, we provide a sampling-based method to get a good approximation. We empirically show the outputs of our approach significantly correlate with human judgments. By applying our method to various tasks, we also find that (1) our approach could effectively detect redundant relations extracted by open information extraction (Open IE) models, that (2) even the most competitive models for relational classification still make mistakes among very similar relations, and that (3) our approach could be incorporated into negative sampling and softmax classification to alleviate these mistakes. The source code and experiment details of this paper can be obtained from https://github.com/thunlp/relation-similarity.",Introduction,"Author contributions: Hao Zhu designed the research; Weize Chen prepared the data, and organized data annotation; Hao Zhu and Xu Han designed the experiments; Weize Chen performed the experiments; Hao Zhu, Weize Chen and Xu Han wrote the paper; Zhiyuan Liu and Maosong Sun proofread the paper. Zhiyuan Liu is the corresponding author. Relations, representing various types of connections between entities or arguments, are the core of expressing relational facts in most general knowledge bases (KBs) BIBREF0 , BIBREF1 . Hence, identifying relations is a crucial problem for several information extraction tasks. Although considerable effort has been devoted to these tasks, some nuances between similar relations are still overlooked, (tab:similarityexample shows an example); on the other hand, some distinct surface forms carrying the same relational semantics are mistaken as different relations. These severe problems motivate us to quantify the similarity between relations in a more effective and robust method. In this paper, we introduce an adaptive and general framework for measuring similarity of the pairs of relations. Suppose for each relation INLINEFORM0 , we have obtained a conditional distribution, INLINEFORM1 ( INLINEFORM2 are head and tail entities, and INLINEFORM3 is a relation), over all head-tail entity pairs given INLINEFORM4 . We could quantify similarity between a pair of relations by the divergence between the conditional probability distributions given these relations. In this paper, this conditional probability is given by a simple feed-forward neural network, which can capture the dependencies between entities conditioned on specific relations. Despite its simplicity, the proposed network is expected to cover various facts, even if the facts are not used for training, owing to the good generalizability of neural networks. For example, our network will assign a fact a higher probability if it is “logical”: e.g., the network might prefer an athlete has the same nationality as same as his/her national team rather than other nations. Intuitively, two similar relations should have similar conditional distributions over head-tail entity pairs INLINEFORM0 , e.g., the entity pairs associated with be trade to and play for are most likely to be athletes and their clubs, whereas those associated with live in are often people and locations. In this paper, we evaluate the similarity between relations based on their conditional distributions over entity pairs. Specifically, we adopt Kullback–Leibler (KL) divergence of both directions as the metric. However, computing exact KL requires iterating over the whole entity pair space INLINEFORM1 , which is quite intractable. Therefore, we further provide a sampling-based method to approximate the similarity score over the entity pair space for computational efficiency. Besides developing a framework for assessing the similarity between relations, our second contribution is that we have done a survey of applications. We present experiments and analysis aimed at answering five questions: (1) How well does the computed similarity score correlate with human judgment about the similarity between relations? How does our approach compare to other possible approaches based on other kinds of relation embeddings to define a similarity? (sec:relationship and sec:human-judgment) (2) Open IE models inevitably extract many redundant relations. How can our approach help reduce such redundancy? (sec:openie) (3) To which extent, quantitatively, does best relational classification models make errors among similar relations? (sec:error-analysis) (4) Could similarity be used in a heuristic method to enhance negative sampling for relation prediction? (sec:training-guidance-relation-prediction) (5) Could similarity be used as an adaptive margin in softmax-margin training method for relation extraction? (sec:training-guidance-relation-extraction) Finally, we conclude with a discussion of valid extensions to our method and other possible applications.",Learning Head-Tail Distribution,"Just as introduced in sec:introduction, we quantify the similarity between relations by their corresponding head-tail entity pair distributions. Consider the typical case that we have got numbers of facts, but they are still sparse among all facts in the real world. How could we obtain a well-generalized distribution over the whole space of possible triples beyond the training facts? This section proposes a method to parameterize such a distribution.",Formal Definition of Fact Distribution,"A fact is a triple INLINEFORM0 , where INLINEFORM1 and INLINEFORM2 are called head and tail entities, INLINEFORM3 is the relation connecting them, INLINEFORM4 and INLINEFORM5 are the sets of entities and relations respectively. We consider a score function INLINEFORM6 maps all triples to a scalar value. As a special case, the function can be factorized into the sum of two parts: INLINEFORM7 . We use INLINEFORM8 to define the unnormalized probability. DISPLAYFORM0  for every triple INLINEFORM0 . The real parameter INLINEFORM1 can be adjusted to obtain difference distributions over facts. In this paper, we only consider locally normalized version of INLINEFORM0 : DISPLAYFORM0  where INLINEFORM0 and INLINEFORM1 are directly parameterized by feed-forward neural networks. Through local normalization, INLINEFORM2 is naturally a valid probability distribution, as the partition function INLINEFORM3 . Therefore, INLINEFORM4 .",Neural architecture design,"Here we introduce our special design of neural networks. For the first part and the second part, we implement the scoring functions introduced in eq:local-normalization as DISPLAYFORM0  where each INLINEFORM0 represents a multi-layer perceptron composed of layers like INLINEFORM1 , INLINEFORM2 , INLINEFORM3 are embeddings of INLINEFORM4 , INLINEFORM5 , and INLINEFORM6 includes weights and biases in all layers.",Training,"Now we discuss the method to perform training. In this paper, we consider joint training. By minimizing the loss function, we compute the model parameters INLINEFORM0 : DISPLAYFORM0  where INLINEFORM0 is a set of triples. The whole set of parameters, INLINEFORM1 . We train these parameters by Adam optimizer BIBREF2 . Training details are shown in sec:trainingdetail.",Quantifying Similarity,"So far, we have talked about how to use neural networks to approximate the natural distribution of facts. The center topic of our paper, quantifying similarity, will be discussed in detail in this section.",Relations as Distributions,"In this paper, we provide a probability view of relations by representing relation INLINEFORM0 as a probability distribution INLINEFORM1 . After training the neural network on a given set of triples, the model is expected to generalize well on the whole INLINEFORM2 space. Note that it is very easy to calculate INLINEFORM0 in our model thanks to local normalization (eq:local-normalization). Therefore, we can compute it by DISPLAYFORM0 ",Defining Similarity,"As the basis of our definition, we hypothesize that the similarity between INLINEFORM0 reflects the similarity between relations. For example, if the conditional distributions of two relations put mass on similar entity pairs, the two relations should be quite similar. If they emphasize different ones, the two should have some differences in meaning. Formally, we define the similarity between two relations as a function of the divergence between the distributions of corresponding head-tail entity pairs: DISPLAYFORM0  where INLINEFORM0 denotes Kullback–Leibler divergence, DISPLAYFORM0  vice versa, and function INLINEFORM0 is a symmetrical function. To keep the coherence between semantic meaning of “similarity” and our definition, INLINEFORM1 should be a monotonically decreasing function. Through this paper, we choose to use an exponential family composed with max function, i.e., INLINEFORM2 . Note that by taking both sides of KL divergence into account, our definition incorporates both the entity pairs with high probability in INLINEFORM3 and INLINEFORM4 . Intuitively, if INLINEFORM5 mainly distributes on a proportion of entities pairs that INLINEFORM6 emphasizes, INLINEFORM7 is only hyponymy of INLINEFORM8 . Considering both sides of KL divergence could help model yield more comprehensive consideration. We will talk about the advantage of this method in detail in sec:relationship.",Calculating Similarity,"Just as introduced in sec:introduction, it is intractable to compute similarity exactly, as involving INLINEFORM0 computation. Hence, we consider the monte-carlo approximation: DISPLAYFORM0  where INLINEFORM0 is a list of entity pairs sampled from INLINEFORM1 . We use sequential sampling to gain INLINEFORM6 , which means we first sample INLINEFORM7 given INLINEFORM8 from INLINEFORM9 , and then sample INLINEFORM10 given INLINEFORM11 and INLINEFORM12 from INLINEFORM13 .",Relationship with other metrics,"Previous work proposed various methods for representing relations as vectors BIBREF3 , BIBREF4 , as matrices BIBREF5 , even as angles BIBREF6 , etc. Based on each of these representations, one could easily define various similarity quantification methods. We show in tab:other-similarity the best one of them in each category of relation presentation. Here we provide two intuitive reasons for using our proposed probability-based similarity: (1) the capacity of a single fixed-size representation is limited — some details about the fact distribution is lost during embedding; (2) directly comparing distributions yields a better interpretability — you can not know about how two relations are different given two relation embeddings, but our model helps you study the detailed differences between probabilities on every entity pair. fig:head-tail-distribution provides an example. Although the two relations talk about the same topic, they have different meanings. TransE embeds them as vectors the closest to each other, while our model can capture the distinction between the distributions corresponds to the two relations, which could be directly noticed from the figure. Embeddings used in this graph are from a trained TransE model.",Dataset Construction,"We show the statistics of the dataset we use in tab:statistics, and the construction procedures will be introduced in this section.",Wikidata,"In Wikidata BIBREF8 , facts can be described as (Head item/property, Property, Tail item/property). To construct a dataset suitable for our task, we only consider the facts whose head entity and tail entity are both items. We first choose the most common 202 relations and 120000 entities from Wikidata as our initial data. Considering that the facts containing the two most frequently appearing relations (P2860: cites, and P31: instance of) occupy half of the initial data, we drop the two relations to downsize the dataset and make the dataset more balanced. Finally, we keep the triples whose head and tail both come from the selected 120000 entities as well as its relation comes from the remaining 200 relations.",ReVerb Extractions,ReVerb BIBREF9 is a program that automatically identifies and extracts binary relationships from English sentences. We use the extractions from running ReVerb on Wikipedia. We only keep the relations appear more than 10 times and their corresponding triples to construct our dataset.,FB15K and TACRED,"FB15K BIBREF3 is a subset of freebase. TACRED BIBREF10 is a large supervised relation extraction dataset obtained via crowdsourcing. We directly use these two dataset, no extra processing steps were applied.",Human Judgments,"Following BIBREF11 , BIBREF12 and the vast amount of previous work on semantic similarity, we ask nine undergraduate subjects to assess the similarity of 360 pairs of relations from a subset of Wikidata BIBREF8 that are chosen to cover from high to low levels of similarity. In our experiment, subjects were asked to rate an integer similarity score from 0 (no similarity) to 4 (perfectly the same) for each pair. The inter-subject correlation, estimated by leaving-one-out method BIBREF13 , is r = INLINEFORM0 , standard deviation = INLINEFORM1 . This important reference value (marked in fig:correlation) could be seen as the highest expected performance for machines BIBREF12 . To get baselines for comparison, we consider other possible methods to define similarity functions, as shown in tab:other-similarity. We compute the correlation between these methods and human judgment scores. As the models we have chosen are the ones work best in knowledge base completion, we do expect the similarity quantification approaches based on them could measure some degree of similarity. As shown in fig:correlation, the three baseline models could achieve moderate ( INLINEFORM0 ) positive correlation. On the other hand, our model shows a stronger correlation ( INLINEFORM1 ) with human judgment, indicating that considering the probability over whole entity pair space helps to gain a similarity closer to human judgments. These results provide evidence for our claim raised in sec:defining-similarity.",Redundant Relation Removal,"Open IE extracts concise token patterns from plain text to represent various relations between entities, e.g.,, (Mark Twain, was born in, Florida). As Open IE is significant for constructing KBs, many effective extractors have been proposed to extract triples, such as Text-Runner BIBREF14 , ReVerb BIBREF9 , and Standford Open IE BIBREF15 . However, these extractors only yield relation patterns between entities, without aggregating and clustering their results. Accordingly, there are a fair amount of redundant relation patterns after extracting those relation patterns. Furthermore, the redundant patterns lead to some redundant relations in KBs. Recently, some efforts are devoted to Open Relation Extraction (Open RE) BIBREF16 , BIBREF17 , BIBREF18 , BIBREF19 , aiming to cluster relation patterns into several relation types instead of redundant relation patterns. Whenas, these Open RE methods adopt distantly supervised labels as golden relation types, suffering from both false positive and false negative problems on the one hand. On the other hand, these methods still rely on the conventional similarity metrics mentioned above. In this section, we will show that our defined similarity quantification could help Open IE by identifying redundant relations. To be specific, we set a toy experiment to remove redundant relations in KBs for a preliminary comparison (sec:toy-experiment). Then, we evaluate our model and baselines on the real-world dataset extracted by Open IE methods (sec:real-experiment). Considering the existing evaluation metric for Open IE and Open RE rely on either labor-intensive annotations or distantly supervised annotations, we propose a metric approximating recall and precision evaluation based on operable human annotations for balancing both efficiency and accuracy.",Toy Experiment,"In this subsection, we propose a toy environment to verify our similarity-based method. Specifically, we construct a dataset from Wikidata and implement Chinese restaurant process to split every relation in the dataset into several sub-relations. Then, we filter out those sub-relations appearing less than 50 times to eventually get 1165 relations. All these split relations are regarded as different ones during training, and then different relation similarity metrics are adopted to merge those sub-relations into one relation. As Figure FIGREF26 shown that the matrices-based approach is less effective than other approaches, we leave this approach out of this experiment. The results are shown in Table TABREF37 .",Real World Experiment,"In this subsection, we evaluate various relation similarity metrics on the real-world Open IE patterns. The dataset are constructed by ReVerb. Different patterns will be regarded as different relations during training, and we also adopt various relation similarity metrics to merge similar relation patterns. Because it is nearly impossible to annotate all pattern pairs for their merging or not, meanwhile it is also inappropriate to take distantly supervised annotations as golden results. Hence, we propose a novel metric approximating recall and precision evaluation based on minimal human annotations for evaluation in this experiment. Recall is defined as the yielding fraction of true positive instances over the total amount of real positive instances. However, we do not have annotations about which pairs of relations are synonymous. Crowdsourcing is a method to obtain a large number of high-quality annotations. Nevertheless, applying crowdsourcing is not trivial in our settings, because it is intractable to enumerate all synonymous pairs in the large space of relation (pattern) pairs INLINEFORM0 in Open IE. A promising method is to use rejection sampling by uniform sampling from the whole space, and only keep the synonymous ones judged by crowdworkers. However, this is not practical either, as the synonymous pairs are sparse in the whole space, resulting in low efficiency. Fortunately, we could use normalized importance sampling as an alternative to get an unbiased estimation of recall. Theorem 1 Suppose every sample INLINEFORM0 has a label INLINEFORM1 , and the model to be evaluated also gives its prediction INLINEFORM2 . The recall can be written as DISPLAYFORM0  where INLINEFORM0 is the uniform distribution over all samples with INLINEFORM1 . If we have a proposal distribution INLINEFORM2 satisfying INLINEFORM3 , we get an unbiased estimation of recall: DISPLAYFORM0  where INLINEFORM0 is a normalized version of INLINEFORM1 , where INLINEFORM2 is the unnormalized version of q, and INLINEFORM3 are i.i.d. drawn from INLINEFORM4 . Similar to eq:recall-expectation, we can write the expectation form of precision: DISPLAYFORM0  where INLINEFORM0 is the uniform distribution over all samples with INLINEFORM1 . As these samples could be found out by performing models on it. We can simply approximate precision by Monte Carlo Sampling: DISPLAYFORM0  where INLINEFORM0 . In our setting, INLINEFORM0 , INLINEFORM1 means INLINEFORM2 and INLINEFORM3 are the same relations, INLINEFORM4 means INLINEFORM5 is larger than a threshold INLINEFORM6 . The results on the ReVerb Extractions dataset that we constructed are described in fig:precision-recall-openie. To approximate recall, we use the similarity scores as the proposal distribution INLINEFORM0 . 500 relation pairs are then drawn from INLINEFORM1 . To approximate precision, we set thresholds at equal intervals. At each threshold, we uniformly sample 50 to 100 relation pairs whose similarity score given by the model is larger than the threshold. We ask 15 undergraduates to judge whether two relations in a relation pair have the same meaning. A relation pair is viewed valid only if 8 of the annotators annotate it as valid. We use the annotations to approximate recall and precision with eq:recall and eq:precision. Apart from the confidential interval of precision shown in the figure, the largest INLINEFORM2 confidential interval among thresholds for recall is INLINEFORM3 . From the result, we could see that our model performs much better than other models' similarity by a very large margin.",Error Analysis for Relational Classification,"In this section, we consider two kinds of relational classification tasks: (1) relation prediction and (2) relation extraction. Relation prediction aims at predicting the relationship between entities with a given set of triples as training data; while relation extraction aims at extracting the relationship between two entities in a sentence.",Relation Prediction,"We hope to design a simple and clear experiment setup to conduct error analysis for relational prediction. Therefore, we consider a typical method TransE BIBREF3 as the subject as well as FB15K BIBREF3 as the dataset. TransE embeds entities and relations as vectors, and train these embeddings by minimizing DISPLAYFORM0  where INLINEFORM0 is the set of training triples, INLINEFORM1 is the distance function, INLINEFORM2 is a negative sample with one element different from INLINEFORM4 uniformly sampled from INLINEFORM5 , and INLINEFORM6 is the margin. During testing, for each entity pair INLINEFORM0 , TransE rank relations according to INLINEFORM1 . For each INLINEFORM2 in the test set, we call the relations with higher rank scores than INLINEFORM3 distracting relations. We then compare the similarity between the golden relation and distracting relations. Note that some entity pairs could correspond to more than one relations, in which case we just do not see them as distracting relations.",Relation Extraction,"For relation extraction, we consider the supervised relation extraction setting and TACRED dataset BIBREF10 . As for the subject model, we use the best model on TACRED dataset — position-aware neural sequence model. This method first passes the sentence into an LSTM and then calculate an attention sum of the hidden states in the LSTM by taking positional features into account. This simple and effective method achieves the best in TACRED dataset.",Which competitive relational classification models do they test?,10ddc5caf36fe9d7438eb5a3936e24580c4ffe6a,infinity,unfamiliar,no,,2cfd959e433f290bb50b55722370f0d22fe090b7,False,,For relation prediction they test TransE and for relation extraction they test position aware neural sequence model,"In this section, we consider two kinds of relational classification tasks: (1) relation prediction and (2) relation extraction. Relation prediction aims at predicting the relationship between entities with a given set of triples as training data; while relation extraction aims at extracting the relationship between two entities in a sentence. We hope to design a simple and clear experiment setup to conduct error analysis for relational prediction. Therefore, we consider a typical method TransE BIBREF3 as the subject as well as FB15K BIBREF3 as the dataset. TransE embeds entities and relations as vectors, and train these embeddings by minimizing DISPLAYFORM0 For relation extraction, we consider the supervised relation extraction setting and TACRED dataset BIBREF10 . As for the subject model, we use the best model on TACRED dataset — position-aware neural sequence model. This method first passes the sentence into an LSTM and then calculate an attention sum of the hidden states in the LSTM by taking positional features into account. This simple and effective method achieves the best in TACRED dataset.","In this section, we consider two kinds of relational classification tasks: (1) relation prediction and (2) relation extraction.  We hope to design a simple and clear experiment setup to conduct error analysis for relational prediction. Therefore, we consider a typical method TransE BIBREF3 as the subject as well as FB15K BIBREF3 as the dataset. For relation extraction, we consider the supervised relation extraction setting and TACRED dataset BIBREF10 . As for the subject model, we use the best model on TACRED dataset — position-aware neural sequence model. ",95bdaf3d6a7bda0b316622f894922a9e97014da0,c1018a31c3272ce74964a3280069f62f314a1a58,,,,,,,,,Which tasks do they apply their method to?,29571867fe00346418b1ec36c3b7685f035e22ce,infinity,unfamiliar,no,,2cfd959e433f290bb50b55722370f0d22fe090b7,False,,,"In this section, we will show that our defined similarity quantification could help Open IE by identifying redundant relations. To be specific, we set a toy experiment to remove redundant relations in KBs for a preliminary comparison (sec:toy-experiment). Then, we evaluate our model and baselines on the real-world dataset extracted by Open IE methods (sec:real-experiment). Considering the existing evaluation metric for Open IE and Open RE rely on either labor-intensive annotations or distantly supervised annotations, we propose a metric approximating recall and precision evaluation based on operable human annotations for balancing both efficiency and accuracy. FLOAT SELECTED: Figure 3: Precision-recall curve on Open IE task comparing our similarity function with vector-based and angle-based similarity. Error bar represents 95% confidential interval. Bootstraping is used to calculate the confidential interval. In this section, we consider two kinds of relational classification tasks: (1) relation prediction and (2) relation extraction. Relation prediction aims at predicting the relationship between entities with a given set of triples as training data; while relation extraction aims at extracting the relationship between two entities in a sentence.","In this section, we will show that our defined similarity quantification could help Open IE by identifying redundant relations. FLOAT SELECTED: Figure 3: Precision-recall curve on Open IE task comparing our similarity function with vector-based and angle-based similarity. Error bar represents 95% confidential interval. Bootstraping is used to calculate the confidential interval. In this section, we consider two kinds of relational classification tasks: (1) relation prediction and (2) relation extraction. ",1f884a8c66e2b03922b0982c043d139474a336a0,c1018a31c3272ce74964a3280069f62f314a1a58,Which knowledge bases do they use?,1a678d081f97531d54b7122254301c20b3531198,infinity,unfamiliar,no,,2cfd959e433f290bb50b55722370f0d22fe090b7,False,,,"We show the statistics of the dataset we use in tab:statistics, and the construction procedures will be introduced in this section. In Wikidata BIBREF8 , facts can be described as (Head item/property, Property, Tail item/property). To construct a dataset suitable for our task, we only consider the facts whose head entity and tail entity are both items. We first choose the most common 202 relations and 120000 entities from Wikidata as our initial data. Considering that the facts containing the two most frequently appearing relations (P2860: cites, and P31: instance of) occupy half of the initial data, we drop the two relations to downsize the dataset and make the dataset more balanced. Finally, we keep the triples whose head and tail both come from the selected 120000 entities as well as its relation comes from the remaining 200 relations. ReVerb BIBREF9 is a program that automatically identifies and extracts binary relationships from English sentences. We use the extractions from running ReVerb on Wikipedia. We only keep the relations appear more than 10 times and their corresponding triples to construct our dataset. FB15K BIBREF3 is a subset of freebase. TACRED BIBREF10 is a large supervised relation extraction dataset obtained via crowdsourcing. We directly use these two dataset, no extra processing steps were applied.","We show the statistics of the dataset we use in tab:statistics, and the construction procedures will be introduced in this section. In Wikidata BIBREF8 , facts can be described as (Head item/property, Property, Tail item/property).  We first choose the most common 202 relations and 120000 entities from Wikidata as our initial data. ReVerb BIBREF9 is a program that automatically identifies and extracts binary relationships from English sentences. We use the extractions from running ReVerb on Wikipedia. FB15K BIBREF3 is a subset of freebase. TACRED BIBREF10 is a large supervised relation extraction dataset obtained via crowdsourcing. We directly use these two dataset, no extra processing steps were applied.",2cd64b80bc3339eefa46c8cae56b6823b13e92f6,c1018a31c3272ce74964a3280069f62f314a1a58,,,,,,,,How do they gather human judgements for similarity between relations?,b9f2a30f5ef664ff845d860cf4bfc2afb0a46e5a,infinity,unfamiliar,no,,2cfd959e433f290bb50b55722370f0d22fe090b7,False,,By assessing similarity of 360 pairs of relations from a subset of Wikidata using an integer similarity score from 0 to 4,"Human Judgments Following BIBREF11 , BIBREF12 and the vast amount of previous work on semantic similarity, we ask nine undergraduate subjects to assess the similarity of 360 pairs of relations from a subset of Wikidata BIBREF8 that are chosen to cover from high to low levels of similarity. In our experiment, subjects were asked to rate an integer similarity score from 0 (no similarity) to 4 (perfectly the same) for each pair. The inter-subject correlation, estimated by leaving-one-out method BIBREF13 , is r = INLINEFORM0 , standard deviation = INLINEFORM1 . This important reference value (marked in fig:correlation) could be seen as the highest expected performance for machines BIBREF12 .","Human Judgments
Following BIBREF11 , BIBREF12 and the vast amount of previous work on semantic similarity, we ask nine undergraduate subjects to assess the similarity of 360 pairs of relations from a subset of Wikidata BIBREF8 that are chosen to cover from high to low levels of similarity. In our experiment, subjects were asked to rate an integer similarity score from 0 (no similarity) to 4 (perfectly the same) for each pair. ",ecdbad0b58727c3e230a18bd851ea3295cece946,c1018a31c3272ce74964a3280069f62f314a1a58,,,,,,,,Which sampling method do they use to approximate similarity between the conditional probability distributions over entity pairs?,3513682d4ee2e64725b956c489cd5b5995a6acf2,infinity,unfamiliar,no,,2cfd959e433f290bb50b55722370f0d22fe090b7,False,monte-carlo sequential sampling,,,"Just as introduced in sec:introduction, it is intractable to compute similarity exactly, as involving INLINEFORM0 computation. Hence, we consider the monte-carlo approximation: DISPLAYFORM0 where INLINEFORM0 is a list of entity pairs sampled from INLINEFORM1 . We use sequential sampling to gain INLINEFORM6 , which means we first sample INLINEFORM7 given INLINEFORM8 from INLINEFORM9 , and then sample INLINEFORM10 given INLINEFORM11 and INLINEFORM12 from INLINEFORM13 .","Just as introduced in sec:introduction, it is intractable to compute similarity exactly, as involving INLINEFORM0 computation. Hence, we consider the monte-carlo approximation: DISPLAYFORM0

where INLINEFORM0 is a list of entity pairs sampled from INLINEFORM1 . We use sequential sampling to gain INLINEFORM6 , which means we first sample INLINEFORM7 given INLINEFORM8 from INLINEFORM9 , and then sample INLINEFORM10 given INLINEFORM11 and INLINEFORM12 from INLINEFORM13 .",c2ffbf5dc43e19a73cd8ed56c0a1bb60344ca7c3,c1018a31c3272ce74964a3280069f62f314a1a58,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1-Table1-1.png,"Table 1: An illustration of the errors made by relation extraction models. The sentence contains obvious patterns indicating the two persons are siblings, but the model predicts it as parents. We introduce an approach to measure the similarity between relations. Our result shows “siblings” is the second most similar one to “parents”. By applying this approach, we could analyze the errors made by models, and help reduce errors.",4-Table2-1.png,Table 2: Methods to define a similarity function with different types of relation representations,4-Figure1-1.png,"Figure 1: Head-tail entity pairs of relation “be an unincorporated community in” (in blue) and “be a small city in” (in red) sampled from our fact distribution model. The coordinates of the points are computed by t-sne (Maaten and Hinton, 2008) on the concatenation of head and tail embeddings8. The two larger blue and red points indicate the embeddings of these two relations.",4-Figure2-1.png,"Figure 2: Spearman correlations between human judgment and models’ outputs. The inter-subject correlation is also shown as a horizontal line with its standard deviation as an error band. Our model shows the strongest positive correlation with human judgment, and, in other words, the smallest margin with human inter-subject agreement. Significance: ***/**/* := p < .001/.01/.05.",5-Table3-1.png,Table 3: Statistics of the triple sets used in this paper.,6-Table4-1.png,Table 4: The experiment results on the toy dataset show that our metric based on probability distribution significantly outperforms other relation similarity metrics.,Results,"fig:averank shows the distribution of similarity ranks of distracting relations of the above mentioned models' outputs on both relation prediction and relation extraction tasks. From fig:averankrp,fig:averankre, we could observe the most distracting relations are the most similar ones, which corroborate our hypothesis that even the best models on these tasks still make mistakes among the most similar relations. This result also highlights the importance of a heuristic method for guiding models to pay more attention to the boundary between similar relations. We also try to do the negative sampling with relation type constraints, but we see no improvement compared with uniform sampling. The details of negative sampling with relation type constraints are presented in sec:relation-type-constraints.",Similarity and Negative Sampling,"Based on the observation presented in sec:erroranalysisresult, we find out that similar relations are often confusing for relation prediction models. Therefore, corrupted triples with similar relations can be used as high-quality negative samples. For a given valid triple INLINEFORM0 , we corrupt the triple by substituting INLINEFORM1 with INLINEFORM2 with the probability, DISPLAYFORM0  where INLINEFORM0 is the temperature of the exponential function, the bigger the INLINEFORM1 is, the flatter the probability distribution is. When the temperature approaches infinite, the sampling process reduces to uniform sampling. In training, we set the initial temperature to a high level and gradually reduce the temperature. Intuitively, it enables the model to distinguish among those obviously different relations in the early stage and gives more and more confusing negative triples as the training processes to help the model distinguish the similar relations. This can be also viewed as a process of curriculum learning BIBREF21 , the data fed to the model gradually changes from simple negative triples to hard ones. We perform relation prediction task on FB15K with TransE. Following BIBREF3 , we use the ""Filtered"" setting protocol, i.e., filtering out the corrupted triples that appear in the dataset. Our sampling method is shown to improve the model's performance, especially on Hit@1 (fig:relationprediction). Training details are described in sec:trainingdetail.",Similarity and Softmax-Margin Loss,"Similar to sec:training-guidance-relation-prediction, we find out that relation extraction models often make wrong preditions on similar relations. In this section, we use similarity as an adaptive margin in softmax-margin loss to improve the performance of relation extraction models. As shown in BIBREF22 , Softmax-Margin Loss can be expressed as DISPLAYFORM0  where INLINEFORM0 denotes a structured output space for INLINEFORM1 , and INLINEFORM2 is INLINEFORM3 example in training data. We can easily incorporate similarity into cost function INLINEFORM0 . In this task, we define the cost function as INLINEFORM1 , where INLINEFORM2 is a hyperparameter. Intuitively, we give a larger margin between similar relations, forcing the model to distinguish among them, and thus making the model perform better. We apply our method to Position-aware Attention LSTM (PA-LSTM) BIBREF10 , and tab:relationextraction shows our method improves the performance of PA-LSTM. Training details are described in sec:trainingdetail.",Related Works,"As many early works devoted to psychology and linguistics, especially those works exploring semantic similarity BIBREF11 , BIBREF12 , researchers have empirically found there are various different categorizations of semantic relations among words and contexts. For promoting research on these different semantic relations, bejar1991cognitive explicitly defining these relations and miller1995wordnet further systematically organize rich semantic relations between words via a database. For identifying correlation and distinction between different semantic relations so as to support learning semantic similarity, various methods have attempted to measure relational similarity BIBREF23 , BIBREF24 , BIBREF25 , BIBREF26 , BIBREF27 , BIBREF28 , BIBREF29 . With the ongoing development of information extraction and effective construction of KBs BIBREF0 , BIBREF1 , BIBREF30 , relations are further defined as various types of latent connections between objects more than semantic relations. These general relations play a core role in expressing relational facts in the real world. Hence, there are accordingly various methods proposed for discovering more relations and their facts, including open information extraction BIBREF31 , BIBREF32 , BIBREF33 , BIBREF34 , BIBREF35 , BIBREF36 , BIBREF37 and relation extraction BIBREF38 , BIBREF39 , BIBREF40 , BIBREF41 , BIBREF42 , BIBREF43 , and relation prediction BIBREF3 , BIBREF44 , BIBREF45 , BIBREF46 , BIBREF47 . For both semantic relations and general relations, identifying them is a crucial problem, requiring systems to provide a fine-grained relation similarity metric. However, the existing methods suffer from sparse data, which makes it difficult to achieve an effective and stable similarity metric. Motivated by this, we propose to measure relation similarity by leveraging their fact distribution so that we can identify nuances between similar relations, and merge those distant surface forms of the same relations, benefitting the tasks mentioned above.",relation prediction relation extraction Open IE,Wikidata ReVerb FB15K TACRED,6-Figure3-1.png,Figure 3: Precision-recall curve on Open IE task comparing our similarity function with vector-based and angle-based similarity. Error bar represents 95% confidential interval. Bootstraping is used to calculate the confidential interval.,7-Figure4-1.png,"Figure 4: Similarity rank distributions of distracting relations on different tasks and datasets. Most of the distracting relations have top similarity rank. Distracting relations are, as defined previously, the relations have a higher rank in the relation classification result than the ground truth.",8-Table5-1.png,Table 5: Improvement of using similarity in softmaxmargin loss.,12-Figure6-1.png,Figure 6: The recall standard deviation of different models.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Conclusion and Future Work,"In this paper, we introduce an effective method to quantify the relation similarity and provide analysis and a survey of applications. We note that there are a wide range of future directions: (1) human prior knowledge could be incorporated into the similarity quantification; (2) similarity between relations could also be considered in multi-modal settings, e.g., extracting relations from images, videos, or even from audios; (3) by analyzing the distributions corresponding to different relations, one can also find some “meta-relations” between relations, such as hypernymy and hyponymy.",Acknowledgements,"This work is supported by the National Natural Science Foundation of China (NSFC No. 61572273, 61532010), the National Key Research and Development Program of China (No. 2018YFB1004503). Chen and Zhu is supported by Tsinghua University Initiative Scientific Research Program, and Chen is also supported by DCST Student Academic Training Program. Han is also supported by 2018 Tencent Rhino-Bird Elite Training Program.",Proofs to theorems in the paper,"If we have a proposal distribution INLINEFORM0 satisfying INLINEFORM1 , then eq:proofrecallfirstpart can be further written as DISPLAYFORM0  Sometimes, it's hard for us to compute normalized probability INLINEFORM0 . To tackle this problem, consider self-normalized importance sampling as an unbiased estimation BIBREF50 , DISPLAYFORM0  where INLINEFORM0 is the normalized version of INLINEFORM1 .",Chinese Restaurant Process,"Specifically, for a relation INLINEFORM0 with currently INLINEFORM1 sub-relations, we turn it to a new sub-relation with probability DISPLAYFORM0  or to the INLINEFORM0 existing sub-relation with probability DISPLAYFORM0  where INLINEFORM0 is the size of INLINEFORM1 existing sub-relation, INLINEFORM2 is the sum of the number of all sub-relationships of INLINEFORM3 , and INLINEFORM4 is a hyperparameter, in which case we use INLINEFORM5 .",Training Details,"In Wikidata and ReVerb Extractions dataset, we manually split a validation set, assuring every entity and relation appears in validation set also appears in training set. While minimizing loss on the training set, we observe the loss on the validation set and stop training as validation loss stops to decrease. Before training our model on any dataset, we use the entity embeddings and relation embeddings produced by TransE on the dataset as the pretrained embeddings for our model.",Training Details on Negative Sampling,"The sampling is launched with an initial temperature of 8192. The temperature drops to half every 200 epochs and remains stable once it hits 16. Optimization is performed using SGD, with a learning rate of 1e-3.",Training Details on Softmax-Margin Loss,"The sampling is launching with an initial temperature of 64. The temperature drops by 20% per epoch, and remains stable once it hits 16. The alpha we use is 9. Optimization is performed using SGD, with a learning rate of 1.",Recall Standard Deviation,"As is shown in fig:recallstd, the max recall standard deviation for our model is 0.4, and 0.11 for TransE.",Negative Samplilng with Relation Type Constraints,"In FB15K, if two relations have same prefix, we regard them as belonging to a same type, e.g., both /film/film/starring./film/performance/actor and /film/actor/film./film/performance/film have prefix film, they belong to same type. Similar to what is mentioned in sec:training-guidance-relation-prediction, we expect the model first to learn to distinguish among obviously different relations, and gradually learn to distinguish similar relations. Therefore, we conduct negative sampling with relation type constraints in two ways.",Add Up Two Uniform Distribution,"For each triple INLINEFORM0 , we have two uniform distribution INLINEFORM1 and INLINEFORM2 . INLINEFORM3 is the uniform distribution over all the relations except for those appear with INLINEFORM4 in the knowledge base, and INLINEFORM5 is the uniform distribution over the relations of the same type as INLINEFORM6 . When corrupting the triple, we sample INLINEFORM7 from the distribution: DISPLAYFORM0  where INLINEFORM0 is a hyperparameter. We set INLINEFORM1 to 1 at the beginning of training, and every INLINEFORM2 epochs, INLINEFORM3 will be multiplied by decrease rate INLINEFORM4 . We do grid search for INLINEFORM5 and INLINEFORM6 , but no improvement is observed.",Add Weight,"We speculate that the unsatisfactory result produced by adding up two uniform distribution is because that for those types with few relations in it, a small change of INLINEFORM0 will result in a significant change in INLINEFORM1 . Therefore, when sampling a negative INLINEFORM2 , we add weights to relations that are of the same type as INLINEFORM3 instead. Concretely, we substitute INLINEFORM4 with INLINEFORM5 with probability INLINEFORM6 , which can be calculated as: DISPLAYFORM0  where INLINEFORM0 denotes all the relations that are the same type as INLINEFORM1 , INLINEFORM2 is a hyperparameter and INLINEFORM3 is a normalizing constant. We set INLINEFORM4 to 0 at the beginning of training, and every INLINEFORM5 epochs, INLINEFORM6 will increase by INLINEFORM7 . We do grid search for INLINEFORM8 and INLINEFORM9 , still no improvement is observed.",,,,,,,,,,,,,,,,,,,,,,,,,Wikidata annotation guidance,We show the guidance provided for the annotators here.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Code-Switching Language Modeling using Syntax-Aware Multi-Task Learning,"Lack of text data has been the major issue on code-switching language modeling. In this paper, we introduce multi-task learning based language model which shares syntax representation of languages to leverage linguistic information and tackle the low resource data issue. Our model jointly learns both language modeling and Part-of-Speech tagging on code-switched utterances. In this way, the model is able to identify the location of code-switching points and improves the prediction of next word. Our approach outperforms standard LSTM based language model, with an improvement of 9.7% and 7.4% in perplexity on SEAME Phase I and Phase II dataset respectively.",Introduction,"Code-switching has received a lot of attention from speech and computational linguistic communities especially on how to automatically recognize text from speech and understand the structure within it. This phenomenon is very common in bilingual and multilingual communities. For decades, linguists studied this phenomenon and found that speakers switch at certain points, not randomly and obeys several constraints which point to the code-switched position in an utterance BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 . These hypotheses have been empirically proven by observing that bilinguals tend to code-switch intra-sententially at certain (morpho)-syntactic boundaries BIBREF5 . BIBREF1 defined the well-known theory that constraints the code-switch between a functional head and its complement is given the strong relationship between the two constituents, which corresponds to a hierarchical structure in terms of Part-of-Speech (POS) tags. BIBREF3 introduced Matrix-Language Model Framework for an intra-sentential case where the primary language is called Matrix Language and the second one called Embedded Language BIBREF2 . A language island was then introduced which is a constituent composed entirely of the language morphemes. From the Matrix-Language Frame Model, both matrix language (ML) island and embedded language (EL) islands are well-formed in their grammars and the EL islands are constrained under ML grammar BIBREF6 . BIBREF7 studied determiner–noun switches in Spanish–English bilinguals . Code-switching can be classified into two categories: intra-sentential and inter-sentential switches BIBREF0 . Intra-sentential switch defines a shift from one language to another language within an utterance. Inter-sentential switch refers to the change between two languages in a single discourse, where the switching occurs after a sentence in the first language has been completed and the next sentence starts with a new language. The example of the intra-sentential switch is shown in (1), and the inter-sentential switch is shown in (2). Language modeling using only word lexicons is not adequate to learn the complexity of code-switching patterns, especially in a low resource setting. Learning at the same time syntactic features such as POS tag and language identifier allows to have a shared grammatical information that constraint the next word prediction. Due to this reason, we propose a multi-task learning framework for code-switching language modeling task which is able to leverage syntactic features such as language and POS tag. The main contribution of this paper is two-fold. First, multi-task learning model is proposed to jointly learn language modeling task and POS sequence tagging task on code-switched utterances. Second, we incorporate language information into POS tags to create bilingual tags - it distinguishes tags between Chinese and English. The POS tag features are shared towards the language model and enrich the features to better learn where to switch. From our experiments result, we found that our method improves the perplexity on SEAME Phase I and Phase II dataset BIBREF8 .",Related Work,"The earliest language modeling research on code-switching data was applying linguistic theories on computational modelings such as Inversion Constraints and Functional Head Constraints on Chinese-English code-switching data BIBREF9 , BIBREF10 . BIBREF11 built a bilingual language model which is trained by interpolating two monolingual language models with statistical machine translation (SMT) based text generation to generate artificial code-switching text. BIBREF12 , BIBREF13 introduced a class-based method using RNNLM for computing the posterior probability and added POS tags in the input. BIBREF14 explored the combination of brown word clusters, open class words, and clusters of open class word embeddings as hand-crafted features for improving the factored language model. In addition, BIBREF15 proposed a generative language modeling with explicit phrase structure. A method of tying input and output embedding helped to reduce the number of parameters in language model and improved the perplexity BIBREF16 . Learning multiple NLP tasks using multi-task learning have been recently used in many domains BIBREF17 , BIBREF18 , BIBREF19 . They presented a joint many-task model to handle multiple NLP tasks and share parameters with growing depth in a single end-to-end model. A work by BIBREF20 showed the potential of combining POS tagging with Named-Entity Recognition task.",Methodology,This section shows how to build the features and how to train our multi-task learning language model. Multi-task learning consists of two NLP tasks: Language modeling and POS sequence tagging.,Feature Representation,"In the model, word lexicons and syntactic features are used as input. Word Lexicons: Sentences are encoded as 1-hot vectors and our vocabulary is built from training data. Syntactic Features: For each language island, phrase within the same language, we extract POS Tags iteratively using Chinese and English Penn Tree Bank Parser BIBREF21 , BIBREF22 . There are 31 English POS Tags and 34 Chinese POS Tags. Chinese words are distinguishable from English words since they have different encoding. We add language information in the POS tag label to discriminate POS tag between two languages.",Model Description,"faFigure FIGREF7 illustrates our multi-task learning extension to recurrent language model. In this multi-task learning setting, the tasks are language modeling and POS tagging. The POS tagging task shares the POS tag vector and the hidden states to LM task, but it does not receive any information from the other loss. Let INLINEFORM0 be the word lexicon in the document and INLINEFORM1 be the POS tag of the corresponding INLINEFORM2 at index INLINEFORM3 . They are mapped into embedding matrices to get their INLINEFORM4 -dimensional vector representations INLINEFORM5 and INLINEFORM6 . The input embedding weights are tied with the output weights. We concatenate INLINEFORM7 and INLINEFORM8 as the input of INLINEFORM9 . The information from the POS tag sequence is shared to the language model through this step. INLINEFORM10 INLINEFORM11  where INLINEFORM0 denotes the concatenation operator, INLINEFORM1 and INLINEFORM2 are the final hidden states of INLINEFORM3 and INLINEFORM4 respectively. INLINEFORM5 and INLINEFORM6 , the hidden states from both LSTMs are summed before predicting the next word. INLINEFORM7 INLINEFORM8  The word distribution of the next word INLINEFORM0 is normalized using softmax function. The model uses cross-entropy losses as error functions INLINEFORM1 and INLINEFORM2 for language modeling task and POS tagging task respectively. We optimize the multi-objective losses using the Back Propagation algorithm and we perform a weighted linear sum of the losses for each individual task. INLINEFORM3  where INLINEFORM0 is the weight of the loss in the training.",Experimental Setup,"In this section, we present the experimental setting for this task Corpus: SEAME (South East Asia Mandarin-English), a conversational Mandarin-English code-switching speech corpus consists of spontaneously spoken interviews and conversations BIBREF8 . Our dataset (LDC2015S04) is the most updated version of the Linguistic Data Consortium (LDC) database. However, the statistics are not identical to BIBREF23 . The corpus consists of two phases. In Phase I, only selected audio segments were transcribed. In Phase II, most of the audio segments were transcribed. According to the authors, it was not possible to restore the original dataset. The authors only used Phase I corpus. Few speaker ids are not in the speaker list provided by the authors BIBREF23 . Therefore as a workaround, we added these ids to the train set. As our future reference, the recording lists are included in the supplementary material. Preprocessing: First, we tokenized English and Chinese word using Stanford NLP toolkit BIBREF24 . Second, all hesitations and punctuations were removed except apostrophe, for examples: “let's"" and “it's"". Table TABREF9 and Table TABREF10 show the statistics of SEAME Phase I and II corpora. Table TABREF11 shows the most common trigger POS tag for Phase II corpus. Training: The baseline model was trained using RNNLM BIBREF25 . Then, we trained our LSTM models with different hidden sizes [200, 500]. All LSTMs have 2 layers and unrolled for 35 steps. The embedding size is equal to the LSTM hidden size. A dropout regularization BIBREF26 was applied to the word embedding vector and POS tag embedding vector, and to the recurrent output BIBREF27 with values between [0.2, 0.4]. We used a batch size of 20 in the training. EOS tag was used to separate every sentence. We chose Stochastic Gradient Descent and started with a learning rate of 20 and if there was no improvement during the evaluation, we reduced the learning rate by a factor of 0.75. The gradient was clipped to a maximum of 0.25. For the multi-task learning, we used different loss weights hyper-parameters INLINEFORM0 in the range of [0.25, 0.5, 0.75]. We tuned our model with the development set and we evaluated our best model using the test set, taking perplexity as the final evaluation metric. Where the latter was calculated by taking the exponential of the error in the negative log-form. INLINEFORM1 ",Results,"Table TABREF14 and Table TABREF15 show the results of multi-task learning with different values of the hyper-parameter INLINEFORM0 . We observe that the multi-task model with INLINEFORM1 achieved the best performance. We compare our multi-task learning model against RNNLM and LSTM baselines. The baselines correspond to recurrent neural networks that are trained with word lexicons. Table TABREF16 and Table TABREF17 present the overall results from different models. The multi-task model performs better than LSTM baseline by 9.7% perplexity in Phase I and 7.4% perplexity in Phase II. The performance of our model in Phase II is also better than the RNNLM (8.9%) and far better than the one presented in BIBREF13 in Phase I. Moreover, the results show that adding shared POS tag representation to INLINEFORM0 does not hurt the performance of the language modeling task. This implies that the syntactic information helps the model to better predict the next word in the sequence. To further verify this hypothesis, we conduct two analysis by visualizing our prediction examples in Figure FIGREF13 : Results with different hyper-parameter settings",Conclusion,"In this paper, we propose a multi-task learning approach for code-switched language modeling. The multi-task learning models achieve the best performance and outperform LSTM baseline with 9.7% and 7.4% improvement in perplexity for Phase I and Phase II SEAME corpus respectively. This implies that by training two different NLP tasks together the model can correctly learn the correlation between them. Indeed, the syntactic information helps the model to be aware of code-switching points and it improves the performance over the language model. Finally, we conclude that multi-task learning has good potential on code-switching language modeling research and there are still rooms for improvements, especially by adding more language pairs and corpora.",Acknowledgments,"This work is partially funded by ITS/319/16FP of the Innovation Technology Commission, HKUST 16214415 & 16248016 of Hong Kong Research Grants Council, and RDC 1718050-0 of EMOS.AI.",Recording Lists,"We split the recording ids into train, development, and test set as the following:",,,,,,,,,,,,,,,,,,,,,,,What is the architecture of the model?,3d583a0675ad34eb7a46767ef5eba5f0ea898aa9,five,familiar,no,,5053f146237e8fc8859ed3984b5d3f02f39266b7,False,,,"Training: The baseline model was trained using RNNLM BIBREF25 . Then, we trained our LSTM models with different hidden sizes [200, 500]. All LSTMs have 2 layers and unrolled for 35 steps. The embedding size is equal to the LSTM hidden size. A dropout regularization BIBREF26 was applied to the word embedding vector and POS tag embedding vector, and to the recurrent output BIBREF27 with values between [0.2, 0.4]. We used a batch size of 20 in the training. EOS tag was used to separate every sentence. We chose Stochastic Gradient Descent and started with a learning rate of 20 and if there was no improvement during the evaluation, we reduced the learning rate by a factor of 0.75. The gradient was clipped to a maximum of 0.25. For the multi-task learning, we used different loss weights hyper-parameters INLINEFORM0 in the range of [0.25, 0.5, 0.75]. We tuned our model with the development set and we evaluated our best model using the test set, taking perplexity as the final evaluation metric. Where the latter was calculated by taking the exponential of the error in the negative log-form. INLINEFORM1 FLOAT SELECTED: Figure 1: Multi-Task Learning Framework","Then, we trained our LSTM models with different hidden sizes [200, 500].  FLOAT SELECTED: Figure 1: Multi-Task Learning Framework",e50f78968d32ffa54e12ebadff914f5bb073ecaf,c1018a31c3272ce74964a3280069f62f314a1a58,,,,,,,,,What languages are explored in the work?,d7d41a1b8bbb1baece89b28962d23ee4457b9c3a,five,familiar,no,,5053f146237e8fc8859ed3984b5d3f02f39266b7,False,,,"In this section, we present the experimental setting for this task Corpus: SEAME (South East Asia Mandarin-English), a conversational Mandarin-English code-switching speech corpus consists of spontaneously spoken interviews and conversations BIBREF8 . Our dataset (LDC2015S04) is the most updated version of the Linguistic Data Consortium (LDC) database. However, the statistics are not identical to BIBREF23 . The corpus consists of two phases. In Phase I, only selected audio segments were transcribed. In Phase II, most of the audio segments were transcribed. According to the authors, it was not possible to restore the original dataset. The authors only used Phase I corpus. Few speaker ids are not in the speaker list provided by the authors BIBREF23 . Therefore as a workaround, we added these ids to the train set. As our future reference, the recording lists are included in the supplementary material.","In this section, we present the experimental setting for this task

Corpus: SEAME (South East Asia Mandarin-English), a conversational Mandarin-English code-switching speech corpus consists of spontaneously spoken interviews and conversations BIBREF8 . ",9fb82a10fd223c314b595e1bd5534f0fc6a7625b,c1018a31c3272ce74964a3280069f62f314a1a58,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3-Figure1-1.png,Figure 1: Multi-Task Learning Framework,3-Table1-1.png,Table 1: Data Statistics in SEAME Phase I,3-Table2-1.png,Table 2: Data Statistics in SEAME Phase II,3-Table3-1.png,Table 3: Code-Switching Trigger Words in SEAME Phase II,4-Table4-1.png,Table 4: Multi-task results with different weighted loss hyper-parameter in Phase I,4-Table5-1.png,Table 5: Multi-task results with different weighted loss hyper-parameter in Phase II,,,,,,,,,Mandarin English,,4-Table7-1.png,Table 7: Results in Phase II,5-Figure2-1.png,Figure 2: Prediction examples in Phase II. Left: Each square shows the target word’s log probability improvement by multi-task model compared to LSTM model (Darker color is better). Right: Each square shows the probability of the next POS tag is Chinese (Darker color represents higher probability),7-Table8-1.png,Table 8: Results in Phase I,7-Table9-1.png,Table 9: Results in Phase II,,,,LSTM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Finding Dominant User Utterances And System Responses in Conversations,"There are several dialog frameworks which allow manual specification of intents and rule based dialog flow. The rule based framework provides good control to dialog designers at the expense of being more time consuming and laborious. The job of a dialog designer can be reduced if we could identify pairs of user intents and corresponding responses automatically from prior conversations between users and agents. In this paper we propose an approach to find these frequent user utterances (which serve as examples for intents) and corresponding agent responses. We propose a novel SimCluster algorithm that extends standard K-means algorithm to simultaneously cluster user utterances and agent utterances by taking their adjacency information into account. The method also aligns these clusters to provide pairs of intents and response groups. We compare our results with those produced by using simple Kmeans clustering on a real dataset and observe upto 10% absolute improvement in F1-scores. Through our experiments on synthetic dataset, we show that our algorithm gains more advantage over K-means algorithm when the data has large variance.",Introduction,"There are several existing works that focus on modelling conversation using prior human to human conversational data BIBREF0 , BIBREF1 , BIBREF2 . BIBREF3 models the conversation from pairs of consecutive tweets. Deep learning based approaches have also been used to model the dialog in an end to end manner BIBREF4 , BIBREF5 . Memory networks have been used by Bordes et al Bor16 to model goal based dialog conversations. More recently, deep reinforcement learning models have been used for generating interactive and coherent dialogs BIBREF6 and negotiation dialogs BIBREF7 . Industry on the other hand has focused on building frameworks that allow manual specification of dialog models such as api.ai, Watson Conversational Services, and Microsoft Bot framework. These frameworks provide ways to specify intents, and a dialog flow. The user utterances are mapped to intents that are passed to a dialog flow manager. The dialog manager generates a response and updates the dialog state. See Figure FIGREF4 for an example of some intents and a dialog flow in a technical support domain. The dialog flow shows that when a user expresses an intent of # laptop_heat, then the system should respond with an utterance “Could you let me know the serial number of your machine ”. The designer needs to specify intents (for example # laptop_heat, # email_not_opening) and also provide corresponding system responses in the dialog flow. This way of specifying a dialog model using intents and corresponding system responses manually is more popular in industry than a data driven approach as it makes dialog model easy to interpret and debug as well as provides a better control to a dialog designer. However, this is very time consuming and laborious and thus involves huge costs. One approach to reduce the task of a dialog designer is to provide her with frequent user intents and possible corresponding system responses in a given domain. This can be done by analysing prior human to human conversations in the domain. Figure FIGREF5 (a) provides some example conversations in the technical support domain between users and agents. In order to identify frequent user intents, one can use existing clustering algorithms to group together all the utterances from the users. Here each cluster would correspond to a new intent and each utterance in the cluster would correspond to an example for the intent. Similarly the agents utterances can be clustered to identify system responses. However, we argue that rather than treating user utterances and agents responses in an isolated manner, there is merit in jointly clustering them. There is adjacency information of these utterances that can be utilized to identify better user intents and system responses. As an example, consider agent utterances A.2 in box A and A.2 in box C in Figure FIGREF5 (a). The utterances “Which operating system do you use?"" and “What OS is installed in your machine"" have no syntactic similarity and therefore may not be grouped together. However the fact that these utterances are adjacent to the similar user utterances “I am unable to start notes email client"" and “Unable to start my email client"" provides some evidence that the agent utterances might be similar. Similarly the user utterances “My system keeps getting rebooted"" and “Machine is booting time and again"" ( box B and D in Figure FIGREF5 (a))- that are syntactically not similar - could be grouped together since the adjacent agent utterances, “Is your machine heating up?"" and “Is the machine heating?"" are similar. Joint clustering of user utterances and agent utterances allow us to align the user utterance clusters with agent utterance clusters. Figure FIGREF5 (b) shows some examples of user utterance clusters and agent utterance clusters along with their alignments. Note that the user utterance clusters can be used by a dialog designer to specify intents, the agent utterance clusters can be used to create system responses and their alignment can be used to create part of the dialog flow. We propose two ways to take adjacency information into account. Firstly we propose a method called SimCluster for jointly or simultaneously clustering user utterances and agent utterances. SimCluster extends the K-means clustering method by incorporating additional penalty terms in the objective function that try to align the clusters together (described in Section SECREF3 ). The algorithm creates initial user utterance clusters as well as agent utterance clusters and then use bi-partite matching to get the best alignment across these clusters. Minimizing the objective function pushes the cluster centroids to move towards the centroids of the aligned clusters. The process implicitly ensures that the similarity of adjacent agent utterances affect the grouping of user utterances and conversely similarity of adjacent user utterances affect the grouping of agent utterances. In our second approach we use the information about neighbouring utterances for creating the vector representation of an utterance. For this we train a sequence to sequence model BIBREF8 to create the vectors (described in Section SECREF5 ). Our experiments described in section SECREF5 show that we achieve upto 10% absolute improvement in F1 scores over standard K-means using SimCluster. Also we observe that clustering of customer utterances gains significantly by using the adjacency information of agent utterances whereas the gain in clustering quality of agent utterances is moderate. This is because the agent utterances typically follow similar syntactic constructs whereas customer utterances are more varied. Considering the agent utterances into account while clustering users utterances is thus helpful. The organization of the rest of the paper is as follows. In Section SECREF2 we describe the related work. In Section SECREF3 we describe our problem formulation for clustering and the associated algorithm. Finally in sections SECREF4 and SECREF5 we discuss our experiments on synthetic and real datasets respectively.",Related Work,The notion of adjacency pairs was introduced by Sacks et al SSE74 to formalize the structure of a dialog. Adjacency pairs have been used to analyze the semantics of the dialog in computational linguistics community BIBREF9 . Clustering has been used for different tasks related to conversation. BIBREF10 considers the task of discovering dialog acts by clustering the raw utterances. We aim to obtain the frequent adjacency pairs through clustering. There have been several works regarding extensions of clustering to different scenarios such as:-,The Proposed Approach,In this section we describe our approach SimCluster that performs clustering in the two domains simultaneously and ensures that the generated clusters can be aligned with each other. We will describe the model in section SECREF9 and the algorithm in Section SECREF11 ., Model,"We consider a problem setting where we are given a collection of pairs of consecutive utterances, with vector representations INLINEFORM0 where INLINEFORM1 s are in speaker 1's domain and INLINEFORM2 s are in speaker 2's domain. We need to simultaneously cluster the utterances in their respective domains to minimize the variations within each domain and also ensure that the clusters for both domains are close together. We denote the clusters for speaker 1's domain by INLINEFORM0 with their respective means INLINEFORM1 . We denote the clusters assignments for INLINEFORM2 by INLINEFORM3 . We denote the clusters for second speaker by INLINEFORM0 with their respective means INLINEFORM1 . We denote the clusters assignments for INLINEFORM2 by INLINEFORM3 . The usual energy function has the terms for distance of points from their corresponding cluster centroids. To be able to ensure that the clusters in each domain are similar, we also consider an alignment between the centroids of the two domains. Since the semantic representations in the two domains are not comparable we consider a notion of induced centroids. We define the induced centroids INLINEFORM0 as the arithmetic means of the points INLINEFORM1 s such that INLINEFORM2 's have the same cluster assigned to them. Similarly, we define INLINEFORM3 as the arithmetic means of INLINEFORM4 s such that INLINEFORM5 s have the same cluster assigned to them. More formally, we define these induced centroids as:- INLINEFORM6  and INLINEFORM0  The alignment between these clusters given by the function INLINEFORM0 , which is a bijective mapping from the cluster indices in speaker 1's domain to those in speaker 2's domain. Though there can be several choices for this alignment function, we consider this alignment to be a matching which maximizes the sum of number of common indices in the aligned clusters. More formally we define INLINEFORM1  Then the matching INLINEFORM0 is defined to be the bijective function which maximizes INLINEFORM1 . We consider a term in the cost function corresponding to the sum of distances between the original centroids and the matched induced centroids. Our overall cost function is now given by:- INLINEFORM2  We explain the above definition via an example. Consider the clusters shown in Figure FIGREF10 . Here the INLINEFORM0 would match INLINEFORM1 to INLINEFORM2 , INLINEFORM3 to INLINEFORM4 and INLINEFORM5 to INLINEFORM6 , giving a match score of 6. Since INLINEFORM7 , INLINEFORM8 and INLINEFORM9 are present in the cluster INLINEFORM10 , INLINEFORM11 is given by INLINEFORM12 . Similarly INLINEFORM13   In a similar manner, INLINEFORM0 s can also be defined. Now the alignment terms are given by:- INLINEFORM1   ",SimCluster Algorithm,"[] SimCluster [1] SimClusterInput: INLINEFORM0 ,k (No. of cluster) Output: A cluster assignment INLINEFORM1 for INLINEFORM2 s and a cluster assignment INLINEFORM3 for INLINEFORM4 s Initialize a set of centroids INLINEFORM5 , and INLINEFORM6 Perform simple clustering for a few iterations For each i, compute INLINEFORM7 as the index j among 1 to k which minimizes INLINEFORM8 . Similarly , compute INLINEFORM9 as the index j' among 1 to k which minimizes INLINEFORM10 . Update the centroids, INLINEFORM11 and INLINEFORM12 as:- INLINEFORM13  and INLINEFORM0   Perform a Hungarian matching between the cluster indices in the two domains with weights N(j,j') on edges from index j to index j'. convergence To minimize the above energy term we adopt an approach similar to Lloyd's clustering algorithm Llo82 . We assume that we are given a set of initial seeds for the cluster centroids INLINEFORM0 and INLINEFORM1 . We repeat the following steps iteratively:- Minimize the energy with respect to cluster assignment keeping centroids unchanged. As in standard K-means algorithm, this is achieved by updating the cluster assignment, INLINEFORM0 for each index i to be the cluster index j which minimizes INLINEFORM1 . Correspondingly for INLINEFORM2 , we pick the cluster index j' which minimizes INLINEFORM3 .  Minimize the energy with respect to the centroids keeping cluster assignment unchanged. To achieve this step we need to minimize the energy function with respect to the centroids INLINEFORM0 and INLINEFORM1 . This is achieved by setting INLINEFORM2 for each j and INLINEFORM3 for each j. Setting INLINEFORM0 , we obtain INLINEFORM1  or equivalently INLINEFORM0   Similarly, setting INLINEFORM0 , we obtain INLINEFORM1  Finally we update the matching between the clusters. To do so, we need to find a bipartite matching match on the cluster indices so as to maximize INLINEFORM0 . We use Hungarian algorithm BIBREF13 to perform the same i.e. we define a bipartite graph with vertices consisting of cluster indices in the two domains. There is an edge from vertex representing cluster indices j (in domain 1) and j' in domain 2, with weight N(j,j'). We find a maximum weight bipartite matching in this graph. Similar to Lloyd's algorithm, each step of the above algorithm decreases the cost function. This ensures that the algorithm achieves a local minima of the cost function if it converges. See Algorithm SECREF11 for a formal description of the approach. The centroid update step of the above algorithm also has an intuitive explanation i.e. we are slightly moving away the centroid towards the matched induced centroid. This is consistent with our goal of aligning the clusters together in the two domains.",Alignment,The algorithm above maintains a mapping between the clusters in each speaker's domain. This mapping serves to give us the alignment between the clusters required to provide a corresponding response for a given user intent.,Experiments on Synthetic Dataset,"We performed experiments on synthetically generated dataset since it gives us a better control over the distribution of the data. Specifically we compared the gains obtained using our approach versus the variance of the distribution. We created dataset from the following generative process. [H] Generative Process [1] Generate data Pick k points INLINEFORM0 as domain -1 means and a corresponding set of k points INLINEFORM1 as domain-2 means, and covariance matrices INLINEFORM2  iter INLINEFORM0 upto num INLINEFORM1 samples Sample class INLINEFORM2 Sample INLINEFORM3 Sample INLINEFORM4 Add q and a so sampled to the list of q,a pairs We generated the dataset from the above sampling process with means selected on a 2 dimensional grid of size INLINEFORM5 with variance set as INLINEFORM6 in each dimension.10000 sample points were generated. The parameter INLINEFORM7 of the above algorithm was set to 0.5 and k was set to 9 (since the points could be generated from one of the 9 gaussians with centroids on a INLINEFORM8 grid). We compared the results with simple K-means clustering with k set to 9. For each of these, the initialization of means was done using INLINEFORM0 sampling approach BIBREF14 .",Evaluation and Results,"To evaluate the clusters we computed the following metrics ARI (Adjusted Rand Index): Standard Rand Index is a metric used to check the clustering quality against a given standard set of clusters by comparing the pairwise clustering decisions. It is defined as INLINEFORM0 , where a is the number of true positive pairs, b is the number of true negative pairs, c is the number of false positive pairs and d is the number of false negative pairs. Adjusted rand index corrects the standard rand index for chance and is defined as INLINEFORM1 BIBREF15 . We compute ARI score for both the source clusters as well as the target clusters. F1 scores: We also report F1 scores for the pairwise clustering decisions. In the above notation we considered the pair-precision as INLINEFORM0 and recall as INLINEFORM1 . The F1 measure is the Harmonic mean given as INLINEFORM2 . We used the gaussian index from which an utterance pair was generated as the ground truth label, which served to provide ground truth clusters for computation of the above evaluation metrics. Table TABREF15 shows a comparison of the results on SimCluster versus K-means algorithm. Here our SimCluster algorithm improves the F1-scores from 0.412 and 0.417 in the two domains to 0.442 and 0.441. The ARI scores also improve from 0.176 and 0.180 to 0.203 and 0.204. We also performed experiments to see how the performance of SimCluster is affected by the variance in the cluster (controlled by the generative process in Algorithm SECREF11 ). Intuitively we expect SimCluster to obtain an advantage over simple K-means when variance is larger. This is because at larger variance, the data points are more likely to be generated away from the centroid due to which they might be clustered incorrectly with the points from neighbouring cluster. However if the corresponding point from the other domain is generated closer to the centroid, it might help in clustering the given data point correctly. We performed these experiments with points generated from Algorithm SECREF11 at differet values of variance. We generated the points with centroids located on a grid of size INLINEFORM0 in each domain. The value of k was set to 9. The experiment was repeated for each value of variance between 0.1 to 1.0 in the intervals of 0.1. Figures FIGREF22 and FIGREF23 show the percentage improvement on ARI score and F1 score respectively achieved by SimCluster (over K-means) versus variance.",Description and preprocessing of dataset,"We have experimented on a dataset containing Twitter conversations between customers and Amazon help. The dataset consisted of 92130 conversations between customers and amazon help. We considered the conversations with exactly two speakers Amazon Help and a customer. Consecutive utterances by the same speaker were concatenated and considered as a single utterance. From these we extracted adjacency pairs of the form of a customer utterance followed by an agent (Amazon Help) utterance. We then selected the utterance pairs from 8 different categories, like late delivery of item, refund, unable to sign into the account, replacement of item, claim of warranty, tracking delivery information etc. A total of 1944 utterance pairs were selected. To create the vector representation we had used two distinct approaches:- Paragraph to vector approach (Doc2Vec) by Le and Mikolov LM14. Here we trained the vectors using distributed memory algorithm and trained for 40 iterations. A window size of 4 was used. We also trained the vectors using sequence to sequence approach BIBREF8 , on the Twitter dataset where we considered the task of predicting the reply of Amazon Help for customer's query and vice versa. The encoded vector from the input sequence forms the corresponding vector representation. For the task of generating the agent's response for customer utterance the encoding from the input sequence (in the trained model) forms the vector representation for the customer utterance. Similarly for the task of generating the previous customer utterance from the agent's response, the intermediate encoding forms the vector representation for the agent utterance. We used an LSTM based 3-layered sequence to sequence model with attention for this task. We ran the K-means clustering algorithm for 5 iterations followed by our SimCluster algorithm for 30 iterations to form clusters in both the (customer and agent) domains. The hyper parameter( INLINEFORM0 ) is chosen based on a validation set. We varied the value of INLINEFORM1 from 0.5 to 1.0 at intervals of 0.025. The initialization of centroids was performed using INLINEFORM2 sampling approach BIBREF14 .",Results,"For the clusters so obtained we have computed F1 and ARI measures as before and compared with the K-means approach. We used the partitioning formed by the 8 categories (from which the utterance pairs were selected) as the ground truth clustering. Table TABREF20 summarizes the results. We observe that for K-means algorithm, the vectors generated from sequence to sequence model perform better than the vectors generated using paragraph to vector for both the domains. This is expected as the vectors generated from sequence to sequence model encode some adjacency information as well. We further observe that the SimCluster approach performs better than the K-means approach for both the vector representations. It improves the F1-scores for Doc2Vec representation from 0.787 and 0.783 to 0.88 and 0.887 in the two domains. Also the F1-scores on Seq2Seq based representation improve from 0.83 and 0.9 to 0.86 and 0.916 using SimCluster. However the gains are much more in case of Doc2Vec representations than Seq2Seq representations since Doc2Vec did not have any information from the other domain where as some amount of this information is already captured by Seq2Seq representation. Moreover it is the clustering of customer utterances which is likely to see an improvement. This is because agent utterances tends to follow a generic pattern while customer utterances tend to be more varied. Considering agent utterances while generating clusters in the user domain thus tends to be more helpful than the other way round. Table TABREF25 shows qualitative results on the same dataset. Column 1 and 2 consists of clusters of utterances in customer domain and agent domain respectively. The utterances with usual font are representative utterances from clusters obtained through K-means clustering. The utterances in bold face indicate the similar utterances which were incorrectly classified in different clusters using K-means but were correctly classified together with the utterances by SimCluster algorithm.",Conclusions,One of the first steps to automate the construction of conversational systems could be to identify the frequent user utterances and their corresponding system responses. In this paper we proposed an approach to compute these groups of utterances by clustering the utterances in both the domains using our novel SimCluster algorithm which seeks to simultaneously cluster the utterances and align the utterances in two domains. Through our experiments on synthetically generated datset we have shown that SimCluster has more advantage over K-means on datasets with larger variance. Our technique improves upon the ARI and F1 scores on a real dataset containing Twitter conversations.,Acknowledgments,"We thank Dr. David Nahamoo (CTO, Speech Technology and Fellow IBM Research ) for his valuable guidance and feedback. We also acknowledge the anonymous reviewers of IJCNLP 2017 for their comments.",,,,,,,,,,,,,,,,,,,Do they study frequent user responses to help automate modelling of those?,3f7a7e81908a763e5ca720f90570c5f224ac64f6,infinity,unfamiliar,no,,2cfd959e433f290bb50b55722370f0d22fe090b7,False,True,,"In order to identify frequent user intents, one can use existing clustering algorithms to group together all the utterances from the users. Here each cluster would correspond to a new intent and each utterance in the cluster would correspond to an example for the intent. Similarly the agents utterances can be clustered to identify system responses. However, we argue that rather than treating user utterances and agents responses in an isolated manner, there is merit in jointly clustering them. There is adjacency information of these utterances that can be utilized to identify better user intents and system responses. As an example, consider agent utterances A.2 in box A and A.2 in box C in Figure FIGREF5 (a). The utterances “Which operating system do you use?"" and “What OS is installed in your machine"" have no syntactic similarity and therefore may not be grouped together. However the fact that these utterances are adjacent to the similar user utterances “I am unable to start notes email client"" and “Unable to start my email client"" provides some evidence that the agent utterances might be similar. Similarly the user utterances “My system keeps getting rebooted"" and “Machine is booting time and again"" ( box B and D in Figure FIGREF5 (a))- that are syntactically not similar - could be grouped together since the adjacent agent utterances, “Is your machine heating up?"" and “Is the machine heating?"" are similar.","Similarly the agents utterances can be clustered to identify system responses. However, we argue that rather than treating user utterances and agents responses in an isolated manner, there is merit in jointly clustering them. There is adjacency information of these utterances that can be utilized to identify better user intents and system responses.",b5256d39f74e32a23745916ba19ada536993c6ea,34c35a1877e453ecaebcf625df3ef788e1953cc4,,,,,,,,,How do they divide text into utterances?,28e7711f94e093137eb8828f0b1eff1b05e4fa38,infinity,unfamiliar,no,,2cfd959e433f290bb50b55722370f0d22fe090b7,True,,,,,b48d4c99485ff7fd118f070ca2588e2139066291,34c35a1877e453ecaebcf625df3ef788e1953cc4,Do they use the same distance metric for both the SimCluster and K-means algorithm?,49b38189b8336ce41d0f0b4c5c9459722736e15b,infinity,unfamiliar,no,,2cfd959e433f290bb50b55722370f0d22fe090b7,False,True,,"We used the gaussian index from which an utterance pair was generated as the ground truth label, which served to provide ground truth clusters for computation of the above evaluation metrics. Table TABREF15 shows a comparison of the results on SimCluster versus K-means algorithm. Here our SimCluster algorithm improves the F1-scores from 0.412 and 0.417 in the two domains to 0.442 and 0.441. The ARI scores also improve from 0.176 and 0.180 to 0.203 and 0.204.",Table TABREF15 shows a comparison of the results on SimCluster versus K-means algorithm. Here our SimCluster algorithm improves the F1-scores from 0.412 and 0.417 in the two domains to 0.442 and 0.441. The ARI scores also improve from 0.176 and 0.180 to 0.203 and 0.204.,20bc3e295844fab5380da0a33210552423bf60cf,34c35a1877e453ecaebcf625df3ef788e1953cc4,,,,,,,,How do they generate the synthetic dataset?,40c2bab4a6bf3c0628079fcf19e8b52f27f51d98,infinity,unfamiliar,no,,2cfd959e433f290bb50b55722370f0d22fe090b7,False,,using generative process,"We performed experiments on synthetically generated dataset since it gives us a better control over the distribution of the data. Specifically we compared the gains obtained using our approach versus the variance of the distribution. We created dataset from the following generative process. [H] Generative Process [1] Generate data Pick k points INLINEFORM0 as domain -1 means and a corresponding set of k points INLINEFORM1 as domain-2 means, and covariance matrices INLINEFORM2 iter INLINEFORM0 upto num INLINEFORM1 samples Sample class INLINEFORM2 Sample INLINEFORM3 Sample INLINEFORM4 Add q and a so sampled to the list of q,a pairs We generated the dataset from the above sampling process with means selected on a 2 dimensional grid of size INLINEFORM5 with variance set as INLINEFORM6 in each dimension.10000 sample points were generated. The parameter INLINEFORM7 of the above algorithm was set to 0.5 and k was set to 9 (since the points could be generated from one of the 9 gaussians with centroids on a INLINEFORM8 grid).","We performed experiments on synthetically generated dataset since it gives us a better control over the distribution of the data. Specifically we compared the gains obtained using our approach versus the variance of the distribution. We created dataset from the following generative process. [H] Generative Process [1] Generate data

Pick k points INLINEFORM0 as domain -1 means and a corresponding set of k points INLINEFORM1 as domain-2 means, and covariance matrices INLINEFORM2

iter INLINEFORM0 upto num INLINEFORM1 samples Sample class INLINEFORM2 Sample INLINEFORM3 Sample INLINEFORM4 Add q and a so sampled to the list of q,a pairs We generated the dataset from the above sampling process with means selected on a 2 dimensional grid of size INLINEFORM5 with variance set as INLINEFORM6 in each dimension.10000 sample points were generated. The parameter INLINEFORM7 of the above algorithm was set to 0.5 and k was set to 9 (since the points could be generated from one of the 9 gaussians with centroids on a INLINEFORM8 grid).",d8e2514bb4d80f73a30335bc26d577d76c1e30a0,34c35a1877e453ecaebcf625df3ef788e1953cc4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2-Figure1-1.png,Figure 1: Some intents and dialog flow,3-Figure2-1.png,Figure 2: Some sample conversations and the obtained clusters,5-Figure3-1.png,Figure 3: Sample clusters with matching,6-Table1-1.png,Table 1: Performance of SimCluster versus K-means clustering on synthetic dataset,7-Figure4-1.png,Figure 4: Improvement in ARI figures achieved by SimCluster versus variance,7-Figure5-1.png,Figure 5: Variation of Improvement in F1 score figures achieved by SimCluster versus variance,,,,,,,,,,,8-Table2-1.png,Table 2: Performance of SimCluster versus K-means clustering on both Doc2Vec as well as seq2seq based vectors,8-Table3-1.png,"Table 3: Sample clusters in user and agent domains. Utterances in bold are those which were not in the given cluster using K-means, but could be correctly classified with the cluster using SimCluster",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The Transference Architecture for Automatic Post-Editing,"In automatic post-editing (APE) it makes sense to condition post-editing (pe) decisions on both the source (src) and the machine translated text (mt) as input. This has led to multi-source encoder based APE approaches. A research challenge now is the search for architectures that best support the capture, preparation and provision of src and mt information and its integration with pe decisions. In this paper we present a new multi-source APE model, called transference. Unlike previous approaches, it (i) uses a transformer encoder block for src, (ii) followed by a decoder block, but without masking for self-attention on mt, which effectively acts as second encoder combining src -> mt, and (iii) feeds this representation into a final decoder block generating pe. Our model outperforms the state-of-the-art by 1 BLEU point on the WMT 2016, 2017, and 2018 English--German APE shared tasks (PBSMT and NMT). We further investigate the importance of our newly introduced second encoder and find that a too small amount of layers does hurt the performance, while reducing the number of layers of the decoder does not matter much.",Introduction,"The performance of state-of-the-art MT systems is not perfect, thus, human interventions are still required to correct machine translated texts into publishable quality translations BIBREF0. Automatic post-editing (APE) is a method that aims to automatically correct errors made by MT systems before performing actual human post-editing (PE) BIBREF1, thereby reducing the translators' workload and increasing productivity BIBREF2. APE systems trained on human PE data serve as MT post-processing modules to improve the overall performance. APE can therefore be viewed as a 2nd-stage MT system, translating predictable error patterns in MT output to their corresponding corrections. APE training data minimally involves MT output ($mt$) and the human post-edited ($pe$) version of $mt$, but additionally using the source ($src$) has been shown to provide further benefits BIBREF3, BIBREF4, BIBREF5. To provide awareness of errors in $mt$ originating from $src$, attention mechanisms BIBREF6 allow modeling of non-local dependencies in the input or output sequences, and importantly also global dependencies between them (in our case $src$, $mt$ and $pe$). The transformer architecture BIBREF7 is built solely upon such attention mechanisms completely replacing recurrence and convolutions. The transformer uses positional encoding to encode the input and output sequences, and computes both self- and cross-attention through so-called multi-head attentions, which are facilitated by parallelization. Such multi-head attention allows to jointly attend to information at different positions from different representation subspaces, e.g. utilizing and combining information from $src$, $mt$, and $pe$. In this paper, we present a multi-source neural APE architecture called transference. Our model contains a source encoder which encodes $src$ information, a second encoder ($enc_{src \rightarrow mt}$) which takes the encoded representation from the source encoder ($enc_{src}$), combines this with the self-attention-based encoding of $mt$ ($enc_{mt}$), and prepares a representation for the decoder ($dec_{pe}$) via cross-attention. Our second encoder ($enc_{src \rightarrow mt}$) can also be viewed as a standard transformer decoding block, however, without masking, which acts as an encoder. We thus recombine the different blocks of the transformer architecture and repurpose them for the APE task in a simple yet effective way. The suggested architecture is inspired by the two-step approach professional translators tend to use during post-editing: first, the source segment is compared to the corresponding translation suggestion (similar to what our $enc_{src \rightarrow mt}$ is doing), then corrections to the MT output are applied based on the encountered errors (in the same way that our $dec_{pe}$ uses the encoded representation of $enc_{src \rightarrow mt}$ to produce the final translation). The paper makes the following contributions: (i) we propose a new multi-encoder model for APE that consists only of standard transformer encoding and decoding blocks, (ii) by using a mix of self- and cross-attention we provide a representation of both $src$ and $mt$ for the decoder, allowing it to better capture errors in $mt$ originating from $src$; this advances the state-of-the-art in APE in terms of BLEU and TER, and (iii), we analyze the effect of varying the number of encoder and decoder layers BIBREF8, indicating that the encoders contribute more than decoders in transformer-based neural APE.",Related Research,"Recent advances in APE research are directed towards neural APE, which was first proposed by Pal:2016:ACL and junczysdowmunt-grundkiewicz:2016:WMT for the single-source APE scenario which does not consider $src$, i.e. $mt \rightarrow pe$. In their work, junczysdowmunt-grundkiewicz:2016:WMT also generated a large synthetic training dataset through back translation, which we also use as additional training data. Exploiting source information as an additional input can help neural APE to disambiguate corrections applied at each time step; this naturally leads to multi-source APE ($\lbrace src, mt\rbrace \rightarrow pe$). A multi-source neural APE system can be configured either by using a single encoder that encodes the concatenation of $src$ and $mt$ BIBREF9 or by using two separate encoders for $src$ and $mt$ and passing the concatenation of both encoders' final states to the decoder BIBREF10. A few approaches to multi-source neural APE were proposed in the WMT 2017 APE shared task. Junczysdowmunt:2017:WMT combine both $mt$ and $src$ in a single neural architecture, exploring different combinations of attention mechanisms including soft attention and hard monotonic attention. Chatterjee-EtAl:2017:WMT2 built upon the two-encoder architecture of multi-source models BIBREF10 by means of concatenating both weighted contexts of encoded $src$ and $mt$. Varis-bojar:2017:WMT compared two multi-source models, one using a single encoder with concatenation of $src$ and $mt$ sentences, and a second one using two character-level encoders for $mt$ and $src$ along with a character-level decoder. Recently, in the WMT 2018 APE shared task, several adaptations of the transformer architecture have been presented for multi-source APE. pal-EtAl:2018:WMT proposed an APE model that uses three self-attention-based encoders. They introduce an additional joint encoder that attends over a combination of the two encoded sequences from $mt$ and $src$. tebbifakhr-EtAl:2018:WMT, the NMT-subtask winner of WMT 2018 ($wmt18^{nmt}_{best}$), employ sequence-level loss functions in order to avoid exposure bias during training and to be consistent with the automatic evaluation metrics. shin-lee:2018:WMT propose that each encoder has its own self-attention and feed-forward layer to process each input separately. On the decoder side, they add two additional multi-head attention layers, one for $src \rightarrow mt$ and another for $src \rightarrow pe$. Thereafter another multi-head attention between the output of those attention layers helps the decoder to capture common words in $mt$ which should remain in $pe$. The APE PBSMT-subtask winner of WMT 2018 ($wmt18^{smt}_{best}$) BIBREF11 also presented another transformer-based multi-source APE which uses two encoders and stacks an additional cross-attention component for $src \rightarrow pe$ above the previous cross-attention for $mt \rightarrow pe$. Comparing shin-lee:2018:WMT's approach with the winner system, there are only two differences in the architecture: (i) the cross-attention order of $src \rightarrow mt$ and $src \rightarrow pe$ in the decoder, and (ii) $wmt18^{smt}_{best}$ additionally shares parameters between two encoders.",Transference Model for APE,"We propose a multi-source transformer model called transference ($\lbrace src,mt\rbrace _{tr} \rightarrow pe$, Figure FIGREF1), which takes advantage of both the encodings of $src$ and $mt$ and attends over a combination of both sequences while generating the post-edited sentence. The second encoder, $enc_{src \rightarrow mt}$, makes use of the first encoder $enc_{src}$ and a sub-encoder $enc_{mt}$ for considering $src$ and $mt$. Here, the $enc_{src}$ encoder and the $dec_{pe}$ decoder are equivalent to the original transformer for neural MT. Our $enc_{src \rightarrow mt}$ follows an architecture similar to the transformer's decoder, the difference being that no masked multi-head self-attention is used to process $mt$. One self-attended encoder for $src$, $\mathbf {s}$ = $(s_1, s_2, \ldots , s_k)$, returns a sequence of continuous representations, $enc_{src}$, and a second self-attended sub-encoder for $mt$, $\mathbf {m}$ = $(m_1, m_2, \ldots , m_l)$, returns another sequence of continuous representations, $enc_{mt}$. Self-attention at this point provides the advantage of aggregating information from all of the words, including $src$ and $mt$, and successively generates a new representation per word informed by the entire $src$ and $mt$ context. The internal $enc_{mt}$ representation performs cross-attention over $enc_{src}$ and prepares a final representation ($enc_{src \rightarrow mt}$) for the decoder ($dec_{pe}$). The decoder then generates the $pe$ output in sequence, $\mathbf {p}$ = $(p_1, p_2, \ldots , p_n)$, one word at a time from left to right by attending to previously generated words as well as the final representations ($enc_{src \rightarrow mt}$) generated by the encoder. To summarize, our multi-source APE implementation extends Vaswani:NIPS2017 by introducing an additional encoding block by which $src$ and $mt$ communicate with the decoder. Our proposed approach differs from the WMT 2018 PBSMT winner system in several ways: (i) we use the original transformer's decoder without modifications; (ii) one of our encoder blocks ($enc_{src \rightarrow mt}$) is identical to the transformer's decoder block but uses no masking in the self-attention layer, thus having one self-attention layer and an additional cross-attention for $src \rightarrow mt$; and (iii) in the decoder layer, the cross-attention is performed between the encoded representation from $enc_{src \rightarrow mt}$ and $pe$. Our approach also differs from the WMT 2018 NMT winner system: (i) $wmt18^{nmt}_{best}$ concatenates the encoded representation of two encoders and passes it as the key to the attention layer of the decoder, and (ii), the system additionally employs sequence-level loss functions based on maximum likelihood estimation and minimum risk training in order to avoid exposure bias during training. The main intuition is that our $enc_{src \rightarrow mt}$ attends over the $src$ and $mt$ and informs the $pe$ to better capture, process, and share information between $src$-$mt$-$pe$, which efficiently models error patterns and the corresponding corrections. Our model performs better than past approaches, as the experiment section will show.",Experiments,"We explore our approach on both APE sub-tasks of WMT 2018, where the 1st-stage MT system to which APE is applied is either a phrase-based statistical machine translation (PBSMT) or a neural machine translation (NMT) model. For the PBSMT task, we compare against four baselines: the raw SMT output provided by the 1st-stage PBSMT system, the best-performing systems from WMT APE 2018 ($\mathbf {wmt18^{smt}_{best}}$), which are a single model and an ensemble model by junczysdowmunt-grundkiewicz:2018:WMT, as well as a transformer trying to directly translate from $src$ to $pe$ (Transformer ($\mathbf {src \rightarrow pe}$)), thus performing translation instead of APE. We evaluate the systems using BLEU BIBREF12 and TER BIBREF13. For the NMT task, we consider two baselines: the raw NMT output provided by the 1st-stage NMT system and the best-performing system from the WMT 2018 NMT APE task ($\mathbf {wmt18^{nmt}_{best}}$) BIBREF14. Apart from the multi-encoder transference architecture described above ($\lbrace src,mt\rbrace _{tr} \rightarrow pe$) and ensembling of this architecture, two simpler versions are also analyzed: first, a `mono-lingual' ($\mathbf {mt \rightarrow pe}$) APE model using only parallel $mt$–$pe$ data and therefore only a single encoder, and second, an identical single-encoder architecture, however, using the concatenated $src$ and $mt$ text as input ($\mathbf {\lbrace src+mt\rbrace \rightarrow pe}$) BIBREF9.",Experiments ::: Data,"For our experiments, we use the English–German WMT 2016 BIBREF4, 2017 BIBREF5 and 2018 BIBREF15 APE task data. All these released APE datasets consist of English–German triplets containing source English text ($src$) from the IT domain, the corresponding German translations ($mt$) from a 1st-stage MT system, and the corresponding human-post-edited version ($pe$). The sizes of the datasets (train; dev; test), in terms of number of sentences, are (12,000; 1,000; 2,000), (11,000; 0; 2,000), and (13,442; 1,000; 1,023), for the 2016 PBSMT, the 2017 PBSMT, and the 2018 NMT data, respectively. One should note that for WMT 2018, we carried out experiments only for the NMT sub-task and ignored the data for the PBSMT task. Since the WMT APE datasets are small in size, we use `artificial training data' BIBREF16 containing 4.5M sentences as additional resources, 4M of which are weakly similar to the WMT 2016 training data, while 500K are very similar according to TER statistics. For experimenting on the NMT data, we additionally use the synthetic eScape APE corpus BIBREF17, consisting of $\sim $7M triples. For cleaning this noisy eScape dataset containing many unrelated language words (e.g. Chinese), we perform the following two steps: (i) we use the cleaning process described in tebbifakhr-EtAl:2018:WMT, and (ii) we use the Moses BIBREF18 corpus cleaning scripts with minimum and maximum number of tokens set to 1 and 100, respectively. After cleaning, we perform punctuation normalization, and then use the Moses tokenizer BIBREF18 to tokenize the eScape corpus with `no-escape' option. Finally, we apply true-casing. The cleaned version of the eScape corpus contains $\sim $6.5M triplets.",Experiments ::: Experiment Setup,"To build models for the PBSMT tasks from 2016 and 2017, we first train a generic APE model using all the training data (4M + 500K + 12K + 11K) described in Section SECREF2. Afterwards, we fine-tune the trained model using the 500K artificial and 23K (12K + 11K) real PE training data. We use the WMT 2016 development data (dev2016) containing 1,000 triplets to validate the models during training. To test our system performance, we use the WMT 2016 and 2017 test data (test2016, test2017) as two sub-experiments, each containing 2,000 triplets ($src$, $mt$ and $pe$). We compare the performance of our system with the four different baseline systems described above: raw MT, $wmt18^{smt}_{best}$ single and ensemble, as well as Transformer ($src \rightarrow pe$). Additionally, we check the performance of our model on the WMT 2018 NMT APE task (where unlike in previous tasks, the 1st-stage MT system is provided by NMT): for this, we explore two experimental setups: (i) we use the PBSMT task's APE model as a generic model which is then fine-tuned to a subset (12k) of the NMT data ($\lbrace src,mt\rbrace ^{nmt}_{tr} \rightarrow pe^{{generic, smt}}_{{}}$). One should note that it has been argued that the inclusion of SMT-specific data could be harmful when training NMT APE models BIBREF11. (ii), we train a completely new generic model on the cleaned eScape data ($\sim $6.5M) along with a subset (12K) of the original training data released for the NMT task ($\lbrace src,mt\rbrace ^{nmt}_{tr} \rightarrow pe^{{generic, nmt}}_{{}}$). The aforementioned 12K NMT data are the first 12K of the overall 13.4K NMT data. The remaining 1.4K are used as validation data. The released development set (dev2018) is used as test data for our experiment, alongside the test2018, for which we could only obtain results for a few models by the WMT 2019 task organizers. We also explore an additional fine-tuning step of $\lbrace src,mt\rbrace ^{nmt}_{tr} \rightarrow pe^{{generic, nmt}}_{{}}$ towards the 12K NMT data (called $\lbrace src,mt\rbrace ^{nmt}_{tr} \rightarrow pe^{{ft}}_{{}}$), and a model averaging the 8 best checkpoints of $\lbrace src,mt\rbrace ^{nmt}_{tr} \rightarrow pe^{{ft}}_{{}}$, which we call $\lbrace src,mt\rbrace ^{nmt}_{tr} \rightarrow pe^{{ft}}_{{avg}}$. Last, we analyze the importance of our second encoder ($enc_{src \rightarrow mt}$), compared to the source encoder ($enc_{src}$) and the decoder ($dec_{pe}$), by reducing and expanding the amount of layers in the encoders and the decoder. Our standard setup, which we use for fine-tuning, ensembling etc., is fixed to 6-6-6 for $N_{src}$-$N_{mt}$-$N_{pe}$ (cf. Figure FIGREF1), where 6 is the value that was proposed by Vaswani:NIPS2017 for the base model. We investigate what happens in terms of APE performance if we change this setting to 6-6-4 and 6-4-6. To handle out-of-vocabulary words and reduce the vocabulary size, instead of considering words, we consider subword units BIBREF19 by using byte-pair encoding (BPE). In the preprocessing step, instead of learning an explicit mapping between BPEs in the $src$, $mt$ and $pe$, we define BPE tokens by jointly processing all triplets. Thus, $src$, $mt$ and $pe$ derive a single BPE vocabulary. Since $mt$ and $pe$ belong to the same language (German) and $src$ is a close language (English), they naturally share a good fraction of BPE tokens, which reduces the vocabulary size to 28k.",Experiments ::: Hyper-parameter Setup,"We follow a similar hyper-parameter setup for all reported systems. All encoders (for $\lbrace src,mt\rbrace _{tr} \rightarrow pe$), and the decoder, are composed of a stack of $N_{src} = N_{mt} = N_{pe} = 6$ identical layers followed by layer normalization. The learning rate is varied throughout the training process, and increasing for the first training steps $warmup_{steps} = 8000$ and afterwards decreasing as described in BIBREF7. All remaining hyper-parameters are set analogously to those of the transformer's base model, except that we do not perform checkpoint averaging. At training time, the batch size is set to 25K tokens, with a maximum sentence length of 256 subwords. After each epoch, the training data is shuffled. During decoding, we perform beam search with a beam size of 4. We use shared embeddings between $mt$ and $pe$ in all our experiments.",Results,"The results of our four models, single-source ($\mathbf {mt \rightarrow pe}$), multi-source single encoder ($\mathbf {\lbrace src + pe\rbrace \rightarrow pe}$), transference ($\mathbf {\lbrace src,mt\rbrace ^{smt}_{tr} \rightarrow pe}$), and ensemble, in comparison to the four baselines, raw SMT, $\mathbf {wmt18^{smt}_{best}}$ BIBREF11 single and ensemble, as well as Transformer ($\mathbf {src \rightarrow pe}$), are presented in Table TABREF5 for test2016 and test2017. Table TABREF9 reports the results obtained by our transference model ($\mathbf {\lbrace src,mt\rbrace ^{nmt}_{tr} \rightarrow pe^{{}}_{{}}}$) on the WMT 2018 NMT data for dev2018 (which we use as a test set) and test2018, compared to the baselines raw NMT and $\mathbf {wmt18^{nmt}_{best}}$.",Results ::: Baselines,"The raw SMT output in Table TABREF5 is a strong black-box PBSMT system (i.e., 1st-stage MT). We report its performance observed with respect to the ground truth ($pe$), i.e., the post-edited version of $mt$. The original PBSMT system scores over 62 BLEU points and below 25 TER on test2016 and test2017. Using a Transformer ($src \rightarrow pe$), we test if APE is really useful, or if potential gains are only achieved due to the good performance of the transformer architecture. While we cannot do a full training of the transformer on the data that the raw MT engine was trained on due to the unavailability of the data, we use our PE datasets in an equivalent experimental setup as for all other models. The results of this system (Exp. 1.2 in Table TABREF5) show that the performance is actually lower across both test sets, -5.52/-9.43 absolute points in BLEU and +5.21/+7.72 absolute in TER, compared to the raw SMT baseline. We report four results from $\mathbf {wmt18^{smt}_{best}}$, (i) $wmt18^{smt}_{best}$ ($single$), which is the core multi-encoder implementation without ensembling but with checkpoint averaging, (ii) $wmt18^{smt}_{best}$ ($x4$) which is an ensemble of four identical `single' models trained with different random initializations. The results of $wmt18^{smt}_{best}$ ($single$) and $wmt18^{smt}_{best}$ ($x4$) (Exp. 1.3 and 1.4) reported in Table TABREF5 are from junczysdowmunt-grundkiewicz:2018:WMT. Since their training procedure slightly differs from ours, we also trained the $wmt18^{smt}_{best}$ system using exactly our experimental setup in order to make a fair comparison. This yields the baselines (iii) $wmt18^{smt,generic}_{best}$ ($single$) (Exp. 1.5), which is similar to $wmt18^{smt}_{best}$ ($single$), however, the training parameters and data are kept in line with our transference general model (Exp. 2.3) and (iv) $wmt18^{smt,ft}_{best}$ ($single$) (Exp. 1.6), which is also trained maintaining the equivalent experimental setup compared to the fine tuned version of the transference general model (Exp. 3.3). Compared to both raw SMT and Transformer ($src \rightarrow pe$) we see strong improvements for this state-of-the-art model, with BLEU scores of at least 68.14 and TER scores of at most 20.98 across the PBSMT testsets. $wmt18^{smt}_{best}$, however, performs better in its original setup (Exp. 1.3 and 1.4) compared to our experimental setup (Exp. 1.5 and 1.6).",Results ::: Single-Encoder Transformer for APE,"The two transformer architectures $\mathbf {mt \rightarrow pe}$ and $\mathbf {\lbrace src+mt\rbrace \rightarrow pe}$ use only a single encoder. Table TABREF5 shows that $\mathbf {mt \rightarrow pe}$ (Exp. 2.1) provides better performance (+4.42 absolute BLEU on test2017) compared to the original SMT, while $\mathbf {\lbrace src+mt\rbrace \rightarrow pe}$ (Exp. 2.2) provides further improvements by additionally using the $src$ information. $\mathbf {\lbrace src+mt\rbrace \rightarrow pe}$ improves over $\mathbf {mt \rightarrow pe}$ by +1.62/+1.35 absolute BLEU points on test2016/test2017. After fine-tuning, both single encoder transformers (Exp. 3.1 and 3.2 in Table TABREF5) show further improvements, +0.87 and +0.31 absolute BLEU points, respectively, for test2017 and a similar improvement for test2016.",Results ::: Transference Transformer for APE,"In contrast to the two models above, our transference architecture uses multiple encoders. To fairly compare to $wmt18^{smt}_{best}$, we retrain the $wmt18^{smt}_{best}$ system with our experimental setup (cf. Exp. 1.5 and 1.6 in Table TABREF5). $wmt18^{smt,generic}_{best}$ (single) is a generic model trained on all the training data; which is afterwards fine-tuned with 500K artificial and 23K real PE data ($wmt18^{smt,ft}_{best}$ (single)). It is to be noted that in terms of performance the data processing method described in junczysdowmunt-grundkiewicz:2018:WMT reported in Exp. 1.3 is better than ours (Exp. 1.6). The fine-tuned version of the $\lbrace src,mt\rbrace ^{smt}_{tr} \rightarrow pe$ model (Exp. 3.3 in Table TABREF5) outperforms $wmt18^{smt}_{best}$ (single) (Exp. 1.3) in BLEU on both test sets, however, the TER score for test2016 increases. One should note that $wmt18^{smt}_{best}$ (single) follows the transformer base model, which is an average of five checkpoints, while our Exp. 3.3 is not. When ensembling the 4 best checkpoints of our $\lbrace src,mt\rbrace ^{smt}_{tr} \rightarrow pe$ model (Exp. 4.1), the result beats the $wmt18^{smt}_{best}$ (x4) system, which is an ensemble of four different randomly initialized $wmt18^{smt}_{best}$ (single) systems. Our $\mathbf {ensemble^{smt} (x3)}$ combines two $\lbrace src,mt\rbrace ^{smt}_{tr} \rightarrow pe$ (Exp. 2.3) models initialized with different random weights with the ensemble of the fine-tuned transference model Exp3.3$^{smt}_{ens4ckpt}$(Exp. 4.1). This ensemble provides the best results for all datasets, providing roughly +1 BLEU point and -0.5 TER when comparing against $wmt18^{smt}_{best}$ (x4). The results on the WMT 2018 NMT datasets (dev2018 and test2018) are presented in Table TABREF9. The raw NMT system serves as one baseline against which we compare the performance of the different models. We evaluate the system hypotheses with respect to the ground truth ($pe$), i.e., the post-edited version of $mt$. The baseline original NMT system scores 76.76 BLEU points and 15.08 TER on dev2018, and 74.73 BLEU points and 16.84 TER on test2018. For the WMT 2018 NMT data we first test our $\lbrace src,mt\rbrace ^{nmt}_{tr} \rightarrow pe^{{generic,smt}}_{{}}$ model, which is the model from Exp. 3.3 fine-tuned towards NMT data as described in Section SECREF3. Table TABREF9 shows that our PBSMT APE model fine-tuned towards NMT (Exp. 7) can even slightly improve over the already very strong NMT system by about +0.3 BLEU and -0.1 TER, although these improvements are not statistically significant. The overall results improve when we train our model on eScape and NMT data instead of using the PBSMT model as a basis. Our proposed generic transference model (Exp. 8, $\lbrace src,mt\rbrace ^{nmt}_{tr} \rightarrow pe^{{generic,nmt}}_{{}}$ shows statistically significant improvements in terms of BLEU and TER compared to the baseline even before fine-tuning, and further improvements after fine-tuning (Exp. 9, $\lbrace src,mt\rbrace ^{nmt}_{tr} \rightarrow pe^{{ft}}_{{}}$). Finally, after averaging the 8 best checkpoints, our $\lbrace src,mt\rbrace ^{nmt}_{tr} \rightarrow pe^{{ft}}_{{avg}}$ model (Exp. 10) also shows consistent improvements in comparison to the baseline and other experimental setups. Overall our fine-tuned model averaging the 8 best checkpoints achieves +1.02 absolute BLEU points and -0.69 absolute TER improvements over the baseline on test2018. Table TABREF9 also shows the performance of our model compared to the winner system of WMT 2018 ($wmt18^{nmt}_{best}$) for the NMT task BIBREF14. $wmt18^{nmt}_{best}$ scores 14.78 in TER and 77.74 in BLEU on the dev2018 and 16.46 in TER and 75.53 in BLEU on the test2018. In comparison to $wmt18^{nmt}_{best}$, our model (Exp. 10) achieves better scores in TER on both the dev2018 and test2018, however, in terms of BLEU our model scores slightly lower for dev2018, while some improvements are achieved on test2018. The number of layers ($N_{src}$-$N_{mt}$-$N_{pe}$) in all encoders and the decoder for these results is fixed to 6-6-6. In Exp. 5.1, and 5.2 in Table TABREF5, we see the results of changing this setting to 6-6-4 and 6-4-6. This can be compared to the results of Exp. 2.3, since no fine-tuning or ensembling was performed for these three experiments. Exp. 5.1 shows that decreasing the number of layers on the decoder side does not hurt the performance. In fact, in the case of test2016, we got some improvement, while for test2017, the scores got slightly worse. In contrast, reducing the $enc_{src \rightarrow mt}$ encoder block's depth (Exp. 5.2) does indeed reduce the performance for all four scores, showing the importance of this second encoder.",Results ::: Analysis of Error Patterns,"In Table TABREF11, we analyze and compare the best performing SMT ($ensemble^{smt} (x3)$) and NMT ($\lbrace src,mt\rbrace ^{nmt}_{tr} \rightarrow pe^{{ft}}_{{avg}}$) model outputs with the original MT outputs on the WMT 2017 (SMT) APE test set and on the WMT 2018 (NMT) development set. Improvements are measured in terms of number of words which need to be (i) inserted (In), (ii) deleted (De), (iii) substituted (Su), and (iv) shifted (Sh), as per TER BIBREF13, in order to turn the MT outputs into reference translations. Our model provides promising results by significantly reducing the required number of edits (24% overall for PBSMT task and 3.6% for NMT task) across all edit operations, thereby leading to reduced post-editing effort and hence improving human post-editing productivity. When comparing PBSMT to NMT, we see that stronger improvements are achieved for PBSMT, probably because the raw SMT is worse than the raw NMT. For PBSMT, similar results are achieved for In, De, and Sh, while less gains are obtained in terms of Su. For NMT, In is improved most, followed by Su, De, and last Sh. For shifts in NMT, the APE system even creates further errors, instead of reducing them, which is an issue we aim to prevent in the future.",Results ::: Discussion,"The proposed transference architecture ($\lbrace src,mt\rbrace ^{smt}_{tr} \rightarrow pe$, Exp. 2.3) shows slightly worse results than $wmt18^{smt}_{best}$ (single) (Exp. 1.3) before fine-tuning, and roughly similar results after fine-tuning (Exp. 3.3). After ensembling, however, our transference model (Exp. 4.2) shows consistent improvements when comparing against the best baseline ensemble $wmt18^{smt}_{best}$ (x4) (Exp. 1.4). Due to the unavailability of the sentence-level scores of $wmt18^{smt}_{best}$ (x4), we could not test if the improvements (roughly +1 BLEU, -0.5 TER) are statistically significant. Interestingly, our approach of taking the model optimized for PBSMT and fine-tuning it to the NMT task (Exp. 7) does not hurt the performance as was reported in the previous literature BIBREF11. In contrast, some small, albeit statistically insignificant improvements over the raw NMT baseline were achieved. When we train the transference architecture directly for the NMT task (Exp. 8), we get slightly better and statistically significant improvements compared to raw NMT. Fine-tuning this NMT model further towards the actual NMT data (Exp. 9), as well as performing checkpoint averaging using the 8 best checkpoints improves the results even further. The reasons for the effectiveness of our approach can be summarized as follows. (1) Our $enc_{src \rightarrow mt}$ contains two attention mechanisms: one is self-attention and another is cross-attention. The self-attention layer is not masked here; therefore, the cross-attention layer in $enc_{src \rightarrow mt}$ is informed by both previous and future time-steps from the self-attended representation of $mt$ ($enc_{mt}$) and additionally from $enc_{src}$. As a result, each state representation of $enc_{src \rightarrow mt}$ is learned from the context of $src$ and $mt$. This might produce better representations for $dec_{pe}$ which can access the combined context. In contrast, in $wmt18^{smt}_{best}$, the $dec_{pe}$ accesses representations from $src$ and $mt$ independently, first using the representation from $mt$ and then using that of $src$. (2) The position-wise feed-forward layer in our $enc_{src \rightarrow mt}$ of the transference model requires processing information from two attention modules, while in the case of $wmt18^{smt}_{best}$, the position-wise feed-forward layer in $dec_{pe}$ needs to process information from three attention modules, which may increase the learning difficulty of the feed-forward layer. (3) Since $pe$ is a post-edited version of $mt$, sharing the same language, $mt$ and $pe$ are quite similar compared to $src$. Therefore, attending over a fine-tuned representation from $mt$ along with $src$, which is what we have done in this work, might be a reason for the better results than those achieved by attending over $src$ directly. Evaluating the influence of the depth of our encoders and decoder show that while the decoder depth appears to have limited importance, reducing the encoder depth indeed hurts performance which is in line with domhan-2018-much.",Conclusions,"In this paper, we presented a multi-encoder transformer-based APE model that repurposes the standard transformer blocks in a simple and effective way for the APE task: first, our transference architecture uses a transformer encoder block for $src$, followed by a decoder block without masking on $mt$ that effectively acts as a second encoder combining $src \rightarrow mt$, and feeds this representation into a final decoder block generating $pe$. The proposed model outperforms the best-performing system of WMT 2018 on the test2016, test2017, dev2018, and test2018 data and provides a new state-of-the-art in APE. Taking a departure from traditional transformer-based encoders, which perform self-attention only, our second encoder also performs cross-attention to produce representations for the decoder based on both $src$ and $mt$. We also show that the encoder plays a more pivotal role than the decoder in transformer-based APE, which could also be the case for transformer-based generation tasks in general. Our architecture is generic and can be used for any multi-source task, e.g., multi-source translation or summarization, etc.",,,,,,,,,,,,,,,What experiment result led to conclussion that reducing the number of layers of the decoder does not matter much?,f9c5799091e7e35a8133eee4d95004e1b35aea00,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,,,"Last, we analyze the importance of our second encoder ($enc_{src \rightarrow mt}$), compared to the source encoder ($enc_{src}$) and the decoder ($dec_{pe}$), by reducing and expanding the amount of layers in the encoders and the decoder. Our standard setup, which we use for fine-tuning, ensembling etc., is fixed to 6-6-6 for $N_{src}$-$N_{mt}$-$N_{pe}$ (cf. Figure FIGREF1), where 6 is the value that was proposed by Vaswani:NIPS2017 for the base model. We investigate what happens in terms of APE performance if we change this setting to 6-6-4 and 6-4-6. To handle out-of-vocabulary words and reduce the vocabulary size, instead of considering words, we consider subword units BIBREF19 by using byte-pair encoding (BPE). In the preprocessing step, instead of learning an explicit mapping between BPEs in the $src$, $mt$ and $pe$, we define BPE tokens by jointly processing all triplets. Thus, $src$, $mt$ and $pe$ derive a single BPE vocabulary. Since $mt$ and $pe$ belong to the same language (German) and $src$ is a close language (English), they naturally share a good fraction of BPE tokens, which reduces the vocabulary size to 28k. The number of layers ($N_{src}$-$N_{mt}$-$N_{pe}$) in all encoders and the decoder for these results is fixed to 6-6-6. In Exp. 5.1, and 5.2 in Table TABREF5, we see the results of changing this setting to 6-6-4 and 6-4-6. This can be compared to the results of Exp. 2.3, since no fine-tuning or ensembling was performed for these three experiments. Exp. 5.1 shows that decreasing the number of layers on the decoder side does not hurt the performance. In fact, in the case of test2016, we got some improvement, while for test2017, the scores got slightly worse. In contrast, reducing the $enc_{src \rightarrow mt}$ encoder block's depth (Exp. 5.2) does indeed reduce the performance for all four scores, showing the importance of this second encoder. FLOAT SELECTED: Table 1: Evaluation results on the WMT APE test set 2016, and test set 2017 for the PBSMT task; (±X) value is the improvement over wmt18smtbest (x4). The last section of the table shows the impact of increasing and decreasing the depth of the encoders and the decoder.","Last, we analyze the importance of our second encoder ($enc_{src \rightarrow mt}$), compared to the source encoder ($enc_{src}$) and the decoder ($dec_{pe}$), by reducing and expanding the amount of layers in the encoders and the decoder. Our standard setup, which we use for fine-tuning, ensembling etc., is fixed to 6-6-6 for $N_{src}$-$N_{mt}$-$N_{pe}$ (cf. Figure FIGREF1), where 6 is the value that was proposed by Vaswani:NIPS2017 for the base model. We investigate what happens in terms of APE performance if we change this setting to 6-6-4 and 6-4-6.  The number of layers ($N_{src}$-$N_{mt}$-$N_{pe}$) in all encoders and the decoder for these results is fixed to 6-6-6. In Exp. 5.1, and 5.2 in Table TABREF5, we see the results of changing this setting to 6-6-4 and 6-4-6. This can be compared to the results of Exp. 2.3, since no fine-tuning or ensembling was performed for these three experiments. Exp. 5.1 shows that decreasing the number of layers on the decoder side does not hurt the performance. In fact, in the case of test2016, we got some improvement, while for test2017, the scores got slightly worse. In contrast, reducing the $enc_{src \rightarrow mt}$ encoder block's depth (Exp. 5.2) does indeed reduce the performance for all four scores, showing the importance of this second encoder. FLOAT SELECTED: Table 1: Evaluation results on the WMT APE test set 2016, and test set 2017 for the PBSMT task; (±X) value is the improvement over wmt18smtbest (x4). The last section of the table shows the impact of increasing and decreasing the depth of the encoders and the decoder.",b9ca56e28bc96cc457a1d083731e3959d1e78778,a0b403873302db7cada39008f04d01155ef68f4f,,,,,,,,,How much is performance hurt when using too small amount of layers in encoder?,04012650a45d56c0013cf45fd9792f43916eaf83,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,,"comparing to the results from reducing the number of layers in the decoder, the BLEU score was 69.93 which is less than 1% in case of test2016 and in case of test2017 it was less by 0.2 %. In terms of TER it had higher score by 0.7 in case of test2016 and 0.1 in case of test2017. ","The number of layers ($N_{src}$-$N_{mt}$-$N_{pe}$) in all encoders and the decoder for these results is fixed to 6-6-6. In Exp. 5.1, and 5.2 in Table TABREF5, we see the results of changing this setting to 6-6-4 and 6-4-6. This can be compared to the results of Exp. 2.3, since no fine-tuning or ensembling was performed for these three experiments. Exp. 5.1 shows that decreasing the number of layers on the decoder side does not hurt the performance. In fact, in the case of test2016, we got some improvement, while for test2017, the scores got slightly worse. In contrast, reducing the $enc_{src \rightarrow mt}$ encoder block's depth (Exp. 5.2) does indeed reduce the performance for all four scores, showing the importance of this second encoder. FLOAT SELECTED: Table 1: Evaluation results on the WMT APE test set 2016, and test set 2017 for the PBSMT task; (±X) value is the improvement over wmt18smtbest (x4). The last section of the table shows the impact of increasing and decreasing the depth of the encoders and the decoder.","Exp. 5.1 shows that decreasing the number of layers on the decoder side does not hurt the performance. In fact, in the case of test2016, we got some improvement, while for test2017, the scores got slightly worse. In contrast, reducing the $enc_{src \rightarrow mt}$ encoder block's depth (Exp. 5.2) does indeed reduce the performance for all four scores, showing the importance of this second encoder. FLOAT SELECTED: Table 1: Evaluation results on the WMT APE test set 2016, and test set 2017 for the PBSMT task; (±X) value is the improvement over wmt18smtbest (x4). The last section of the table shows the impact of increasing and decreasing the depth of the encoders and the decoder.",fab85e57ab7cf64eea51fd8daaeec88dc5ca43e5,a0b403873302db7cada39008f04d01155ef68f4f,What was previous state of the art model for automatic post editing?,7889ec45b996be0b8bf7360d08f84daf3644f115,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,,,"Recently, in the WMT 2018 APE shared task, several adaptations of the transformer architecture have been presented for multi-source APE. pal-EtAl:2018:WMT proposed an APE model that uses three self-attention-based encoders. They introduce an additional joint encoder that attends over a combination of the two encoded sequences from $mt$ and $src$. tebbifakhr-EtAl:2018:WMT, the NMT-subtask winner of WMT 2018 ($wmt18^{nmt}_{best}$), employ sequence-level loss functions in order to avoid exposure bias during training and to be consistent with the automatic evaluation metrics. shin-lee:2018:WMT propose that each encoder has its own self-attention and feed-forward layer to process each input separately. On the decoder side, they add two additional multi-head attention layers, one for $src \rightarrow mt$ and another for $src \rightarrow pe$. Thereafter another multi-head attention between the output of those attention layers helps the decoder to capture common words in $mt$ which should remain in $pe$. The APE PBSMT-subtask winner of WMT 2018 ($wmt18^{smt}_{best}$) BIBREF11 also presented another transformer-based multi-source APE which uses two encoders and stacks an additional cross-attention component for $src \rightarrow pe$ above the previous cross-attention for $mt \rightarrow pe$. Comparing shin-lee:2018:WMT's approach with the winner system, there are only two differences in the architecture: (i) the cross-attention order of $src \rightarrow mt$ and $src \rightarrow pe$ in the decoder, and (ii) $wmt18^{smt}_{best}$ additionally shares parameters between two encoders.","Recently, in the WMT 2018 APE shared task, several adaptations of the transformer architecture have been presented for multi-source APE. pal-EtAl:2018:WMT proposed an APE model that uses three self-attention-based encoders. They introduce an additional joint encoder that attends over a combination of the two encoded sequences from $mt$ and $src$. tebbifakhr-EtAl:2018:WMT, the NMT-subtask winner of WMT 2018 ($wmt18^{nmt}_{best}$), employ sequence-level loss functions in order to avoid exposure bias during training and to be consistent with the automatic evaluation metrics. shin-lee:2018:WMT propose that each encoder has its own self-attention and feed-forward layer to process each input separately. On the decoder side, they add two additional multi-head attention layers, one for $src \rightarrow mt$ and another for $src \rightarrow pe$. Thereafter another multi-head attention between the output of those attention layers helps the decoder to capture common words in $mt$ which should remain in $pe$. The APE PBSMT-subtask winner of WMT 2018 ($wmt18^{smt}_{best}$) BIBREF11 also presented another transformer-based multi-source APE which uses two encoders and stacks an additional cross-attention component for $src \rightarrow pe$ above the previous cross-attention for $mt \rightarrow pe$. Comparing shin-lee:2018:WMT's approach with the winner system, there are only two differences in the architecture: (i) the cross-attention order of $src \rightarrow mt$ and $src \rightarrow pe$ in the decoder, and (ii) $wmt18^{smt}_{best}$ additionally shares parameters between two encoders. Recently, in the WMT 2018 APE shared task, several adaptations of the transformer architecture have been presented for multi-source APE. pal-EtAl:2018:WMT proposed an APE model that uses three self-attention-based encoders. They introduce an additional joint encoder that attends over a combination of the two encoded sequences from $mt$ and $src$. tebbifakhr-EtAl:2018:WMT, the NMT-subtask winner of WMT 2018 ($wmt18^{nmt}_{best}$), employ sequence-level loss functions in order to avoid exposure bias during training and to be consistent with the automatic evaluation metrics. shin-lee:2018:WMT propose that each encoder has its own self-attention and feed-forward layer to process each input separately. On the decoder side, they add two additional multi-head attention layers, one for $src \rightarrow mt$ and another for $src \rightarrow pe$. Thereafter another multi-head attention between the output of those attention layers helps the decoder to capture common words in $mt$ which should remain in $pe$. The APE PBSMT-subtask winner of WMT 2018 ($wmt18^{smt}_{best}$) BIBREF11 also presented another transformer-based multi-source APE which uses two encoders and stacks an additional cross-attention component for $src \rightarrow pe$ above the previous cross-attention for $mt \rightarrow pe$. Comparing shin-lee:2018:WMT's approach with the winner system, there are only two differences in the architecture: (i) the cross-attention order of $src \rightarrow mt$ and $src \rightarrow pe$ in the decoder, and (ii) $wmt18^{smt}_{best}$ additionally shares parameters between two encoders.",e1ea054e476335bec3cd9d5c81e2c0c43f02dedc,a0b403873302db7cada39008f04d01155ef68f4f,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3-Figure1-1.png,"Figure 1: The transference model architecture for APE ({src,mt}tr → pe).",6-Table1-1.png,"Table 1: Evaluation results on the WMT APE test set 2016, and test set 2017 for the PBSMT task; (±X) value is the improvement over wmt18smtbest (x4). The last section of the table shows the impact of increasing and decreasing the depth of the encoders and the decoder.",7-Table2-1.png,Table 2: Evaluation results on the WMT APE 2018 development set for the NMT task (Exp. 10 results were obtained by the WMT 2019 task organizers).,8-Table3-1.png,Table 3: % of error reduction in terms of different edit operations achieved by our best systems compared to the raw MT baselines.,,,,,,,,,,,,,,"pal-EtAl:2018:WMT proposed an APE model that uses three self-attention-based encoders tebbifakhr-EtAl:2018:WMT, the NMT-subtask winner of WMT 2018 ($wmt18^{nmt}_{best}$), employ sequence-level loss functions in order to avoid exposure bias during training and to be consistent with the automatic evaluation metrics. shin-lee:2018:WMT propose that each encoder has its own self-attention and feed-forward layer to process each input separately.  The APE PBSMT-subtask winner of WMT 2018 ($wmt18^{smt}_{best}$) BIBREF11 also presented another transformer-based multi-source APE which uses two encoders and stacks an additional cross-attention component for $src \rightarrow pe$ above the previous cross-attention for $mt \rightarrow pe$.",,,,,,,,,,,,Exp. 5.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Question Answering based Clinical Text Structuring Using Pre-trained Language Model,"Clinical text structuring is a critical and fundamental task for clinical research. Traditional methods such as taskspecific end-to-end models and pipeline models usually suffer from the lack of dataset and error propagation. In this paper, we present a question answering based clinical text structuring (QA-CTS) task to unify different specific tasks and make dataset shareable. A novel model that aims to introduce domain-specific features (e.g., clinical named entity information) into pre-trained language model is also proposed for QA-CTS task. Experimental results on Chinese pathology reports collected from Ruijing Hospital demonstrate our presented QA-CTS task is very effective to improve the performance on specific tasks. Our proposed model also competes favorably with strong baseline models in specific tasks.",Introduction,"Clinical text structuring (CTS) is a critical task for fetching medical research data from electronic health records (EHRs), where structural patient medical data, such as whether the patient has specific symptoms, diseases, or what the tumor size is, how far from the tumor is cut at during the surgery, or what the specific laboratory test result is, are obtained. It is important to extract structured data from clinical text because bio-medical systems or bio-medical researches greatly rely on structured data but they cannot obtain them directly. In addition, clinical text often contains abundant healthcare information. CTS is able to provide large-scale extracted structured data for enormous down-stream clinical researches. However, end-to-end CTS is a very challenging task. Different CTS tasks often have non-uniform output formats, such as specific-class classifications (e.g. tumor stage), strings in the original text (e.g. result for a laboratory test) and inferred values from part of the original text (e.g. calculated tumor size). Researchers have to construct different models for it, which is already costly, and hence it calls for a lot of labeled data for each model. Moreover, labeling necessary amount of data for training neural network requires expensive labor cost. To handle it, researchers turn to some rule-based structuring methods which often have lower labor cost. Traditionally, CTS tasks can be addressed by rule and dictionary based methods BIBREF0, BIBREF1, BIBREF2, task-specific end-to-end methods BIBREF3, BIBREF4, BIBREF5, BIBREF6 and pipeline methods BIBREF7, BIBREF8, BIBREF9. Rule and dictionary based methods suffer from costly human-designed extraction rules, while task-specific end-to-end methods have non-uniform output formats and require task-specific training dataset. Pipeline methods break down the entire process into several pieces which improves the performance and generality. However, when the pipeline depth grows, error propagation will have a greater impact on the performance. To reduce the pipeline depth and break the barrier of non-uniform output formats, we present a question answering based clinical text structuring (QA-CTS) task (see Fig. FIGREF1). Unlike the traditional CTS task, our QA-CTS task aims to discover the most related text from original paragraph text. For some cases, it is already the final answer in deed (e.g., extracting sub-string). While for other cases, it needs several steps to obtain the final answer, such as entity names conversion and negative words recognition. Our presented QA-CTS task unifies the output format of the traditional CTS task and make the training data shareable, thus enriching the training data. The main contributions of this work can be summarized as follows. We first present a question answering based clinical text structuring (QA-CTS) task, which unifies different specific tasks and make dataset shareable. We also propose an effective model to integrate clinical named entity information into pre-trained language model. Experimental results show that QA-CTS task leads to significant improvement due to shared dataset. Our proposed model also achieves significantly better performance than the strong baseline methods. In addition, we also show that two-stage training mechanism has a great improvement on QA-CTS task. The rest of the paper is organized as follows. We briefly review the related work on clinical text structuring in Section SECREF2. Then, we present question answer based clinical text structuring task in Section SECREF3. In Section SECREF4, we present an effective model for this task. Section SECREF5 is devoted to computational studies and several investigations on the key issues of our proposed model. Finally, conclusions are given in Section SECREF6.",Related Work ::: Clinical Text Structuring,"Clinical text structuring is a final problem which is highly related to practical applications. Most of existing studies are case-by-case. Few of them are developed for the general purpose structuring task. These studies can be roughly divided into three categories: rule and dictionary based methods, task-specific end-to-end methods and pipeline methods. Rule and dictionary based methods BIBREF0, BIBREF1, BIBREF2 rely extremely on heuristics and handcrafted extraction rules which is more of an art than a science and incurring extensive trial-and-error experiments. Fukuda et al. BIBREF0 identified protein names from biological papers by dictionaries and several features of protein names. Wang et al. BIBREF1 developed some linguistic rules (i.e. normalised/expanded term matching and substring term matching) to map specific terminology to SNOMED CT. Song et al. BIBREF2 proposed a hybrid dictionary-based bio-entity extraction technique and expands the bio-entity dictionary by combining different data sources and improves the recall rate through the shortest path edit distance algorithm. This kind of approach features its interpretability and easy modifiability. However, with the increase of the rule amount, supplementing new rules to existing system will turn to be a rule disaster. Task-specific end-to-end methods BIBREF3, BIBREF4 use large amount of data to automatically model the specific task. Topaz et al. BIBREF3 constructed an automated wound information identification model with five output. Tan et al. BIBREF4 identified patients undergoing radical cystectomy for bladder cancer. Although they achieved good performance, none of their models could be used to another task due to output format difference. This makes building a new model for a new task a costly job. Pipeline methods BIBREF7, BIBREF8, BIBREF9 break down the entire task into several basic natural language processing tasks. Bill et al. BIBREF7 focused on attributes extraction which mainly relied on dependency parsing and named entity recognition BIBREF10, BIBREF11, BIBREF12. Meanwhile, Fonferko et al. BIBREF9 used more components like noun phrase chunking BIBREF13, BIBREF14, BIBREF15, part-of-speech tagging BIBREF16, BIBREF17, BIBREF18, sentence splitter, named entity linking BIBREF19, BIBREF20, BIBREF21, relation extraction BIBREF22, BIBREF23. This kind of method focus on language itself, so it can handle tasks more general. However, as the depth of pipeline grows, it is obvious that error propagation will be more and more serious. In contrary, using less components to decrease the pipeline depth will lead to a poor performance. So the upper limit of this method depends mainly on the worst component.",Related Work ::: Pre-trained Language Model,"Recently, some works focused on pre-trained language representation models to capture language information from text and then utilizing the information to improve the performance of specific natural language processing tasks BIBREF24, BIBREF25, BIBREF26, BIBREF27 which makes language model a shared model to all natural language processing tasks. Radford et al. BIBREF24 proposed a framework for fine-tuning pre-trained language model. Peters et al. BIBREF25 proposed ELMo which concatenates forward and backward language models in a shallow manner. Devlin et al. BIBREF26 used bidirectional Transformers to model deep interactions between the two directions. Yang et al. BIBREF27 replaced the fixed forward or backward factorization order with all possible permutations of the factorization order and avoided using the [MASK] tag which causes pretrain-finetune discrepancy that BERT is subject to. The main motivation of introducing pre-trained language model is to solve the shortage of labeled data and polysemy problem. Although polysemy problem is not a common phenomenon in biomedical domain, shortage of labeled data is always a non-trivial problem. Lee et al. BIBREF28 applied BERT on large-scale biomedical unannotated data and achieved improvement on biomedical named entity recognition, relation extraction and question answering. Kim et al. BIBREF29 adapted BioBERT into multi-type named entity recognition and discovered new entities. Both of them demonstrates the usefulness of introducing pre-trained language model into biomedical domain.",Question Answering based Clinical Text Structuring,"Given a sequence of paragraph text $X=<x_1, x_2, ..., x_n>$, clinical text structuring (CTS) can be regarded to extract or generate a key-value pair where key $Q$ is typically a query term such as proximal resection margin and value $V$ is a result of query term $Q$ according to the paragraph text $X$. Generally, researchers solve CTS problem in two steps. Firstly, the answer-related text is pick out. And then several steps such as entity names conversion and negative words recognition are deployed to generate the final answer. While final answer varies from task to task, which truly causes non-uniform output formats, finding the answer-related text is a common action among all tasks. Traditional methods regard both the steps as a whole. In this paper, we focus on finding the answer-related substring $Xs = <X_i, X_i+1, X_i+2, ... X_j> (1 <= i < j <= n)$ from paragraph text $X$. For example, given sentence UTF8gkai“远端胃切除标本：小弯长11.5cm，大弯长17.0cm。距上切端6.0cm、下切端8.0cm"" (Distal gastrectomy specimen: measuring 11.5cm in length along the lesser curvature, 17.0cm in length along the greater curvature; 6.0cm from the proximal resection margin, and 8.0cm from the distal resection margin) and query UTF8gkai“上切缘距离""(proximal resection margin), the answer should be 6.0cm which is located in original text from index 32 to 37. With such definition, it unifies the output format of CTS tasks and therefore make the training data shareable, in order to reduce the training data quantity requirement. Since BERT BIBREF26 has already demonstrated the usefulness of shared model, we suppose extracting commonality of this problem and unifying the output format will make the model more powerful than dedicated model and meanwhile, for a specific clinical task, use the data for other tasks to supplement the training data.",The Proposed Model for QA-CTS Task,"In this section, we present an effective model for the question answering based clinical text structuring (QA-CTS). As shown in Fig. FIGREF8, paragraph text $X$ is first passed to a clinical named entity recognition (CNER) model BIBREF12 to capture named entity information and obtain one-hot CNER output tagging sequence for query text $I_{nq}$ and paragraph text $I_{nt}$ with BIEOS (Begin, Inside, End, Outside, Single) tag scheme. $I_{nq}$ and $I_{nt}$ are then integrated together into $I_n$. Meanwhile, the paragraph text $X$ and query text $Q$ are organized and passed to contextualized representation model which is pre-trained language model BERT BIBREF26 here to obtain the contextualized representation vector $V_s$ of both text and query. Afterwards, $V_s$ and $I_n$ are integrated together and fed into a feed forward network to calculate the start and end index of answer-related text. Here we define this calculation problem as a classification for each word to be the start or end word.",The Proposed Model for QA-CTS Task ::: Contextualized Representation of Sentence Text and Query Text,"For any clinical free-text paragraph $X$ and query $Q$, contextualized representation is to generate the encoded vector of both of them. Here we use pre-trained language model BERT-base BIBREF26 model to capture contextual information. The text input is constructed as `[CLS] $Q$ [SEP] $X$ [SEP]'. For Chinese sentence, each word in this input will be mapped to a pre-trained embedding $e_i$. To tell the model $Q$ and $X$ is two different sentence, a sentence type input is generated which is a binary label sequence to denote what sentence each character in the input belongs to. Positional encoding and mask matrix is also constructed automatically to bring in absolute position information and eliminate the impact of zero padding respectively. Then a hidden vector $V_s$ which contains both query and text information is generated through BERT-base model.",The Proposed Model for QA-CTS Task ::: Clinical Named Entity Information,"Since BERT is trained on general corpus, its performance on biomedical domain can be improved by introducing biomedical domain-specific features. In this paper, we introduce clinical named entity information into the model. The CNER task aims to identify and classify important clinical terms such as diseases, symptoms, treatments, exams, and body parts from Chinese EHRs. It can be regarded as a sequence labeling task. A CNER model typically outputs a sequence of tags. Each character of the original sentence will be tagged a label following a tag scheme. In this paper we recognize the entities by the model of our previous work BIBREF12 but trained on another corpus which has 44 entity types including operations, numbers, unit words, examinations, symptoms, negative words, etc. An illustrative example of named entity information sequence is demonstrated in Table TABREF2. In Table TABREF2, UTF8gkai“远端胃切除"" is tagged as an operation, `11.5' is a number word and `cm' is an unit word. The named entity tag sequence is organized in one-hot type. We denote the sequence for clinical sentence and query term as $I_{nt}$ and $I_{nq}$, respectively.",The Proposed Model for QA-CTS Task ::: Integration Method,"There are two ways to integrate two named entity information vectors $I_{nt}$ and $I_{nq}$ or hidden contextualized representation $V_s$ and named entity information $I_n$, where $I_n = [I_{nt}; I_{nq}]$. The first one is to concatenate them together because they have sequence output with a common dimension. The second one is to transform them into a new hidden representation. For the concatenation method, the integrated representation is described as follows. While for the transformation method, we use multi-head attention BIBREF30 to encode the two vectors. It can be defined as follows where $h$ is the number of heads and $W_o$ is used to projects back the dimension of concatenated matrix. $Attention$ denotes the traditional attention and it can be defined as follows. where $d_k$ is the length of hidden vector.",The Proposed Model for QA-CTS Task ::: Final Prediction,"The final step is to use integrated representation $H_i$ to predict the start and end index of answer-related text. Here we define this calculation problem as a classification for each word to be the start or end word. We use a feed forward network (FFN) to compress and calculate the score of each word $H_f$ which makes the dimension to $\left\langle l_s, 2\right\rangle $ where $l_s$ denotes the length of sequence. Then we permute the two dimensions for softmax calculation. The calculation process of loss function can be defined as followed. where $O_s = softmax(permute(H_f)_0)$ denotes the probability score of each word to be the start word and similarly $O_e = softmax(permute(H_f)_1)$ denotes the end. $y_s$ and $y_e$ denotes the true answer of the output for start word and end word respectively.",The Proposed Model for QA-CTS Task ::: Two-Stage Training Mechanism,"Two-stage training mechanism is previously applied on bilinear model in fine-grained visual recognition BIBREF31, BIBREF32, BIBREF33. Two CNNs are deployed in the model. One is trained at first for coarse-graind features while freezing the parameter of the other. Then unfreeze the other one and train the entire model in a low learning rate for fetching fine-grained features. Inspired by this and due to the large amount of parameters in BERT model, to speed up the training process, we fine tune the BERT model with new prediction layer first to achieve a better contextualized representation performance. Then we deploy the proposed model and load the fine tuned BERT weights, attach named entity information layers and retrain the model.",Experimental Studies,"In this section, we devote to experimentally evaluating our proposed task and approach. The best results in tables are in bold.",Experimental Studies ::: Dataset and Evaluation Metrics,"Our dataset is annotated based on Chinese pathology reports provided by the Department of Gastrointestinal Surgery, Ruijin Hospital. It contains 17,833 sentences, 826,987 characters and 2,714 question-answer pairs. All question-answer pairs are annotated and reviewed by four clinicians with three types of questions, namely tumor size, proximal resection margin and distal resection margin. These annotated instances have been partitioned into 1,899 training instances (12,412 sentences) and 815 test instances (5,421 sentences). Each instance has one or several sentences. Detailed statistics of different types of entities are listed in Table TABREF20. In the following experiments, two widely-used performance measures (i.e., EM-score BIBREF34 and (macro-averaged) F$_1$-score BIBREF35) are used to evaluate the methods. The Exact Match (EM-score) metric measures the percentage of predictions that match any one of the ground truth answers exactly. The F$_1$-score metric is a looser metric measures the average overlap between the prediction and ground truth answer.",Experimental Studies ::: Experimental Settings,"To implement deep neural network models, we utilize the Keras library BIBREF36 with TensorFlow BIBREF37 backend. Each model is run on a single NVIDIA GeForce GTX 1080 Ti GPU. The models are trained by Adam optimization algorithm BIBREF38 whose parameters are the same as the default settings except for learning rate set to $5\times 10^{-5}$. Batch size is set to 3 or 4 due to the lack of graphical memory. We select BERT-base as the pre-trained language model in this paper. Due to the high cost of pre-training BERT language model, we directly adopt parameters pre-trained by Google in Chinese general corpus. The named entity recognition is applied on both pathology report texts and query texts.",Experimental Studies ::: Comparison with State-of-the-art Methods,"Since BERT has already achieved the state-of-the-art performance of question-answering, in this section we compare our proposed model with state-of-the-art question answering models (i.e. QANet BIBREF39) and BERT-Base BIBREF26. As BERT has two versions: BERT-Base and BERT-Large, due to the lack of computational resource, we can only compare with BERT-Base model instead of BERT-Large. Prediction layer is attached at the end of the original BERT-Base model and we fine tune it on our dataset. In this section, the named entity integration method is chosen to pure concatenation (Concatenate the named entity information on pathology report text and query text first and then concatenate contextualized representation and concatenated named entity information). Comparative results are summarized in Table TABREF23. Table TABREF23 indicates that our proposed model achieved the best performance both in EM-score and F$_1$-score with EM-score of 91.84% and F$_1$-score of 93.75%. QANet outperformed BERT-Base with 3.56% score in F$_1$-score but underperformed it with 0.75% score in EM-score. Compared with BERT-Base, our model led to a 5.64% performance improvement in EM-score and 3.69% in F$_1$-score. Although our model didn't outperform much with QANet in F$_1$-score (only 0.13%), our model significantly outperformed it with 6.39% score in EM-score.",Experimental Studies ::: Ablation Analysis,"To further investigate the effects of named entity information and two-stage training mechanism for our model, we apply ablation analysis to see the improvement brought by each of them, where $\times $ refers to removing that part from our model. As demonstrated in Table TABREF25, with named entity information enabled, two-stage training mechanism improved the result by 4.36% in EM-score and 3.8% in F$_1$-score. Without two-stage training mechanism, named entity information led to an improvement by 1.28% in EM-score but it also led to a weak deterioration by 0.12% in F$_1$-score. With both of them enabled, our proposed model achieved a 5.64% score improvement in EM-score and a 3.69% score improvement in F$_1$-score. The experimental results show that both named entity information and two-stage training mechanism are helpful to our model.",Experimental Studies ::: Comparisons Between Two Integration Methods,"There are two methods to integrate named entity information into existing model, we experimentally compare these two integration methods. As named entity recognition has been applied on both pathology report text and query text, there will be two integration here. One is for two named entity information and the other is for contextualized representation and integrated named entity information. For multi-head attention BIBREF30, we set heads number $h = 16$ with 256-dimension hidden vector size for each head. From Table TABREF27, we can observe that applying concatenation on both periods achieved the best performance on both EM-score and F$_1$-score. Unfortunately, applying multi-head attention on both period one and period two can not reach convergence in our experiments. This probably because it makes the model too complex to train. The difference on other two methods are the order of concatenation and multi-head attention. Applying multi-head attention on two named entity information $I_{nt}$ and $I_{nq}$ first achieved a better performance with 89.87% in EM-score and 92.88% in F$_1$-score. Applying Concatenation first can only achieve 80.74% in EM-score and 84.42% in F$_1$-score. This is probably due to the processing depth of hidden vectors and dataset size. BERT's output has been modified after many layers but named entity information representation is very close to input. With big amount of parameters in multi-head attention, it requires massive training to find out the optimal parameters. However, our dataset is significantly smaller than what pre-trained BERT uses. This probably can also explain why applying multi-head attention method on both periods can not converge. Although Table TABREF27 shows the best integration method is concatenation, multi-head attention still has great potential. Due to the lack of computational resources, our experiment fixed the head number and hidden vector size. However, tuning these hyper parameters may have impact on the result. Tuning integration method and try to utilize larger datasets may give help to improving the performance.",Experimental Studies ::: Data Integration Analysis,"To investigate how shared task and shared model can benefit, we split our dataset by query types, train our proposed model with different datasets and demonstrate their performance on different datasets. Firstly, we investigate the performance on model without two-stage training and named entity information. As indicated in Table TABREF30, The model trained by mixed data outperforms 2 of the 3 original tasks in EM-score with 81.55% for proximal resection margin and 86.85% for distal resection margin. The performance on tumor size declined by 1.57% score in EM-score and 3.14% score in F$_1$-score but they were still above 90%. 0.69% and 0.37% score improvement in EM-score was brought by shared model for proximal and distal resection margin prediction. Meanwhile F$_1$-score for those two tasks declined 3.11% and 0.77% score. Then we investigate the performance on model with two-stage training and named entity information. In this experiment, pre-training process only use the specific dataset not the mixed data. From Table TABREF31 we can observe that the performance on proximal and distal resection margin achieved the best performance on both EM-score and F$_1$-score. Compared with Table TABREF30, the best performance on proximal resection margin improved by 6.9% in EM-score and 7.94% in F$_1$-score. Meanwhile, the best performance on distal resection margin improved by 5.56% in EM-score and 6.32% in F$_1$-score. Other performances also usually improved a lot. This proves the usefulness of two-stage training and named entity information as well. Lastly, we fine tune the model for each task with a pre-trained parameter. Table TABREF32 summarizes the result. (Add some explanations for the Table TABREF32). Comparing Table TABREF32 with Table TABREF31, using mixed-data pre-trained parameters can significantly improve the model performance than task-specific data trained model. Except tumor size, the result was improved by 0.52% score in EM-score, 1.39% score in F$_1$-score for proximal resection margin and 2.6% score in EM-score, 2.96% score in F$_1$-score for distal resection margin. This proves mixed-data pre-trained parameters can lead to a great benefit for specific task. Meanwhile, the model performance on other tasks which are not trained in the final stage was also improved from around 0 to 60 or 70 percent. This proves that there is commonality between different tasks and our proposed QA-CTS task make this learnable. In conclusion, to achieve the best performance for a specific dataset, pre-training the model in multiple datasets and then fine tuning the model on the specific dataset is the best way.",Conclusion,"In this paper, we present a question answering based clinical text structuring (QA-CTS) task, which unifies different clinical text structuring tasks and utilize different datasets. A novel model is also proposed to integrate named entity information into a pre-trained language model and adapt it to QA-CTS task. Initially, sequential results of named entity recognition on both paragraph and query texts are integrated together. Contextualized representation on both paragraph and query texts are transformed by a pre-trained language model. Then, the integrated named entity information and contextualized representation are integrated together and fed into a feed forward network for final prediction. Experimental results on real-world dataset demonstrate that our proposed model competes favorably with strong baseline models in all three specific tasks. The shared task and shared model introduced by QA-CTS task has also been proved to be useful for improving the performance on most of the task-specific datasets. In conclusion, the best way to achieve the best performance for a specific dataset is to pre-train the model in multiple datasets and then fine tune it on the specific dataset.",Acknowledgment,"We would like to thank Ting Li and Xizhou Hong (Ruijin Hospital) who have helped us very much in data fetching and data cleansing. This work is supported by the National Key R&D Program of China for “Precision Medical Research"" (No. 2018YFC0910500).",,,,,What data is the language model pretrained on?,71a7153e12879defa186bfb6dbafe79c74265e10,infinity,research,no,question answering,ecca0cede84b7af8a918852311d36346b07f0668,False,,,"To implement deep neural network models, we utilize the Keras library BIBREF36 with TensorFlow BIBREF37 backend. Each model is run on a single NVIDIA GeForce GTX 1080 Ti GPU. The models are trained by Adam optimization algorithm BIBREF38 whose parameters are the same as the default settings except for learning rate set to $5\times 10^{-5}$. Batch size is set to 3 or 4 due to the lack of graphical memory. We select BERT-base as the pre-trained language model in this paper. Due to the high cost of pre-training BERT language model, we directly adopt parameters pre-trained by Google in Chinese general corpus. The named entity recognition is applied on both pathology report texts and query texts.","Due to the high cost of pre-training BERT language model, we directly adopt parameters pre-trained by Google in Chinese general corpus. The named entity recognition is applied on both pathology report texts and query texts.",0ab604dbe114dba174da645cc06a713e12a1fd9d,a0b403873302db7cada39008f04d01155ef68f4f,True,,,,,,1f1495d06d0abe86ee52124ec9f2f0b25a536147,258ee4069f740c400c0049a2580945a1cc7f044c,What baselines is the proposed model compared against?,85d1831c28d3c19c84472589a252e28e9884500f,infinity,research,no,question answering,ecca0cede84b7af8a918852311d36346b07f0668,False,,,"Experimental Studies ::: Comparison with State-of-the-art Methods Since BERT has already achieved the state-of-the-art performance of question-answering, in this section we compare our proposed model with state-of-the-art question answering models (i.e. QANet BIBREF39) and BERT-Base BIBREF26. As BERT has two versions: BERT-Base and BERT-Large, due to the lack of computational resource, we can only compare with BERT-Base model instead of BERT-Large. Prediction layer is attached at the end of the original BERT-Base model and we fine tune it on our dataset. In this section, the named entity integration method is chosen to pure concatenation (Concatenate the named entity information on pathology report text and query text first and then concatenate contextualized representation and concatenated named entity information). Comparative results are summarized in Table TABREF23.","Experimental Studies ::: Comparison with State-of-the-art Methods
Since BERT has already achieved the state-of-the-art performance of question-answering, in this section we compare our proposed model with state-of-the-art question answering models (i.e. QANet BIBREF39) and BERT-Base BIBREF26. As BERT has two versions: BERT-Base and BERT-Large, due to the lack of computational resource, we can only compare with BERT-Base model instead of BERT-Large.",0de2087bf0e46b14042de2a6e707bbf544a04556,258ee4069f740c400c0049a2580945a1cc7f044c,How is the clinical text structuring task defined?,1959e0ebc21fafdf1dd20c6ea054161ba7446f61,infinity,research,no,question answering,ecca0cede84b7af8a918852311d36346b07f0668,False,,,"FLOAT SELECTED: Fig. 1. An illustrative example of QA-CTS task. Clinical text structuring (CTS) is a critical task for fetching medical research data from electronic health records (EHRs), where structural patient medical data, such as whether the patient has specific symptoms, diseases, or what the tumor size is, how far from the tumor is cut at during the surgery, or what the specific laboratory test result is, are obtained. It is important to extract structured data from clinical text because bio-medical systems or bio-medical researches greatly rely on structured data but they cannot obtain them directly. In addition, clinical text often contains abundant healthcare information. CTS is able to provide large-scale extracted structured data for enormous down-stream clinical researches. To reduce the pipeline depth and break the barrier of non-uniform output formats, we present a question answering based clinical text structuring (QA-CTS) task (see Fig. FIGREF1). Unlike the traditional CTS task, our QA-CTS task aims to discover the most related text from original paragraph text. For some cases, it is already the final answer in deed (e.g., extracting sub-string). While for other cases, it needs several steps to obtain the final answer, such as entity names conversion and negative words recognition. Our presented QA-CTS task unifies the output format of the traditional CTS task and make the training data shareable, thus enriching the training data. The main contributions of this work can be summarized as follows.","FLOAT SELECTED: Fig. 1. An illustrative example of QA-CTS task. Clinical text structuring (CTS) is a critical task for fetching medical research data from electronic health records (EHRs), where structural patient medical data, such as whether the patient has specific symptoms, diseases, or what the tumor size is, how far from the tumor is cut at during the surgery, or what the specific laboratory test result is, are obtained. It is important to extract structured data from clinical text because bio-medical systems or bio-medical researches greatly rely on structured data but they cannot obtain them directly. In addition, clinical text often contains abundant healthcare information. CTS is able to provide large-scale extracted structured data for enormous down-stream clinical researches. Unlike the traditional CTS task, our QA-CTS task aims to discover the most related text from original paragraph text. For some cases, it is already the final answer in deed (e.g., extracting sub-string). While for other cases, it needs several steps to obtain the final answer, such as entity names conversion and negative words recognition. Our presented QA-CTS task unifies the output format of the traditional CTS task and make the training data shareable, thus enriching the training data. ",6d56080358bb7f22dd764934ffcd6d4e93fef0b2,a0b403873302db7cada39008f04d01155ef68f4f,False,,CTS is extracting structural data from medical research data (unstructured). Authors define QA-CTS task that aims to discover most related text from original text.,"Clinical text structuring (CTS) is a critical task for fetching medical research data from electronic health records (EHRs), where structural patient medical data, such as whether the patient has specific symptoms, diseases, or what the tumor size is, how far from the tumor is cut at during the surgery, or what the specific laboratory test result is, are obtained. It is important to extract structured data from clinical text because bio-medical systems or bio-medical researches greatly rely on structured data but they cannot obtain them directly. In addition, clinical text often contains abundant healthcare information. CTS is able to provide large-scale extracted structured data for enormous down-stream clinical researches. However, end-to-end CTS is a very challenging task. Different CTS tasks often have non-uniform output formats, such as specific-class classifications (e.g. tumor stage), strings in the original text (e.g. result for a laboratory test) and inferred values from part of the original text (e.g. calculated tumor size). Researchers have to construct different models for it, which is already costly, and hence it calls for a lot of labeled data for each model. Moreover, labeling necessary amount of data for training neural network requires expensive labor cost. To handle it, researchers turn to some rule-based structuring methods which often have lower labor cost. To reduce the pipeline depth and break the barrier of non-uniform output formats, we present a question answering based clinical text structuring (QA-CTS) task (see Fig. FIGREF1). Unlike the traditional CTS task, our QA-CTS task aims to discover the most related text from original paragraph text. For some cases, it is already the final answer in deed (e.g., extracting sub-string). While for other cases, it needs several steps to obtain the final answer, such as entity names conversion and negative words recognition. Our presented QA-CTS task unifies the output format of the traditional CTS task and make the training data shareable, thus enriching the training data. The main contributions of this work can be summarized as follows.","Clinical text structuring (CTS) is a critical task for fetching medical research data from electronic health records (EHRs), where structural patient medical data, such as whether the patient has specific symptoms, diseases, or what the tumor size is, how far from the tumor is cut at during the surgery, or what the specific laboratory test result is, are obtained. It is important to extract structured data from clinical text because bio-medical systems or bio-medical researches greatly rely on structured data but they cannot obtain them directly. However, end-to-end CTS is a very challenging task. Different CTS tasks often have non-uniform output formats, such as specific-class classifications (e.g. tumor stage), strings in the original text (e.g. result for a laboratory test) and inferred values from part of the original text (e.g. calculated tumor size). To reduce the pipeline depth and break the barrier of non-uniform output formats, we present a question answering based clinical text structuring (QA-CTS) task (see Fig. FIGREF1). Unlike the traditional CTS task, our QA-CTS task aims to discover the most related text from original paragraph text.",da233cce57e642941da2446d3e053349c2ab1a15,258ee4069f740c400c0049a2580945a1cc7f044c,What are the specific tasks being unified?,77cf4379106463b6ebcb5eb8fa5bb25450fa5fb8,infinity,research,no,question answering,ecca0cede84b7af8a918852311d36346b07f0668,False,,,"To reduce the pipeline depth and break the barrier of non-uniform output formats, we present a question answering based clinical text structuring (QA-CTS) task (see Fig. FIGREF1). Unlike the traditional CTS task, our QA-CTS task aims to discover the most related text from original paragraph text. For some cases, it is already the final answer in deed (e.g., extracting sub-string). While for other cases, it needs several steps to obtain the final answer, such as entity names conversion and negative words recognition. Our presented QA-CTS task unifies the output format of the traditional CTS task and make the training data shareable, thus enriching the training data. The main contributions of this work can be summarized as follows. Our dataset is annotated based on Chinese pathology reports provided by the Department of Gastrointestinal Surgery, Ruijin Hospital. It contains 17,833 sentences, 826,987 characters and 2,714 question-answer pairs. All question-answer pairs are annotated and reviewed by four clinicians with three types of questions, namely tumor size, proximal resection margin and distal resection margin. These annotated instances have been partitioned into 1,899 training instances (12,412 sentences) and 815 test instances (5,421 sentences). Each instance has one or several sentences. Detailed statistics of different types of entities are listed in Table TABREF20. In this paper, we present a question answering based clinical text structuring (QA-CTS) task, which unifies different clinical text structuring tasks and utilize different datasets. A novel model is also proposed to integrate named entity information into a pre-trained language model and adapt it to QA-CTS task. Initially, sequential results of named entity recognition on both paragraph and query texts are integrated together. Contextualized representation on both paragraph and query texts are transformed by a pre-trained language model. Then, the integrated named entity information and contextualized representation are integrated together and fed into a feed forward network for final prediction. Experimental results on real-world dataset demonstrate that our proposed model competes favorably with strong baseline models in all three specific tasks. The shared task and shared model introduced by QA-CTS task has also been proved to be useful for improving the performance on most of the task-specific datasets. In conclusion, the best way to achieve the best performance for a specific dataset is to pre-train the model in multiple datasets and then fine tune it on the specific dataset.","Our presented QA-CTS task unifies the output format of the traditional CTS task and make the training data shareable, thus enriching the training data. All question-answer pairs are annotated and reviewed by four clinicians with three types of questions, namely tumor size, proximal resection margin and distal resection margin.  Experimental results on real-world dataset demonstrate that our proposed model competes favorably with strong baseline models in all three specific tasks.",7138d812ea70084e7610e5a2422039da1404afd7,a0b403873302db7cada39008f04d01155ef68f4f,True,,,,,b732d5561babcf37393ebf6cbb051d04b0b66bd5,258ee4069f740c400c0049a2580945a1cc7f044c,"Is all text in this dataset a question, or are there unrelated sentences in between questions?",06095a4dee77e9a570837b35fc38e77228664f91,five,familiar,no,Question Answering,2a18a3656984d04249f100633e4c1003417a2255,False,,,the dataset consists of pathology reports including sentences and questions and answers about tumor size and resection margins so it does include additional sentences ,"Our dataset is annotated based on Chinese pathology reports provided by the Department of Gastrointestinal Surgery, Ruijin Hospital. It contains 17,833 sentences, 826,987 characters and 2,714 question-answer pairs. All question-answer pairs are annotated and reviewed by four clinicians with three types of questions, namely tumor size, proximal resection margin and distal resection margin. These annotated instances have been partitioned into 1,899 training instances (12,412 sentences) and 815 test instances (5,421 sentences). Each instance has one or several sentences. Detailed statistics of different types of entities are listed in Table TABREF20.","Our dataset is annotated based on Chinese pathology reports provided by the Department of Gastrointestinal Surgery, Ruijin Hospital. It contains 17,833 sentences, 826,987 characters and 2,714 question-answer pairs. All question-answer pairs are annotated and reviewed by four clinicians with three types of questions, namely tumor size, proximal resection margin and distal resection margin. ",1d4d4965fd44fefbfed0b3267ef5875572994b66,fa716cd87ce6fd6905e2f23f09b262e90413167f,How many questions are in the dataset?,19c9cfbc4f29104200393e848b7b9be41913a7ac,five,familiar,no,Question Answering,2a18a3656984d04249f100633e4c1003417a2255,False,,"2,714 ","Our dataset is annotated based on Chinese pathology reports provided by the Department of Gastrointestinal Surgery, Ruijin Hospital. It contains 17,833 sentences, 826,987 characters and 2,714 question-answer pairs. All question-answer pairs are annotated and reviewed by four clinicians with three types of questions, namely tumor size, proximal resection margin and distal resection margin. These annotated instances have been partitioned into 1,899 training instances (12,412 sentences) and 815 test instances (5,421 sentences). Each instance has one or several sentences. Detailed statistics of different types of entities are listed in Table TABREF20.","Our dataset is annotated based on Chinese pathology reports provided by the Department of Gastrointestinal Surgery, Ruijin Hospital. It contains 17,833 sentences, 826,987 characters and 2,714 question-answer pairs.",229cc59d1545c9e8f47d43053465e2dfd1b763cc,fa716cd87ce6fd6905e2f23f09b262e90413167f,What is the perWhat are the tasks evaluated?,6743c1dd7764fc652cfe2ea29097ea09b5544bc3,five,familiar,no,Question Answering,2a18a3656984d04249f100633e4c1003417a2255,True,,,,,e2fe2a3438f28758724d992502a44615051eda90,fa716cd87ce6fd6905e2f23f09b262e90413167f,Are there privacy concerns with clinical data?,14323046220b2aea8f15fba86819cbccc389ed8b,five,familiar,no,Question Answering,2a18a3656984d04249f100633e4c1003417a2255,True,,,,,2a73264b743b6dd183c200f7dcd04aed4029f015,fa716cd87ce6fd6905e2f23f09b262e90413167f,How they introduce domain-specific features into pre-trained language model?,08a5f8d36298b57f6a4fcb4b6ae5796dc5d944a4,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,integrate clinical named entity information into pre-trained language model,,,"We first present a question answering based clinical text structuring (QA-CTS) task, which unifies different specific tasks and make dataset shareable. We also propose an effective model to integrate clinical named entity information into pre-trained language model. In this section, we present an effective model for the question answering based clinical text structuring (QA-CTS). As shown in Fig. FIGREF8, paragraph text $X$ is first passed to a clinical named entity recognition (CNER) model BIBREF12 to capture named entity information and obtain one-hot CNER output tagging sequence for query text $I_{nq}$ and paragraph text $I_{nt}$ with BIEOS (Begin, Inside, End, Outside, Single) tag scheme. $I_{nq}$ and $I_{nt}$ are then integrated together into $I_n$. Meanwhile, the paragraph text $X$ and query text $Q$ are organized and passed to contextualized representation model which is pre-trained language model BERT BIBREF26 here to obtain the contextualized representation vector $V_s$ of both text and query. Afterwards, $V_s$ and $I_n$ are integrated together and fed into a feed forward network to calculate the start and end index of answer-related text. Here we define this calculation problem as a classification for each word to be the start or end word.","We also propose an effective model to integrate clinical named entity information into pre-trained language model. In this section, we present an effective model for the question answering based clinical text structuring (QA-CTS). As shown in Fig. FIGREF8, paragraph text $X$ is first passed to a clinical named entity recognition (CNER) model BIBREF12 to capture named entity information and obtain one-hot CNER output tagging sequence for query text $I_{nq}$ and paragraph text $I_{nt}$ with BIEOS (Begin, Inside, End, Outside, Single) tag scheme. $I_{nq}$ and $I_{nt}$ are then integrated together into $I_n$. Meanwhile, the paragraph text $X$ and query text $Q$ are organized and passed to contextualized representation model which is pre-trained language model BERT BIBREF26 here to obtain the contextualized representation vector $V_s$ of both text and query. Afterwards, $V_s$ and $I_n$ are integrated together and fed into a feed forward network to calculate the start and end index of answer-related text.",5f125408e657282669f90a1866d8227c0f94332e,e70d8110563d53282f1a26e823d27e6f235772db,1-Figure1-1.png,Fig. 1. An illustrative example of QA-CTS task.,2-TableI-1.png,TABLE I AN ILLUSTRATIVE EXAMPLE OF NAMED ENTITY FEATURE TAGS,3-Figure2-1.png,Fig. 2. The architecture of our proposed model for QA-CTS task,4-TableII-1.png,TABLE II STATISTICS OF DIFFERENT TYPES OF QUESTION ANSWER INSTANCES,5-TableV-1.png,TABLE V COMPARATIVE RESULTS FOR DIFFERENT INTEGRATION METHOD OF OUR PROPOSED MODEL,5-TableIII-1.png,TABLE III COMPARATIVE RESULTS BETWEEN BERT AND OUR PROPOSED MODEL,,,,,,,,,BERT-Base QANet,"Clinical text structuring (CTS) is a critical task for fetching medical research data from electronic health records (EHRs), where structural patient medical data, such as whether the patient has specific symptoms, diseases, or what the tumor size is, how far from the tumor is cut at during the surgery, or what the specific laboratory test result is, are obtained. Unlike the traditional CTS task, our QA-CTS task aims to discover the most related text from original paragraph text. ",6-TableVI-1.png,TABLE VI COMPARATIVE RESULTS FOR DATA INTEGRATION ANALYSIS (WITHOUT TWO-STAGE TRAINING AND NAMED ENTITY INFORMATION),6-TableVII-1.png,TABLE VII COMPARATIVE RESULTS FOR DATA INTEGRATION ANALYSIS (WITH TWO-STAGE TRAINING AND NAMED ENTITY INFORMATION),6-TableVIII-1.png,TABLE VIII COMPARATIVE RESULTS FOR DATA INTEGRATION ANALYSIS (USING MIXED-DATA PRE-TRAINED PARAMETERS),,,,,,Chinese general corpus,False,QANet BIBREF39 BERT-Base BIBREF26,,,"Since BERT has already achieved the state-of-the-art performance of question-answering, in this section we compare our proposed model with state-of-the-art question answering models (i.e. QANet BIBREF39) and BERT-Base BIBREF26. As BERT has two versions: BERT-Base and BERT-Large, due to the lack of computational resource, we can only compare with BERT-Base model instead of BERT-Large. Prediction layer is attached at the end of the original BERT-Base model and we fine tune it on our dataset. In this section, the named entity integration method is chosen to pure concatenation (Concatenate the named entity information on pathology report text and query text first and then concatenate contextualized representation and concatenated named entity information). Comparative results are summarized in Table TABREF23. FLOAT SELECTED: TABLE III COMPARATIVE RESULTS BETWEEN BERT AND OUR PROPOSED MODEL","Since BERT has already achieved the state-of-the-art performance of question-answering, in this section we compare our proposed model with state-of-the-art question answering models (i.e. QANet BIBREF39) and BERT-Base BIBREF26. As BERT has two versions: BERT-Base and BERT-Large, due to the lack of computational resource, we can only compare with BERT-Base model instead of BERT-Large.  FLOAT SELECTED: TABLE III COMPARATIVE RESULTS BETWEEN BERT AND OUR PROPOSED MODEL",c14d9acff1d3e6f47901e7104a7f01a10a727050,a0b403873302db7cada39008f04d01155ef68f4f," three types of questions, namely tumor size, proximal resection margin and distal resection margin",How big is QA-CTS task dataset?,975a4ac9773a4af551142c324b64a0858670d06e,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,"17,833 sentences, 826,987 characters and 2,714 question-answer pairs",,,"Our dataset is annotated based on Chinese pathology reports provided by the Department of Gastrointestinal Surgery, Ruijin Hospital. It contains 17,833 sentences, 826,987 characters and 2,714 question-answer pairs. All question-answer pairs are annotated and reviewed by four clinicians with three types of questions, namely tumor size, proximal resection margin and distal resection margin. These annotated instances have been partitioned into 1,899 training instances (12,412 sentences) and 815 test instances (5,421 sentences). Each instance has one or several sentences. Detailed statistics of different types of entities are listed in Table TABREF20.","Our dataset is annotated based on Chinese pathology reports provided by the Department of Gastrointestinal Surgery, Ruijin Hospital. It contains 17,833 sentences, 826,987 characters and 2,714 question-answer pairs. ",24c7023a5221b509d34dd6703d6e0607b2777e78,e70d8110563d53282f1a26e823d27e6f235772db,How big is dataset of pathology reports collected from Ruijing Hospital?,326e08a0f5753b90622902bd4a9c94849a24b773,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,"17,833 sentences, 826,987 characters and 2,714 question-answer pairs",,,"Our dataset is annotated based on Chinese pathology reports provided by the Department of Gastrointestinal Surgery, Ruijin Hospital. It contains 17,833 sentences, 826,987 characters and 2,714 question-answer pairs. All question-answer pairs are annotated and reviewed by four clinicians with three types of questions, namely tumor size, proximal resection margin and distal resection margin. These annotated instances have been partitioned into 1,899 training instances (12,412 sentences) and 815 test instances (5,421 sentences). Each instance has one or several sentences. Detailed statistics of different types of entities are listed in Table TABREF20.","Our dataset is annotated based on Chinese pathology reports provided by the Department of Gastrointestinal Surgery, Ruijin Hospital. It contains 17,833 sentences, 826,987 characters and 2,714 question-answer pairs.",d046d9ea83c5ffe607465e2fbc8817131c11e037,e70d8110563d53282f1a26e823d27e6f235772db,What are strong baseline models in specific tasks?,bd78483a746fda4805a7678286f82d9621bc45cf,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,state-of-the-art question answering models (i.e. QANet BIBREF39) and BERT-Base BIBREF26,,,"Since BERT has already achieved the state-of-the-art performance of question-answering, in this section we compare our proposed model with state-of-the-art question answering models (i.e. QANet BIBREF39) and BERT-Base BIBREF26. As BERT has two versions: BERT-Base and BERT-Large, due to the lack of computational resource, we can only compare with BERT-Base model instead of BERT-Large. Prediction layer is attached at the end of the original BERT-Base model and we fine tune it on our dataset. In this section, the named entity integration method is chosen to pure concatenation (Concatenate the named entity information on pathology report text and query text first and then concatenate contextualized representation and concatenated named entity information). Comparative results are summarized in Table TABREF23.","Since BERT has already achieved the state-of-the-art performance of question-answering, in this section we compare our proposed model with state-of-the-art question answering models (i.e. QANet BIBREF39) and BERT-Base BIBREF26. As BERT has two versions: BERT-Base and BERT-Large, due to the lack of computational resource, we can only compare with BERT-Base model instead of BERT-Large.",b3a3d6e707a67bab827053b40e446f30e416887f,e70d8110563d53282f1a26e823d27e6f235772db,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Markov Chain Monte-Carlo Phylogenetic Inference Construction in Computational Historical Linguistics,"More and more languages in the world are under study nowadays, as a result, the traditional way of historical linguistics study is facing some challenges. For example, the linguistic comparative research among languages needs manual annotation, which becomes more and more impossible with the increasing amount of language data coming out all around the world. Although it could hardly replace linguists work, the automatic computational methods have been taken into consideration and it can help people reduce their workload. One of the most important work in historical linguistics is word comparison from different languages and find the cognate words for them, which means people try to figure out if the two languages are related to each other or not. In this paper, I am going to use computational method to cluster the languages and use Markov Chain Monte Carlo (MCMC) method to build the language typology relationship tree based on the clusters.",Introduction,"Bayesian inference of phylogeny has great impact on evolutionary biology. It is believed that all the species are related through a history of a common descent BIBREF0, that is to say, the reason we have various wildlife, including human beings, is because of evolution. We can show the process of evolution and solve the problems, like what is the phylogeny of life, by showing a phylogenetic tree (see Figure FIGREF1). As a matter of fact, any research of DNA sequences or protein pattern taken from the species or an individual wildlife can start with a phylogenetic analysis. Language change, which is regarded as just like life evolution, can similarly borrow the phylogenetic analysis to discover the process in which how languages change. It is accepted that all human languages start from the proto-language which is the ancestor of modern languages. For example, Germanic languages (like English, German, Dutch), Romance languages (like Spanish, French and Portuguese), Slavic languages (like Russian, Bosnian and Polish) are from Indo-European languages, which was developed from proto-Indo-European languages. Therefore, we can borrow the computational method from biology and evolution to apply to the language topology. Not only Natural Language Processing (NLP), more and more linguistic branch disciplines begin to have digital datasets and use computational methods to solve some problems. Historical linguistics recently have dramatically increasing digital datasets. The availability of new data and researches of different language from different language families start to challenge the traditional way to carry on the study of historical linguistics. The comparative method has been the core method for linguistic reconstruction for the past 200 years, and is based on manually identifying systematic phonetic correspondences between many words in pair of languages BIBREF2, because different culture has different writing system and different way to spell their words. That is why International Phonetic Alphabet was created to help linguists analyze different languages in parallel. However, there are too few scholars, i.e., historical linguists, to analyze the world's over 7500 type of languages BIBREF2, BIBREF3, including thousands of languages that have not been studied and are facing extinction. Thereafter, computational methods can help people do researches on unstudied languages faster and give a possible solution. Phylogenetic inference of human languages task is composed with two parts: cognate set detection and phylogenetic tree construction. Cognate set detection automatically assists people put language words with similar or possible evolutionary patterns to one cluster. The phylogenetic tree construction task build trees given the information from the clusters. In the following, I will divided the whole paper into two main steps: the way I implement cognate detection would be discussed in section 2. After that, I will use the cluster data to carry on the phylogenetic inference program and build the relationship trees, which I will describe the way that how I finished this part in section 3. I will show the results and evaluate them in section 4, and make a conclusion in the last section 5.",Cognate Detection,"A great number of algorithms and mechanisms to antomatic cognate detection which could be applied in historical linguistics have been used and tested if they are working by many linguists and computer scientists BIBREF2, BIBREF4, BIBREF5, BIBREF6, BIBREF7. In detail, many of these works are very similar to each other, which consist of two main stages. For the first stage, they first extract the words with same meaning from the wordlists of different languages, either same or different language families, and compare them and use the distance calculation matrix to compute how similar they are. Regarding the second stage, a flat cluster algorithm or a network partitioning algorithm is used to partition all words into cognate sets, and also take the information in the matrix of word pairs as basis BIBREF4, BIBREF5. However, the methods in which those researchers use to compare the word pairs are totally different in that people could use different methods to pre-process their language datasets, or even use different algorithms to finish the comparison and clustering task. For example, intuitively, people will start the automated word comparison by computing the distance between the words, such as word embedding in NLP, GloVe BIBREF8, which computes the semantic similarities between the two words. In computational historical linguistics, phonetic segments are used to calculate how close the two words are instead, because the semantics of a single word is not changing as easy as phonetics is. The problem is since the program involves the data pre-processing, then the whole dataset would be traverse twice and the computation would be a big problem when the dataset is about to be over 100 languages. Consequently, people began to think about a faster method to help.",Cognate Detection ::: Consonant Class Matching Algorithm (CCM),"The first linear time method was proposed by BIBREF9, and later modified by BIBREF6. The algorithm compares the word pairs by their consonant class. A consonant class is hereby understood as a rough partitioning of speech sounds into groups that are conveniently used by historical linguists when comparing languages BIBREF4. Table TABREF3 shows some of the international phonetic alphabet (IPA) (for more details about IPA, please refer to BIBREF10). After getting the IPA of the word pairs from the wordlist, the algorithm is to determine if they are cognate by judging their first two consonants class match each other or not. However, since the algorithm only compares the first two consonant classes, the accuracy of this algorithm is relatively low. I have two reasons for this: (a) In linguistics, the number of possible sounds in human languages in the world, excluding artificial languages, amounts to the thousands BIBREF5. It is unrealistic to enroll all the sounds into the system. If we enroll all the sounds in the algorithm to simulate the language change process, we need to get a new matrix in which the probabilities of one sound switching to the other sound will be calculated, which is very time-consuming. (b) comparing the first two consonant classes are not sufficient to determine if the two words in pair are derived from the same cognate word.",Cognate Detection ::: Edit Distance,"The Edit Distance approach is to take the normalized Levenshtein distance BIBREF11, which is a concept in information theory. It aims to measure the difference of two sequences by calculating the minimum number of character or string edits, such as insertion, deletion, which are coincidentally two basic language phonetic change. The distance could be used as a probability to estimate how possible one word changes to the other one.",Cognate Detection ::: Sound Class Algorithm (SCA),"This algorithm is for pairwise and multiple alignment analysis BIBREF2. It not only takes an expanded sound class into account, but it considers the prosodic aspect of each word. As a result, it is able to align within morpheme boundaries instead of the sound segments, suppose the morpheme information is the prior knowledge and we already have it.",Cognate Detection ::: LexStat,"The previous three methods use the same strategy to put the words from different languages into clusters, i.e., UPGMA clustering algorithm, while LexStat uses language-specific scoring schemes which are derived from a Monte-Carlo permutation of the data BIBREF5. The word pairs from different languages are first aligned and scored, and the MC permutation shuffled the word pairs according to their scores. The scores could be calculated by the frequencies of daily use by native speakers. Thus, a distribution of sound-correspondence frequencies is generated. Then, the distribution is used to compare with an attested distribution and then converted into a language-specific scoring scheme for all word pairs. Following the algorithms above, with the consideration of both the advantages and disadvantages of them, in this project, I am going to use a modified method: sound-class based skip-grams with bipartite networks (BipSkip). The whole procedure is quite straightforward and could be divided into three steps. First step: the word pair and their skip-grams are two sets of the bipartite networks. The second step is optional, which is to refine the bipartite network. Before I run the program, I will be asked to input a threshold, which determines if the program should delete the skip-gram nodes linked to fewer word nodes than the threshold itself. According to the experiment, even though I did not input any threshold as one of the parameters, the algorithm could still give the same answer but with more executing time. In the last step, the final generated bipartite graph would be connected to a monopartite graph and partitioned into cognate sets with the help of graph partitioning algorithms. Here I would use Informap algorithm BIBREF12. To make a comparison to this method, I am using CCM and SCA for distance measurement in this experiment, too. UPGMA algorithm would be used accordingly in these two cases.",Bayesian Phylogenetic Inference,"Methods for Bayesian phylogenetic inference in evolutionary biology and historical linguistics are all based on the following Bayes rule BIBREF4, BIBREF13: or Where $f$ means the probability density function, $\Lambda $ consists of the tree topology $\tau $, the branch length vector of the tree $T$ and the substitution model parameter $\theta $; $X$ is a data matrix with dimension $N*K$, within which there are $N$ rows indicating $N$ different words from different kinds of languages and they can be put into $K$ cluster in a language family. Figure FIGREF8 shows an example of the matrix. As we can see, the data matrix is a binary matrix with every element $i_{ij}$. 1 means language $i$ could be classified into cognate $j$, while 0 means language $i$ does not belong to cluster $j$. Based on the shape of the data matrix, to get the parameter ($\Lambda = \tau , X, \theta $) for tree generation, we need to sum over the whole matrix and will have to compute all possible topologies of $\frac{(2N-3)!}{2^{N-2}(N-2)!}$. This method is practical only for a small amount of dataset and the calculation is not easy once it is over 5 languages BIBREF13. In addition, based on the Bayesian formula, if we use prior probability $Pr(tree)$ and likelihood $Pr(Data|Tree)$ to produce the posterior probability, it seems that the posterior probability is very easy and convenient to formulate based on the Bayesian formula, but to compute this probability and get the greatest estimate of the final tree, the machine has to compute and compare all the possible trees and in each tree, the machine will compute all the combination of branches with different length. Metropolis-Hastings (MH) algorithm BIBREF14 , one of the Markov Chain Monte Carlo techniques, would be a good tool to overcome this computation difficulty by avoiding summing over all of the topologies by evaluating the posterior probability $f(\Lambda |X)$. The likelihood in this case from one data to the next parameter is calculated by the prunning algorithm. Prunning algorithm, or we can call it K81 model BIBREF15, is a Markov model of DNA evolution, and this model is a description of the DNA in evolution as a string of four discrete state, i.e., G, C, A, T. Fortunately, language model is very similar to DNA model in that both of them are discrete models , in the language model, we only apply this model in the binary dataset.",Bayesian Phylogenetic Inference ::: Markov Chain Monte Carlo (MCMC),"MCMC method is helpful here since it generates a probability distribution $\pi =\lbrace \pi _{i}\rbrace , i=0,1,...,$. We construct a Markov chain with the stationary distribution being posterior probability distribution of the parameters. A new tree will be proposed by stochastically perturbing the current tree and the new tree would be accepted or rejected. If the new tree is accepted, then repeated the algorithm, which is subjected to the future perturbation. If this Markov chain is constructed and configured properly, the proportion of the time that any of the trees is visited is a valid approximation of the posterior probability of the tree BIBREF16. Besides, $\pi _{i}$ is sometimes still not easy to calculate, but we can get the function $\pi = \frac{\pi _{j}}{\pi _{i}}$ directly instead. MH algorithm is the method that could help us solve this function. This algorithm tells us that given sample states $i,j\in \Omega $, and every state $i\in \Omega $ has a distribution $\pi =\lbrace \pi _{i}\rbrace $. There is a potential transition from state $i$ to $j$ and the transition probability is $g_{ij}$, while the acceptance transition probability, meaning a move from state $i$ to $j$, is $\alpha _{ij}$. We can easily get the properties as shown below: Therefore, the chain $\textbf {P}=\lbrace p_{ij}\rbrace $ has $\pi $ as its stationary distribution and it satisfies A sufficient solution for this chain to generate $\pi $ as its stationary distribution is that $\lbrace g_{ij}\rbrace $ be irreducible and aperiodic BIBREF13, BIBREF17. MH algorithm aims to sample parameters from the posterior distribution, which could help us generate the posterior distribution of phylogenetic trees and each state of this Markov Chain is probably labelled as cognate set. We can get the simple relationship as $\pi _{i}=f(\tau =i|X)$. To calculate $\pi = \frac{\pi _{j}}{\pi _{i}}$, we have ratio as shown below: We can have $\alpha _{ij}$ We put $\alpha _{ij}$ the method and have algorithm 1. Under this method, the Markov chain can finally work and the time is shortened greatly, but due to the large scale of the dataset, the new problem is the chain has a large probability that it can get stuck in a local maxima if the posterior probability density function has multiple peaks. To solve this problem, BIBREF18 proposed Metropolis-coupled Markov Chain Monte-Carlo methods (MC3) aiming to solve the problem when Markov chain has to move across a distribution with multiple peaks, which requires $n$ chains run simultaneously. In the iterations, we can get $n$ different stationary distributions $\pi _{n}$, but only the first one $\pi _{1}$ would be the target density and the rest of them would be treated for improving mixing. To 'heat' some of these chains, the posterior probability of the corresponding chains are raised to the power of $\beta $. For instance, if the probability density function is $f(\Lambda |X)$, then the heated distribution is $f(\Lambda |X)^{\beta }$, where $\beta \in (0,1)$. Once we get a heated version of Markov chain, this technique actually make new states easily get accepted. In comparison, we can have the heated version of the chain: In the ratio, if $f(\Lambda |X) > f(\Lambda ^{\prime }|x)$, this will trigger the increase of each state to the power of $\beta $. As a result, the heated chain is more likely to accept new states than other cold chains. The MC3 algorithm needs very expensive computation, since it has to work with multiple CPU cores to run at the same time. Based on the information above, to avoid both shortcomings with cheaper computation, I am going to use MH algorithm with simulated annealing BIBREF19. This method is one of the modified version of MH algorithm with change of the acceptance ratio, similar to MC3 but only with single CPU. We are going to use the equation, where $T_{i}$ represents temperature. In this method, I will have an initial temperature $T_{0}$, which was set very high at the beginning, indicating heated Markov chain and could be decreased in the following steps until it is set to be $T_{i}\rightarrow 0$. Regarding the change of $T_{i}$ value, i.e., to cool down the Markov chain, a cool-down algorithm is inserted in the method. According to BIBREF19, a linear cool-down algorithm in which the $T_{0}$ initial value was extremely high at the very beginning and the algorithm decreases it in every iteration in $T_{i} = \lambda T_{i-1}$ with $\lambda $ being set between $[0.85,0.96]$ BIBREF20. In the experiment, I reduce the temperature to cool down the chain until $T_{i} = 10^{-5}$. The final state of this whole Markov chain is regarded to be the maximuma-posterior (MAP) estimate of the inference. The Bayesian analysis in this project uses binary datasets with all states 0 and 1. The Generalized Time Reversible Model (GTR model) BIBREF21, BIBREF22 is employed to compute the transition probabilities between states. When I build the phylogenetic trees, the tree branch length and also the shape of the tree are continuous variables, a move satisfying exponential distribution would be used as the potential transition function. When the program is launched, a uniform move will randomly choose two states in $\Omega $ and propose a new frequency with the summation of the frequencies of the states that are not changed. I use the Subprunning and Regrafting move and Nearest Neighbor Interchange BIBREF23 to operate on the nodes and the leaves to build the new trees.",Experiments ::: Settings and Implementation,"It is not easy to figure out which kind of skip-gram and sound-class system would generate the satisfying results, so I design my experiment as follows. (1) During the training process, I would use datasets proposed by BIBREF7, I will give the training dataset in table TABREF14, and compute skip-gram by length 4. Although there are languages repeated in both training language dataset and testing language dataset, such as Chinese in Sino-Tibet and Austronesian, I manually tick out the repeated data from both dataset and also the concept from both datasets are not overlapping each other. (2) Regarding sound-class, I would refer to SCA sound class BIBREF7 (3) Set the threshold in step 2 as 0.2 (20%). (3) The evaluation matrix is B-Cubes BIBREF24. The F-score based on the configuration above is, when I use connected components partitioning, 85.4%, and 85.2% when I use Infomap partitioning algorithm.",Experiments ::: Evaluation Methods,"To evaluate the cognate detection, I choose to use B-Cubed algorithm proposed by BIBREF25. The algorithm is completely based on clusters in way of precision, recall and F1-score. The precision and recall score for each cluster are calculated separately and the final recall and precision score for the entire output are from the combination of each cluster/entity in the output. This evalution matrix has been implemented by LingPy. To test if the phylogenetic tree generation module is working good or not, I utilize Generalized Quartet Distance (GQD) matrix, which compares the tree from historical linguistic experts and the one generated by machine. A quartet distance is used to measure how similar the two phylogenetic trees are, which is defined to count the number of quartet that differ between two trees. A quartet is defined as a set of four leaves selected from a set of leaves without replacement BIBREF26, i.e., suppose I have $n$ leaves in the tree, then I need to compare $\binom{n}{4}$ pairs of quartets in both trees. The only 4 topologies are shown in figure FIGREF16. Given a tree $\tau $ with $n$ leaves, I are able to divide $\tau $ into sets of stars $S(\tau )$ and sets of butterflies $B(\tau )$. Now I can define the similarities between the two trees GQD as follows, where tree $\tau _{g}$ is the ground truth tree from historical linguists theoretically. Practically, I used golden standard tree from Glottolog BIBREF28. Glottolog provides a detailed list about understudied languages, including their language families. This dataset collects 7,592 understudied spoken L1 language. Linguists began to use this dataset to build phylogenetic trees with higher accuracy, since the cognate sets are annotated manually. For example, BIBREF4 uses this as a ground truth to compare with the phylogenetic tree constructed with the cognate sets generated automatically. Their result shows their tree has higher quality if I just use annotated cognate sets, which support the motivation of the automated cognate detection of the unstudied language, helping linguists to study the less studied language more efficiently.",Experiments ::: Results and Discussion,"Cognate Detection Table TABREF19 shows the statistics in test data developed by BIBREF2. The result of the BipSkip approach for cognate detection is shown in the table TABREF20 as well as the result of using SCA and CCM. As shown in the tables, we can see that the BipSkip approach is not the quickest method, although it is more user-friendly. CCM is surprsingly fastest method with slightly higher precision than BipSkip approach, especially in Austronesian and Sino-Tibetan. Indo-European languages stays almost the same, my guess is because the languages in Indo-European language family are more phonetically similar to each other than other languages in their corresponding language families, as a result of which the three methods would perform almost the same. SCA algorithm is not recommended here in that it costs the longest time and the largest spece since I need to prepare the expanded sound class and also some morphological features. Phylogenetic Inference The result of phylogenetic inference in modified MH algorithm is shown in table TABREF22. I designed a branch of experiments, changing the settings and get some good results. I set the initial temperature as $T_{0} = \lbrace 10, 20, 30, 40, 50, 60, 70, 80, 90, 100\rbrace $. During the project, I will take down the number of iteration, the time it takes to finish running the program. Table TABREF21 is the ground truth results, testing on golden standard cognates from Glottolog. It is hard to determine which one is outperforming the other two. Overall, Indo-European costs the shortest time and fewer iterations, since in the three methods this language family always has the highest the accuracy. In addition, the cognate sets from the automatic approaches is easier to build phylogenetic trees than the manually annotated standard cognate sets, from the result, the automatic methods obviously shorten the time of automatic phylogenetic inference. ",Conclusion,"Obviously, the result is not surprisingly good. We can observe from the data, the accuracy for each language, some of them are only slightly over 50%. Among the five language families in the testing data, Indo-european has more accuracy than the other four language families, due to the similar phonetic features. Also, some places where native people used to live and was conquered by immigrates, for example languages in the islands in south pacific, Hawaii, Indonesia, etc, their accuracy is obviously high and easy to cluster by cognate and find their relationship. Some native languages in Australia, Pama-Nyungan language family whose languages are mainly on Autralian continent is surprisingly lower than any other southern pacific islands languages. From this exmperiment, we can obviously use this method to help historcial linguists to make an analysis of language development and change, but the result is not very accurate basically. How do we solve the problem? The current datasets only includes the main class of phonetic alphabet, I think it is necessary to enroll some language phonetic change background knowledge to let the machine recognize the probability of change from phoneme $A$ to $B$, such as Great Vowel Shift, etc.",Appendix,"In this appendix, I am going to show the part of resulting tree generated from Indo-european language testing dataset. The whole tree structure is very big which involves 34 trees totally. I am only showning the first four trees. The number on the edge is the probability that they are related. The labels at the end of the tree are language type: such as poli represents polish, norw represents Norwegian; lati represents Latin. The number behind the language type is the index of the word in the wordlist.",,,,,,,,,,,,,,,,,Is the proposed method compared to previous methods?,0bd992a6a218331aa771d922e3c7bb60b653949a,two,familiar,no,markov model,18f4d5a2eb93a969d55361267e74aa0c4f6f82fe,False,True,,"Following the algorithms above, with the consideration of both the advantages and disadvantages of them, in this project, I am going to use a modified method: sound-class based skip-grams with bipartite networks (BipSkip). The whole procedure is quite straightforward and could be divided into three steps. First step: the word pair and their skip-grams are two sets of the bipartite networks. The second step is optional, which is to refine the bipartite network. Before I run the program, I will be asked to input a threshold, which determines if the program should delete the skip-gram nodes linked to fewer word nodes than the threshold itself. According to the experiment, even though I did not input any threshold as one of the parameters, the algorithm could still give the same answer but with more executing time. In the last step, the final generated bipartite graph would be connected to a monopartite graph and partitioned into cognate sets with the help of graph partitioning algorithms. Here I would use Informap algorithm BIBREF12. To make a comparison to this method, I am using CCM and SCA for distance measurement in this experiment, too. UPGMA algorithm would be used accordingly in these two cases.","Here I would use Informap algorithm BIBREF12. To make a comparison to this method, I am using CCM and SCA for distance measurement in this experiment, too. UPGMA algorithm would be used accordingly in these two cases.",34234921a8a643669cbdde180a0e6cfb94d6f8b5,a0b403873302db7cada39008f04d01155ef68f4f,,,,,,,,,What metrics are used to evaluate results?,052d19b456f1795acbb8463312251869cc5b38da,two,familiar,no,markov model,18f4d5a2eb93a969d55361267e74aa0c4f6f82fe,True,,,,,7321d3322293ebd424a2c8c7ef9cc123bc8b628c,a0b403873302db7cada39008f04d01155ef68f4f,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1-Figure1-1.png,Figure 1: The evolution phylogenetic tree of the carnivores. The data in [26] are used to generate the tree.,2-Table1-1.png,"TABLE 1: Some examples of consonants in IPA, also used in the experiments.",3-Figure2-1.png,Figure 2: Data matrix example,4-Table2-1.png,TABLE 2: Training data from [11],4-Table3-1.png,TABLE 3: Test data from [1],4-Table4-1.png,TABLE 4: Three methods to extract the cognate on test data.,,,,,,,,,,,4-Table5-1.png,TABLE 5: showing the results for golden standard cognates from [3],5-Table6-1.png,TABLE 6: Three methods to extract the cognate on test data.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Torch-Struct: Deep Structured Prediction Library,"The literature on structured prediction for NLP describes a rich collection of distributions and algorithms over sequences, segmentations, alignments, and trees; however, these algorithms are difficult to utilize in deep learning frameworks. We introduce Torch-Struct, a library for structured prediction designed to take advantage of and integrate with vectorized, auto-differentiation based frameworks. Torch-Struct includes a broad collection of probabilistic structures accessed through a simple and flexible distribution-based API that connects to any deep learning model. The library utilizes batched, vectorized operations and exploits auto-differentiation to produce readable, fast, and testable code. Internally, we also include a number of general-purpose optimizations to provide cross-algorithm efficiency. Experiments show significant performance gains over fast baselines and case-studies demonstrate the benefits of the library. Torch-Struct is available at this https URL.",Introduction,"Structured prediction is an area of machine learning focusing on representations of spaces with combinatorial structure, and algorithms for inference and parameter estimation over these structures. Core methods include both tractable exact approaches like dynamic programming and spanning tree algorithms as well as heuristic techniques such linear programming relaxations and greedy search. Structured prediction has played a key role in the history of natural language processing. Example methods include techniques for sequence labeling and segmentation BIBREF0, BIBREF4, discriminative dependency and constituency parsing BIBREF10, BIBREF8, unsupervised learning for labeling and alignment BIBREF11, BIBREF12, approximate translation decoding with beam search BIBREF9, among many others. In recent years, research into deep structured prediction has studied how these approaches can be integrated with neural networks and pretrained models. One line of work has utilized structured prediction as the final final layer for deep models BIBREF13, BIBREF14. Another has incorporated structured prediction within deep learning models, exploring novel models for latent-structure learning, unsupervised learning, or model control BIBREF15, BIBREF16, BIBREF17. We aspire to make both of these use-cases as easy to use as standard neural networks. The practical challenge of employing structured prediction is that many required algorithms are difficult to implement efficiently and correctly. Most projects reimplement custom versions of standard algorithms or focus particularly on a single well-defined model class. This research style makes it difficult to combine and try out new approaches, a problem that has compounded with the complexity of research in deep structured prediction. With this challenge in mind, we introduce Torch-Struct with three specific contributions: Modularity: models are represented as distributions with a standard flexible API integrated into a deep learning framework. Completeness: a broad array of classical algorithms are implemented and new models can easily be added in Python. Efficiency: implementations target computational/memory efficiency for GPUs and the backend includes extensions for optimization. In this system description, we first motivate the approach taken by the library, then present a technical description of the methods used, and finally present several example use cases.",Related Work,"Several software libraries target structured prediction. Optimization tools, such as SVM-struct BIBREF18, focus on parameter estimation. Model libraries, such as CRFSuite BIBREF19 or CRF++ BIBREF20, implement inference for a fixed set of popular models, such as linear-chain CRFs. General-purpose inference libraries, such as PyStruct BIBREF21 or TurboParser BIBREF22, utilize external solvers for (primarily MAP) inference such as integer linear programming solvers and ADMM. Probabilistic programming languages, for example languages that integrate with deep learning such as Pyro BIBREF23, allow for specification and inference over some discrete domains. Most ambitiously, inference libraries such as Dyna BIBREF24 allow for declarative specifications of dynamic programming algorithms to support inference for generic algorithms. Torch-Struct takes a different approach and integrates a library of optimized structured distributions into a vectorized deep learning system. We begin by motivating this approach with a case study.",Motivating Case Study,"While structured prediction is traditionally presented at the output layer, recent applications have deployed structured models broadly within neural networks BIBREF15, BIBREF25, BIBREF16. Torch-Struct aims to encourage this general use case. To illustrate, we consider a latent tree model. ListOps BIBREF26 is a dataset of mathematical functions. Each data point consists of a prefix expression $x$ and its result $y$, e.g. Models such as a flat RNN will fail to capture the hierarchical structure of this task. However, if a model can induce an explicit latent $z$, the parse tree of the expression, then the task is easy to learn by a tree-RNN model $p(y | x, z)$ BIBREF16, BIBREF27. A popular approach is a latent-tree RL model which we briefly summarize. The objective is to maximize the probability of the correct prediction under the expectation of a prior tree model, $p(z|x ;\phi )$, Computing the expectation is intractable so policy gradient is used. First a tree is sampled $\tilde{z} \sim p(z | x;\phi )$, then the gradient with respect to $\phi $ is approximated as, where $b$ is a variance reduction baseline. A common choice is the self-critical baseline BIBREF28, Finally an entropy regularization term is added to the objective encourage exploration of different trees, $ O + \lambda \mathbb {H}(p(z\ |\ x;\phi ))$. Even in this brief overview, we can see how complex a latent structured learning problem can be. To compute these terms, we need 5 different properties of the tree model $p(z\ | x; \phi )$: [description]font= [itemsep=-2pt] Policy gradient, $\tilde{z} \sim p(z \ |\ x ; \phi )$ Score policy samples, $p(z \ | \ x; \phi )$ Backpropagation, $\frac{\partial }{\partial \phi } p(z\ |\ x; \phi )$ Self-critical, $\arg \max _z p(z \ |\ x;\phi )$ Objective regularizer, $\mathbb {H}(p(z\ |\ x;\phi ))$ For structured models, each of these terms is non-trivial to compute. A goal of Torch-Struct is to make it seamless to deploy structured models for these complex settings. To demonstrate this, Torch-Struct includes an implementation of this latent-tree approach. With a minimal amount of user code, the implementation achieves near perfect accuracy on the ListOps dataset.",Library Design,"The library design of Torch-Struct follows the distributions API used by both TensorFlow and PyTorch BIBREF29. For each structured model in the library, we define a conditional random field (CRF) distribution object. From a user's standpoint, this object provides all necessary distributional properties. Given log-potentials (scores) output from a deep network $\ell $, the user can request samples $z \sim \textsc {CRF}(\ell )$, probabilities $\textsc {CRF}(z;\ell )$, modes $\arg \max _z \textsc {CRF}(\ell )$, or other distributional properties such as $\mathbb {H}(\textsc {CRF}(\ell ))$. The library is agnostic to how these are utilized, and when possible, they allow for backpropagation to update the input network. The same distributional object can be used for standard output prediction as for more complex operations like attention or reinforcement learning. Figure FIGREF11 demonstrates this API for a binary tree CRF over an ordered sequence, such as $p(z \ | \ y ;\phi )$ from the previous section. The distribution takes in log-potentials $\ell $ which score each possible span in the input. The distribution converts these to probabilities of a specific tree. This distribution can be queried for predicting over the set of trees, sampling a tree for model structure, or even computing entropy over all trees. Table TABREF2 shows all of the structures and distributions implemented in Torch-Struct. While each is internally implemented using different specialized algorithms and optimizations, from the user's perspective they all utilize the same external distributional API, and pass a generic set of distributional tests. This approach hides the internal complexity of the inference procedure, while giving the user full access to the model.",Technical Approach ::: Conditional Random Fields,"We now describe the technical approach underlying the library. To establish notation first consider the implementation of a categorical distribution, Cat($\ell $), with one-hot categories $z$ with $z_i = 1$ from a set $\cal Z$ and probabilities given by the softmax, Define the log-partition as $A(\ell ) = \mathrm {LSE}(\ell )$, i.e. log of the denominator, where $\mathrm {LSE}$ is the log-sum-exp operator. Computing probabilities or sampling from this distribution, requires enumerating $\cal Z$ to compute the log-partition $A$. A useful identity is that derivatives of $A$ yield category probabilities, Other distributional properties can be similarly extracted from variants of the log-partition. For instance, define $A^*(\ell ) = \log \max _{j=1}^K \exp \ell _j$ then: $\mathbb {I}(z^*_i = 1) = \frac{\partial }{\partial \ell _i} A^*(\ell ) $. Conditional random fields, CRF($\ell $), extend the softmax to combinatorial spaces where ${\cal Z}$ is exponentially sized. Each $z$, is now represented as a binary vector over polynomial-sized set of parts, $\cal P$, i.e. ${\cal Z} \subset \lbrace 0, 1\rbrace ^{|\cal P|}$. Similarly log-potentials are now defined over parts $\ell \in \mathbb {R}^{|\cal P|}$. For instance, in Figure FIGREF11 each span is a part and the $\ell $ vector is shown in the top-left figure. Define the probability of a structure $z$ as, Computing probabilities or sampling from this distribution, requires computing the log-partition term $A$. In general computing this term is now intractable, however for many core algorithms in NLP there are exist efficient combinatorial algorithms for this term (as enumerated in Table TABREF2). Derivatives of the log-partition again provide distributional properties. For instance, the marginal probabilities of parts are given by, Similarly derivatives of $A^*$ correspond to whether a part appears in the argmax structure. $\mathbb {I}(z^*_p = 1) = \frac{\partial }{\partial \ell _p} A^*(\ell ) $. While these gradient identities are well-known BIBREF30, they are not commonly deployed. Computing CRF properties is typically done through two-step specialized algorithms, such as forward-backward, inside-outside, or similar variants such as viterbi-backpointers BIBREF31. In our experiments, we found that using these identities with auto-differentiation on GPU was often faster, and much simpler, than custom two-pass approaches. Torch-Struct is thus designed around using gradients for distributional computations.",Technical Approach ::: Dynamic Programming and Semirings,"Torch-Struct is a collection of generic algorithms for CRF inference. Each CRF distribution object, $\textsc {CRF}(\ell )$, is constructed by providing $\ell \in \mathbb {R}^{|{\cal P}|}$ where the parts $\cal P$ are specific to the type of distribution. Internally, each distribution is implemented through a single Python function for computing the log-partition function $A(\ell )$. From this function, the library uses auto-differentiation and the identities from the previous section, to define a complete distribution object. The core models implemented by the library are shown in Table TABREF2. To make the approach concrete, we consider the example of a linear-chain CRF. latent](a)$z_1$; latent, right = of a](b)$z_2$; latent, right = of b](c)$z_3$; (a) – (b) – (c); The model has $C$ labels per node with a length $T=2$ edges utilizing a first-order linear-chain (Markov) model. This model has $2\times C \times C$ parts corresponding to edges in the chain, and thus requires $\ell \in \mathbb {R}^{2\times C \times C}$. The log-partition function $A(\ell )$ factors into two reduce computations, Computing this function left-to-right using dynamic programming yield the standard forward algorithm for sequence models. As we have seen, the gradient with respect to $\ell $ produces marginals for each part, i.e. the probability of a specific labeled edge. We can further extend the same function to support generic semiring dynamic programming BIBREF34. A semiring is defined by a pair $(\oplus , \otimes )$ with commutative $\oplus $, distribution, and appropriate identities. The log-partition utilizes $\oplus , \otimes = \mathrm {LSE}, +$, but we can substitute alternatives. For instance, utilizing the log-max semiring $(\max , +)$ in the forward algorithm yields the max score. As we have seen, its gradient with respect to $\ell $ is the argmax sequence, negating the need for a separate argmax (Viterbi) algorithm. Some distributional properties cannot be computed directly through gradient identities but still use a forward-backward style compute structure. For instance, sampling requires first computing the log-partition term and then sampling each part, (forward filtering / backward sampling). We can compute this value by overriding each backpropagation operation for the $\bigoplus $ to instead compute a sample. Table TABREF16 shows the set of semirings and backpropagation steps for computing different terms of interest. We note that many of the terms necessary in the case-study can be computed with variant semirings, negating the need for specialized algorithms.",Optimizations,"Torch-Struct aims for computational and memory efficiency. Implemented naively, dynamic programming algorithms in Python are prohibitively slow. As such Torch-Struct provides key primitives to help batch and vectorize these algorithms to take advantage of GPU computation and to minimize the overhead of backpropagating through chart-based dynamic programmming. Figure FIGREF17 shows the impact of these optimizations on the core algorithms.",Optimizations ::: a) Parallel Scan Inference,"The commutative properties of semiring algorithms allow flexibility in the order in which we compute $A(\ell )$. Typical implementations of dynamic programming algorithms are serial in the length of the sequence. On parallel hardware, an appealing approach is a parallel scan ordering BIBREF35, typically used for computing prefix sums. To compute, $A(\ell )$ in this manner we first pad the sequence length $T$ out to the nearest power of two, and then compute a balanced parallel tree over the parts, shown in Figure FIGREF21. Concretely each node layer would compute a semiring matrix multiplication, e.g. $ \bigoplus _c \ell _{t, \cdot , c} \otimes \ell _{t^{\prime }, c, \cdot }$. Under this approach, we only need $O(\log N)$ steps in Python and can use parallel GPU operations for the rest. Similar parallel approach can also be used for computing sequence alignment and semi-Markov models.",Optimizations ::: b) Vectorized Parsing,"Computational complexity is even more of an issue for parsing algorithms, which cannot be as easily parallelized. The log-partition for parsing is computed with the Inside algorithm. This algorithm must compute each width from 1 through T in serial; however it is important to parallelize each inner step. Assuming we have computed all inside spans of width less than $d$, computing the inside span of width $d$ requires computing for all $i$, In order to vectorize this loop over $i, j$, we reindex the chart. Instead of using a single chart $C$, we split it into two parts: one right-facing $C_r[i, d] = C[i, i+d]$ and one left facing, $C_l[i+d, T-d] = C[i, i+d]$. After this reindexing, the update can be written. Unlike the original, this formula can easily be computed as a vectorized semiring dot product. This allows use to compute $C_r[\cdot , d]$ in one operation. Variants of this same approach can be used for all the parsing models employed.",Optimizations ::: c) Semiring Matrix Operations,"The two previous optimizations reduce most of the cost to semiring matrix multiplication. In the specific case of the $(\sum , \times )$ semiring these can be computed very efficiently using matrix multiplication, which is highly-tuned on GPU hardware. Unfortunately for other semirings, such as log and max, these operations are either slow or very memory inefficient. For instance, for matrices $T$ and $U$ of sized $N \times M$ and $M \times O$, we can broadcast with $\otimes $ to a tensor of size $N \times M \times O$ and then reduce dim $M$ by $\bigoplus $ at a huge memory cost. To avoid this issue, we implement custom CUDA kernels targeting fast and memory efficient tensor operations. For log, this corresponds to computing, where $q = \max _n T_{m,n} + U_{n, o}$. To optimize this operation on GPU we utilize the TVM language BIBREF36 to layout the CUDA loops and tune it to hardware.",Conclusion and Future Work,"We present Torch-Struct, a library for deep structured prediction. The library achieves modularity through its adoption of a generic distributional API, completeness by utilizing CRFs and semirings to make it easy to add new algorithms, and efficiency through core optimizations to vectorize important dynamic programming steps. In addition to the problems discussed so far, Torch-Struct also includes several other example implementations including supervised dependency parsing with BERT, unsupervised tagging, structured attention, and connectionist temporal classification (CTC) for speech. The full library is available at https://github.com/harvardnlp/pytorch-struct. In the future, we hope to support research and production applications employing structured models. We also believe the library provides a strong foundation for building generic tools for interpretablity, control, and visualization through its probabilistic API. Finally, we hope to explore further optimizations to make core algorithms competitive with highly-optimized neural network components.",Acknowledgements,"We thank Yoon Kim, Xiang Lisa Li, Sebastian Gehrmann, Yuntian Deng, and Justin Chiu for discussion and feedback on the project. The project was supported by NSF CAREER 1845664, NSF 1901030, and research awards by Sony and AWS.",,,,,,,,,,,,,,,,,,,Does API provide ability to connect to models written in some other deep learning framework?,1d9b953a324fe0cfbe8e59dcff7a44a2f93c568d,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,True,,"The library design of Torch-Struct follows the distributions API used by both TensorFlow and PyTorch BIBREF29. For each structured model in the library, we define a conditional random field (CRF) distribution object. From a user's standpoint, this object provides all necessary distributional properties. Given log-potentials (scores) output from a deep network $\ell $, the user can request samples $z \sim \textsc {CRF}(\ell )$, probabilities $\textsc {CRF}(z;\ell )$, modes $\arg \max _z \textsc {CRF}(\ell )$, or other distributional properties such as $\mathbb {H}(\textsc {CRF}(\ell ))$. The library is agnostic to how these are utilized, and when possible, they allow for backpropagation to update the input network. The same distributional object can be used for standard output prediction as for more complex operations like attention or reinforcement learning.",The library design of Torch-Struct follows the distributions API used by both TensorFlow and PyTorch BIBREF29.,83b0d2c9df28b611f74cbc625a6fa50df1bba8ae,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,Is this library implemented into Torch or is framework agnostic?,093039f974805952636c19c12af3549aa422ec43,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,,It uses deep learning framework (pytorch),"With this challenge in mind, we introduce Torch-Struct with three specific contributions: Modularity: models are represented as distributions with a standard flexible API integrated into a deep learning framework. Completeness: a broad array of classical algorithms are implemented and new models can easily be added in Python. Efficiency: implementations target computational/memory efficiency for GPUs and the backend includes extensions for optimization.","With this challenge in mind, we introduce Torch-Struct with three specific contributions:

Modularity: models are represented as distributions with a standard flexible API integrated into a deep learning framework.

Completeness: a broad array of classical algorithms are implemented and new models can easily be added in Python.

Efficiency: implementations target computational/memory efficiency for GPUs and the backend includes extensions for optimization.",363475920554b38997e8edef0aafd969ed8e7fcc,258ee4069f740c400c0049a2580945a1cc7f044c,What baselines are used in experiments?,8df89988adff57279db10992846728ec4f500eaa,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,,,"Optimizations ::: a) Parallel Scan Inference The commutative properties of semiring algorithms allow flexibility in the order in which we compute $A(\ell )$. Typical implementations of dynamic programming algorithms are serial in the length of the sequence. On parallel hardware, an appealing approach is a parallel scan ordering BIBREF35, typically used for computing prefix sums. To compute, $A(\ell )$ in this manner we first pad the sequence length $T$ out to the nearest power of two, and then compute a balanced parallel tree over the parts, shown in Figure FIGREF21. Concretely each node layer would compute a semiring matrix multiplication, e.g. $ \bigoplus _c \ell _{t, \cdot , c} \otimes \ell _{t^{\prime }, c, \cdot }$. Under this approach, we only need $O(\log N)$ steps in Python and can use parallel GPU operations for the rest. Similar parallel approach can also be used for computing sequence alignment and semi-Markov models. Optimizations ::: b) Vectorized Parsing Computational complexity is even more of an issue for parsing algorithms, which cannot be as easily parallelized. The log-partition for parsing is computed with the Inside algorithm. This algorithm must compute each width from 1 through T in serial; however it is important to parallelize each inner step. Assuming we have computed all inside spans of width less than $d$, computing the inside span of width $d$ requires computing for all $i$, Optimizations ::: c) Semiring Matrix Operations The two previous optimizations reduce most of the cost to semiring matrix multiplication. In the specific case of the $(\sum , \times )$ semiring these can be computed very efficiently using matrix multiplication, which is highly-tuned on GPU hardware. Unfortunately for other semirings, such as log and max, these operations are either slow or very memory inefficient. For instance, for matrices $T$ and $U$ of sized $N \times M$ and $M \times O$, we can broadcast with $\otimes $ to a tensor of size $N \times M \times O$ and then reduce dim $M$ by $\bigoplus $ at a huge memory cost. To avoid this issue, we implement custom CUDA kernels targeting fast and memory efficient tensor operations. For log, this corresponds to computing,","Parallel Scan Inference
The commutative properties of semiring algorithms allow flexibility in the order in which we compute $A(\ell )$. Typical implementations of dynamic programming algorithms are serial in the length of the sequence. Vectorized Parsing
Computational complexity is even more of an issue for parsing algorithms, which cannot be as easily parallelized. Semiring Matrix Operations
The two previous optimizations reduce most of the cost to semiring matrix multiplication. In the specific case of the $(\sum , \times )$ semiring these can be computed very efficiently using matrix multiplication, which is highly-tuned on GPU hardware. Unfortunately for other semirings, such as log and max, these operations are either slow or very memory inefficient. For instance, for matrices $T$ and $U$ of sized $N \times M$ and $M \times O$, we can broadcast with $\otimes $ to a tensor of size $N \times M \times O$ and then reduce dim $M$ by $\bigoplus $ at a huge memory cost.",41a5e7f9002bc00be615405addaa6e72f4201759,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,What general-purpose optimizations are included?,94edac71eea1e78add678fb5ed2d08526b51016b,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,,,"Optimizations ::: a) Parallel Scan Inference Optimizations ::: b) Vectorized Parsing Optimizations ::: c) Semiring Matrix Operations Torch-Struct aims for computational and memory efficiency. Implemented naively, dynamic programming algorithms in Python are prohibitively slow. As such Torch-Struct provides key primitives to help batch and vectorize these algorithms to take advantage of GPU computation and to minimize the overhead of backpropagating through chart-based dynamic programmming. Figure FIGREF17 shows the impact of these optimizations on the core algorithms.","a) Parallel Scan Inference b) Vectorized Parsing c) Semiring Matrix Operations Torch-Struct aims for computational and memory efficiency. Implemented naively, dynamic programming algorithms in Python are prohibitively slow. As such Torch-Struct provides key primitives to help batch and vectorize these algorithms to take advantage of GPU computation and to minimize the overhead of backpropagating through chart-based dynamic programmming.",0f255bdea6c34801b2ab038ea6710f9481bc417a,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1-Figure1-1.png,Figure 1: Distribution of binary trees over an 1000- token sequence. Coloring shows the marginal probabilities of every span. Torch-Struct is an optimized collection of common CRF distributions used in NLP designed to integrate with deep learning frameworks.,2-Table1-1.png,"Table 1: Models and algorithms implemented in Torch-Struct. Notation is developed in Section 5. Parts are described in terms of sequence lengths N,M , label size C, segment length K, and layers / grammar size L,G. Lines of code (LoC) is from the log-partition (A(`)) implementation. T/S is the tokens per second of a batched computation, computed with batch 32, N = 25, C = 20,K = 5, L = 3 (K80 GPU run on Google Colab).",3-Figure2-1.png,Figure 2: Latent Tree CRF example. (a) Logpotentials ` for each part/span. (b) Marginals for CRF(`) computed by backpropagation. (c) Mode tree argmaxz CRF(z; `). (d) Sampled tree z ∼ CRF(`).,4-Table2-1.png,Table 2: (Top) Semirings implemented in Torch-Struct. Backprop/Gradients gives overridden backpropagation computation and value computed by this combination. (Bot) Example of gradients from different semirings on sequence alignment with dynamic time warping.,6-Figure3-1.png,Figure 3: Speed impact of optimizations. Time is given in seconds for 10 runs with batch 16 executed on Google Colab. (a) Speed of a linear-chain forward with 20 classes for lengths up to 500. Compares left-to-right ordering to parallel scan. (b) Speed of CKY inside with lengths up to 80. Compares inner loop versus vectorization. (c) Speed of linear-chain forward of length 20 with up to 100 classes. Compares broadcast-reduction versus CUDA semiring kernel. (Baseline memory is exhausted after 100 classes.),6-Figure4-1.png,Figure 4: Parallel scan implementation of the linearchain CRF inference algorithm. Here ⊕ ⊗ represents a semiring matrix operation and I is padding.,,,,,,,,,,"Typical implementations of dynamic programming algorithms are serial in the length of the sequence Computational complexity is even more of an issue for parsing algorithms, which cannot be as easily parallelized Unfortunately for other semirings, such as log and max, these operations are either slow or very memory inefficient",,,,,,,,,,,,,,,,,,,,,Parallel Scan Inference Vectorized Parsing Semiring Matrix Operations,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Evaluation of basic modules for isolated spelling error correction in Polish texts,"Spelling error correction is an important problem in natural language processing, as a prerequisite for good performance in downstream tasks as well as an important feature in user-facing applications. For texts in Polish language, there exist works on specific error correction solutions, often developed for dealing with specialized corpora, but not evaluations of many different approaches on big resources of errors. We begin to address this problem by testing some basic and promising methods on PlEWi, a corpus of annotated spelling extracted from Polish Wikipedia. These modules may be further combined with appropriate solutions for error detection and context awareness. Following our results, combining edit distance with cosine distance of semantic vectors may be suggested for interpretable systems, while an LSTM, particularly enhanced by ELMo embeddings, seems to offer the best raw performance.",Introduction,"Spelling error correction is a fundamental NLP task. Most language processing applications benefit greatly from being provided clean texts for their best performance. Human users of computers also often expect competent help in making spelling of their texts correct. Because of the lack of tests of many common spelling correction methods for Polish, it is useful to establish how they perform in a simple scenario. We constrain ourselves to the pure task of isolated correction of non-word errors. They are traditionally separated in error correction literature BIBREF0 . Non-word errors are here incorrect word forms that not only differ from what was intended, but also do not constitute another, existing word themselves. Much of the initial research on error correction focused on this simple task, tackled without means of taking the context of the nearest words into account. It is true that, especially in the case of neural networks, it is often possible and desirable to combine problems of error detection, correction and context awareness into one task trained with a supervised training procedure. In language correction research for English language also grammatical and regular spelling errors have been treated uniformly with much success BIBREF1 . However, when more traditional methods are used, because of their predictability and interpretability for example, one can mix and match various approaches to dealing with the subproblems of detection, correction and context handling (often equivalent to employing some kind of a language model). We call it a modular approach to building spelling error correction systems. There is recent research where this paradigm was applied, interestingly, to convolutional networks trained separately for various subtasks BIBREF2 . In similar setups it is more useful to assess abilities of various solutions in isolation. The exact architecture of a spelling correction system should depend on characteristics of texts it will work on. Similar considerations eliminated from our focus handcrafted solutions for the whole spelling correction pipeline, primarily the LanguageTool BIBREF3 . Its performance in fixing spelling of Polish tweets was already tested BIBREF4 . For our purposes it would be given an unfair advantage, since it is a rule-based system making heavy use of words in context of the error.",Problems of spelling correction for Polish,"Published work on language correction for Polish dates back at least to 1970s, when simplest Levenshtein distance solutions were used for cleaning mainframe inputs BIBREF5 , BIBREF6 . Spelling correction tests described in literature have tended to focus on one approach applied to a specific corpus. Limited examples include works on spellchecking mammography reports and tweets BIBREF7 , BIBREF4 . These works emphasized the importance of tailoring correction systems to specific problems of corpora they are applied to. For example, mammography reports suffer from poor typing, which in this case is a repetitive work done in relative hurry. Tweets, on the other hand, tend to contain emoticons and neologisms that can trick solutions based on rules and dictionaries, such as LanguageTool. The latter is, by itself, fairly well suited for Polish texts, since a number of extensions to the structure of this application was inspired by problems with morphology of Polish language BIBREF3 . These existing works pointed out more general, potentially useful qualities specific to spelling errors in Polish language texts. It is, primarily, the problem of leaving out diacritical signs, or, more rarely, adding them in wrong places. This phenomenon stems from using a variant of the US keyboard layout, where combinations of AltGr with some alphabetic keys produces characters unique to Polish. When the user forgets or neglects to press the AltGr key, typos such as writing *olowek instead of ołówek appear. In fact, BIBREF4 managed to get substantial performance on Twitter corpus by using this ”diacritical swapping” alone.",Baseline methods,"The methods that we evaluated are baselines are the ones we consider to be basic and with moderate potential of yielding particularly good results. Probably the most straightforward approach to error correction is selecting known words from a dictionary that are within the smallest edit distance from the error. We used the Levenshtein distance metric BIBREF8 implemented in Apache Lucene library BIBREF9 . It is a version of edit distance that treats deletions, insertions and replacements as adding one unit distance, without giving a special treatment to character swaps. The SGJP – Grammatical Dictionary of Polish BIBREF10 was used as the reference vocabulary. Another simple approach is the aforementioned diacritical swapping, which is a term that we introduce here for referring to a solution inspired by the work of BIBREF4 . Namely, from the incorrect form we try to produce all strings obtainable by either adding or removing diacritical marks from characters. We then exclude options that are not present in SGJP, and select as the correction the one within the smallest edit distance from the error. It is possible for the number of such diacritically-swapped options to become very big. For example, the token Modlin-Zegrze-Pultusk-Różan-Ostrołęka-Łomża-Osowiec (taken from PlEWi corpus of spelling errors, see below) can yield over INLINEFORM0 states with this method, such as Módłiń-Żęgrzę-Pułtuśk-Roźąń-Óśtróleką-Lómzą-Óśówięć. The actual correction here is just fixing the ł in Pułtusk. Hence we only try to correct in this way tokens that are shorter than 17 characters.",Vector distance,"A promising method, adapted from work on correcting texts by English language learners BIBREF11 , expands on the concept of selecting a correction nearest to the spelling error according to some notion of distance. Here, the Levenshtein distance is used in a weighted sum to cosine distance between word vectors. This is based on the observation that trained vectors models of distributional semantics contain also representations of spelling errors, if they were not pruned. Their representations tend to be similar to those of their correct counterparts. For example, the token enginir will appear in similar contexts as engineer, and therefore will be assigned a similar vector embedding. The distance between two tokens INLINEFORM0 and INLINEFORM1 is thus defined as INLINEFORM2  Here INLINEFORM0 is just Levenshtein distance between strings, and INLINEFORM1 – cosine distance between vectors. INLINEFORM2 denotes the word vector for INLINEFORM3 . Both distance metrics are in our case roughly in the range [0,1] thanks to the scaling of edit distance performed automatically by Apache Lucene. We used a pretrained set of word embeddings of Polish BIBREF12 , obtained with the flavor word2vec procedure using skipgrams and negative sampling BIBREF13 .",Recurrent neural networks,"Another powerful approach, if conceptually simple in linguistic terms, is using a character-based recurrent neural network. Here, we test uni- and bidirectional Long Short-Term Memory networks BIBREF14 that are fed characters of the error as their input and are expected to output its correct form, character after character. This is similar to traditional solutions conceptualizing the spelling error as a chain of characters, which are used as evidence to predict the most likely chain of replacements (original characters). This was done with n-gram methods, Markov chains and other probabilistic models BIBREF15 . Since nowadays neural networks enjoy a large awareness as an element of software infrastructure, with actively maintained packages readily available, their evaluation seems to be the most practically useful. We used the PyTorch BIBREF16 implementation of LSTM in particular. The bidirectional version BIBREF17 of LSTM reads the character chains forward and backwards at the same time. Predictions from networks running in both directions are averaged. In order to provide the network an additional, broad picture peek at the whole error form we also evaluated a setup where the internal state of LSTM cells, instead of being initialized randomly, is computed from an ELMo embedding BIBREF18 of the token. The ELMo embedder is capable of integrating linguistic information carried by the whole form (probably often not much in case of errors), as well as the string as a character chain. The latter is processed with a convolutional neural network. How this representation is constructed is informed by the whole corpus on which the embedder was trained. The pretrained ELMo model that we used BIBREF19 was trained on Wikipedia and Common Crawl corpora of Polish. The ELMo embedding network outputs three layers as matrices, which are supposed to reflect subsequent compositional layers of language, from phonetic phenomena at the bottom to lexical ones at the top. A weighted sum of these layers is computed, with weights trained along with the LSTM error-correcting network. Then we apply a trained linear transformation, followed by INLINEFORM0 non-linearity: INLINEFORM1  (applied cellwise) in order to obtain the initial setting of parameters for the main LSTM. Our ELMo-augmented LSTM is bidirectional.",Experimental setup,"PlEWi BIBREF20 is an early version of WikEd BIBREF21 error corpus, containing error type annotations allowing us to select only non-word errors for evaluation. Specifically, PlEWi supplied 550,755 [error, correction] pairs, from which 298,715 were unique. The corpus contains data extracted from histories of page versions of Polish Wikipedia. An algorithm designed by the corpus author determined where the changes were correcting spelling errors, as opposed to expanding content and disagreements among Wikipedia editors. The corpus features texts that are descriptive rather than conversational, contain relatively many proper names and are more likely to have been at least skimmed by the authors before submitting for online publication. Error cases provided by PlEWi are, therefore, not a balanced representation of spelling errors in written Polish language. PlEWi does have the advantage of scale in comparison to existing literature, such as BIBREF4 operating on a set of only 740 annotated errors in tweets. All methods were tested on a test subset of 25% of cases, with 75% left for training (where needed) and 5% for development. The methods that required training – namely recurrent neural networks – had their loss measured as cross-entropy loss measure between correct character labels and predictions. This value was minimized with Adam algorithm BIBREF22 . The networks were trained for 35 epochs.",Results,"The experimental results are presented in Table TABREF4 . Diacritic swapping showed a remarkably poor performance, despite promising mentions in existing literature. This might be explained by the already mentioned feature of Wikipedia edits, which can be expected to be to some degree self-reviewed before submission. This can very well limit the number of most trivial mistakes. On the other hand, the vector distance method was able to bring a discernible improvement over pure Levenshtein distance, comparable even with the most basic LSTM. It is possible that assigning more fine-tuned weights to edit distance and semantic distance would make the quality of predictions even higher. The idea of using vector space measurements explicitly can be also expanded if we were to consider the problem of contextualizing corrections. For example, the semantic distance of proposed corrections to the nearest words is likely to carry much information about their appropriateness. Looking from another angle, searching for words that seem semantically off in context may be a good heuristic for detecting errors that are not nonword (that is, they lead to wrong forms appearing in text which are nevertheless in-vocabulary). The good performance of recurrent network methods is hardly a surprise, given observed effectiveness of neural networks in many NLP tasks in the recent decade. It seems that bidirectional LSTM augmented with ELMo may already hit the limit for correcting Polish spelling errors without contextual information. While it improves accuracy in comparison to LSTM initialized withrandom noise, it makes the test cross-entropy slightly worse, which hints at overfitting. The perplexity measures actually increase sharply for more sophisticated architectures. Perplexity should show how little probability is assigned by the model to true answers. We measure it as INLINEFORM0  where INLINEFORM0 is a sequence of INLINEFORM1 characters, forming the correct version of the word, and INLINEFORM2 is the estimated probability of the INLINEFORM3 th character, given previous predicted characters and the incorrect form. The observed increase of perplexity for increasingly accurate models is most likely due to more refined predicted probability distributions, which go beyond just assigning the bulk of probability to the best answer. Interesting insights can be gained from weights assigned by optimization to layers of ELMo network, which are taken as the word form embedding (Table TABREF5 ). The first layer, and the one that is nearest to input of the network, is given relatively the least importance, while the middle one dominates both others taken together. This suggests that in error correction, at least for Polish, the middle level of morphemes and other characteristic character chunks is more important than phenomena that are low-level or tied to some specific words. This observation should be taken into account in further research on practical solutions for spelling correction.",Conclusion,"Among the methods tested the bidirectional LSTM, especially initialized by ELMo embeddings, offers the best accuracy and raw performance. Adding ELMo to a straightforward PyTorch implementation of LSTM may be easier now than at the time of performing our tests, as since then the authors of ELMoForManyLangs package BIBREF19 improved their programmatic interface. However, if a more interpretable and explainable output is required, some version of vector distance combined with edit distance may be the best direction. It should be noted that this method produces multiple candidate corrections with their similarity scores, as opposed to only one “best guess“ correction that can be obtained from a character-based LSTM. This is important in applications where it is up to humans to the make the final decision, and they are only to be aided by a machine. It is desirable for further reasearch to expand the corpus material into a wider and more representative set of texts. Nevertheless, the solution for any practical case has to be tailored to its characteristic error patterns. Works on language correction for English show that available corpora can be ”boosted” BIBREF1 , i.e. expanded by generating new errors consistent with a generative model inferred from the data. This may greatly aid in developing models that are dependent on learning from error corpora. A deliberate omission in this paper are the elements accompanying most real-word error correction solutions. Some fairly obvious approaches to integrating evidence from context include n-grams and Markov chains, although the possibility of using measurements in spaces of semantic vectors was already mentioned in this article. Similarly, non-word errors can be easily detected with comparing tokens against reference vocabulary, but in practice one should have ways of detecting mistakes masquerading as real words and fixing bad segmentation (tokens that are glued together or improperly separated). Testing how performant are various methods for dealing with these problems in Polish language is left for future research.",,,,,,,,,,,,,,,,,,,,,,,,,,,What is the difference in performance between the interpretable system (e.g. vectors and cosine distance) and LSTM with ELMo system?,44104668796a6ca10e2ea3ecf706541da1cec2cf,five,,,,e8b24c3133e0bec0a6465e1f13acd3a5ed816b37,False,,Accuracy of best interpretible system was 0.3945 while accuracy of LSTM-ELMo net was 0.6818.,"The experimental results are presented in Table TABREF4 . Diacritic swapping showed a remarkably poor performance, despite promising mentions in existing literature. This might be explained by the already mentioned feature of Wikipedia edits, which can be expected to be to some degree self-reviewed before submission. This can very well limit the number of most trivial mistakes. FLOAT SELECTED: Table 1: Test results for all the methods used. The loss measure is cross-entropy.",The experimental results are presented in Table TABREF4 . FLOAT SELECTED: Table 1: Test results for all the methods used. The loss measure is cross-entropy.,91f989a06bf11f012960b7cdad07de1c33d7d969,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,What solutions are proposed for error detection and context awareness?,bbcd77aac74989f820e84488c52f3767d0405d51,five,,,,e8b24c3133e0bec0a6465e1f13acd3a5ed816b37,True,,,,,645cb2f15db2bd0a712c0159a71fd64f152c98d3,258ee4069f740c400c0049a2580945a1cc7f044c,How is PIEWi annotated?,6a31bd676054222faf46229fc1d283322478a020,five,,,,e8b24c3133e0bec0a6465e1f13acd3a5ed816b37,False,,,"PlEWi BIBREF20 is an early version of WikEd BIBREF21 error corpus, containing error type annotations allowing us to select only non-word errors for evaluation. Specifically, PlEWi supplied 550,755 [error, correction] pairs, from which 298,715 were unique. The corpus contains data extracted from histories of page versions of Polish Wikipedia. An algorithm designed by the corpus author determined where the changes were correcting spelling errors, as opposed to expanding content and disagreements among Wikipedia editors.","Specifically, PlEWi supplied 550,755 [error, correction] pairs, from which 298,715 were unique.",1afa01b50f65043288ee2dc5ca7f521c49bf4694,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,What methods are tested in PIEWi?,e4d16050f0b457c93e590261732a20401def9cde,five,,,,e8b24c3133e0bec0a6465e1f13acd3a5ed816b37,False,,,"The methods that we evaluated are baselines are the ones we consider to be basic and with moderate potential of yielding particularly good results. Probably the most straightforward approach to error correction is selecting known words from a dictionary that are within the smallest edit distance from the error. We used the Levenshtein distance metric BIBREF8 implemented in Apache Lucene library BIBREF9 . It is a version of edit distance that treats deletions, insertions and replacements as adding one unit distance, without giving a special treatment to character swaps. The SGJP – Grammatical Dictionary of Polish BIBREF10 was used as the reference vocabulary. Another simple approach is the aforementioned diacritical swapping, which is a term that we introduce here for referring to a solution inspired by the work of BIBREF4 . Namely, from the incorrect form we try to produce all strings obtainable by either adding or removing diacritical marks from characters. We then exclude options that are not present in SGJP, and select as the correction the one within the smallest edit distance from the error. It is possible for the number of such diacritically-swapped options to become very big. For example, the token Modlin-Zegrze-Pultusk-Różan-Ostrołęka-Łomża-Osowiec (taken from PlEWi corpus of spelling errors, see below) can yield over INLINEFORM0 states with this method, such as Módłiń-Żęgrzę-Pułtuśk-Roźąń-Óśtróleką-Lómzą-Óśówięć. The actual correction here is just fixing the ł in Pułtusk. Hence we only try to correct in this way tokens that are shorter than 17 characters. A promising method, adapted from work on correcting texts by English language learners BIBREF11 , expands on the concept of selecting a correction nearest to the spelling error according to some notion of distance. Here, the Levenshtein distance is used in a weighted sum to cosine distance between word vectors. This is based on the observation that trained vectors models of distributional semantics contain also representations of spelling errors, if they were not pruned. Their representations tend to be similar to those of their correct counterparts. For example, the token enginir will appear in similar contexts as engineer, and therefore will be assigned a similar vector embedding. (applied cellwise) in order to obtain the initial setting of parameters for the main LSTM. Our ELMo-augmented LSTM is bidirectional.","We used the Levenshtein distance metric BIBREF8 implemented in Apache Lucene library BIBREF9 . Another simple approach is the aforementioned diacritical swapping, which is a term that we introduce here for referring to a solution inspired by the work of BIBREF4 . A promising method, adapted from work on correcting texts by English language learners BIBREF11 , expands on the concept of selecting a correction nearest to the spelling error according to some notion of distance. Here, the Levenshtein distance is used in a weighted sum to cosine distance between word vectors. Our ELMo-augmented LSTM is bidirectional.",abc39352a914939a293c4c3a9ea06fc6ee432add,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,Which specific error correction solutions have been proposed for specialized corpora in the past?,b25e7137f49f77e7e67ee2f40ca585d3a377f8b5,five,,,,e8b24c3133e0bec0a6465e1f13acd3a5ed816b37,False,"spellchecking mammography reports and tweets BIBREF7 , BIBREF4",,,"Published work on language correction for Polish dates back at least to 1970s, when simplest Levenshtein distance solutions were used for cleaning mainframe inputs BIBREF5 , BIBREF6 . Spelling correction tests described in literature have tended to focus on one approach applied to a specific corpus. Limited examples include works on spellchecking mammography reports and tweets BIBREF7 , BIBREF4 . These works emphasized the importance of tailoring correction systems to specific problems of corpora they are applied to. For example, mammography reports suffer from poor typing, which in this case is a repetitive work done in relative hurry. Tweets, on the other hand, tend to contain emoticons and neologisms that can trick solutions based on rules and dictionaries, such as LanguageTool. The latter is, by itself, fairly well suited for Polish texts, since a number of extensions to the structure of this application was inspired by problems with morphology of Polish language BIBREF3 .","Spelling correction tests described in literature have tended to focus on one approach applied to a specific corpus. Limited examples include works on spellchecking mammography reports and tweets BIBREF7 , BIBREF4 . These works emphasized the importance of tailoring correction systems to specific problems of corpora they are applied to. For example, mammography reports suffer from poor typing, which in this case is a repetitive work done in relative hurry. Tweets, on the other hand, tend to contain emoticons and neologisms that can trick solutions based on rules and dictionaries, such as LanguageTool. The latter is, by itself, fairly well suited for Polish texts, since a number of extensions to the structure of this application was inspired by problems with morphology of Polish language BIBREF3 .",b632e06c7bb1119cf80527670e985d1f07f6e97d,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3-Table1-1.png,Table 1: Test results for all the methods used. The loss measure is cross-entropy.,3-Table2-1.png,Table 2: Discovered optimal weights for summing layers of ELMo embedding for initializing an error-correcting LSTM. The layers are numbered from the one that directly processes character and word input to the most abstract one.,,,,,,,,,,,,,,,,,,"[error, correction] pairs",,,,,,,,,,,,,,,,,,,,,Levenshtein distance metric BIBREF8 diacritical swapping Levenshtein distance is used in a weighted sum to cosine distance between word vectors ELMo-augmented LSTM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Community Identity and User Engagement in a Multi-Community Landscape,"A community's identity defines and shapes its internal dynamics. Our current understanding of this interplay is mostly limited to glimpses gathered from isolated studies of individual communities. In this work we provide a systematic exploration of the nature of this relation across a wide variety of online communities. To this end we introduce a quantitative, language-based typology reflecting two key aspects of a community's identity: how distinctive, and how temporally dynamic it is. By mapping almost 300 Reddit communities into the landscape induced by this typology, we reveal regularities in how patterns of user engagement vary with the characteristics of a community. Our results suggest that the way new and existing users engage with a community depends strongly and systematically on the nature of the collective identity it fosters, in ways that are highly consequential to community maintainers. For example, communities with distinctive and highly dynamic identities are more likely to retain their users. However, such niche communities also exhibit much larger acculturation gaps between existing users and newcomers, which potentially hinder the integration of the latter. More generally, our methodology reveals differences in how various social phenomena manifest across communities, and shows that structuring the multi-community landscape can lead to a better understanding of the systematic nature of this diversity.",Introduction,"“If each city is like a game of chess, the day when I have learned the rules, I shall finally possess my empire, even if I shall never succeed in knowing all the cities it contains.”  — Italo Calvino, Invisible Cities A community's identity—defined through the common interests and shared experiences of its users—shapes various facets of the social dynamics within it BIBREF0 , BIBREF1 , BIBREF2 . Numerous instances of this interplay between a community's identity and social dynamics have been extensively studied in the context of individual online communities BIBREF3 , BIBREF4 , BIBREF5 . However, the sheer variety of online platforms complicates the task of generalizing insights beyond these isolated, single-community glimpses. A new way to reason about the variation across multiple communities is needed in order to systematically characterize the relationship between properties of a community and the dynamics taking place within. One especially important component of community dynamics is user engagement. We can aim to understand why users join certain communities BIBREF6 , what factors influence user retention BIBREF7 , and how users react to innovation BIBREF5 . While striking patterns of user engagement have been uncovered in prior case studies of individual communities BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 , BIBREF12 , we do not know whether these observations hold beyond these cases, or when we can draw analogies between different communities. Are there certain types of communities where we can expect similar or contrasting engagement patterns? To address such questions quantitatively we need to provide structure to the diverse and complex space of online communities. Organizing the multi-community landscape would allow us to both characterize individual points within this space, and reason about systematic variations in patterns of user engagement across the space. Present work: Structuring the multi-community space. In order to systematically understand the relationship between community identityand user engagement we introduce a quantitative typology of online communities. Our typology is based on two key aspects of community identity: how distinctive—or niche—a community's interests are relative to other communities, and how dynamic—or volatile—these interests are over time. These axes aim to capture the salience of a community's identity and dynamics of its temporal evolution. Our main insight in implementing this typology automatically and at scale is that the language used within a community can simultaneously capture how distinctive and dynamic its interests are. This language-based approach draws on a wealth of literature characterizing linguistic variation in online communities and its relationship to community and user identity BIBREF16 , BIBREF5 , BIBREF17 , BIBREF18 , BIBREF19 . Basing our typology on language is also convenient since it renders our framework immediately applicable to a wide variety of online communities, where communication is primarily recorded in a textual format. Using our framework, we map almost 300 Reddit communities onto the landscape defined by the two axes of our typology (Section SECREF2 ). We find that this mapping induces conceptually sound categorizations that effectively capture key aspects of community-level social dynamics. In particular, we quantitatively validate the effectiveness of our mapping by showing that our two-dimensional typology encodes signals that are predictive of community-level rates of user retention, complementing strong activity-based features. Engagement and community identity. We apply our framework to understand how two important aspects of user engagement in a community—the community's propensity to retain its users (Section SECREF3 ), and its permeability to new members (Section SECREF4 )—vary according to the type of collective identity it fosters. We find that communities that are characterized by specialized, constantly-updating content have higher user retention rates, but also exhibit larger linguistic gaps that separate newcomers from established members. More closely examining factors that could contribute to this linguistic gap, we find that especially within distinctive communities, established users have an increased propensity to engage with the community's specialized content, compared to newcomers (Section SECREF5 ). Interestingly, while established members of distinctive communities more avidly respond to temporal updates than newcomers, in more generic communities it is the outsiders who engage more with volatile content, perhaps suggesting that such content may serve as an entry-point to the community (but not necessarily a reason to stay). Such insights into the relation between collective identity and user engagement can be informative to community maintainers seeking to better understand growth patterns within their online communities. More generally, our methodology stands as an example of how sociological questions can be addressed in a multi-community setting. In performing our analyses across a rich variety of communities, we reveal both the diversity of phenomena that can occur, as well as the systematic nature of this diversity.",A typology of community identity,"A community's identity derives from its members' common interests and shared experiences BIBREF15 , BIBREF20 . In this work, we structure the multi-community landscape along these two key dimensions of community identity: how distinctive a community's interests are, and how dynamic the community is over time. We now proceed to outline our quantitative typology, which maps communities along these two dimensions. We start by providing an intuition through inspecting a few example communities. We then introduce a generalizable language-based methodology and use it to map a large set of Reddit communities onto the landscape defined by our typology of community identity.",Overview and intuition,"In order to illustrate the diversity within the multi-community space, and to provide an intuition for the underlying structure captured by the proposed typology, we first examine a few example communities and draw attention to some key social dynamics that occur within them. We consider four communities from Reddit: in Seahawks, fans of the Seahawks football team gather to discuss games and players; in BabyBumps, expecting mothers trade advice and updates on their pregnancy; Cooking consists of recipe ideas and general discussion about cooking; while in pics, users share various images of random things (like eels and hornets). We note that these communities are topically contrasting and foster fairly disjoint user bases. Additionally, these communities exhibit varied patterns of user engagement. While Seahawks maintains a devoted set of users from month to month, pics is dominated by transient users who post a few times and then depart. Discussions within these communities also span varied sets of interests. Some of these interests are more specific to the community than others: risotto, for example, is seldom a discussion point beyond Cooking. Additionally, some interests consistently recur, while others are specific to a particular time: kitchens are a consistent focus point for cooking, but mint is only in season during spring. Coupling specificity and consistency we find interests such as easter, which isn't particularly specific to BabyBumps but gains prominence in that community around Easter (see Figure FIGREF3 .A for further examples). These specific interests provide a window into the nature of the communities' interests as a whole, and by extension their community identities. Overall, discussions in Cooking focus on topics which are highly distinctive and consistently recur (like risotto). In contrast, discussions in Seahawks are highly dynamic, rapidly shifting over time as new games occur and players are traded in and out. In the remainder of this section we formally introduce a methodology for mapping communities in this space defined by their distinctiveness and dynamicity (examples in Figure FIGREF3 .B).",Language-based formalization,"Our approach follows the intuition that a distinctive community will use language that is particularly specific, or unique, to that community. Similarly, a dynamic community will use volatile language that rapidly changes across successive windows of time. To capture this intuition automatically, we start by defining word-level measures of specificity and volatility. We then extend these word-level primitives to characterize entire comments, and the community itself. Our characterizations of words in a community are motivated by methodology from prior literature that compares the frequency of a word in a particular setting to its frequency in some background distribution, in order to identify instances of linguistic variation BIBREF21 , BIBREF19 . Our particular framework makes this comparison by way of pointwise mutual information (PMI). In the following, we use INLINEFORM0 to denote one community within a set INLINEFORM1 of communities, and INLINEFORM2 to denote one time period within the entire history INLINEFORM3 of INLINEFORM4 . We account for temporal as well as inter-community variation by computing word-level measures for each time period of each community's history, INLINEFORM5 . Given a word INLINEFORM6 used within a particular community INLINEFORM7 at time INLINEFORM8 , we define two word-level measures: Specificity. We quantify the specificity INLINEFORM0 of INLINEFORM1 to INLINEFORM2 by calculating the PMI of INLINEFORM3 and INLINEFORM4 , relative to INLINEFORM5 , INLINEFORM6  where INLINEFORM0 is INLINEFORM1 's frequency in INLINEFORM2 . INLINEFORM3 is specific to INLINEFORM4 if it occurs more frequently in INLINEFORM5 than in the entire set INLINEFORM6 , hence distinguishing this community from the rest. A word INLINEFORM7 whose occurrence is decoupled from INLINEFORM8 , and thus has INLINEFORM9 close to 0, is said to be generic. We compute values of INLINEFORM0 for each time period INLINEFORM1 in INLINEFORM2 ; in the above description we drop the time-based subscripts for clarity. Volatility. We quantify the volatility INLINEFORM0 of INLINEFORM1 to INLINEFORM2 as the PMI of INLINEFORM3 and INLINEFORM4 relative to INLINEFORM5 , the entire history of INLINEFORM6 : INLINEFORM7  A word INLINEFORM0 is volatile at time INLINEFORM1 in INLINEFORM2 if it occurs more frequently at INLINEFORM3 than in the entire history INLINEFORM4 , behaving as a fad within a small window of time. A word that occurs with similar frequency across time, and hence has INLINEFORM5 close to 0, is said to be stable. Extending to utterances. Using our word-level primitives, we define the specificity of an utterance INLINEFORM0 in INLINEFORM1 , INLINEFORM2 as the average specificity of each word in the utterance. The volatility of utterances is defined analogously. ",Community-level measures,"Having described these word-level measures, we now proceed to establish the primary axes of our typology: Distinctiveness. A community with a very distinctive identity will tend to have distinctive interests, expressed through specialized language. Formally, we define the distinctiveness of a community INLINEFORM0 as the average specificity of all utterances in INLINEFORM1 . We refer to a community with a less distinctive identity as being generic. Dynamicity. A highly dynamic community constantly shifts interests from one time window to another, and these temporal variations are reflected in its use of volatile language. Formally, we define the dynamicity of a community INLINEFORM0 as the average volatility of all utterances in INLINEFORM1 . We refer to a community whose language is relatively consistent throughout time as being stable. In our subsequent analyses, we focus mostly on examing the average distinctiveness and dynamicity of a community over time, denoted INLINEFORM0 and INLINEFORM1 .",Applying the typology to Reddit,"We now explain how our typology can be applied to the particular setting of Reddit, and describe the overall behaviour of our linguistic axes in this context. Dataset description. Reddit is a popular website where users form and participate in discussion-based communities called subreddits. Within these communities, users post content—such as images, URLs, or questions—which often spark vibrant lengthy discussions in thread-based comment sections. The website contains many highly active subreddits with thousands of active subscribers. These communities span an extremely rich variety of topical interests, as represented by the examples described earlier. They also vary along a rich multitude of structural dimensions, such as the number of users, the amount of conversation and social interaction, and the social norms determining which types of content become popular. The diversity and scope of Reddit's multicommunity ecosystem make it an ideal landscape in which to closely examine the relation between varying community identities and social dynamics. Our full dataset consists of all subreddits on Reddit from January 2013 to December 2014, for which there are at least 500 words in the vocabulary used to estimate our measures, in at least 4 months of the subreddit's history. We compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk of the contributions are in a foreign language. This results in 283 communities ( INLINEFORM0 ), for a total of 4,872 community-months ( INLINEFORM1 ). Estimating linguistic measures. We estimate word frequencies INLINEFORM0 , and by extension each downstream measure, in a carefully controlled manner in order to ensure we capture robust and meaningful linguistic behaviour. First, we only consider top-level comments which are initial responses to a post, as the content of lower-level responses might reflect conventions of dialogue more than a community's high-level interests. Next, in order to prevent a few highly active users from dominating our frequency estimates, we count each unique word once per user, ignoring successive uses of the same word by the same user. This ensures that our word-level characterizations are not skewed by a small subset of highly active contributors. In our subsequent analyses, we will only look at these measures computed over the nouns used in comments. In principle, our framework can be applied to any choice of vocabulary. However, in the case of Reddit using nouns provides a convenient degree of interpretability. We can easily understand the implication of a community preferentially mentioning a noun such as gamer or feminist, but interpreting the overuse of verbs or function words such as take or of is less straightforward. Additionally, in focusing on nouns we adopt the view emphasized in modern “third wave” accounts of sociolinguistic variation, that stylistic variation is inseparable from topical content BIBREF23 . In the case of online communities, the choice of what people choose to talk about serves as a primary signal of social identity. That said, a typology based on more purely stylistic differences is an interesting avenue for future work. Accounting for rare words. One complication when using measures such as PMI, which are based off of ratios of frequencies, is that estimates for very infrequent words could be overemphasized BIBREF24 . Words that only appear a few times in a community tend to score at the extreme ends of our measures (e.g. as highly specific or highly generic), obfuscating the impact of more frequent words in the community. To address this issue, we discard the long tail of infrequent words in our analyses, using only the top 5th percentile of words, by frequency within each INLINEFORM0 , to score comments and communities. Typology output on Reddit. The distribution of INLINEFORM0 and INLINEFORM1 across Reddit communities is shown in Figure FIGREF3 .B, along with examples of communities at the extremes of our typology. We find that interpretable groupings of communities emerge at various points within our axes. For instance, highly distinctive and dynamic communities tend to focus on rapidly-updating interests like sports teams and games, while generic and consistent communities tend to be large “link-sharing” hubs where users generally post content with no clear dominating themes. More examples of communities at the extremes of our typology are shown in Table TABREF9 . We note that these groupings capture abstract properties of a community's content that go beyond its topic. For instance, our typology relates topically contrasting communities such as yugioh (which is about a popular trading card game) and Seahawks through the shared trait that their content is particularly distinctive. Additionally, the axes can clarify differences between topically similar communities: while startrek and thewalkingdead both focus on TV shows, startrek is less dynamic than the median community, while thewalkingdead is among the most dynamic communities, as the show was still airing during the years considered.",Community identity and user retention,"We have seen that our typology produces qualitatively satisfying groupings of communities according to the nature of their collective identity. This section shows that there is an informative and highly predictive relationship between a community's position in this typology and its user engagement patterns. We find that communities with distinctive and dynamic identities have higher rates of user engagement, and further show that a community's position in our identity-based landscape holds important predictive information that is complementary to a strong activity baseline. In particular user retention is one of the most crucial aspects of engagement and is critical to community maintenance BIBREF2 . We quantify how successful communities are at retaining users in terms of both short and long-term commitment. Our results indicate that rates of user retention vary drastically, yet systematically according to how distinctive and dynamic a community is (Figure FIGREF3 ). We find a strong, explanatory relationship between the temporal consistency of a community's identity and rates of user engagement: dynamic communities that continually update and renew their discussion content tend to have far higher rates of user engagement. The relationship between distinctiveness and engagement is less universal, but still highly informative: niche communities tend to engender strong, focused interest from users at one particular point in time, though this does not necessarily translate into long-term retention.",Community-type and monthly retention,"We find that dynamic communities, such as Seahawks or starcraft, have substantially higher rates of monthly user retention than more stable communities (Spearman's INLINEFORM0 = 0.70, INLINEFORM1 0.001, computed with community points averaged over months; Figure FIGREF11 .A, left). Similarly, more distinctive communities, like Cooking and Naruto, exhibit moderately higher monthly retention rates than more generic communities (Spearman's INLINEFORM2 = 0.33, INLINEFORM3 0.001; Figure FIGREF11 .A, right). Monthly retention is formally defined as the proportion of users who contribute in month INLINEFORM0 and then return to contribute again in month INLINEFORM1 . Each monthly datapoint is treated as unique and the trends in Figure FIGREF11 show 95% bootstrapped confidence intervals, cluster-resampled at the level of subreddit BIBREF25 , to account for differences in the number of months each subreddit contributes to the data. Importantly, we find that in the task of predicting community-level user retention our identity-based typology holds additional predictive value on top of strong baseline features based on community-size (# contributing users) and activity levels (mean # contributions per user), which are commonly used for churn prediction BIBREF7 . We compared out-of-sample predictive performance via leave-one-community-out cross validation using random forest regressors with ensembles of size 100, and otherwise default hyperparameters BIBREF26 . A model predicting average monthly retention based on a community's average distinctiveness and dynamicity achieves an average mean squared error ( INLINEFORM0 ) of INLINEFORM1 and INLINEFORM2 , while an analogous model predicting based on a community's size and average activity level (both log-transformed) achieves INLINEFORM4 and INLINEFORM5 . The difference between the two models is not statistically significant ( INLINEFORM6 , Wilcoxon signed-rank test). However, combining features from both models results in a large and statistically significant improvement over each independent model ( INLINEFORM7 , INLINEFORM8 , INLINEFORM9 Bonferroni-corrected pairwise Wilcoxon tests). These results indicate that our typology can explain variance in community-level retention rates, and provides information beyond what is present in standard activity-based features.",Community-type and user tenure,"As with monthly retention, we find a strong positive relationship between a community's dynamicity and the average number of months that a user will stay in that community (Spearman's INLINEFORM0 = 0.41, INLINEFORM1 0.001, computed over all community points; Figure FIGREF11 .B, left). This verifies that the short-term trend observed for monthly retention translates into longer-term engagement and suggests that long-term user retention might be strongly driven by the extent to which a community continually provides novel content. Interestingly, there is no significant relationship between distinctiveness and long-term engagement (Spearman's INLINEFORM2 = 0.03, INLINEFORM3 0.77; Figure FIGREF11 .B, right). Thus, while highly distinctive communities like RandomActsOfMakeup may generate focused commitment from users over a short period of time, such communities are unlikely to retain long-term users unless they also have sufficiently dynamic content. To measure user tenures we focused on one slice of data (May, 2013) and measured how many months a user spends in each community, on average—the average number of months between a user's first and last comment in each community. We have activity data up until May 2015, so the maximum tenure is 24 months in this set-up, which is exceptionally long relative to the average community member (throughout our entire data less than INLINEFORM0 of users have tenures of more than 24 months in any community).",Community identity and acculturation,"The previous section shows that there is a strong connection between the nature of a community's identity and its basic user engagement patterns. In this section, we probe the relationship between a community's identity and how permeable, or accessible, it is to outsiders. We measure this phenomenon using what we call the acculturation gap, which compares the extent to which engaged vs. non-engaged users employ community-specific language. While previous work has found this gap to be large and predictive of future user engagement in two beer-review communities BIBREF5 , we find that the size of the acculturation gap depends strongly on the nature of a community's identity, with the gap being most pronounced in stable, highly distinctive communities (Figure FIGREF13 ). This finding has important implications for our understanding of online communities. Though many works have analyzed the dynamics of “linguistic belonging” in online communities BIBREF16 , BIBREF28 , BIBREF5 , BIBREF17 , our results suggest that the process of linguistically fitting in is highly contingent on the nature of a community's identity. At one extreme, in generic communities like pics or worldnews there is no distinctive, linguistic identity for users to adopt. To measure the acculturation gap for a community, we follow Danescu-Niculescu-Mizil et al danescu-niculescu-mizilno2013 and build “snapshot language models” (SLMs) for each community, which capture the linguistic state of a community at one point of time. Using these language models we can capture how linguistically close a particular utterance is to the community by measuring the cross-entropy of this utterance relative to the SLM: DISPLAYFORM0  where INLINEFORM0 is the probability assigned to bigram INLINEFORM1 from comment INLINEFORM2 in community-month INLINEFORM3 . We build the SLMs by randomly sampling 200 active users—defined as users with at least 5 comments in the respective community and month. For each of these 200 active users we select 5 random 10-word spans from 5 unique comments. To ensure robustness and maximize data efficiency, we construct 100 SLMs for each community-month pair that has enough data, bootstrap-resampling from the set of active users. We compute a basic measure of the acculturation gap for a community-month INLINEFORM0 as the relative difference of the cross-entropy of comments by users active in INLINEFORM1 with that of singleton comments by outsiders—i.e., users who only ever commented once in INLINEFORM2 , but who are still active in Reddit in general: DISPLAYFORM0   INLINEFORM0 denotes the distribution over singleton comments, INLINEFORM1 denotes the distribution over comments from users active in INLINEFORM2 , and INLINEFORM3 the expected values of the cross-entropy over these respective distributions. For each bootstrap-sampled SLM we compute the cross-entropy of 50 comments by active users (10 comments from 5 randomly sampled active users, who were not used to construct the SLM) and 50 comments from randomly-sampled outsiders. Figure FIGREF13 .A shows that the acculturation gap varies substantially with how distinctive and dynamic a community is. Highly distinctive communities have far higher acculturation gaps, while dynamicity exhibits a non-linear relationship: relatively stable communities have a higher linguistic `entry barrier', as do very dynamic ones. Thus, in communities like IAmA (a general Q&A forum) that are very generic, with content that is highly, but not extremely dynamic, outsiders are at no disadvantage in matching the community's language. In contrast, the acculturation gap is large in stable, distinctive communities like Cooking that have consistent community-specific language. The gap is also large in extremely dynamic communities like Seahawks, which perhaps require more attention or interest on the part of active users to keep up-to-date with recent trends in content. These results show that phenomena like the acculturation gap, which were previously observed in individual communities BIBREF28 , BIBREF5 , cannot be easily generalized to a larger, heterogeneous set of communities. At the same time, we see that structuring the space of possible communities enables us to observe systematic patterns in how such phenomena vary.",Community identity and content affinity,"Through the acculturation gap, we have shown that communities exhibit large yet systematic variations in their permeability to outsiders. We now turn to understanding the divide in commenting behaviour between outsiders and active community members at a finer granularity, by focusing on two particular ways in which such gaps might manifest among users: through different levels of engagement with specific content and with temporally volatile content. Echoing previous results, we find that community type mediates the extent and nature of the divide in content affinity. While in distinctive communities active members have a higher affinity for both community-specific content and for highly volatile content, the opposite is true for generic communities, where it is the outsiders who engage more with volatile content. We quantify these divides in content affinity by measuring differences in the language of the comments written by active users and outsiders. Concretely, for each community INLINEFORM0 , we define the specificity gap INLINEFORM1 as the relative difference between the average specificity of comments authored by active members, and by outsiders, where these measures are macroaveraged over users. Large, positive INLINEFORM2 then occur in communities where active users tend to engage with substantially more community-specific content than outsiders. We analogously define the volatility gap INLINEFORM0 as the relative difference in volatilities of active member and outsider comments. Large, positive values of INLINEFORM1 characterize communities where active users tend to have more volatile interests than outsiders, while negative values indicate communities where active users tend to have more stable interests. We find that in 94% of communities, INLINEFORM0 , indicating (somewhat unsurprisingly) that in almost all communities, active users tend to engage with more community-specific content than outsiders. However, the magnitude of this divide can vary greatly: for instance, in Homebrewing, which is dedicated to brewing beer, the divide is very pronounced ( INLINEFORM1 0.33) compared to funny, a large hub where users share humorous content ( INLINEFORM2 0.011). The nature of the volatility gap is comparatively more varied. In Homebrewing ( INLINEFORM0 0.16), as in 68% of communities, active users tend to write more volatile comments than outsiders ( INLINEFORM1 0). However, communities like funny ( INLINEFORM2 -0.16), where active users contribute relatively stable comments compared to outsiders ( INLINEFORM3 0), are also well-represented on Reddit. To understand whether these variations manifest systematically across communities, we examine the relationship between divides in content affinity and community type. In particular, following the intuition that active users have a relatively high affinity for a community's niche, we expect that the distinctiveness of a community will be a salient mediator of specificity and volatility gaps. Indeed, we find a strong correlation between a community's distinctiveness and its specificity gap (Spearman's INLINEFORM0 0.34, INLINEFORM1 0.001). We also find a strong correlation between distinctiveness and community volatility gaps (Spearman's INLINEFORM0 0.53, INLINEFORM1 0.001). In particular, we see that among the most distinctive communities (i.e., the top third of communities by distinctiveness), active users tend to write more volatile comments than outsiders (mean INLINEFORM2 0.098), while across the most generic communities (i.e., the bottom third), active users tend to write more stable comments (mean INLINEFORM3 -0.047, Mann-Whitney U test INLINEFORM4 0.001). The relative affinity of outsiders for volatile content in these communities indicates that temporally ephemeral content might serve as an entry point into such a community, without necessarily engaging users in the long term.",Further related work,"Our language-based typology and analysis of user engagement draws on and contributes to several distinct research threads, in addition to the many foundational studies cited in the previous sections. Multicommunity studies. Our investigation of user engagement in multicommunity settings follows prior literature which has examined differences in user and community dynamics across various online groups, such as email listservs. Such studies have primarily related variations in user behaviour to structural features such as group size and volume of content BIBREF30 , BIBREF31 , BIBREF32 , BIBREF33 . In focusing on the linguistic content of communities, we extend this research by providing a content-based framework through which user engagement can be examined. Reddit has been a particularly useful setting for studying multiple communities in prior work. Such studies have mostly focused on characterizing how individual users engage across a multi-community platform BIBREF34 , BIBREF35 , or on specific user engagement patterns such as loyalty to particular communities BIBREF22 . We complement these studies by seeking to understand how features of communities can mediate a broad array of user engagement patterns within them. Typologies of online communities. Prior attempts to typologize online communities have primarily been qualitative and based on hand-designed categories, making them difficult to apply at scale. These typologies often hinge on having some well-defined function the community serves, such as supporting a business or non-profit cause BIBREF36 , which can be difficult or impossible to identify in massive, anonymous multi-community settings. Other typologies emphasize differences in communication platforms and other functional requirements BIBREF37 , BIBREF38 , which are important but preclude analyzing differences between communities within the same multi-community platform. Similarly, previous computational methods of characterizing multiple communities have relied on the presence of markers such as affixes in community names BIBREF35 , or platform-specific affordances such as evaluation mechanisms BIBREF39 . Our typology is also distinguished from community detection techniques that rely on structural or functional categorizations BIBREF40 , BIBREF41 . While the focus of those studies is to identify and characterize sub-communities within a larger social network, our typology provides a characterization of pre-defined communities based on the nature of their identity. Broader work on collective identity. Our focus on community identity dovetails with a long line of research on collective identity and user engagement, in both online and offline communities BIBREF42 , BIBREF1 , BIBREF2 . These studies focus on individual-level psychological manifestations of collective (or social) identity, and their relationship to user engagement BIBREF42 , BIBREF43 , BIBREF44 , BIBREF0 . In contrast, we seek to characterize community identities at an aggregate level and in an interpretable manner, with the goal of systematically organizing the diverse space of online communities. Typologies of this kind are critical to these broader, social-psychological studies of collective identity: they allow researchers to systematically analyze how the psychological manifestations and implications of collective identity vary across diverse sets of communities.",Conclusion and future work,"Our current understanding of engagement patterns in online communities is patched up from glimpses offered by several disparate studies focusing on a few individual communities. This work calls into attention the need for a method to systematically reason about similarities and differences across communities. By proposing a way to structure the multi-community space, we find not only that radically contrasting engagement patterns emerge in different parts of this space, but also that this variation can be at least partly explained by the type of identity each community fosters. Our choice in this work is to structure the multi-community space according to a typology based on community identity, as reflected in language use. We show that this effectively explains cross-community variation of three different user engagement measures—retention, acculturation and content affinity—and complements measures based on activity and size with additional interpretable information. For example, we find that in niche communities established members are more likely to engage with volatile content than outsiders, while the opposite is true in generic communities. Such insights can be useful for community maintainers seeking to understand engagement patterns in their own communities. One main area of future research is to examine the temporal dynamics in the multi-community landscape. By averaging our measures of distinctiveness and dynamicity across time, our present study treated community identity as a static property. However, as communities experience internal changes and respond to external events, we can expect the nature of their identity to shift as well. For instance, the relative consistency of harrypotter may be disrupted by the release of a new novel, while Seahawks may foster different identities during and between football seasons. Conversely, a community's type may also mediate the impact of new events. Moving beyond a static view of community identity could enable us to better understand how temporal phenomena such as linguistic change manifest across different communities, and also provide a more nuanced view of user engagement—for instance, are communities more welcoming to newcomers at certain points in their lifecycle? Another important avenue of future work is to explore other ways of mapping the landscape of online communities. For example, combining structural properties of communities BIBREF40 with topical information BIBREF35 and with our identity-based measures could further characterize and explain variations in user engagement patterns. Furthermore, extending the present analyses to even more diverse communities supported by different platforms (e.g., GitHub, StackExchange, Wikipedia) could enable the characterization of more complex behavioral patterns such as collaboration and altruism, which become salient in different multicommunity landscapes.",Acknowledgements,"The authors thank Liye Fu, Jack Hessel, David Jurgens and Lillian Lee for their helpful comments. This research has been supported in part by a Discovery and Innovation Research Seed Award from the Office of the Vice Provost for Research at Cornell, NSF CNS-1010921, IIS-1149837, IIS-1514268 NIH BD2K, ARO MURI, DARPA XDATA, DARPA SIMPLEX, DARPA NGS2, Stanford Data Science Initiative, SAP Stanford Graduate Fellowship, NSERC PGS-D, Boeing, Lightspeed, and Volkswagen. ",,,,,,,,,,,,,,,Do they report results only on English data?,003f884d3893532f8c302431c9f70be6f64d9be8,five,,,,e8b24c3133e0bec0a6465e1f13acd3a5ed816b37,False,False,,"Our full dataset consists of all subreddits on Reddit from January 2013 to December 2014, for which there are at least 500 words in the vocabulary used to estimate our measures, in at least 4 months of the subreddit's history. We compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk of the contributions are in a foreign language. This results in 283 communities ( INLINEFORM0 ), for a total of 4,872 community-months ( INLINEFORM1 ).","We compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk of the contributions are in a foreign language. ",04ae0cc420f69540ca11707ab8ecc07a89f803f7,c1018a31c3272ce74964a3280069f62f314a1a58,True,,,,,,31d8f8ed7ba40b27c480f7caf7cfb48fba47bb07,34c35a1877e453ecaebcf625df3ef788e1953cc4,How do the various social phenomena examined manifest in different types of communities?,bb97537a0a7c8f12a3f65eba73cefa6abcd2f2b2,five,,,,e8b24c3133e0bec0a6465e1f13acd3a5ed816b37,False,,"Dynamic communities have substantially higher rates of monthly user retention than more stable communities. More distinctive communities exhibit moderately higher monthly retention rates than more generic communities. There is also a strong positive relationship between a community's dynamicity and the average number of months that a user will stay in that community - a short-term trend observed for monthly retention translates into longer-term engagement and suggests that long-term user retention might be strongly driven by the extent to which a community continually provides novel content.
","We find that dynamic communities, such as Seahawks or starcraft, have substantially higher rates of monthly user retention than more stable communities (Spearman's INLINEFORM0 = 0.70, INLINEFORM1 0.001, computed with community points averaged over months; Figure FIGREF11 .A, left). Similarly, more distinctive communities, like Cooking and Naruto, exhibit moderately higher monthly retention rates than more generic communities (Spearman's INLINEFORM2 = 0.33, INLINEFORM3 0.001; Figure FIGREF11 .A, right). As with monthly retention, we find a strong positive relationship between a community's dynamicity and the average number of months that a user will stay in that community (Spearman's INLINEFORM0 = 0.41, INLINEFORM1 0.001, computed over all community points; Figure FIGREF11 .B, left). This verifies that the short-term trend observed for monthly retention translates into longer-term engagement and suggests that long-term user retention might be strongly driven by the extent to which a community continually provides novel content. Interestingly, there is no significant relationship between distinctiveness and long-term engagement (Spearman's INLINEFORM2 = 0.03, INLINEFORM3 0.77; Figure FIGREF11 .B, right). Thus, while highly distinctive communities like RandomActsOfMakeup may generate focused commitment from users over a short period of time, such communities are unlikely to retain long-term users unless they also have sufficiently dynamic content.","We find that dynamic communities, such as Seahawks or starcraft, have substantially higher rates of monthly user retention than more stable communities (Spearman's INLINEFORM0 = 0.70, INLINEFORM1 0.001, computed with community points averaged over months; Figure FIGREF11 .A, left). Similarly, more distinctive communities, like Cooking and Naruto, exhibit moderately higher monthly retention rates than more generic communities (Spearman's INLINEFORM2 = 0.33, INLINEFORM3 0.001; Figure FIGREF11 .A, right). As with monthly retention, we find a strong positive relationship between a community's dynamicity and the average number of months that a user will stay in that community (Spearman's INLINEFORM0 = 0.41, INLINEFORM1 0.001, computed over all community points; Figure FIGREF11 .B, left). This verifies that the short-term trend observed for monthly retention translates into longer-term engagement and suggests that long-term user retention might be strongly driven by the extent to which a community continually provides novel content. Interestingly, there is no significant relationship between distinctiveness and long-term engagement (Spearman's INLINEFORM2 = 0.03, INLINEFORM3 0.77; Figure FIGREF11 .B, right). Thus, while highly distinctive communities like RandomActsOfMakeup may generate focused commitment from users over a short period of time, such communities are unlikely to retain long-term users unless they also have sufficiently dynamic content.",8a080f37fbbb5c6700422a346b944ef535fa725b,34c35a1877e453ecaebcf625df3ef788e1953cc4,What patterns do they observe about how user engagement varies with the characteristics of a community?,eea089baedc0ce80731c8fdcb064b82f584f483a,five,,,,e8b24c3133e0bec0a6465e1f13acd3a5ed816b37,False,,,"Engagement and community identity. We apply our framework to understand how two important aspects of user engagement in a community—the community's propensity to retain its users (Section SECREF3 ), and its permeability to new members (Section SECREF4 )—vary according to the type of collective identity it fosters. We find that communities that are characterized by specialized, constantly-updating content have higher user retention rates, but also exhibit larger linguistic gaps that separate newcomers from established members. More closely examining factors that could contribute to this linguistic gap, we find that especially within distinctive communities, established users have an increased propensity to engage with the community's specialized content, compared to newcomers (Section SECREF5 ). Interestingly, while established members of distinctive communities more avidly respond to temporal updates than newcomers, in more generic communities it is the outsiders who engage more with volatile content, perhaps suggesting that such content may serve as an entry-point to the community (but not necessarily a reason to stay). Such insights into the relation between collective identity and user engagement can be informative to community maintainers seeking to better understand growth patterns within their online communities.","We find that communities that are characterized by specialized, constantly-updating content have higher user retention rates, but also exhibit larger linguistic gaps that separate newcomers from established members. More closely examining factors that could contribute to this linguistic gap, we find that especially within distinctive communities, established users have an increased propensity to engage with the community's specialized content, compared to newcomers (Section SECREF5 ). ",f64ff06cfd16f9bd339512a6e85f0a7bc8b670f4,34c35a1877e453ecaebcf625df3ef788e1953cc4,,,,,,,,How did the select the 300 Reddit communities for comparison?,edb2d24d6d10af13931b3a47a6543bd469752f0c,five,,,,e8b24c3133e0bec0a6465e1f13acd3a5ed816b37,False,,They selected all the subreddits from January 2013 to December 2014 with at least 500 words in the vocabulary and at least 4 months of the subreddit's history. They also removed communities with the bulk of the contributions are in foreign language.,"Our full dataset consists of all subreddits on Reddit from January 2013 to December 2014, for which there are at least 500 words in the vocabulary used to estimate our measures, in at least 4 months of the subreddit's history. We compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk of the contributions are in a foreign language. This results in 283 communities ( INLINEFORM0 ), for a total of 4,872 community-months ( INLINEFORM1 ).","Our full dataset consists of all subreddits on Reddit from January 2013 to December 2014, for which there are at least 500 words in the vocabulary used to estimate our measures, in at least 4 months of the subreddit's history. We compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk of the contributions are in a foreign language. ",2c804f9b9543e3b085fbd1fff87f0fde688f1484,c1018a31c3272ce74964a3280069f62f314a1a58,False,,"They collect subreddits from January 2013 to December 2014,2 for which there are at
least 500 words in the vocabulary used to estimate the measures,
in at least 4 months of the subreddit’s history. They compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk of the contributions are in a foreign language.","Our full dataset consists of all subreddits on Reddit from January 2013 to December 2014, for which there are at least 500 words in the vocabulary used to estimate our measures, in at least 4 months of the subreddit's history. We compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk of the contributions are in a foreign language. This results in 283 communities ( INLINEFORM0 ), for a total of 4,872 community-months ( INLINEFORM1 ).","Our full dataset consists of all subreddits on Reddit from January 2013 to December 2014, for which there are at least 500 words in the vocabulary used to estimate our measures, in at least 4 months of the subreddit's history. We compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk of the contributions are in a foreign language. This results in 283 communities ( INLINEFORM0 ), for a total of 4,872 community-months ( INLINEFORM1 ).",78de92427e9e37b0dfdc19f57b735e65cec40e0a,34c35a1877e453ecaebcf625df3ef788e1953cc4,How do the authors measure how temporally dynamic a community is?,938cf30c4f1d14fa182e82919e16072fdbcf2a82,five,,,,e8b24c3133e0bec0a6465e1f13acd3a5ed816b37,False,the average volatility of all utterances,,,"Dynamicity. A highly dynamic community constantly shifts interests from one time window to another, and these temporal variations are reflected in its use of volatile language. Formally, we define the dynamicity of a community INLINEFORM0 as the average volatility of all utterances in INLINEFORM1 . We refer to a community whose language is relatively consistent throughout time as being stable.",". A highly dynamic community constantly shifts interests from one time window to another, and these temporal variations are reflected in its use of volatile language. Formally, we define the dynamicity of a community INLINEFORM0 as the average volatility of all utterances in INLINEFORM1 . ",62d30e963bf86e9b2d454adbd4b2c4dc3107cd11,34c35a1877e453ecaebcf625df3ef788e1953cc4,How do the authors measure how distinctive a community is?,93f4ad6568207c9bd10d712a52f8de25b3ebadd4,five,,,,e8b24c3133e0bec0a6465e1f13acd3a5ed816b37,False,,,"Distinctiveness. A community with a very distinctive identity will tend to have distinctive interests, expressed through specialized language. Formally, we define the distinctiveness of a community INLINEFORM0 as the average specificity of all utterances in INLINEFORM1 . We refer to a community with a less distinctive identity as being generic.","A community with a very distinctive identity will tend to have distinctive interests, expressed through specialized language. Formally, we define the distinctiveness of a community INLINEFORM0 as the average specificity of all utterances in INLINEFORM1 ",21484dfac315192bb69aee597ebf5d100ff5925b,34c35a1877e453ecaebcf625df3ef788e1953cc4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3-Figure1-1.png,"Figure 1: A: Within a community certain words are more community-specific and temporally volatile than others. For instance, words like onesies are highly specific to the BabyBumps community (top left corner), while words like easter are temporally ephemeral. B: Extending these word-level measures to communities, we can measure the overall distinctiveness and dynamicity of a community, which are highly associated with user retention rates (colored heatmap; see Section 3). Communities like Seahawks (a football team) and Cooking use highly distinctive language. Moreover, Seahawks uses very dynamic language, as the discussion continually shifts throughout the football season. In contrast, the content of Cooking remains stable over time, as does the content of pics; though these communities do have ephemeral fads, the overall themes discussed generally remain stable.",4-Table1-1.png,Table 1: Examples of communities on Reddit which occur at the extremes (top and bottom quartiles) of our typology.,5-Figure2-1.png,"Figure 2: A: The monthly retention rate for communities differs drastically according to their position in our identity-based typology, with dynamicity being the strongest signal of higher user retention (x-axes bin community-months by percentiles; in all subsequent plots, error bars indicate 95% bootstrapped confidence intervals). B: Dynamicity also correlates with long-term user retention, measured as the number of months the average user spends in the community; however, distinctiveness does not correlate with this longer-term variant of user retention.",6-Figure3-1.png,"Figure 3: A: There is substantial variation in the direction and magnitude of the acculturation gap, which quantifies the extent to which established members of a community are linguistically differentiated from outsiders. Among 60% of communities this gap is positive, indicating that established users match the community’s language more than outsiders. B: The size of the acculturation gap varies systematically according to how dynamic and distinctive a community is. Distinctive communities exhibit larger gaps; as do relatively stable, and very dynamic communities.",,,,,,,,,,,,,,"communities that are characterized by specialized, constantly-updating content have higher user retention rates, but also exhibit larger linguistic gaps that separate newcomers from established members within distinctive communities, established users have an increased propensity to engage with the community's specialized content, compared to newcomers ",,,,,,,,,,, the average specificity of all utterances,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Brundlefly at SemEval-2016 Task 12: Recurrent Neural Networks vs. Joint Inference for Clinical Temporal Information Extraction,"We submitted two systems to the SemEval-2016 Task 12: Clinical TempEval challenge, participating in Phase 1, where we identified text spans of time and event expressions in clinical notes and Phase 2, where we predicted a relation between an event and its parent document creation time. For temporal entity extraction, we find that a joint inference-based approach using structured prediction outperforms a vanilla recurrent neural network that incorporates word embeddings trained on a variety of large clinical document sets. For document creation time relations, we find that a combination of date canonicalization and distant supervision rules for predicting relations on both events and time expressions improves classification, though gains are limited, likely due to the small scale of training data.",Introduction,"This work discusses two information extraction systems for identifying temporal information in clinical text, submitted to SemEval-2016 Task 12 : Clinical TempEval BIBREF0 . We participated in tasks from both phases: (1) identifying text spans of time and event mentions; and (2) predicting relations between clinical events and document creation time. Temporal information extraction is the task of constructing a timeline or ordering of all events in a given document. In the clinical domain, this is a key requirement for medical reasoning systems as well as longitudinal research into the progression of disease. While timestamps and the structured nature of the electronic medical record (EMR) directly capture some aspects of time, a large amount of information on the progression of disease is found in the unstructured text component of the EMR where temporal structure is less obvious. We examine a deep-learning approach to sequence labeling using a vanilla recurrent neural network (RNN) with word embeddings, as well as a joint inference, structured prediction approach using Stanford's knowledge base construction framework DeepDive BIBREF1 . Our DeepDive application outperformed the RNN and scored similarly to 2015's best-in-class extraction systems, even though it only used a small set of context window and dictionary features. Extraction performance, however lagged this year's best system submission. For document creation time relations, we again use DeepDive. Our system examined a simple temporal distant supervision rule for labeling time expressions and linking them to nearby event mentions via inference rules. Overall system performance was better than this year's median submission, but again fell short of the best system.",Methods and Materials,"Phase 1 of the challenge required parsing clinical documents to identify Timex3 and Event temporal entity mentions in text. Timex3 entities are expressions of time, ranging from concrete dates to phrases describing intervals like “the last few months."" Event entities are broadly defined as anything relevant to a patient's clinical timeline, e.g., diagnoses, illnesses, procedures. Entity mentions are tagged using a document collection of clinic and pathology notes from the Mayo Clinic called the THYME (Temporal History of Your Medical Events) corpus BIBREF2 . We treat Phase 1 as a sequence labeling task and examine several models for labeling entities. We discuss our submitted tagger which uses a vanilla RNN and compare its performance to a DeepDive-based system, which lets us encode domain knowledge and sequence structure into a probabilistic graphic model. For Phase 2, we are given all test set entities and asked to identify the temporal relationship between an Event mention and corresponding document creation time. This relation is represented as a classification problem, assigning event attributes from the label set {Before, Overlap, Before/Overlap, After}. We use DeepDive to define several inference rules for leveraging neighboring pairs of Event and Timex3 mentions to better reason about temporal labels.",Recurrent Neural Networks,"Vanilla (or Elman-type) RNNs are recursive neural networks with a linear chain structure BIBREF3 . RNNs are similar to classical feedforward neural networks, except that they incorporate an additional hidden context layer that forms a time-lagged, recurrent connection (a directed cycle) to the primary hidden layer. In the canonical RNN design, the output of the hidden layer at time step INLINEFORM0 is retained in the context layer and fed back into the hidden layer at INLINEFORM1 this enables the RNN to explicitly model some aspects of sequence history. (see Figure FIGREF4 ). Each word in our vocabulary is represented as an INLINEFORM0 -dimensional vector in a lookup table of INLINEFORM1 x INLINEFORM2 parameters (i.e., our learned embedding matrix). Input features then consist of a concatenation of these embeddings to represent a context window surrounding our target word. The output layer then emits a probability distribution in the dimension of the candidate label set. The lookup table is shared across all input instances and updated during training. Formally our RNN definition follows BIBREF4 :  INLINEFORM0  where INLINEFORM0 is our concatenated context window of word embeddings, INLINEFORM1 is our hidden layer, INLINEFORM2 is the input-to-hidden layer matrix, INLINEFORM3 is the hidden layer-to-context layer matrix, and INLINEFORM4 is the activation function (logistic in this work).  INLINEFORM0  The output layer INLINEFORM0 consists of a softmax activation function INLINEFORM1   INLINEFORM0 INLINEFORM1  where INLINEFORM0 is the output layer matrix. Training is done using batch gradient descent using one sentence per batch. Our RNN implementation is based on code available as part of Theano v0.7 BIBREF5 . For baseline RNN models, all embedding parameters are initialized randomly in the range [-1.0, 1.0]. For all other word-based models, embedding vectors are initialized or pre-trained with parameters trained on different clinical corpora. Pre-training generally improves classification performance over random initialization and provides a mechanism to leverage large collections of unlabeled data for use in semi-supervised learning BIBREF6 . We create word embeddings using two collections of clinical documents: the MIMIC-III database containing 2.4M notes from critical care patients at Beth Israel Deaconess Medical Center BIBREF7 ; and the University of Iowa Hospitals and Clinics (UIHC) corpus, containing 15M predominantly inpatient notes (see Table TABREF6 ). All word embeddings in this document are trained with word2vec BIBREF8 using the Skip-gram model, trained with a 10 token window size. We generated 100 and 300 dimensional embeddings based on prior work tuning representation sizes in clinical domains BIBREF9 . We train RNN models for three tasks in Phase 1: a character-level RNN for tokenization; and two word-level RNNs for POS tagging and entity labeling. Word-level RNNs are pre-trained with the embeddings described above, while character-level RNNs are randomly initialized. All words are normalized by lowercasing tokens and replacing digits with N, e.g., 01-Apr-2016 becomes NN-apr-NNNN to improve generalizability and restrict vocabulary size. Characters are left as unnormalized input. In the test data set, unknown words/characters are represented using the special token <UNK> . All hyperparameters were selected using a randomized grid search.  Tokenization: Word tokenization and sentence boundary detection are done simultaneously using a character-level RNN. Each character is assigned a tag from 3 classes: WORD(W) if a character is a member of a token that does not end a sentence; END(E) for a token that does end a sentence, and whitespace O. We use IOB2 tagging to encode the range of token spans. Models are trained using THYME syntactic annotations from colon and brain cancer notes. Training data consists of all sentences, padded with 5 characters from the left and right neighboring sentences. Each character is represented by a 16-dimensional embedding (from an alphabet of 90 characters) and an 11 character context window. The final prediction task input is one, long character sequence per-document. We found that the tokenizer consistently made errors conflating E and W classes (e.g., B-W, I-E, I-E) so after tagging, we enforce an additional consistency constraint on B-* and I-* tags so that contiguous BEGIN/INSIDE spans share the same class. Part-of-speech Tagging: We trained a POS tagger using THYME syntactic annotations. A model using 100-dimensional UIHC-CN embeddings (clinic notes) and a context window of INLINEFORM0 2 words performed best on held out test data, with an accuracy of 97.67% and F INLINEFORM1 = 0.973. TIMEX3 and EVENT Span Tagging: We train separate models for each entity type, testing different pre-training schemes using 100 and 300-dimensional embeddings trained on our large, unlabeled clinical corpora. Both tasks use context windows of INLINEFORM0 2 words (i.e., concatenated input of 5 INLINEFORM1 -d word embeddings) and a learning rate of 0.01. We use 80 hidden units for 100-dimensional embeddings models and 256 units for 300-dimensional models. Output tags are in the IOB2 tagging format.",DeepDive,"DeepDive developers build domain knowledge into applications using a combination of distant supervision rules, which use heuristics to generate noisy training examples, and inference rules which use factors to define relationships between random variables. This design pattern allows us to quickly encode domain knowledge into a probabilistic graphical model and do joint inference over a large space of random variables. For example, we want to capture the relationship between Event entities and their closest Timex3 mentions in text since that provides some information about when the Event occurred relative to document creation time. Timex3s lack a class DocRelTime, but we can use a distant supervision rule to generate a noisy label that we then leverage to predict neighboring Event labels. We also know that the set of all Event/Timex3 mentions within a given note section, such as patient history, provides discriminative information that should be shared across labels in that section. DeepDive lets us easily define these structures by linking random variables (in this case all entity class labels) with factors, directly encoding domain knowledge into our learning algorithm. Phase 1: Our baseline tagger consists of three inference rules: logistic regression, conditional random fields (CRF), and skip-chain CRF BIBREF10 . In CRFs, factor edges link adjoining words in a linear chain structure, capturing label dependencies between neighboring words. Skip-chain CRFs generalize this idea to include skip edges, which can connect non-neighboring words. For example, we can link labels for all identical words in a given window of sentences. We use DeepDive's feature library, ddlib, to generate common textual features like context windows and dictionary membership. We explored combinations of left/right windows of 2 neighboring words and POS tags, letter case, and entity dictionaries for all vocabulary identified by the challenge's baseline memorization rule, i.e., all phrases that are labeled as true entities INLINEFORM0 50% of the time in the training set. Feature Ablation Tests We evaluate 3 feature set combinations to determine how each contributes predictive power to our system. Run 1: dictionary features, letter case Run 2: dictionary features, letter case, context window ( INLINEFORM0 2 normalized words) Run 3: dictionary features, letter case, context window ( INLINEFORM0 2 normalized words), POS tags Phase 2: In order to predict the relationship between an event and the creation time of its parent document, we assign a DocRelTime random variable to every Timex3 and Event mention. For Events, these values are provided by the training data, for Timex3s we have to compute class labels. Around 42% of Timex3 mentions are simple dates (“12/29/08"", “October 16"", etc.) and can be naively canonicalized to a universal timestamp. This is done using regular expressions to identify common date patterns and heuristics to deal with incomplete dates. The missing year in “October 16"", for example, can be filled in using the nearest preceding date mention; if that isn't available we use the document creation year. These mentions are then assigned a class using the parent document's DocTime value and any revision timestamps. Other Timex3 mentions are more ambiguous so we use a distant supervision approach. Phrases like “currently"" and “today's"" tend to occur near Events that overlap the current document creation time, while “ago"" or “ INLINEFORM0 -years"" indicate past events. These dominant temporal associations can be learned from training data and then used to label Timex3s. Finally, we define a logistic regression rule to predict entity DocRelTime values as well as specify a linear skip-chain factor over Event mentions and their nearest Timex3 neighbor, encoding the baseline system heuristic directly as an inference rule.",Phase 1,"Word tokenization performance was high, F INLINEFORM0 =0.993 while sentence boundary detection was lower with F INLINEFORM1 = 0.938 (document micro average F INLINEFORM2 = 0.985). Tokenization errors were largely confined to splitting numbers and hyphenated words (“ex-smoker"" vs. “ex - smoker"") which has minimal impact on upstream sequence labeling. Sentence boundary errors were largely missed terminal words, creating longer sentences, which is preferable to short, less informative sequences in terms of impact on RNN mini-batches. Tables TABREF13 and TABREF14 contain results for all sequence labeling models. For Timex3 spans, the best RNN ensemble model performed poorly compared to the winning system (0.706 vs. 0.795). DeepDive runs 2-3 performed as well as 2015's best system, but also fell short of the top system (0.730 vs. 0.795). Event spans were easier to tag and RNN models compared favorably with DeepDive, the former scoring higher recall and the latter higher precision. Both approaches scored below this year's best system (0.885 vs. 0.903).",Phase 2,"Finally, Table TABREF16 contains our DocRelTime relation extraction. Our simple distant supervision rule leads to better performance than then median system submission, but also falls substantially short of current state of the art.",Discussion,"Randomly initialized RNNs generally weren't competitive to our best performing structured prediction models (DeepDive runs 2-3) which isn't surprising considering the small amount of training data available compared to typical deep-learning contexts. There was a statistically significant improvement for RNNs pre-trained with clinical text word2vec embeddings, reflecting the consensus that embeddings capture some syntactic and semantic information that must otherwise be manually encoded as features. Performance was virtually the same across all embedding types, independent of corpus size, note type, etc. While embeddings trained on more data perform better in semantic tasks like synonym detection, its unclear if that representational strength is important here. Similar performance might also just reflect the general ubiquity with which temporal vocabulary occurs in all clinical note contexts. Alternatively, vanilla RNNs rarely achieve state-of-the-art performance in sequence labeling tasks due to well-known issues surrounding the vanishing or exploding gradient effect BIBREF12 . More sophisticated recurrent architectures with gated units such as Long Short-Term Memory (LSTM), BIBREF13 and gated recurrent unit BIBREF14 or recursive structures like Tree-LSTM BIBREF15 have shown strong representational power in other sequence labeling tasks. Such approaches might perform better in this setting. DeepDive's feature generator libraries let us easily create a large space of binary features and then let regularization address overfitting. In our extraction system, just using a context window of INLINEFORM0 2 words and dictionaries representing the baseline memorization rules was enough to achieve median system performance. POS tag features had no statistically significant impact on performance in either Event/Timex3 extraction. For classifying an Event's document creation time relation, our DeepDive application essentially implements the joint inference version of the baseline memorization rule, leveraging entity proximity to increase predictive performance. A simple distant supervision rule that canonicalizes Timex3 timestamps and predicts nearby Event's lead to a slight performance boost, suggesting that using a larger collection of unlabeled note data could lead to further increases. While our systems did not achieve current state-of-the-art performance, DeepDive matched last year's top submission for Timex3 and Event tagging with very little upfront engineering – around a week of dedicated development time. One of the primary goals of this work was to avoid an over-engineered extraction pipeline, instead relying on feature generation libraries or deep learning approaches to model underlying structure. Both systems explored in this work were successful to some extent, though future work remains in order to close the performance gap between these approaches and current state-of-the-art systems.",Acknowledgments,"This work was supported by the Mobilize Center, a National Institutes of Health Big Data to Knowledge (BD2K) Center of Excellence supported through Grant U54EB020405.",,,,,,,,,,,,,,,,,,,,,,,,,,,How do they obtain distant supervision rules for predicting relations?,4519afe91b1042876d7c021487d98e2d72a09861,infinity,familiar,no,,2cfd959e433f290bb50b55722370f0d22fe090b7,False,,,"Phase 2: In order to predict the relationship between an event and the creation time of its parent document, we assign a DocRelTime random variable to every Timex3 and Event mention. For Events, these values are provided by the training data, for Timex3s we have to compute class labels. Around 42% of Timex3 mentions are simple dates (“12/29/08"", “October 16"", etc.) and can be naively canonicalized to a universal timestamp. This is done using regular expressions to identify common date patterns and heuristics to deal with incomplete dates. The missing year in “October 16"", for example, can be filled in using the nearest preceding date mention; if that isn't available we use the document creation year. These mentions are then assigned a class using the parent document's DocTime value and any revision timestamps. Other Timex3 mentions are more ambiguous so we use a distant supervision approach. Phrases like “currently"" and “today's"" tend to occur near Events that overlap the current document creation time, while “ago"" or “ INLINEFORM0 -years"" indicate past events. These dominant temporal associations can be learned from training data and then used to label Timex3s. Finally, we define a logistic regression rule to predict entity DocRelTime values as well as specify a linear skip-chain factor over Event mentions and their nearest Timex3 neighbor, encoding the baseline system heuristic directly as an inference rule.","Other Timex3 mentions are more ambiguous so we use a distant supervision approach. Phrases like “currently"" and “today's"" tend to occur near Events that overlap the current document creation time, while “ago"" or “ INLINEFORM0 -years"" indicate past events. These dominant temporal associations can be learned from training data and then used to label Timex3s. Finally, we define a logistic regression rule to predict entity DocRelTime values as well as specify a linear skip-chain factor over Event mentions and their nearest Timex3 neighbor, encoding the baseline system heuristic directly as an inference rule.",4263624f3b58869f1621f3a05c63ae6c0ad27560,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,Which structured prediction approach do they adopt for temporal entity extraction?,0cfaca6f3f33ebdb338c5f991f6a7a33ff33844d,infinity,familiar,no,,2cfd959e433f290bb50b55722370f0d22fe090b7,False,,,"We examine a deep-learning approach to sequence labeling using a vanilla recurrent neural network (RNN) with word embeddings, as well as a joint inference, structured prediction approach using Stanford's knowledge base construction framework DeepDive BIBREF1 . Our DeepDive application outperformed the RNN and scored similarly to 2015's best-in-class extraction systems, even though it only used a small set of context window and dictionary features. Extraction performance, however lagged this year's best system submission. For document creation time relations, we again use DeepDive. Our system examined a simple temporal distant supervision rule for labeling time expressions and linking them to nearby event mentions via inference rules. Overall system performance was better than this year's median submission, but again fell short of the best system.","We examine a deep-learning approach to sequence labeling using a vanilla recurrent neural network (RNN) with word embeddings, as well as a joint inference, structured prediction approach using Stanford's knowledge base construction framework DeepDive BIBREF1 .",51d13f3e249efadac26e81000d509e11ba0f5a9c,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2-Figure1-1.png,"Figure 1: Simple Recurrent Neural Network. U is the input × hidden layer weight matrix. V is the context layer × hidden layer matrix, and W is the output weight matrix. Dotted lines indicate recurrent edge weights.",3-Table1-1.png,Table 1: Summary statistics for embedding corpora.,5-Table2-1.png,"Table 2: TIMEX3 spans extraction performance for the test set (mean of 5 runs) [1] Baseline and BluLab scores are provided in (Bethard et al., 2015)",5-Table4-1.png,Table 4: Phase 2: EVENT Document Creation Time Relation extraction measures (baseline precision/recall scores not provided).,5-Table3-1.png,Table 3: EVENT spans extraction performance.,,,,,,,,,,,DeepDive BIBREF1,,,,,,,,,,,,,dominant temporal associations can be learned from training data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The STEM-ECR Dataset: Grounding Scientific Entity References in STEM Scholarly Content to Authoritative Encyclopedic and Lexicographic Sources,"We introduce the STEM (Science, Technology, Engineering, and Medicine) Dataset for Scientific Entity Extraction, Classification, and Resolution, version 1.0 (STEM-ECR v1.0). The STEM-ECR v1.0 dataset has been developed to provide a benchmark for the evaluation of scientific entity extraction, classification, and resolution tasks in a domain-independent fashion. It comprises abstracts in 10 STEM disciplines that were found to be the most prolific ones on a major publishing platform. We describe the creation of such a multidisciplinary corpus and highlight the obtained findings in terms of the following features: 1) a generic conceptual formalism for scientific entities in a multidisciplinary scientific context; 2) the feasibility of the domain-independent human annotation of scientific entities under such a generic formalism; 3) a performance benchmark obtainable for automatic extraction of multidisciplinary scientific entities using BERT-based neural models; 4) a delineated 3-step entity resolution procedure for human annotation of the scientific entities via encyclopedic entity linking and lexicographic word sense disambiguation; and 5) human evaluations of Babelfy returned encyclopedic links and lexicographic senses for our entities. Our findings cumulatively indicate that human annotation and automatic learning of multidisciplinary scientific concepts as well as their semantic disambiguation in a wide-ranging setting as STEM is reasonable.",,1.1em, ::: ,1.1.1em, :::  ::: ,"1.1.1.1em Jennifer D'Souza, Anett Hoppe, Arthur Brack, Mohamad Yaser Jaradeh, Sören Auer, Ralph Ewerth TIB Leibniz Information Centre for Science and Technology, Hannover, Germany {jennifer.dsouza,anett.hoppe,arthur.brack,yaser.jaradeh,auer,ralph.ewerth}@tib.eu We introduce the STEM (Science, Technology, Engineering, and Medicine) Dataset for Scientific Entity Extraction, Classification, and Resolution, version 1.0 (STEM-ECR v1.0). The STEM-ECR v1.0 dataset has been developed to provide a benchmark for the evaluation of scientific entity extraction, classification, and resolution tasks in a domain-independent fashion. It comprises abstracts in 10 STEM disciplines that were found to be the most prolific ones on a major publishing platform. We describe the creation of such a multidisciplinary corpus and highlight the obtained findings in terms of the following features: 1) a generic conceptual formalism for scientific entities in a multidisciplinary scientific context; 2) the feasibility of the domain-independent human annotation of scientific entities under such a generic formalism; 3) a performance benchmark obtainable for automatic extraction of multidisciplinary scientific entities using BERT-based neural models; 4) a delineated 3-step entity resolution procedure for human annotation of the scientific entities via encyclopedic entity linking and lexicographic word sense disambiguation; and 5) human evaluations of Babelfy returned encyclopedic links and lexicographic senses for our entities. Our findings cumulatively indicate that human annotation and automatic learning of multidisciplinary scientific concepts as well as their semantic disambiguation in a wide-ranging setting as STEM is reasonable. Entity Recognition, Entity Classification, Entity Resolution, Entity Linking, Word Sense Disambiguation, Evaluation Corpus, Language Resource",Scientific Entity Annotations,"By starting with a STEM corpus of scholarly abstracts for annotating with scientific entities, we differ from existing work addressing this task since we are going beyond the domain restriction that so far seems to encompass scientific IE. For entity annotations, we rely on existing scientific concept formalisms BIBREF0, BIBREF1, BIBREF2 that appear to propose generic scientific concept types that can bridge the domains we consider, thereby offering a uniform entity selection framework. In the following subsections, we describe our annotation task in detail, after which we conclude with benchmark results.",Scientific Entity Annotations ::: Our Annotation Process,"The corpus for computing inter-annotator agreement was annotated by two postdoctoral researchers in Computer Science. To develop annotation guidelines, a small pilot annotation exercise was performed on 10 abstracts (one per domain) with a set of surmised generically applicable scientific concepts such as Task, Process, Material, Object, Method, Data, Model, Results, etc., taken from existing work. Over the course of three annotation trials, these concepts were iteratively pruned where concepts that did not cover all domains were dropped, resulting in four finalized concepts, viz. Process, Method, Material, and Data as our resultant set of generic scientific concepts (see Table TABREF3 for their definitions). The subsequent annotation task entailed linguistic considerations for the precise selection of entities as one of the four scientific concepts based on their part-of-speech tag or phrase type. Process entities were verbs (e.g., “prune” in Agr), verb phrases (e.g., “integrating results” in Mat), or noun phrases (e.g. “this transport process” in Bio); Method entities comprised noun phrases containing phrase endings such as simulation, method, algorithm, scheme, technique, system, etc.; Material were nouns or noun phrases (e.g., “forest trees” in Agr, “electrons” in Ast or Che, “tephra” in ES); and majority of the Data entities were numbers otherwise noun phrases (e.g., “(2.5$\pm $1.5)kms$^{-1}$” representing a velocity value in Ast, “plant available P status” in Agr). Summarily, the resulting annotation guidelines hinged upon the following five considerations: To ensure consistent scientific entity spans, entities were annotated as definite noun phrases where possible. In later stages, the extraneous determiners and articles could be dropped as deemed appropriate. Coreferring lexical units for scientific entities in the context of a single abstract were annotated with the same concept type. Quantifiable lexical units such as numbers (e.g., years 1999, measurements 4km) or even as phrases (e.g., vascular risk) were annotated as Data. Where possible, the most precise text reference (i.e., phrases with qualifiers) regarding materials used in the experiment were annotated. For instance, “carbon atoms in graphene” was annotated as a single Material entity and not separately as “carbon atoms,” “graphene.” Any confusion in classifying scientific entities as one of four types was resolved using the following concept precedence: Method $>$ Process $>$ Data $>$ Material, where the concept appearing earlier in the list was preferred. After finalizing the concepts and updating the guidelines, the final annotation task proceeded in two phases In phase I, five abstracts per domain (i.e. 50 abstracts) were annotated by both annotators and the inter-annotator agreement was computed using Cohen's $\kappa $ BIBREF4. Results showed a moderate inter-annotator agreement at 0.52 $\kappa $. Next, in phase II, one of the annotators interviewed subject specialists in each of the ten domains about the choice of concepts and her annotation decisions on their respective domain corpus. The feedback from the interviews were systematically categorized into error types and these errors were discussed by both annotators. Following these discussions, the 50 abstracts from phase I were independently reannotated. The annotators could obtain substantial overall agreement of 0.76 $\kappa $ after phase II. In Table TABREF16, we report the IAA scores obtained per domain and overall. The scores show that the annotators had a substantial agreement in seven domains, while only a moderate agreement was reached in three domains, viz. Agr, Mat, and Ast.",Scientific Entity Annotations ::: Our Annotation Process ::: Annotation Error Analysis,"We discuss some of the changes the interviewer annotator made in phase II after consultation with the subject experts. In total, 21% of the phase I annotations were changed: Process accounted for a major proportion (nearly 54%) of the changes. Considerable inconsistency was found in annotating verbs like “increasing”, “decreasing”, “enhancing”, etc., as Process or not. Interviews with subject experts confirmed that they were a relevant detail to the research investigation and hence should be annotated. So 61% of the Process changes came from additionally annotating these verbs. Material was the second predominantly changed concept in phase II, accounting for 23% of the overall changes. Nearly 32% of the changes under Material came from consistently reannotating phrases about models, tools, and systems; accounting for another 22% of its changes, where spatial locations were an essential part of the investigation such as in the Ast and ES domains, they were decided to be included in the phase II set as Material. Finally, there were some changes that emerged from lack of domain expertise. This was mainly in the medical domain (4.3% of the overall changes) in resolving confusion in annotating Process and Method concept types. Most of the remaining changes were based on the treatment of conjunctive spans or lists. Subsequently, the remaining 60 abstracts (six per domain) were annotated by one annotator. This last phase also involved reconciliation of the earlier annotated 50 abstracts to obtain a gold standard corpus.",Scientific Entity Annotations ::: Our Annotation Process ::: Annotated Corpus Characteristics,"Table TABREF17 shows our annotated corpus characteristics. Our corpus comprises a total of 6,127 scientific entities, including 2,112 Process, 258 Method, 2,099 Material, and 1,658 Data entities. The number of entities per abstract directly correlates with the length of the abstracts (Pearson's R 0.97). Among the concepts, Process and Material directly correlate with abstract length (R 0.8 and 0.83, respectively), while Data has only a slight correlation (R 0.35) and Method has no correlation (R 0.02). In Figure FIGREF18, we show an example instance of a manually created text graph from the scientific entities in one abstract. The graph highlights that linguistic relations such as synonymy, hypernymy, meronymy, as well as OpenIE relations are poignant even between scientific entities.",Scientific Entity Annotations ::: Performance Benchmark,"In the second stage of the study, we perform word sense disambiguation and link our entities to authoritative sources.",Scientific Entity Resolution,"Aside from the four scientific concepts facilitating a common understanding of scientific entities in a multidisciplinary setting, the fact that they are just four made the human annotation task feasible. Utilizing additional concepts would have resulted in a prohibitively expensive human annotation task. Nevertheless, there are existing datasets (particularly in the biomedical domain, e.g., GENIA BIBREF6) that have adopted the conceptual framework in rich domain-specific semantic ontologies. Our work, while related, is different since we target the annotation of multidisciplinary scientific entities that facilitates a low annotation entrance barrier to producing such data. This is beneficial since it enables the task to be performed in a domain-independent manner by researchers, but perhaps not crowdworkers, unless screening tests for a certain level of scientific expertise are created. Nonetheless, we recognize that the four categories might be too limiting for real-world usage. Further, the scientific entities from stage 1 remain susceptible to subjective interpretation without additional information. Therefore, in a similar vein to adopting domain-specific ontologies, we now perform entity linking (EL) to the Wikipedia and word sense disambiguation (WSD) to Wiktionary.",Scientific Entity Resolution ::: Our Annotation Process,The same pair of annotators as before were involved in this stage of the study to determine the annotation agreement.,Scientific Entity Resolution ::: Our Annotation Process ::: Annotation Task Tools,"During the annotation procedure, each annotator was shown the entities, grouped by domain and file name, in Google Excel Sheet columns alongside a view of the current abstract of entities being annotated in the BRAT interface stenetorp2012brat for context information about the entities. For entity resolution, i.e. linking and disambiguation, the annotators had local installations of specific time-stamped Wikipedia and Wiktionary dumps to enable future persistent references to the links since the Wiki sources are actively revised. They queried the local dumps using the DKPro JWPL tool BIBREF8 for Wikipedia and the DKPro JWKTL tool BIBREF9 for Wiktionary, where both tools enable optimized search through the large Wiki data volume.",Scientific Entity Resolution ::: Our Annotation Process ::: Annotation Procedure for Entity Resolution,"Through iterative pilot annotation trials on the same pilot dataset as before, the annotators delineated an ordered annotation procedure depicted in the flowchart in Figure FIGREF28. There are two main annotation phases, viz. a preprocessing phase (determining linkability, determining whether an entity is decomposable into shorter collocations), and the entity resolution phase. The actual annotation task then proceeded, in which to compute agreement scores, the annotators worked on the same set of 50 scholarly abstracts that they had used earlier to compute the scores for the scientific entity annotations.",Scientific Entity Resolution ::: Our Annotation Process ::: Annotation Procedure for Entity Resolution ::: Linkability.,"In this first step, entities that conveyed a sense of scientific jargon were deemed linkable. A natural question that arises, in the context of the Linkability criteria, is: Which stage 1 annotated scientific entities were now deemed unlinkable? They were 1) Data entities that are numbers; 2) entities that are coreference mentions which, as isolated units, lost their precise sense (e.g., “development”); and 3) Process verbs (e.g., “decreasing”, “reconstruct”, etc.). Still, having identified these cases, a caveat remained: except for entities of type Data, the remaining decisions made in this step involved a certain degree of subjectivity because, for instance, not all Process verbs were unlinkable (e.g., “flooding”). Nonetheless, at the end of this step, the annotators obtained a high IAA score at 0.89 $\kappa $. From the agreement scores, we found that the Linkability decisions could be made reliably and consistently on the data.",Scientific Entity Resolution ::: Our Annotation Process ::: Annotation Procedure for Entity Resolution ::: Splitting phrases into shorter collocations.,"While preference was given to annotating non-compositional noun phrases as scientific entities in stage 1, consecutive occurrences of entities of the same concept type separated only by prepositions or conjunctions were merged into longer spans. As examples, consider the phrases “geysers on south polar region,” and “plume of water ice molecules and dust” in Figure FIGREF18. These phrases, respectively, can be meaningfully split as “geysers” and “south polar region” for the first example, and “plume”, “water ice molecules”, and “dust” for the second. As demonstrated in these examples, the stage 1 entities we split in this step are syntactically-flexible multi-word expressions which did not have a strict constraint on composition BIBREF10. For such expressions, we query Wikipedia or Google to identify their splits judging from the number of results returned and whether, in the results, the phrases appeared in authoritative sources (e.g., as overview topics in publishing platforms such as ScienceDirect). Since search engines operate on a vast amount of data, they are a reliable source for determining phrases with a strong statistical regularity, i.e. determining collocations. With a focus on obtaining agreement scores for entity resolution, the annotators bypass this stage for computing independent agreement and attempted it mutually as follows. One annotator determined all splits, wherever required, first. The second annotator acted as judge by going through all the splits and proposed new splits in case of disagreement. The disagreements were discussed by both annotators and the previous steps were repeated iteratively until the dataset was uniformly split. After this stage, both annotators have the same set of entities for resolution.",Scientific Entity Resolution ::: Our Annotation Process ::: Annotation Procedure for Entity Resolution ::: Entity Resolution (ER) Annotation.,"In this stage, the annotators resolved each entity from the previous step to encyclopedic and lexicographic knowledge bases. While, in principle, multiple knowledge sources can be leveraged, this study only examines scientific entities in the context of their Wiki-linkability. Wikipedia, as the largest online encyclopedia (with nearly 5.9 million English articles) offers a wide coverage of real-world entities, and based on its vast community of editors with editing patterns at the rate of 1.8 edits per second, is considered a reliable source of information. It is pervasively adopted in automatic EL tasks BIBREF11, BIBREF12, BIBREF13 to disambiguate the names of people, places, organizations, etc., to their real-world identities. We shift from this focus on proper names as the traditional Wikification EL purpose has been, to its, thus far, seemingly less tapped-in conceptual encyclopedic knowledge of nominal scientific entities. Wiktionary is the largest freely available dictionary resource. Owing to its vast community of curators, it rivals the traditional expert-curated lexicographic resource WordNet BIBREF14 in terms of coverage and updates, where the latter evolves more slowly. For English, Wiktionary has nine times as many entries and at least five times as many senses compared to WordNet. As a more pertinent neologism in the context of our STEM data, consider the sense of term “dropout” as a method for regularizing the neural network algorithms which is already present in Wiktionary. While WSD has been traditionally used WordNet for its high-quality semantic network and longer prevalence in the linguistics community (c.f Navigli navigli2009word for a comprehensive survey), we adopt Wiktionary thus maintaining our focus on collaboratively curated resources. In WSD, entities from all parts-of-speech are enriched w.r.t. language and wordsmithing. But it excludes in-depth factual and encyclopedic information, which otherwise is contained in Wikipedia. Thus, Wikipedia and Wiktionary are viewed as largely complementary.",Scientific Entity Resolution ::: Our Annotation Process ::: Annotation Procedure for Entity Resolution ::: ER Annotation Task formalism.,"Given a scholarly abstract $A$ comprising a set of entities $E = \lbrace e_{1}, ... ,e_{N}\rbrace $, the annotation goal is to produce a mapping from $E$ to a set of Wikipedia pages ($p_1,...,p_N$) and Wiktionary senses ($s_1,...,s_N$) as $R = \lbrace (p_1,s_1), ... , (p_N,s_N)\rbrace $. For entities without a mapping, the corresponding $p$ or $s$ refers to Nil. The annotators followed comprehensive guidelines for ER including exceptions. E.g., the conjunctive phrase “acid/alkaline phosphatase activity” was semantically treated as the following two phrases “acid phosphatase activity” or “alkaline phosphatase activity” for EL, however, in the text it was retained as “acid” and “alkaline phosphatase activity.” Since WSD is performed over exact word-forms without assuming any semantic extension, it was not performed for “acid.” Annotations were also made for complex forms of reference such as meronymy (e.g., space instrument “CAPS” to spacecraft “wiki:Cassini Huygens” of which it is a part), or hypernymy (e.g., “parents” in “genepool parents” to “wiki:Ancestor”). As a result of the annotation task, the annotators obtained 82.87% rate of agreement in the EL task and a $\kappa $ score of 0.86 in the WSD task. Contrary to WSD expectations as a challenging linguistics task BIBREF15, we show high agreement; this we attribute to the entities' direct scientific sense and availability in Wiktionary (e.g., “dropout”). Subsequently, the ER annotation for the remaining 60 abstracts (six per domain) were performed by one annotator. This last phase also involved reconciliation of the earlier annotated 50 abstracts to obtain a gold standard corpus.",Scientific Entity Resolution ::: Our Annotation Process ::: Annotated Corpus Characteristics,"In this stage 2 corpus, linkability of the scientific entities was determined at 74.6%. Of these, 61.7% were split into shorter collocations, at 1.74 splits per split entity. Detailed statistics are presented in Table TABREF36. In the table, the domains are ranked by the total number of their linkable entities (fourth column). Ast has the highest proportion of linked entities at 87.3% which comprises 10.4% of all the linked entities and disambiguated entities at 71.4% forming 8.5% of the overall disambiguated entities. From an EL perspective, we surmize that articles on space topics are well represented in Wikipedia. For WSD, Bio, ES, and Med predictably have the least proportion of disambiguated entities at 52.3%, 54.6%, and 55.5%, respectively, since of all our domains these especially rely on high degree scientific jargon, while WSD generally tends to be linguistically oriented in a generic sense. As a summary, linked and disambiguated entities had a high correlation with the total linkable entities ($R$ 0.98 and 0.89, respectively). In Table TABREF37, the ER annotation results are shown as POS tag distributions. The POS tags were obtained from Wiktionary, where entities that couldn't be disambiguated are tagged as SW (Single Word) or MWE (Multi-Word Expression). These tags have a coarser granularity compared to the traditionally followed Penn Treebank tags with some unconventional tagging patterns (e.g., “North Sea” as NNP, “in vivo” as ADJ). From the distributions, except for nouns being the most EL and WSD instances, the rest of the table differs significantly between the two tasks in a sense reflecting the nature of the tasks. While MWE are the second highest EL instances, its corresponding PHRASE type is least represented in WSD. In contrast, while adverbs are the second highest in WSD, they are least in EL.",Scientific Entity Resolution ::: Evaluation,We do not observe a significant impact of the long-tailed list phenomenon of unresolved entities in our data (c.f Table TABREF36 only 17% did not have EL annotations). Results on more recent publications should perhaps serve more conclusive in this respect for new concepts introduced–the abstracts in our dataset were published between 2012 and 2014.,Conclusion,"The STEM-ECR v1.0 corpus of scientific abstracts offers multidisciplinary Process, Method, Material, and Data entities that are disambiguated using Wiki-based encyclopedic and lexicographic sources thus facilitating links between scientific publications and real-world knowledge (see the concepts enrichment we obtain from Wikipedia for our entities in Figure ). We have found that these Wikipedia categories do enable a semantic enrichment of our entities over our generic four concept formalism as Process, Material, Method, and Data (as an illustration, the top 30 Wiki categories for each of our four generic concept types are shown in the Appendix). Further, considering the various domains in our multidisciplinary STEM corpus, notably, the inclusion of understudied domains like Mathematics, Astronomy, Earth Science, and Material Science makes our corpus particularly unique w.r.t. the investigation of their scientific entities. This is a step toward exploring domain independence in scientific IE. Our corpus can be leveraged for machine learning experiments in several settings: as a vital active-learning test-bed for curating more varied entity representations BIBREF16; to explore domain-independence versus domain-dependence aspects in scientific IE; for EL and WSD extensions to other ontologies or lexicographic sources; and as a knowledge resource to train a reading machine (such as PIKES BIBREF17 or FRED BIBREF18) that generate more knowledge from massive streams of interdisciplinary scientific articles. We plan to extend this corpus with relations to enable building knowledge representation models such as knowledge graphs in a domain-independent manner.",Acknowledgements,We thank the anonymous reviewers for their comments and suggestions. We also thank the subject specialists at TIB for their helpful feedback in the first part of this study. This work was co-funded by the European Research Council for the project ScienceGRAPH (Grant agreement ID: 819536) and by the TIB Leibniz Information Centre for Science and Technology.,Appendix: Supplemental Material,"A.1. Proportion of the Generic Scientific Entities To offer better insights to our STEM corpus for its scientific entity annotations made in part 1, in Figure FIGREF40 below, we visually depict the proportion of Process, Method, Material, and Data entities per domain. The Figure serves a complementary view to our corpus compared with the dataset statistics shown in Table TABREF17. It shows that the Ast domain has the highest proportion of scientific entities overall. On the other hand, per generic type, Bio has the most Process entities, CS has the most Method entities, Ast has the most Material closely followed by Agr, and Eng has the most Data. A.2. Cohen's $\kappa $ Computation Setup in Section 4.1.2 Linkability. Given the stage 1 scientific entities, the annotators could make one of two decisions: a) an entity is linkable; or b) an entity is unlinkable. These decisions were assigned numeric indexes, i.e. 1 for decision (a) and -1 for decision (b) and can take on one of four possible combinations based on the two annotators decisions: (1,1), (1,-1), (-1,1), and (-1,-1). The $\kappa $ scores were then computed on this data representation. WSD Agreement. In order to compute the WSD agreement, the Wiktionary structure for organizing the words needed to be taken into account. It's structure is as follows. Each word in the Wiktionary lexicographic resource is categorized based on etymology, and within each etymological category, by the various part-of-speech tags the word can take. Finally, within each POS type, is a gloss list where each gloss corresponds to a unique word sense. Given the above-mentioned Wiktionary structure, the initial setup for the blind WSD annotation task entailed that the annotators were given the same reference POS tags within an etymology for the split single-word entities in the corpus. Next, as data to compute $\kappa $ scores, each annotator-assigned gloss sense was given a numeric index and agreement was computed based on matches or non-matches between indexes. A.3. Per-domain Inter-annotator Agreement for Entity Resolution To supplement the overall Inter-Annotator Agreement (IAA) scores reported in Section 4.1.2 `Entity Resolution (ER) Annotation' for the EL and WSD tasks, in Table TABREF43 below, we additionally report the IAA scores for our ER tasks (i.e., EL and WSD) per domain in the STEM-ECR corpus. First, considering the domains where the highest ER agreement scores were obtained. For EL, the IAA score was highest in the MS domain. While for WSD, the IAA score was highest in the Bio domain. Next, considering the domains where the agreement was least for the two tasks. We found the the EL agreement was least for CS and the WSD agreement was least for Mat. In the case of low EL agreement, it can be attributed to two main cases: only one of the annotators found a link; or the annotators linked to related pages on the same theme as the entity (e.g., wiki:Rule-based_modeling versus wiki:Rule-based_machine_learning for “rule-based system”). And in the case of the low WSD agreement obtained on Mat, we see that owing to broad terms like “set,” “matrix,” “groups,” etc., in the domain which could be disambiguated to more than one Wiktionary sense correctly, the IAA agreement was low. A.4. Babelfy's Precision ($P$) and Recall ($R$) Computation for Entity Resolution in Figure For the $P$ and $R$ scores reported in Figure , the true positives (TP), false negatives (FN), true negatives (TN), and false positives (FP) were computed as follows: TP = human-annotated entities that have a EL/WSD match with Babelfy results (for Nil, a match is considered as no result from the automatic system); FN = human-annotated entities that have no EL/WSD match with Babelfy results; TN = spurious Babelfy-created strings as entities that do not have a EL/WSD result; and FP = spurious Babelfy-created entities that have a EL/WSD result. A.5. Top 30 Wikipedia Categories for Process, Method, Material, and Data In part 1 of the study, we categorized the scientific entities by our four generic concept formalism, comprising Process, Method, Material, and Data. Linking the entities to Wikipedia further enables their broadened categorization. While in Figure is depicted the rich set of Wikipedia categories obtained overall, here, in Tables TABREF44 and TABREF45, we show the top 30 Wikipedia categories for the scientific entities by their four concept types. we observe the most of the Wikipedia categories pertinently broaden the semantic expressivity of each of our four concepts. Further that in each type, they are diverse reflecting the underlying data domains in our corpus. As examples, consider the Wikipedia categories for the Data scientific entities: “SIBaseQuantities” category over the entity “Kelvin” in Che; “FluidDynamics” in Eng and MS domains; and “SolarCalendars” in the Ast domain.",How large is the dataset?,252677c93feb2cb0379009b680f0b4562b064270,two,unfamiliar,no,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,,"Table TABREF17 shows our annotated corpus characteristics. Our corpus comprises a total of 6,127 scientific entities, including 2,112 Process, 258 Method, 2,099 Material, and 1,658 Data entities. The number of entities per abstract directly correlates with the length of the abstracts (Pearson's R 0.97). Among the concepts, Process and Material directly correlate with abstract length (R 0.8 and 0.83, respectively), while Data has only a slight correlation (R 0.35) and Method has no correlation (R 0.02).","Our corpus comprises a total of 6,127 scientific entities, including 2,112 Process, 258 Method, 2,099 Material, and 1,658 Data entities. ",ac3e227e7737ab6389012789a84c84d37c463138,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3-Table2-1.png,"Table 2: Per-domain and Overall inter-annotator agreement (Cohen’s Kappa κ) for PROCESS, METHOD, MATERIAL, and METHOD scientific concept annotation",4-Table3-1.png,Table 3: The annotated corpus characteristics in terms of size and the number of scientific concept phrases,4-Figure1-1.png,"Figure 1: A text graph of the abstract of the article ‘The Cassini Enceladus encounters 2005–2010 in the view of energetic electron measurements’ (Krupp et al., 2012). Nodes are color-coded by concept type: orange corresponds to PROCESS, green to MATERIAL, and blue to DATA. No METHOD concepts were identified in this abstract.",5-Figure2-1.png,Figure 2: Flowchart depicting our Entity Resolution annotation steps. The boxes shaded in grey represent the steps where annotator agreement scores are computed.,5-Table4-1.png,Table 4: The domain-independent scientific entity extraction system results per-domain,7-Figure3-1.png,"Figure 3: Percentage P , R, and F1 per domain of the Babelfy system for disambiguating to BabelNet (as shaded blue bars BP, BR, and BF) and for linking to DBpedia (as shaded orange bars DP, DR, DF)",,,,,,,,,,,7-Table5-1.png,Table 5: The annotated corpus characteristics in terms of its Entity Linking (EL) and Word Sense Disambiguation (WSD) annotations,7-Table6-1.png,Table 6: Part-of-speech tag distribution in our corpus. N - Noun; MWE - multiword expression; SW - single word; ADJ - adjective; SYM - symbol; V - verb; NNP - proper noun; R - adverb; INIT - initialism; PREP - preposition,8-Figure5-1.png,Figure 5: Wikipedia categories cloud on scientific entities,8-Figure4-1.png,"Figure 4: Percentage recall per domain of four popular online Entity Linking systems, viz. Babelfy (2014a), TagMe (Ferragina and Scaiella, 2010), DBpedia Spotlight (Mendes et al., 2011), and Falcon (Sakor et al., 2019)",9-Figure6-1.png,Figure 6: Percentage proportion of scientific entities in our STEM corpus per domain and per generic concept type,,"6,127 scientific entities, including 2,112 Process, 258 Method, 2,099 Material, and 1,658 Data entities",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,9-Table7-1.png,Table 7: Per-domain inter-annotator agreement for Entity Linking to Wikipedia in terms of the percentage rate of agreement score (roa) and of Word Sense Disambiguation to Wiktionary in terms of the Cohen’s kappa score (κ),10-Table8-1.png,Table 8: Top 30 Wikipedia categories pertaining to PROCESS and METHOD scientific entities in the STEM-ECR corpus,,,,,,,,10-Table9-1.png,Table 9: Top 30 Wikipedia categories pertaining to MATERIAL and DATA scientific entities in the STEM-ECR corpus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AraNet: A Deep Learning Toolkit for Arabic Social Media,"We describe AraNet, a collection of deep learning Arabic social media processing tools. Namely, we exploit an extensive host of publicly available and novel social media datasets to train bidirectional encoders from transformer models (BERT) to predict age, dialect, gender, emotion, irony, and sentiment. AraNet delivers state-of-the-art performance on a number of the cited tasks and competitively on others. In addition, AraNet has the advantage of being exclusively based on a deep learning framework and hence feature engineering free. To the best of our knowledge, AraNet is the first to performs predictions across such a wide range of tasks for Arabic NLP and thus meets a critical needs. We publicly release AraNet to accelerate research and facilitate comparisons across the different tasks.",Introduction,"The proliferation of social media has made it possible to study large online communities at scale, thus making important discoveries that can facilitate decision making, guide policies, improve health and well-being, aid disaster response, etc. The wide host of languages, languages varieties, and dialects used on social media and the nuanced differences between users of various backgrounds (e.g., different age groups, gender identities) make it especially difficult to derive sufficiently valuable insights based on single prediction tasks. For these reasons, it would be desirable to offer NLP tools that can help stitch together a complete picture of an event across different geographical regions as impacting, and being impacted by, individuals of different identities. We offer AraNet as one such tool for Arabic social media processing.",Introduction ::: ,"For Arabic, a collection of languages and varieties spoken by a wide population of $\sim 400$ million native speakers covering a vast geographical region (shown in Figure FIGREF2), no such suite of tools currently exists. Many works have focused on sentiment analysis, e.g., BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF6, BIBREF7 and dialect identification BIBREF8, BIBREF9, BIBREF10, BIBREF11, BIBREF12, BIBREF13. However, there is generally rarity of resources on other tasks such as gender and age detection. This motivates our toolkit, which we hope can meet the current critical need for studying Arabic communities online. This is especially valuable given the waves of protests, uprisings, and revolutions that have been sweeping the region during the last decade. Although we create new models for tasks such as sentiment analysis and gender detection as part of AraNet, our focus is more on putting together the toolkit itself and providing strong baselines that can be compared to. Hence, although we provide some baseline models for some of the tasks, we do not explicitly compare to previous research since most existing works either exploit smaller data (and so it will not be a fair comparison), use methods pre-dating BERT (and so will likely be outperformed by our models) . For many of the tasks we model, there have not been standard benchmarks for comparisons across models. This makes it difficult to measure progress and identify areas worthy of allocating efforts and budgets. As such, by publishing our toolkit models, we believe model-based comparisons will be one way to relieve this bottleneck. For these reasons, we also package models from our recent works on dialect BIBREF12 and irony BIBREF14 as part of AraNet . The rest of the paper is organized as follows: In Section SECREF2 we describe our methods. In Section SECREF3, we describe or refer to published literature for the dataset we exploit for each task and provide results our corresponding model acquires. Section SECREF4 is about AraNet design and use, and we overview related works in Section SECREF5 We conclude in Section SECREF6",Methods,"Supervised BERT. Across all our tasks, we use Bidirectional Encoder Representations from Transformers (BERT). BERT BIBREF15, dispenses with recurrence and convolution. It is based on a multi-layer bidirectional Transformer encoder BIBREF16, with multi-head attention. It uses masked language models to enable pre-trained deep bidirectional representations, in addition to a binary next sentence prediction task. The pre-trained BERT can be easily fine-tuned on a wide host of sentence-level and token-level tasks. All our models are trained in a fully supervised fashion, with dialect id being the only task where we leverage semi-supervised learning. We briefly outline our semi-supervised methods next. Self-Training. Only for the dialect id task, we investigate augmenting our human-labeled training data with automatically-predicted data from self-training. Self-training is a wrapper method for semi-supervised learning BIBREF17, BIBREF18 where a classifier is initially trained on a (usually small) set of labeled samples $\textbf {\textit {D}}^{l}$, then is used to classify an unlabeled sample set $\textbf {\textit {D}}^{u}$. Most confident predictions acquired by the original supervised model are added to the labeled set, and the model is iteratively re-trained. We perform self-training using different confidence thresholds and choose different percentages from predicted data to add to our train. We only report best settings here and the reader is referred to our winning system on the MADAR shared task for more details on these different settings BIBREF12. Implementation & Models Parameters. For all our tasks, we use the BERT-Base Multilingual Cased model released by the authors . The model is trained on 104 languages (including Arabic) with 12 layer, 768 hidden units each, 12 attention heads, and has 110M parameters in entire model. The model has 119,547 shared WordPieces vocabulary, and was pre-trained on the entire Wikipedia for each language. For fine-tuning, we use a maximum sequence size of 50 tokens and a batch size of 32. We set the learning rate to $2e-5$ and train for 15 epochs and choose the best model based on performance on a development set. We use the same hyper-parameters in all of our BERT models. We fine-tune BERT on each respective labeled dataset for each task. For BERT input, we apply WordPiece tokenization, setting the maximal sequence length to 50 words/WordPieces. For all tasks, we use a TensorFlow implementation. An exception is the sentiment analysis task, where we used a PyTorch implementation with the same hyper-parameters but with a learning rate $2e-6$. Pre-processing. Most of our training data in all tasks come from Twitter. Exceptions are in some of the datasets we use for sentiment analysis, which we point out in Section SECREF23. Our pre-processing thus incorporates methods to clean tweets, other datasets (e.g., from the news domain) being much less noisy. For pre-processing, we remove all usernames, URLs, and diacritics in the data.",Data and Models ::: Age and Gender,"Arab-Tweet. For modeling age and gender, we use Arap-Tweet BIBREF19 , which we will refer to as Arab-Tweet. Arab-tweet is a tweet dataset of 11 Arabic regions from 17 different countries. For each region, data from 100 Twitter users were crawled. Users needed to have posted at least 2,000 and were selected based on an initial list of seed words characteristic of each region. The seed list included words such as <برشة> /barsha/ ‘many’ for Tunisian Arabic and <وايد> /wayed/ ‘many’ for Gulf Arabic. BIBREF19 employed human annotators to verify that users do belong to each respective region. Annotators also assigned gender labels from the set male, female and age group labels from the set under-25, 25-to34, above-35 at the user-level, which in turn is assigned at tweet level. Tweets with less than 3 words and re-tweets were removed. Refer to BIBREF19 for details about how annotation was carried out. We provide a description of the data in Table TABREF10. Table TABREF10 also provides class breakdown across our splits.We note that BIBREF19 do not report classification models exploiting the data.",Data and Models ::: Age and Gender ::: ,"We shuffle the Arab-tweet dataset and split it into 80% training (TRAIN), 10% development (DEV), and 10% test (TEST). The distribution of classes in our splits is in Table TABREF10. For pre-processing, we reduce 2 or more consecutive repetitions of the same character into only 2 and remove diacritics. With this dataset, we train a small unidirectional GRU (small-GRU) with a single 500-units hidden layer and $dropout=0.5$ as a baseline. Small-GRU is trained with the TRAIN set, batch size = 8, and up to 30 words of each sequence. Each word in the input sequence is represented as a trainable 300-dimension vector. We use the top 100K words which are weighted by mutual information as our vocabulary in the embedding layer. We evaluate the model on TEST set. Table TABREF14 show small-GRU obtain36.29% XX acc on age classification, and 53.37% acc on gender detection. We also report the accuracy of fine-tuned BERT models on TEST set in Table TABREF14. We can find that BERT models significantly perform better than our baseline on the two tasks. It improve with 15.13% (for age) and 11.93% acc (for gender) over the small-GRU. UBC Twitter Gender Dataset. We also develop an in-house Twitter dataset for gender. We manually labeled 1,989 users from each of the 21 Arab countries. The data had 1,246 “male"", 528 “female"", and 215 unknown users. We remove the “unknown"" category and balance the dataset to have 528 from each of the two `male"" and “female"" categories. We ended with 69,509 tweets for `male"" and 67,511 tweets for “female"". We split the users into 80% TRAIN set (110,750 tweets for 845 users), 10% DEV set (14,158 tweets for 106 users), and 10% TEST set (12,112 tweets for 105 users). We, then, model this dataset with BERT-Base, Multilingual Cased model and evaluate on development and test sets. Table TABREF15 shows that fine-tuned model obtains 62.42% acc on DEV and 60.54% acc on TEST. We also combine the Arab-tweet gender dataset with our UBC-Twitter dataset for gender on training, development, and test, respectively, to obtain new TRAIN, DEV, and TEST. We fine-tune the BERT-Base, Multilingual Cased model with the combined TRAIN and evaluate on combined DEV and TEST. As Table TABREF15 shows, the model obtains 65.32% acc on combined DEV set, and 65.32% acc on combined TEST set. This is the model we package in AraNet .",Data and Models ::: Dialect,"The dialect identification model in AraNet is based on our winning system in the MADAR shared task 2 BIBREF20 as described in BIBREF12. The corpus is divided into train, dev and test, and the organizers masked test set labels. We lost some tweets from training data when we crawled using tweet ids, ultimately acquiring 2,036 (TRAIN-A), 281 (DEV) and 466 (TEST). We also make use of the task 1 corpus (95,000 sentences BIBREF21). More specifically, we concatenate the task 1 data to the training data of task 2, to create TRAIN-B. Again, note that TEST labels were only released to participants after the official task evaluation. Table TABREF17 shows statistics of the data. We used tweets from 21 Arab countries as distributed by task organizers, except that we lost some tweets when we crawled using tweet ids. We had 2,036 (TRAIN-A), 281 (DEV) and 466 (TEST). For our experiments, we also make use of the task 1 corpus (95,000 sentences BIBREF21). More specifically, we concatenate the task 1 data to the training data of task 2, to create TRAIN-B. Note that both DEV and TEST across our experiments are exclusively the data released in task 2, as described above. TEST labels were only released to participants after the official task evaluation. Table TABREF17 shows statistics of the data. More information about the data is in BIBREF21. We use TRAIN-A to perform supervised modeling with BERT and TRAIN-B for self training, under various conditions. We refer the reader to BIBREF12 for more information about our different experimental settings on dialect id. We acquire our best results with self-training, with a classification accuracy of 49.39% and F1 score at 35.44. This is the winning system model in the MADAR shared task and we showed in BIBREF12 that our tweet-level predictions can be ported to user-level prediction. On user-level detection, our models perform superbly, with 77.40% acc and 71.70% F1 score on unseen MADAR blind test data.",Data and Models ::: Emotion,"We make use of two datasets, the LAMA-DINA dataset from BIBREF22, a Twitter dataset with a combination of gold labels from BIBREF23 and distant supervision labels. The tweets are labeled with the Plutchik 8 primary emotions from the set: {anger, anticipation, disgust, fear, joy, sadness, surprise, trust}. The distant supervision approach depends on use of seed phrases with the Arabic first person pronoun انا> (Eng. “I"") + a seed word expressing an emotion, e.g., فرحان> (Eng. “happy""). The manually labeled part of the data comprises tweets carrying the seed phrases verified by human annotators $9,064$ tweets for inclusion of the respective emotion. The rest of the dataset is only labeled using distant supervision (LAMA-DIST) ($182,605$ tweets) . For more information about the dataset, readers are referred to BIBREF22. The data distribution over the emotion classes is in Table TABREF20. We combine LAMA+DINA and LAMA-DIST training set and refer to this new training set as LAMA-D2 (189,903 tweets). We fine-tune BERT-Based, Multilingual Cased on the LAMA-D2 and evaluate the model with same DEV and TEST sets from LAMA+DINA. On DEV set, the fine-tuned BERT model obtains 61.43% on accuracy and 58.83 on $F_1$ score. On TEST set, we acquire 62.38% acc and 60.32% $F_1$ score.",Data and Models ::: Irony,"We use the dataset for irony identification on Arabic tweets released by IDAT@FIRE2019 shared-task BIBREF24. The shared task dataset contains 5,030 tweets related to different political issues and events in the Middle East taking place between 2011 and 2018. Tweets are collected using pre-defined keywords (i.e., targeted political figures or events) and the positive class involves ironic hashtags such as #sokhria, #tahakoum, and #maskhara (Arabic variants for “irony""). Duplicates, retweets, and non-intelligible tweets are removed by organizers. Tweets involve both MSA as well as dialects at various degrees of granularity such as Egyptian, Gulf, and Levantine. IDAT@FIRE2019 BIBREF24 is set up as a binary classification task where tweets are assigned labels from the set {ironic, non-ironic}. A total of 4,024 tweets were released by organizers as training data. In addition, 1,006 tweets were used by organizers as test data. Test labels were not release; and teams were expected to submit the predictions produced by their systems on the test split. For our models, we split the 4,024 released training data into 90% TRAIN ($n$=3,621 tweets; `ironic'=1,882 and `non-ironic'=1,739) and 10% DEV ($n$=403 tweets; `ironic'=209 and `non-ironic'=194). We use the same small-GRU architecture of Section 3.1 as our baselines. We fine-tune BERT-Based, Multilingual Cased model on our TRAIN, and evaluate on DEV. The small-GRU obtain 73.70% accuracy and 73.47% $F_1$ score. BERT model significantly out-performance than small-GRU, which achieve 81.64% accuracy and 81.62% $F_1$ score.",Data and Models ::: Sentiment,"We collect 15 datasets related to sentiment analysis of Arabic, including MSA and dialects BIBREF25, BIBREF26, BIBREF27, BIBREF1, BIBREF28, BIBREF29, BIBREF30, BIBREF31, BIBREF32, BIBREF33, BIBREF34. Table TABREF28 shows all the corpora we use. These datasets involve different types of sentiment analysis tasks such as binary classification (i.e., negative or positive), 3-way classification (i.e., negative, neutral, or positive), and subjective language detection. To combine these datasets for binary sentiment classification, we normalize different types of label to binary labels in the set $\lbrace `positive^{\prime }, `negative^{\prime }\rbrace $ by following rules: {Positive, Pos, or High-Pos} to `positive'; {Negative, Neg, or High-Neg} to `negative'; Exclude samples which label is not `positive' or `negative' such as `obj', `mixed', `neut', or `neutral'. After label normalization, we obtain 126,766 samples. We split this datase into 80% training (TRAIN), 10% development (DEV), and 10% test (TEST). The distribution of classes in our splits is presented in Table TABREF27. We fine-tune pre-trained BERT on the TRAIN set using PyTorch implementation with $2e-6$ learning rate and 15 epochs, as explained in Section SECREF2. Our best model on the DEV set obtains 80.24% acc and 80.24% $F_1$. We evaluate this best model on TEST set and obtain 77.31% acc and 76.67% $F_1$.",AraNet Design and Use,"AraNet consists of identifier tools including age, gender, dialect, emotion, irony and sentiment. Each tool comes with an embedded model. The tool comes with modules for performing normalization and tokenization. AraNet can be used as a Python library or a command-line tool: Python Library: Importing AraNet module as a Python library provides identifiers’ functions. Prediction is based on a text or a path to a file and returns the identified class label. It also returns the probability distribution over all available class labels if needed. Figure FIGREF34 shows two examples of using the tool as Python library. Command-line tool: AraNet provides scripts supporting both command-line and interactive mode. Command-line mode accepts a text or file path. Interaction mode is good for quick interactive line-by-line experiments and also pipeline redirections. AraNet is available through pip or from source on GitHub with detailed documentation.",Related Works,"As we pointed out earlier, there has been several works on some of the tasks but less on others. By far, Arabic sentiment analysis has been the most popular task. Several works have been performed for MSA BIBREF35, BIBREF0 and dialectal BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF6, BIBREF7 sentiment analysis. A number of works have also been published for dialect detection, including BIBREF9, BIBREF10, BIBREF8, BIBREF11. Some works have been performed on the tasks of age detection BIBREF19, BIBREF36, gender detection BIBREF19, BIBREF36, irony identification BIBREF37, BIBREF24, and emotion analysis BIBREF38, BIBREF22. A number of tools exist for Arabic natural language processing,including Penn Arabic treebank BIBREF39, POS tagger BIBREF40, BIBREF41, Buckwalter Morphological Analyzer BIBREF42 and Mazajak BIBREF7 for sentiment analysis .",Conclusion,"We presented AraNet, a deep learning toolkit for a host of Arabic social media processing. AraNet predicts age, dialect, gender, emotion, irony, and sentiment from social media posts. It delivers state-of-the-art and competitive performance on these tasks and has the advantage of using a unified, simple framework based on the recently-developed BERT model. AraNet has the potential to alleviate issues related to comparing across different Arabic social media NLP tasks, by providing one way to test new models against AraNet predictions. Our toolkit can be used to make important discoveries on the wide region of the Arab world, and can enhance our understating of Arab online communication. AraNet will be publicly available upon acceptance.",,,,,,,,,,,,,,,,,,,Did they experiment on all the tasks?,2419b38624201d678c530eba877c0c016cccd49f,,unfamiliar,no,,50d8b4a941c26b89482c94ab324b5a274f9ced66,False,True,,"Implementation & Models Parameters. For all our tasks, we use the BERT-Base Multilingual Cased model released by the authors . The model is trained on 104 languages (including Arabic) with 12 layer, 768 hidden units each, 12 attention heads, and has 110M parameters in entire model. The model has 119,547 shared WordPieces vocabulary, and was pre-trained on the entire Wikipedia for each language. For fine-tuning, we use a maximum sequence size of 50 tokens and a batch size of 32. We set the learning rate to $2e-5$ and train for 15 epochs and choose the best model based on performance on a development set. We use the same hyper-parameters in all of our BERT models. We fine-tune BERT on each respective labeled dataset for each task. For BERT input, we apply WordPiece tokenization, setting the maximal sequence length to 50 words/WordPieces. For all tasks, we use a TensorFlow implementation. An exception is the sentiment analysis task, where we used a PyTorch implementation with the same hyper-parameters but with a learning rate $2e-6$. We presented AraNet, a deep learning toolkit for a host of Arabic social media processing. AraNet predicts age, dialect, gender, emotion, irony, and sentiment from social media posts. It delivers state-of-the-art and competitive performance on these tasks and has the advantage of using a unified, simple framework based on the recently-developed BERT model. AraNet has the potential to alleviate issues related to comparing across different Arabic social media NLP tasks, by providing one way to test new models against AraNet predictions. Our toolkit can be used to make important discoveries on the wide region of the Arab world, and can enhance our understating of Arab online communication. AraNet will be publicly available upon acceptance.","For all tasks, we use a TensorFlow implementation. AraNet predicts age, dialect, gender, emotion, irony, and sentiment from social media posts. It delivers state-of-the-art and competitive performance on these tasks and has the advantage of using a unified, simple framework based on the recently-developed BERT model. ",1e6f1f17079c5fd39dec880a5002fb9fc8d59412,34c35a1877e453ecaebcf625df3ef788e1953cc4,,,,,,,,,What models did they compare to?,b99d100d17e2a121c3c8ff789971ce66d1d40a4d,,unfamiliar,no,,50d8b4a941c26b89482c94ab324b5a274f9ced66,False,,,"Although we create new models for tasks such as sentiment analysis and gender detection as part of AraNet, our focus is more on putting together the toolkit itself and providing strong baselines that can be compared to. Hence, although we provide some baseline models for some of the tasks, we do not explicitly compare to previous research since most existing works either exploit smaller data (and so it will not be a fair comparison), use methods pre-dating BERT (and so will likely be outperformed by our models) . For many of the tasks we model, there have not been standard benchmarks for comparisons across models. This makes it difficult to measure progress and identify areas worthy of allocating efforts and budgets. As such, by publishing our toolkit models, we believe model-based comparisons will be one way to relieve this bottleneck. For these reasons, we also package models from our recent works on dialect BIBREF12 and irony BIBREF14 as part of AraNet .","Although we create new models for tasks such as sentiment analysis and gender detection as part of AraNet, our focus is more on putting together the toolkit itself and providing strong baselines that can be compared to. Hence, although we provide some baseline models for some of the tasks, we do not explicitly compare to previous research since most existing works either exploit smaller data (and so it will not be a fair comparison), use methods pre-dating BERT (and so will likely be outperformed by our models) . For many of the tasks we model, there have not been standard benchmarks for comparisons across models. This makes it difficult to measure progress and identify areas worthy of allocating efforts and budgets.",9b44463b816b36f0753046936b967760414f856d,34c35a1877e453ecaebcf625df3ef788e1953cc4,What datasets are used in training?,578d0b23cb983b445b1a256a34f969b34d332075,,unfamiliar,no,,50d8b4a941c26b89482c94ab324b5a274f9ced66,False,,,"Arab-Tweet. For modeling age and gender, we use Arap-Tweet BIBREF19 , which we will refer to as Arab-Tweet. Arab-tweet is a tweet dataset of 11 Arabic regions from 17 different countries. For each region, data from 100 Twitter users were crawled. Users needed to have posted at least 2,000 and were selected based on an initial list of seed words characteristic of each region. The seed list included words such as <برشة> /barsha/ ‘many’ for Tunisian Arabic and <وايد> /wayed/ ‘many’ for Gulf Arabic. BIBREF19 employed human annotators to verify that users do belong to each respective region. Annotators also assigned gender labels from the set male, female and age group labels from the set under-25, 25-to34, above-35 at the user-level, which in turn is assigned at tweet level. Tweets with less than 3 words and re-tweets were removed. Refer to BIBREF19 for details about how annotation was carried out. We provide a description of the data in Table TABREF10. Table TABREF10 also provides class breakdown across our splits.We note that BIBREF19 do not report classification models exploiting the data. UBC Twitter Gender Dataset. We also develop an in-house Twitter dataset for gender. We manually labeled 1,989 users from each of the 21 Arab countries. The data had 1,246 “male"", 528 “female"", and 215 unknown users. We remove the “unknown"" category and balance the dataset to have 528 from each of the two `male"" and “female"" categories. We ended with 69,509 tweets for `male"" and 67,511 tweets for “female"". We split the users into 80% TRAIN set (110,750 tweets for 845 users), 10% DEV set (14,158 tweets for 106 users), and 10% TEST set (12,112 tweets for 105 users). We, then, model this dataset with BERT-Base, Multilingual Cased model and evaluate on development and test sets. Table TABREF15 shows that fine-tuned model obtains 62.42% acc on DEV and 60.54% acc on TEST. The dialect identification model in AraNet is based on our winning system in the MADAR shared task 2 BIBREF20 as described in BIBREF12. The corpus is divided into train, dev and test, and the organizers masked test set labels. We lost some tweets from training data when we crawled using tweet ids, ultimately acquiring 2,036 (TRAIN-A), 281 (DEV) and 466 (TEST). We also make use of the task 1 corpus (95,000 sentences BIBREF21). More specifically, we concatenate the task 1 data to the training data of task 2, to create TRAIN-B. Again, note that TEST labels were only released to participants after the official task evaluation. Table TABREF17 shows statistics of the data. We used tweets from 21 Arab countries as distributed by task organizers, except that we lost some tweets when we crawled using tweet ids. We had 2,036 (TRAIN-A), 281 (DEV) and 466 (TEST). For our experiments, we also make use of the task 1 corpus (95,000 sentences BIBREF21). More specifically, we concatenate the task 1 data to the training data of task 2, to create TRAIN-B. Note that both DEV and TEST across our experiments are exclusively the data released in task 2, as described above. TEST labels were only released to participants after the official task evaluation. Table TABREF17 shows statistics of the data. More information about the data is in BIBREF21. We use TRAIN-A to perform supervised modeling with BERT and TRAIN-B for self training, under various conditions. We refer the reader to BIBREF12 for more information about our different experimental settings on dialect id. We acquire our best results with self-training, with a classification accuracy of 49.39% and F1 score at 35.44. This is the winning system model in the MADAR shared task and we showed in BIBREF12 that our tweet-level predictions can be ported to user-level prediction. On user-level detection, our models perform superbly, with 77.40% acc and 71.70% F1 score on unseen MADAR blind test data. We make use of two datasets, the LAMA-DINA dataset from BIBREF22, a Twitter dataset with a combination of gold labels from BIBREF23 and distant supervision labels. The tweets are labeled with the Plutchik 8 primary emotions from the set: {anger, anticipation, disgust, fear, joy, sadness, surprise, trust}. The distant supervision approach depends on use of seed phrases with the Arabic first person pronoun انا> (Eng. “I"") + a seed word expressing an emotion, e.g., فرحان> (Eng. “happy""). The manually labeled part of the data comprises tweets carrying the seed phrases verified by human annotators $9,064$ tweets for inclusion of the respective emotion. The rest of the dataset is only labeled using distant supervision (LAMA-DIST) ($182,605$ tweets) . For more information about the dataset, readers are referred to BIBREF22. The data distribution over the emotion classes is in Table TABREF20. We combine LAMA+DINA and LAMA-DIST training set and refer to this new training set as LAMA-D2 (189,903 tweets). We fine-tune BERT-Based, Multilingual Cased on the LAMA-D2 and evaluate the model with same DEV and TEST sets from LAMA+DINA. On DEV set, the fine-tuned BERT model obtains 61.43% on accuracy and 58.83 on $F_1$ score. On TEST set, we acquire 62.38% acc and 60.32% $F_1$ score. We use the dataset for irony identification on Arabic tweets released by IDAT@FIRE2019 shared-task BIBREF24. The shared task dataset contains 5,030 tweets related to different political issues and events in the Middle East taking place between 2011 and 2018. Tweets are collected using pre-defined keywords (i.e., targeted political figures or events) and the positive class involves ironic hashtags such as #sokhria, #tahakoum, and #maskhara (Arabic variants for “irony""). Duplicates, retweets, and non-intelligible tweets are removed by organizers. Tweets involve both MSA as well as dialects at various degrees of granularity such as Egyptian, Gulf, and Levantine. We collect 15 datasets related to sentiment analysis of Arabic, including MSA and dialects BIBREF25, BIBREF26, BIBREF27, BIBREF1, BIBREF28, BIBREF29, BIBREF30, BIBREF31, BIBREF32, BIBREF33, BIBREF34. Table TABREF28 shows all the corpora we use. These datasets involve different types of sentiment analysis tasks such as binary classification (i.e., negative or positive), 3-way classification (i.e., negative, neutral, or positive), and subjective language detection. To combine these datasets for binary sentiment classification, we normalize different types of label to binary labels in the set $\lbrace `positive^{\prime }, `negative^{\prime }\rbrace $ by following rules:","Arab-Tweet. For modeling age and gender, we use Arap-Tweet BIBREF19 , which we will refer to as Arab-Tweet. UBC Twitter Gender Dataset. We also develop an in-house Twitter dataset for gender. We manually labeled 1,989 users from each of the 21 Arab countries. The dialect identification model in AraNet is based on our winning system in the MADAR shared task 2 BIBREF20 as described in BIBREF12. The corpus is divided into train, dev and test, and the organizers masked test set labels. We make use of two datasets, the LAMA-DINA dataset from BIBREF22, a Twitter dataset with a combination of gold labels from BIBREF23 and distant supervision labels. The rest of the dataset is only labeled using distant supervision (LAMA-DIST) ($182,605$ tweets) . For more information about the dataset, readers are referred to BIBREF22.  We use the dataset for irony identification on Arabic tweets released by IDAT@FIRE2019 shared-task BIBREF24. We collect 15 datasets related to sentiment analysis of Arabic, including MSA and dialects BIBREF25, BIBREF26, BIBREF27, BIBREF1, BIBREF28, BIBREF29, BIBREF30, BIBREF31, BIBREF32, BIBREF33, BIBREF34. Table TABREF28 shows all the corpora we use. ",5d7ee4a4ac4dfc9729570afa0aa189959fe9de25,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,,"Arab-Tweet. For modeling age and gender, we use Arap-Tweet BIBREF19 , which we will refer to as Arab-Tweet. Arab-tweet is a tweet dataset of 11 Arabic regions from 17 different countries. For each region, data from 100 Twitter users were crawled. Users needed to have posted at least 2,000 and were selected based on an initial list of seed words characteristic of each region. The seed list included words such as <برشة> /barsha/ ‘many’ for Tunisian Arabic and <وايد> /wayed/ ‘many’ for Gulf Arabic. BIBREF19 employed human annotators to verify that users do belong to each respective region. Annotators also assigned gender labels from the set male, female and age group labels from the set under-25, 25-to34, above-35 at the user-level, which in turn is assigned at tweet level. Tweets with less than 3 words and re-tweets were removed. Refer to BIBREF19 for details about how annotation was carried out. We provide a description of the data in Table TABREF10. Table TABREF10 also provides class breakdown across our splits.We note that BIBREF19 do not report classification models exploiting the data. UBC Twitter Gender Dataset. We also develop an in-house Twitter dataset for gender. We manually labeled 1,989 users from each of the 21 Arab countries. The data had 1,246 “male"", 528 “female"", and 215 unknown users. We remove the “unknown"" category and balance the dataset to have 528 from each of the two `male"" and “female"" categories. We ended with 69,509 tweets for `male"" and 67,511 tweets for “female"". We split the users into 80% TRAIN set (110,750 tweets for 845 users), 10% DEV set (14,158 tweets for 106 users), and 10% TEST set (12,112 tweets for 105 users). We, then, model this dataset with BERT-Base, Multilingual Cased model and evaluate on development and test sets. Table TABREF15 shows that fine-tuned model obtains 62.42% acc on DEV and 60.54% acc on TEST. The dialect identification model in AraNet is based on our winning system in the MADAR shared task 2 BIBREF20 as described in BIBREF12. The corpus is divided into train, dev and test, and the organizers masked test set labels. We lost some tweets from training data when we crawled using tweet ids, ultimately acquiring 2,036 (TRAIN-A), 281 (DEV) and 466 (TEST). We also make use of the task 1 corpus (95,000 sentences BIBREF21). More specifically, we concatenate the task 1 data to the training data of task 2, to create TRAIN-B. Again, note that TEST labels were only released to participants after the official task evaluation. Table TABREF17 shows statistics of the data. We used tweets from 21 Arab countries as distributed by task organizers, except that we lost some tweets when we crawled using tweet ids. We had 2,036 (TRAIN-A), 281 (DEV) and 466 (TEST). For our experiments, we also make use of the task 1 corpus (95,000 sentences BIBREF21). More specifically, we concatenate the task 1 data to the training data of task 2, to create TRAIN-B. Note that both DEV and TEST across our experiments are exclusively the data released in task 2, as described above. TEST labels were only released to participants after the official task evaluation. Table TABREF17 shows statistics of the data. More information about the data is in BIBREF21. We use TRAIN-A to perform supervised modeling with BERT and TRAIN-B for self training, under various conditions. We refer the reader to BIBREF12 for more information about our different experimental settings on dialect id. We acquire our best results with self-training, with a classification accuracy of 49.39% and F1 score at 35.44. This is the winning system model in the MADAR shared task and we showed in BIBREF12 that our tweet-level predictions can be ported to user-level prediction. On user-level detection, our models perform superbly, with 77.40% acc and 71.70% F1 score on unseen MADAR blind test data. Data and Models ::: Emotion We make use of two datasets, the LAMA-DINA dataset from BIBREF22, a Twitter dataset with a combination of gold labels from BIBREF23 and distant supervision labels. The tweets are labeled with the Plutchik 8 primary emotions from the set: {anger, anticipation, disgust, fear, joy, sadness, surprise, trust}. The distant supervision approach depends on use of seed phrases with the Arabic first person pronoun انا> (Eng. “I"") + a seed word expressing an emotion, e.g., فرحان> (Eng. “happy""). The manually labeled part of the data comprises tweets carrying the seed phrases verified by human annotators $9,064$ tweets for inclusion of the respective emotion. The rest of the dataset is only labeled using distant supervision (LAMA-DIST) ($182,605$ tweets) . For more information about the dataset, readers are referred to BIBREF22. The data distribution over the emotion classes is in Table TABREF20. We combine LAMA+DINA and LAMA-DIST training set and refer to this new training set as LAMA-D2 (189,903 tweets). We fine-tune BERT-Based, Multilingual Cased on the LAMA-D2 and evaluate the model with same DEV and TEST sets from LAMA+DINA. On DEV set, the fine-tuned BERT model obtains 61.43% on accuracy and 58.83 on $F_1$ score. On TEST set, we acquire 62.38% acc and 60.32% $F_1$ score. We use the dataset for irony identification on Arabic tweets released by IDAT@FIRE2019 shared-task BIBREF24. The shared task dataset contains 5,030 tweets related to different political issues and events in the Middle East taking place between 2011 and 2018. Tweets are collected using pre-defined keywords (i.e., targeted political figures or events) and the positive class involves ironic hashtags such as #sokhria, #tahakoum, and #maskhara (Arabic variants for “irony""). Duplicates, retweets, and non-intelligible tweets are removed by organizers. Tweets involve both MSA as well as dialects at various degrees of granularity such as Egyptian, Gulf, and Levantine. We collect 15 datasets related to sentiment analysis of Arabic, including MSA and dialects BIBREF25, BIBREF26, BIBREF27, BIBREF1, BIBREF28, BIBREF29, BIBREF30, BIBREF31, BIBREF32, BIBREF33, BIBREF34. Table TABREF28 shows all the corpora we use. These datasets involve different types of sentiment analysis tasks such as binary classification (i.e., negative or positive), 3-way classification (i.e., negative, neutral, or positive), and subjective language detection. To combine these datasets for binary sentiment classification, we normalize different types of label to binary labels in the set $\lbrace `positive^{\prime }, `negative^{\prime }\rbrace $ by following rules:","For modeling age and gender, we use Arap-Tweet BIBREF19 , which we will refer to as Arab-Tweet. UBC Twitter Gender Dataset. We also develop an in-house Twitter dataset for gender.  The dialect identification model in AraNet is based on our winning system in the MADAR shared task 2 BIBREF20 as described in BIBREF12.  Emotion
We make use of two datasets, the LAMA-DINA dataset from BIBREF22, a Twitter dataset with a combination of gold labels from BIBREF23 and distant supervision labels.  We use the dataset for irony identification on Arabic tweets released by IDAT@FIRE2019 shared-task BIBREF24. We collect 15 datasets related to sentiment analysis of Arabic, including MSA and dialects BIBREF25, BIBREF26, BIBREF27, BIBREF1, BIBREF28, BIBREF29, BIBREF30, BIBREF31, BIBREF32, BIBREF33, BIBREF34.",bd0a4cc46a4c8bef5c8b1c2f8d7e8b72d4f81308,34c35a1877e453ecaebcf625df3ef788e1953cc4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1-Figure1-1.png,Figure 1: A map of Arab countries. Our different datasets cover varying regions of the Arab world as we describe in each section.,3-Table1-1.png,Table 1: Distribution of age and gender classes in our Arab-Tweet data splits,3-Table2-1.png,Table 2: Model performance in accuracy of Arab-Tweet age and gender classification tasks.,3-Table4-1.png,Table 4: Distribution of classes within the MADAR twitter corpus.,4-Table6-1.png,Table 6: Model performance of irony detection.,4-Table5-1.png,Table 5: Class distribution of LAMA+DINA and LAMADIST datasets.,,,,,,,,," we do not explicitly compare to previous research since most existing works either exploit smaller data (and so it will not be a fair comparison), use methods pre-dating BERT (and so will likely be outperformed by our models)","Arap-Tweet BIBREF19  an in-house Twitter dataset for gender the MADAR shared task 2 BIBREF20 the LAMA-DINA dataset from BIBREF22 LAMA-DIST Arabic tweets released by IDAT@FIRE2019 shared-task BIBREF24 BIBREF25, BIBREF26, BIBREF27, BIBREF1, BIBREF28, BIBREF29, BIBREF30, BIBREF31, BIBREF32, BIBREF33, BIBREF34",4-Table7-1.png,Table 7: Distribution of sentiment analysis classes in our data splits.,5-Table8-1.png,"Table 8: Sentiment Analysis Datasets. SA: Sentiment Analysis, SSA: Subjective Sentiment Analysis.",5-Figure2-1.png,Figure 2: AraNet usage and output as Python library.,5-Figure3-1.png,"Figure 3: AraNet usage examples as command-line mode, pipeline and interactive mode.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,," Arap-Tweet  UBC Twitter Gender Dataset MADAR  LAMA-DINA  IDAT@FIRE2019 15 datasets related to sentiment analysis of Arabic, including MSA and dialects",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bayesian Sparsification of Recurrent Neural Networks,Recurrent neural networks show state-of-the-art results in many text analysis tasks but often require a lot of memory to store their weights. Recently proposed Sparse Variational Dropout eliminates the majority of the weights in a feed-forward neural network without significant loss of quality. We apply this technique to sparsify recurrent neural networks. To account for recurrent specifics we also rely on Binary Variational Dropout for RNN. We report 99.5% sparsity level on sentiment analysis task without a quality drop and up to 87% sparsity level on language modeling task with slight loss of accuracy.,Introduction,"Recurrent neural networks (RNNs) are among the most powerful models for natural language processing, speech recognition, question-answering systems and other problems with sequential data BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . For complex tasks such as machine translation BIBREF5 or speech recognition BIBREF3 modern RNN architectures incorporate a huge number of parameters. To use these models on portable devices with limited memory, for instance, smartphones, the model compression is desired. High compression level may also lead to an acceleration of RNNs. In addition, compression regularizes RNNs and helps to avoid overfitting. There are a lot of RNNs compression methods based on specific weight matrix representations BIBREF7 , BIBREF8 or sparsification via pruning BIBREF9 . In this paper we focus on RNNs compression via sparsification. Most of the methods from this group are heuristic and require time-consuming hyperparameters tuning. Recently Molchanov et. al. dmolch proposed a principled method based on variational dropout for sparsification of fully connected and convolutional networks. A probabilistic model was described in which parameters controlling sparsity are tuned automatically during neural network training. This model called Sparse Variational Dropout (Sparse VD) leads to extremely sparse solutions without a significant quality drop. However, this technique was not previously investigated for RNNs. In this paper we apply Sparse VD to recurrent neural networks. To take into account the specifics of RNNs we rely on some insights underlined in the paper by Gal & Ghahramani gal where they explain the proper way to use binary dropout in RNNs from the Bayesian point of view. In the experiments we show that LSTMs with Sparse VD yield high sparsity level with just a slight drop in quality. We achieved 99.5% sparsity level on sentiment analysis task and up to 87.6% in character level language modeling experiment.",Bayesian Neural Networks,"Consider a neural network with weights $\omega $ modeling the dependency of the target variables $y=\lbrace y^1, \dots , y^\ell \rbrace $ on the corresponding input objects $X = \lbrace x^1, \dots , x^\ell \rbrace $ . In a Bayesian neural network the weights $\omega $ are treated as random variables. With the prior distribution $p(\omega )$ we search for the posterior distribution $p(\omega |X, y)$ that will help to find expected target value during inference. In the case of neural networks, true posterior is usually intractable but it can be approximated by some parametric distribution $q_\lambda (\omega )$ . The quality of this approximation is measured by the KL-divergence $KL(q_\lambda (\omega )||p(\omega |X, y))$ . The optimal parameter $\lambda $ can be found by maximization of the variational lower bound w.r.t. $\lambda $ :  $$\mathcal {L}=\sum _{i=1}^\ell \mathbb {E}_{q_\lambda (\omega )} \log p(y^i|x^i, \omega ) - KL(q_\lambda (\omega )||p(\omega ))$$   (Eq. 2)  The expected log-likelihood term in ( 2 ) is usually approximated by Monte-Carlo sampling. To make the MC estimation unbiased, the weights are parametrized by a deterministic function: $\omega = g(\lambda , \xi )$ , where $\xi $ is sampled from some non-parametric distribution (the reparameterization trick BIBREF10 ). The KL-divergence term in ( 2 ) acts as a regularizer and is usually computed or approximated analytically.",Sparse Variational Dropout,"Dropout BIBREF11 is a standard technique for regularization of neural networks. It implies that inputs of each layer are multiplied by a randomly generated noise vector. The elements of this vector are usually sampled from Bernoulli or Gaussian distribution with the parameters tuned using cross-validation. Kingma et al. kingma interpreted Gaussian dropout from a Bayesian perspective that allowed to tune dropout rate automatically during model training. Later this model was extended to sparsify fully connected and convolutional neural networks resulting in a model called Sparse Variational Dropout (Sparse VD) BIBREF0 . Consider one dense layer of a feed-forward neural network with an input of the size $n$ , an output of the size $m$ and a weight matrix $W$ . Following Kingma et al. kingma, in Sparse VD the prior on the weights is a fully factorized log-uniform distribution $p(|w_{ij}|) \propto \frac{1}{|w_{ij}|}$ and the posterior is searched in the form of fully factorized normal distribution:  $$q(w_{ij}|m_{ij}, \alpha _{ij}) = \mathcal {N}(m_{ij}, \alpha _{ij} m^2_{ij}).$$   (Eq. 4)  Employment of such form of the posterior distribution is equivalent to putting multiplicative BIBREF12 or additive BIBREF0 normal noise on the weights in the following manner:  $$w_{ij} = m_{ij} \xi _{ij}, \quad \xi _{ij}\sim \mathcal {N}(1, \alpha _{ij}),$$   (Eq. 5)  $$w_{ij} = m_{ij} + \epsilon _{ij}, \quad \epsilon _{ij}\sim \mathcal {N}(0, \sigma ^2_{ij}), \quad \alpha _{ij} = \frac{\sigma ^2_{ij}}{m^2_{ij}}.$$   (Eq. 6)  The representation ( 6 ) is called additive reparameterization BIBREF0 . It reduces the variance of the gradients of $\mathcal {L}$ w. r. t. $m_{ij}$ . Moreover, since a sum of normal distributions is a normal distribution with computable parameters, the noise may be applied to the preactivation (input vector times weight matrix $W$ ) instead of $W$ . This trick is called the local reparameterization trick BIBREF13 , BIBREF12 and it reduces the variance of the gradients even further and makes training more efficient. In Sparse VD optimization of the variational lower bound ( 2 ) is performed w. r. t. $\lbrace M, \log \sigma \rbrace $ . The KL-divergence factorizes over the weights and its terms depend only on $\alpha _{ij}$ because of the specific choice of the prior BIBREF12 :  $$KL(q(w_{ij}|m_{ij}, \alpha _{ij})||p(w_{ij}))=k(\alpha _{ij}).$$   (Eq. 7)  Each term can be approximated as follows BIBREF0 :  $${\begin{array}{c}k(\alpha ) \approx 0.64 \sigma (1.87 + 1.49\log \alpha )-\\
\:\:\:\,- 0.5 \log (1 + \alpha ^{-1}) + C.

\end{array}}$$   (Eq. 8)  KL-divergence term encourages large values of $\alpha _{ij}$ . If $\alpha _{ij} \rightarrow \infty $ for a weight $w_{ij}$ , the posterior over this weight is a high-variance normal distribution and it is beneficial for model to put $m_{ij} = 0$ as well as $\sigma _{ij}=\alpha _{ij} m^2_{ij}=0$ to avoid inaccurate predictions. As a result, the posterior over $w_{ij}$ approaches zero-centered $\delta $ -function, the weight does not affect the network's output and can be ignored.",Dropout for Recurrent Neural Networks,"Yet another Bayesian model was proposed by Gal & Ghahramani bindrop to explain the binary dropout. On this base, a recipe how to apply a binary dropout to the RNNs properly was proposed by Gal & Ghahramani gal. The recurrent neural network takes a sequence $x = [x_0, \dots , x_T]$ , $x_t\in \mathbb {R}^n$ as an input and maps it into the sequence of hidden states:  $${\begin{array}{c}h_{t} = f_h(x_t, h_{t-1}) = g_h(x_{t} W^x + h_{t-1} W^h + b_1)\\
h_i \in \mathbb {R}^m, \, h_0 = \bar{0}
\end{array}}$$   (Eq. 10)  Throughout the paper, we assume that the output of the RNN depends only on the last hidden state:  $$y = f_y(h_T) = g_y(h_T W^y + b_2).$$   (Eq. 11)  Here $g_h$ and $g_y$ are some nonlinear functions. However, all the techniques we discuss further can be easily applied to the more complex setting, e. g. language model with several outputs for one input sequence (one output for each time step). Gal & Ghahramani gal considered RNNs as Bayesian networks. The prior on the recurrent layer weights $\omega =\lbrace W^x, W^h\rbrace $ is a fully factorized standard normal distribution. The posterior is factorized over the rows of weights, and each factor is searched as a mixture of two normal distributions: $
q(w^x_k|m^x_k) = p^x \mathcal {N}(0, \sigma ^2 I) + (1-p^x) \mathcal {N}(m^x_k, \sigma ^2 I),\quad \:
$  $$q(w^h_j|m^h_j) = p^h \mathcal {N}(0, \sigma ^2 I) + (1-p^h) \mathcal {N}(m^h_j, \sigma ^2 I),$$   (Eq. 12)  Under assumption $\sigma \approx 0$ sampling the row of weights from such posterior means putting all the weights from this row either to 0 (drop the corresponding input neuron) or to some learned values. Thus this model is a probabilistic analog of binary dropout with dropout rates $p^x$ and $p^h$ . After unfolding the recurrence in the network, the maximization of the variational lower bound for such model looks as follows:  $$\sum _{i=1}^\ell \int q(\omega |M) \log \Bigl (y^i\big |f_y\bigl (f_h(x^i_T, f_h(\dots f_h (x^i_1, h^i_0))\bigr )\Bigr ) d \omega - \\
-KL\Bigl (q(\omega |M)\big \Vert p(\omega )\Bigr ) \rightarrow \max _{M}$$   (Eq. 13)  Each integral in the first part of ( 13 ) is estimated with MC integration with a single sample $\hat{\omega }_i \sim q(\omega |M)$ . To make this estimation unbiased: (a) the weights sample $\hat{\omega }_i$ should remain the same for all time steps $t=\overline{1, T}$ for a fixed object; (b) dropout rates $p^x$ and $p^h$ should be fixed because the distribution we are sampling from depends on them. The KL-divergence term from ( 13 ) is approximately equivalent to $L_2$ regularization of the variational parameters $M$ . Finally, this probabilistic model leads to the following dropout application in RNNs: we sample a binary mask for the input and hidden neurons, one mask per object for all moments of time, and optimize the $L_2$ -regularized log-likelihood with the dropout rates and the weight of $L_2$ -regularization chosen using cross-validation. Also, the same dropout technique may be applied to forward connections in RNNs, for example in embedding and dense layers BIBREF1 . The same technique can be applied to more complex architectures like LSTM in which the information flow between input and hidden units is controlled by the gate elements:  $$i = sigm(h_{t-1}W^h_i + x_t W^x_i) \quad o = sigm(h_{t-1}W^h_o + x_t W^x_o)$$   (Eq. 14)  $$f = sigm(h_{t-1}W^h_f + x_t W^x_f) \quad 
g = tanh(h_{t-1} W^h_g + x_t W^x_g)$$   (Eq. 15)  Here binary dropout masks for input and hidden neurons are generated 4 times: individually for each of the gates $i,o,f$ and input modulation $g$ .",Variational Dropout for RNN sparsification,"Dropout for RNNs proposed by Gal & Ghahramani gal helps to avoid overfitting but is very sensitive to the choice of the dropout rates. On the other hand, Sparse VD allows automatic tuning of the Gaussian dropout parameters individually for each weight which results in the model sparsification. We combine these two techniques to sparsify and regularize RNNs. Following Molchanov et al. dmolch, we use the fully factorized log-uniform prior and approximate the posterior with a fully factorized normal distribution over the weights $\omega =\lbrace W^x, W^h\rbrace $ :  $${\begin{array}{c}q(w^x_{ki}|m^x_{ki}, \sigma ^x_{ki}) = \mathcal {N}\bigl (m^x_{ki}, {\sigma ^x_{ki}}^2\bigr ), \:\\
q(w^h_{ji}|m^h_{ji}, \sigma ^h_{ji}) = \mathcal {N}\bigl (m^h_{ji}, {\sigma ^h_{ji}}^2\bigr ), \end{array}}$$   (Eq. 17)  where $\sigma ^x_{ki}$ and $\sigma ^h_{ji}$ have the same meaning as in additive reparameterization ( 6 ). To train the model, we maximize the variational lower bound approximation  $$\sum _{i=1}^\ell \int q(\omega |M, \sigma ) \log \Bigl (y^i\big |f_y\bigl (f_h(x^i_T, f_h(\dots f_h (x^i_1, h^i_0))\bigr )\Bigr ) d \omega - \\
- \sum _{k,i=1}^{n,m} k\biggl (\frac{{\sigma ^x_{ki}}^2}{{m^x_{ki}}^2}\biggr ) - \sum _{j,i=1}^{m,m} k\biggl (\frac{{\sigma ^h_{ji}}^2}{{m^h_{ji}}^2}\biggr )$$   (Eq. 18)  w. r. t. $\lbrace M, \log \sigma \rbrace $ using stochastic mini-batch methods. Here the recurrence in the expected log-likelihood term is unfolded as in ( 13 ) and the KL is approximated using ( 8 ). The integral in ( 18 ) is estimated with a single sample $\hat{\omega }_i \sim q(\omega |M, \alpha )$ per input sequence. We use the reparameterization trick (for unbiased integral estimation) and additive reparameterization (for gradients variance reduction) to sample both input-to-hidden and hidden-to-hidden weight matrices $\widehat{W}^x, \widehat{W}^h$ . To reduce the variance of the gradients and for more computational efficiency we also apply the local reparameterization trick to input-to-hidden matrix $\widehat{W}^x$ moving the noise from the weights to the preactivations:  $${\begin{array}{c}(x_t \widehat{W}^x)_j = \sum _{k=1}^n x_{t,k} m^x_{kj} +
\epsilon _j \sqrt{\sum _{k=1}^n x^2_{t,k} {\sigma ^x_{kj}}^2}\:, \\
\epsilon _j \sim \mathcal {N}(0, 1).
\end{array}}$$   (Eq. 19)  As a result, only 2-dimensional noise on input-to-hidden connections is required for each mini-batch: we generate one noise vector of length $m$ for each object in a mini-batch. The local reparameterization trick cannot be applied to the hidden-to-hidden matrix $W^h$ . We use the same sample $\widehat{W}^h$ for all moments of time, therefore in the multiplication $h_{t-1} \widehat{W}^h$ the vector $h_{t-1}$ depends on $\widehat{W}^h$ and the rule about the sum of normally distributed random variables cannot be applied. Since usage of 3-dimensional noise (2 dimensions of $\widehat{W}^h$ and a mini-batch size) is too resource-consuming we sample one noise matrix for all objects in a mini-batch for efficiency:  $$\hat{w}^h_{ji}=m^h_{ji}+\sigma ^j_{ji}\epsilon ^h_{ji},\quad \epsilon ^h_{ji} \sim \mathcal {N}(0, 1).$$   (Eq. 20)  The final framework works as follows: we sample Gaussian additive noise on the input-to-hidden preactivations (one per input sequence) and hidden-to-hidden weight matrix (one per mini-batch), optimize the variational lower bound ( 18 ) w. r. t. $\lbrace M, \log \sigma \rbrace $ , and for many weights we obtain the posterior in the form of a zero-centered $\delta $ -function because the KL-divergence encourages sparsity. These weights can then be safely removed from the model. In LSTM the same prior-posterior pair is consisered for all input-to-hidden and hidden-to-hidden matrices and all computations stay the same. The noise matrices for input-to-hidden and hidden-to-hidden connections are generated individually for each of the gates $i,o,f$ and input modulation $g$ .",Experiments,"We perform experiments with LSTM as the most popular recurrent architecture nowadays. We use Theano BIBREF14 and Lasagne BIBREF15 for implementation. The source code will be available soon at https://github.com/tipt0p/SparseBayesianRNN. We demonstrate the effectiveness of our approach on two diverse problems: Character Level Language Modeling and Sentiment Analysis. Our results show that Sparse Variational Dropout leads to a high level of sparsity in recurrent models without a significant quality drop. We use the dropout technique of Gal & Ghahramani gal as a baseline because it is the most similar dropout technique to our approach and denote it VBD (variational binary dropout). According to Molchanov et al. dmolch, training neural networks with Sparse Variational Dropout from a random initialization is troublesome, as a lot of weights may become pruned away before they could possibly learn something useful from the data. We observe the same effect in our experiments with LSTMs, especially with more complex models. LSTM trained from a random initialization may have high sparsity level, but also have a noticeable quality drop. To overcome this issue we start from pre-trained models that we obtain by training networks without Sparse Variational Dropout for several epochs. Weights in models with Sparse Variational Dropout cannot converge exactly to zero because of the stochastic nature of the training procedure. To obtain sparse networks we explicitly put weights with high corresponding dropout rates to 0 during testing as in Molchanov et al. dmolch. We use the value $\log \alpha = 3$ as a threshold. For all weights that we sparsify using Sparse Variational Dropout, we initialize $\log {\sigma ^2}$ with -6. We optimize our networks using Adam BIBREF16 . Networks without any dropout overfit for both our tasks, therefore, we present results for them with early stopping. Throughout experiments we use the mean values of the weights to evaluate the model quality (we do not sample weights from posterior on the evaluating phase). This is a common practice when working with dropout.",Sentiment Analysis,"Following Gal & Ghahramani gal we evaluated our approach on the sentiment analysis regression task. The dataset is constructed based on Cornell film reviews corpus collected by Pang & Lee regrdata. It consists of approximately 10 thousands non-overlapping segments of 200 words from the reviews. The task is to predict corresponding film scores from 0 to 1. We use the provided train and test partitions. We use networks with one embedding layer of 128 units, one LSTM layer of 128 hidden units, and finally, a fully connected layer applied to the last output of the LSTM (resulting in a scalar output). All weights are initialized in the same way as in Gal & Ghahramani gal. We train our networks using batches of size 128 and a learning rate of 0.001 for 1000 epochs. We also clip the gradients with threshold 0.1. For all layers with VBD we use dropout rate 0.3 and weight decay $10^{-3}$ (these parameters are chosen using cross validation). As a baseline, we train the network without any dropout and with VBD on all layers. In this experiment, our goal is to check the applicability of Sparse VD for recurrent networks, therefore we apply it only to LSTM layer. For embedding and dense layers we use VBD. We try both start training of the network with Sparse VD from random initialization and from two different pre-trained models. The first pre-trained model is obtained after 4 epochs of training of the network without any dropout. The second one is obtained after 200 epochs of training of the network with VBD on all layers. We choose number of pretraining epochs using models quality on cross-validation. The results are shown in Table 1 . In this task our approach achieves extremely high sparsity level both from random initialization and from pre-trained models. Sparse VD networks trained from pre-trained models achieve even better quality than baselines. Note that models already have this sparsity level after approximately 20 epochs.",Character Level Language Modeling,"Following Mikolov et al. mikolov11 we use the Penn Treebank Corpus to train our Language Model (LM). The dataset contains approximately 6 million characters and a vocabulary of 50 characters. We use the provided train, validation and test partitions. We use networks with one LSTM layer of 1000 hidden units to solve the character level LM task. All weight matrices of the networks are initialized orthogonally and all biases are initialized with zeros. Initial values of hidden and cell elements are trainable and also initialized with zeros. We train our networks on non-overlapping sequences of 100 characters in batches of 64 using a learning rate of 0.002 for 50 epochs, and clip gradients with threshold 1. For all layers with VBD we use dropout rate 0.25 and do not use weight decay (these parameters are chosen using quality of VDB model on validation set). As a baseline, we train the network without any dropout and with VBD only on recurrent weights (hidden-to-hidden). Semeniuta et al. semeniuta16 showed that for this particular task applying dropout for feed-forward connections additionally to VBD on recurrent ones does not improve the network quality. We observe the same effect in our experiments. In this experiment we try to sparsify both LSTM and dense layers therefore we apply Sparse VD for all layers. We try both start training of the network with Sparse VD from random initialization and from two different pre-trained models. The first pre-trained model is obtained after 11 epochs of training of the network without any dropout. The second one is obtained after 50 epochs of training of the network with VBD on recurrent connections. We choose the number of pretraining epochs using models quality on validation set. The results are shown in Table 2 . Here we do not achieve such extreme sparsity level as in the previous experiment. This effect may be a consequence of the higher complexity of the task. Also in LM problem we have several outputs for one input sequence (one output for each time step) instead of one output in Sentiment regression. As a result the log-likelihood part of the loss function is much stronger for LM task and regularizer can not sparsify the network so effectively. Here we see that the balance between the likelihood and the regularizer varies a lot for different tasks with RNNs and should be explored futher. Fig. 1 and 2 show the progress of test quality and network sparsity level through the training process. Sparse VD network trained from random initialization underfits and therefore has a slight quality drop in comparison to baseline network without regularization. Sparse VD networks trained from pre-trained models achieve much higher quality but have lower sparsity levels than the one trained from random initialization. Better pretrained models are harder to sparsify. The quality of the model pretrained with VBD drops on the first epoches while the sparsity grows, and the model does not fully recover later.",Regularization of RNNs,"Deep neural networks often suffer from overfitting, and different regularization techniques are used to improve their generalization ability. Dropout BIBREF11 is a popular method of neural networks regularization. The first successful implementations of this method for RNNs BIBREF17 , BIBREF18 applied dropout only for feed-forward connections and not recurrent ones. Introducing dropout in recurrent connections may lead to a better regularization technique but its straightforward implementation may results in underfitting and memory loss through time BIBREF19 . Several ways of dropout application for recurrent connections in LSTM were proposed recently BIBREF20 , BIBREF1 , BIBREF19 . These methods inject binary noise into different parts of LSTM units. Semeniuta et al. semeniuta16 shows that proper implementation of dropout for recurrent connections is important not only for effective regularization but also to avoid vanishing gradients. Bayer et al. bayer13 successfully applied fast dropout BIBREF13 , a deterministic approximation of dropout, to RNNs. Krueger et al. zoneout introduced zoneout which forces some hidden units to maintain their previous values, like in feedforward stochastic depth networks BIBREF21 .",Compression of RNNs,"Reducing RNN size is an important and rapidly developing area of research. One possible concept is to represent large RNN weight matrix by some approximation of the smaller size. For example, Tjandra et. al. tjandra use Tensor Train decomposition of the weight matrices and Le et al. kroneker approximate this matrix with Kronecker product. Hubara et. al itay limit the weights and activations to binary values proposing a way how to compute gradients w. r. t. them. Another concept is to start with a large network and to reduce its size during or after training. The most popular approach here is pruning: the weights of the RNN are cut off on some threshold. Narang et al. pruning choose threshold using several hyperparameters that control the frequency, the rate and the duration of the weights eliminating.",Discussion and future work,"When applying Sparse VD to RNNs we rely on the dropout for RNNs proposed by Gal & Ghahramani gal. The reason is that this dropout technique for RNNs is the closest one to Sparse VD approach. However, there are several other dropout methods for recurrent networks that outperform this baseline BIBREF19 , BIBREF22 . Comparison with them is our future work. Combining Sparse VD with these latest dropout recipes is also an interesting research direction. The challenge here is that the noise should be put on the neurons or gates instead of the weights as in our model. However, there are several recent papers BIBREF23 , BIBREF24 where group sparsity methods are proposed for fully connected and convolutional networks. These methods can be used to solve the underlined problem. The comparison of our approach with other RNN sparsification techniques is still a work-in-progress. It would be interesting to perform this comparison on larger networks, for example, for speech recognition task. One more curious direction of the research is to sparsify not only recurrent layer but an embedding layer too. It may have a lot of parameters in the tasks with large dictionary, such as word based language modeling.",Acknowledgements," We would like to thank Dmitry Molchanov and Arsenii Ashukha for valuable feedback. Nadezhda Chirkova has been supported by Russian Academic Excellence Project `5-100', and Ekaterina Lobacheva has been supported by Russian Science Foundation grant 17-71-20072. We would also like to thank the Department of Algorithms and Theory of Programming, Faculty of Innovation and High Technology in Moscow Institute of Physics and Technology for provided computational resources.",,,,,,,,,,,,,,,,,,,What is binary variational dropout?,dee7383a92c78ea49859a2d5ff2a9d0a794c1f0f,two,unfamiliar,no,,50d8b4a941c26b89482c94ab324b5a274f9ced66,False,,,We use the dropout technique of Gal & Ghahramani gal as a baseline because it is the most similar dropout technique to our approach and denote it VBD (variational binary dropout).,We use the dropout technique of Gal & Ghahramani gal as a baseline because it is the most similar dropout technique to our approach and denote it VBD (variational binary dropout).,be1be88d22f08356a6321afab50db95228600712,35491e1e579f6d147f4793edce4c1a80ab2410e7,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4-Table1-1.png,Table 1. Results on sentiment regression task. Prediction quality is reported in MSE (the lower the better). Sparsity levels reported for W x and Wh separately in percents of zero weights. For Sparse VD methods initialization types are reported in brackets.,5-Table2-1.png,"Table 2. Results on character level Language Modelling task. Prediction quality is reported in bits-per-character (lower is better). Sparsity levels reported for W x, Wh and W y separately in percents of zero weights. For Sparse VD methods initialization types are reported in brackets.",6-Figure1-1.png,"Figure 1. Results on character level Language Modelling task: prediction quality on the test set in bits-per-character (lower is better). Left — training from random initialization, right — training from pretrained initialization. Stars correspond to pretrained models. For Sparse VD methods initialization types are reported in brackets.",6-Figure2-1.png,"Figure 2. Results on character level Language Modelling task: sparsity levels for W y,W x,Wh in LSTM. Initialization types are reported in brackets.",,,,,,,,,,,,,,,,,,,,,,,,,,the dropout technique of Gal & Ghahramani gal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Variational Transformers for Diverse Response Generation,"Despite the great promise of Transformers in many sequence modeling tasks (e.g., machine translation), their deterministic nature hinders them from generalizing to high entropy tasks such as dialogue response generation. Previous work proposes to capture the variability of dialogue responses with a recurrent neural network (RNN)-based conditional variational autoencoder (CVAE). However, the autoregressive computation of the RNN limits the training efficiency. Therefore, we propose the Variational Transformer (VT), a variational self-attentive feed-forward sequence model. The VT combines the parallelizability and global receptive field of the Transformer with the variational nature of the CVAE by incorporating stochastic latent variables into Transformers. We explore two types of the VT: 1) modeling the discourse-level diversity with a global latent variable; and 2) augmenting the Transformer decoder with a sequence of fine-grained latent variables. Then, the proposed models are evaluated on three conversational datasets with both automatic metric and human evaluation. The experimental results show that our models improve standard Transformers and other baselines in terms of diversity, semantic relevance, and human judgment.",Introduction,"Convolutional and fully-attentional feed-forward architectures, such as Transformers BIBREF0, have emerged as effective alternatives to RNNs BIBREF1 in wide range of NLP tasks. These architectures remove the computational temporal dependency during the training and effectively address the long-standing vanishing gradients problem of recurrent models by processing all inputs simultaneously. Notably, transformers apply a fully attention strategy, where each token in the sequence is informed by other tokens via a self-attention mechanism. It acts as an effectively global receptive field across the whole sequences which absence in RNNs. Despite the powerful modeling capability of trasnformers, they often fail to model one-to-many relation in dialogue response generation tasks BIBREF2 due to their deterministic nature. As a result, they generate dull and generic response (e.g., “I am not sure""), especially with greedy and beam search, which are widely used in other sequence modeling tasks. There have been attempts to generate diverse and informative dialogue responses by incorporating latent variable(s) into the RNN encoder-decoder architecture. In particular BIBREF2 adapt a conditional variational autoencoder (CVAE) to capture discourse-level variations of dialogue, while BIBREF3 and BIBREF4 integrates latent variables in the hidden states of the RNN decoder. However, the inherently sequential computation of aforementioned models limit the efficiency for large scale training. In this paper, we introduce the Variational Transformer (VT) a variational self-attentive feed-forward sequence model to address the aforementioned issues. The VT combine the parallelizability and global receptive field of the transformer with the variational nature of CVAE by incorporating stochastic latent variables into transformers. We explore two types of VT: 1) Global Variational Transformer (GVT), and 2) Sequential Variational Transformer. The GVT is the extension of CVAE in BIBREF2, which modeling the discourse-level diversity with a global latent variable, While SVT, inspired by variational autoregressive models BIBREF3, BIBREF4, incorporates a sequence of latent variables into decoding process by using a novel variational decoder layer. Unlike previous approaches BIBREF2, BIBREF3, BIBREF4, SVT uses Non-causal Multi-head Attention, which attend to future tokens for computing posterior latent variables instead of using an additional encoder. The proposed VT architectures integrate stochastic latent variables into Transformers. The experimental results on a three conversation dataset demonstrate that our models can generate more informative and coherent responses.",Related work ::: Neural Conversational Models,"Conversational systems has been widely studied BIBREF5, BIBREF6, BIBREF7, BIBREF8. Compare to rule-based systems BIBREF5, BIBREF6, sequence-to-sequence conversation models achieve superior performance in terms of scalable training and generalization ability BIBREF7. However, it has been pointed out that encoder-decoder models tend to generate generic and repetitive responses like “I am sorry"" BIBREF9. To address this issue, there have been three main lines of work. The first is adding additional information (e.g., persona) as input to guild model generate more informative responses BIBREF10, BIBREF11. The second modifies the learning objective to promote more diverse generation BIBREF9, and the third integrates stochastic latent variables into Seq2Seq models by using the CVAE framework BIBREF12, BIBREF2. Our work comes within this third line introducing a novel model, the Variational Transformer, to improve dialogue response generation.",Related work ::: Conditional Variational Autoencoders,"Many works have attempted to combine CVAEs with encoder-decoder architectures for sequence generation tasks. BIBREF13 propose a variational encoder-decoder model for neural machine translation, while BIBREF14 apply variational recurrent neural networks (VRNN) BIBREF15 for text summarization. BIBREF2 and BIBREF16 explore incorporating meta features into CVAE framework in dialogue response generation tasks. BIBREF3 and BIBREF4 propose variational autoregressive decoders which enhanced by highly multi-modal latent variables to capture the high variability in dialogue responses. BIBREF17 further augment variational autoregressive decoders with dynamic memory networks for improving generation quality. We unify the previous successful ideas of CVAE, and explore the combinations of CVAE and Transformer.",Related work ::: Fully Attentional Networks,"Taking advantage of the parallel-in-time structure and global receptive field, Transformers BIBREF0 have recently been shown to achieve impressive results on various sequence modeling tasks. Based on this, several follow-up models have been presented. The Image Transformer BIBREF18 has been proposed for image generation, while the MultiModel BIBREF19 integrates convolution, attention and sparsely-gated mixture-of-expert blocks into a single deep-learning model for simultaneously learning multiple tasks from various domains. BIBREF20 proposed a fully attentional mixture-of-expert model (MoEL) for empathetic dialogue modeling. The Universal Transformer BIBREF1 incorporates the recurrent inductive bias of RNNs into the standard Transformer, and achieves better result on a wide range of algorithmic and language understanding tasks. BIBREF21 introduce the Latent Transformer (LT) for non-autoregressive machine translation. During training, the LT first autoencodes a target sequence into a shorter sequence discrete latent variables. Then a parallel decoder decodes the target using discrete latent variables and an input sequence. Different from the LT BIBREF21, the VT generates continuous latent variables during the decoding process.",Preliminaries ::: Conditional Variational Autoencoder for Dialogue Generation,"The CVAE framework BIBREF22 represents a dyadic conversation via three random variables: the input condition $c$, including conversation context and meta features (meta features can be ignored when not available); a latent variable $z$; and the target response $x$. A CVAE can be efficiently trained with Stochastic Gradient Variational Bayes (SGVB) BIBREF23 by maximizing the variational lower bound of $x$ given c, according to: The typical CVAE consists of a prior network $p_{\theta }(z | c)$, which is used to approximate $p(z | c)$, a recognition network $p_{\phi }(z | c, x)$, which is used to approximate posterior distribution $q(z | c, x)$, and a decoder $p_{\theta }(x | z, c)$, which is used to approximate $p(x | z, c)$. By assuming z follows multivariate Gaussian distribution with a diagonal co-variance matrix, the evidence lower bound (ELBO) can be written as where $\mathcal {L}_{REC}$ denotes the reconstruction loss and $\mathcal {L}_{KL}$ denotes the Kullback-Leibler (KL) divergence between the posterior and prior. In dialogue generation tasks, previous works BIBREF2, BIBREF16 apply RNN encoders (with GRU or LSTM cell) to encode dialogue contexts and responses separately. The condition $c$ is represented by the concatenation of the last hidden state of the context encoder and the meta features (e.g., topic, emotion), while the response $x$ is represented by the last hidden state of response encoder. Then the prior network $p_{\theta }(z | c)$ and the recognition network $p_{\phi }(z | c, x)$ parameterized by multi-layer perceptrons (MLPs) are applied to approximate the means and the log variances of the prior latent distribution $\mathcal {N}\left(z ; \mu ^{\prime }, \sigma ^{\prime 2} \mathbf {I}\right)$ and posterior latent distribution $\mathcal {N}\left(z ; \mu , \sigma ^{2} \mathbf {I}\right)$. With the reparameterization trick BIBREF23, we can obtain samples of the prior latent variable (for testing) from $\mathcal {N}\left(z ; \mu ^{\prime }, \sigma ^{\prime 2} \mathbf {I}\right)$ and samples of the posterior latent variable (for training) from $\mathcal {N}\left(z ; \mu , \sigma ^{2} \mathbf {I}\right)$. Finally, an RNN decoder use $z$ and $c$ as the initial state to predicts the response $x$. The vanishing latent variable problem BIBREF24 is a common issue in RNN-based CVAEs. That is, the powerful autoregressive RNN decoder first learns to ignore the latent variable, and decodes the response by only condition on the previous tokens. Thus the latent variable fails to encode the meaningful information, and the CVAE deteriorates to seq2seq model. To alleviate this issue, KL annealing BIBREF24 and bag-of-word loss BIBREF2 have been proposed, and have shown effectiveness in various dialogue tasks BIBREF2, BIBREF16.",Preliminaries ::: CVAE with Transformer,"The aforementioned RNN-based CVAE framework integrate the latent variable into the initial state of RNN decoder, while in transformer, it is more flexible to incorporate the latent variable embedding into the first input token of the decoder to generate the initial state. The overall architecture of GVT is depicted in Figure FIGREF9. Different from RNNs, the Transformer encoder maps an input sequence of symbol representations to a sequence of contextualized representations BIBREF0. In order to get fixed dimension representations of the response and context, we add a special token $CLS$ at the beginning of the input sequence as in BERT BIBREF25, to compute the weighted sum of the output representations via self-attention. Thus the output representation of the token $CLS$ is considered as the representation of the whole sequence. Then we introduce a recognition network and a prior network to compute the posterior latent variable and prior latent variable as in BIBREF2, BIBREF16. We add the latent variable sample $z$ and meta features $m$ (can be ignored when not available) into $e_{SOS}$, the embedding of the start-of-sequence token $SOS$: Finally, the transformer decoder decodes the response $x$ sequentially while attending to the new embedding $e^{\prime }_{SOS}$ of token $SOS$ with latent information. This design enhances the CVAE framework with the global receptive field, and each position of the GVT can directly access the latent information via the multi-head self-attention mechanism. However, we still observe that the GVT suffers the vanishing latent variable problem as RNN-based CVAE because the decoder can bypass the latent information by paying less attention to the $SOS$ token. Hence, we apply the KL annealing, and bag-of-word auxiliary loss $\mathcal {L}_{bow}$ as in BIBREF2, BIBREF16 to preserve the useful information of the latent variable. Therefore, the learning objective of the GVT is defined as follows:",Sequential Variational Transformer,"In order to augment the capacity of the latent variable with multi-modal distributions and to better utilize the latent information, we further explore incorporating a sequence of latent variables in decoding process. We introduce Sequential Variational Transformer (SVT) with a novel variational decoder layer which generate latent variables for each position: $z=\left(z_{1}, \dots , z_{T}\right)$. Similar to BIBREF3, we interpret the latent variables as a generation plan for the future sequence. Unlike previous CVAE models which use an extra encoder to encode the response separately BIBREF2, BIBREF16 or use a backward RNN to encode the future sequence for each time step BIBREF3, BIBREF4, SVT uses a Non-causal Multi-head Attention which leaks the future information to the recognition network for computing the posterior latent variables. As shown in Figure FIGREF13, the SVT shares the same encoder as the standard Transformer BIBREF0, while its decoder consists of a variational decoder layer followed by a stack of $N$ standard Transformer decoder layers. The variational decoder layer has two paths for computing the posterior latent variable and prior latent variable respectively. We denote them as Posterior Path and Prior Path.",Sequential Variational Transformer ::: Prior Path,"The Prior Path (solid line in Figure FIGREF13) has a masked multi-head self-attention sub-layer which performs causal attention on the shifted response, followed by a multi-head self-attention sub-layer which performs encoder-decoder multi-head attention on the context encoder. The last sub-layer is composed of a MLP prior network which approximates a sequence of prior latent variable for each position, and a Position-wise Feed-Forward Network (FFN) which fuse the latent information $z$ with the observed information representation $o^P$ before the prior network (shown in Figure FIGREF13). Specifically, we concatenate $o^P$ with $z$ as the input to the FNN, and the FNN pass the fused representation to the next layer. Same as BIBREF0, in the variational decoder layer, each sub-layer is followed by a residual connection and layer normalization. That is, the output of each sub-layer is $LayerNorm(x + Sublayer(x))$. We decompose the response $x$ as $x = \left(x_1, \cdots , x_T\right)$ and the latent variable $z$ as $z=\left(z_{1}, \dots , z_{T}\right)$. The prior model produces latent variables at each position $z_t$ by not only conditioning on the input condition $c$ (the concatenation of context and meta features), but also conditioning on the observed response tokens $x_{1:t-1}$. By assuming $z_t$ follows a multivariate Gaussian distribution, the prior model becomes: where",Sequential Variational Transformer ::: Posterior Path,"The only difference between the Posterior Path (dash line in Figure FIGREF13) and Prior Path is that the mask is removed from the masked multi-head attention. Thus the masked (casual) multi-head attention become non-casual multi-head attention, which allows each position to attend to the subsequent positions. Then, the second multi-head attention sub-layer (shared the same weight with prior path) performs posterior attention on the encoder and passes the posterior observed information $o_R$ to the recognition network. The recognition network produces the posterior latent variable for each position $z_t$ as: where During the training, the posterior path guides the learning of prior path via KL divergence constraint: In the training phase, the posterior latent variables from Equation DISPLAY_FORM17 are passed to the FFN, while in the testing phase the Posterior Path will be blocked and the posterior latent variables will be replaced with the prior latent variables from Equation DISPLAY_FORM15. During the decoding process, each response token $x_t$ is generated by conditioning on observed response tokens $x_{1:t-1}$, latent variables $z_{1:t}$, and the input condition $c$. The decoding process of the SVT is:",Sequential Variational Transformer ::: Auxiliary Loss,"As we expect the latent variables to be a generation plan for the future sequence, we inject such bias into latent variables by using an auxiliary loss: Sequential-Bag-of-Word (SBOW) which proposed by BIBREF4. The idea of the SBOW auxiliary objective is to sequentially predict the bag of succeeding target words $x_{t:T}$ by using latent variable $z_t$. In our case, the succeeding words prediction also leverages the observed information $c$ and $x_{1:t-1}$. Thus the auxiliary loss at each position is computed by: where $f_{aux}$ is a feed-forward neural network with the softmax output.",Sequential Variational Transformer ::: Learning,"The evidence lower bound (ELBO) objective of SVT is the sum of the reconstruction loss $\mathcal {L}_{REC}(t)$ and Kullback-Leibler divergence loss $\mathcal {L}_{KL}(t)$ at each position: We regularize the ELBO learning objective with an auxiliary loss $\mathcal {L}_{sbow}$ to enhance the expressiveness of the latent variables. Therefore, the final learning objective is formulated as follows: where,",Experiments ::: Dataset,"We evaluate the proposed models on three conversationet dataset such as MojiTalk BIBREF16, PersonaChat BIBREF11, Empathetic-Dialogues BIBREF26.",Experiments ::: Dataset ::: MojiTalk,"dataset consists of 596,959 post and response pairs from Twitter. Each response is labeled by one emoji which indicates the response emotion. There are 64 emoji labels in total with unbalanced distribution. We use the preprocessed data and vocabulary released from BIBREF16 and follow the same split of train/validation/test set.",Experiments ::: Dataset ::: PersonaChat & Empathetic-Dialogues,"are one-to-one multi-turn conversation datasets. In PersonaChat (Persona), the conversations are revolve around personas which are established by four to six persona sentences. While in Empathetic-Dialogues (ED), the conversation are mostly about situation that happened to one of the speaker and another speaker is trying to understand the feeling and reply accordingly. Both datasets are about modeling social skills and the goal is to make user more engaging. Therefore, we combine the train/validation/test set of two datasets.",Experiments ::: Baselines,We compare the proposed models with the following baselines:,Experiments ::: Baselines ::: Seq2Seq.,An attention-based sequence-to-sequence model with the emoji vector as additional input as discribed in MojiTalk BIBREF16.,Experiments ::: Baselines ::: CVAE.,"An RNN-based conditional variational autoencoder for dialogue response generation BIBREF16, which uses a multivariate Gaussian latent variable to model the response and concatenate it with the last hidden state of the encoder as the initial state of the decoder. KL annealing, early stopping strategy and bag-of-word auxiliary loss are applied during the training. We use the implementation released by BIBREF16.",Experiments ::: Baselines ::: Transformer.,A transformer BIBREF0 trained by using a Maximum Likelihood Estimation (MLE) objective and can be considered as the base model for both the GVT and SVT.,Experiments ::: Hyper-parameters and Training Setup,"We use a 4-layer Transformer as our base model. The hidden size is set to be 300 everywhere, and the word embedding is initialized with the 300-dimensional pre-trained GloVe embeddings for both encoder and decoder. The multi-head attention sub-layers are made up of 4 attention heads each with embedding dimension 64. The size of latent variable is 300. The recognition network and the prior network are parameterized by 3-layer MLPs with 512 hidden dimension. Following the training setup of BIBREF16, we first train our baseline transformer model with the MLE objective and use it to initialize its counterparts in both GVT and SVT. Then the models are trained end-to-end by the Adam optimizer with the initial learning rate $2\times 10^{-4}$. KL annealing and early stopping strategy are applied as in BIBREF16. In the test time, we use greedy decoding strategy for all models.",Experiments ::: Automatic Evaluation ::: PPL & KLD.,The evaluation metrics include Perplexity (PPL) and Kullback-Leibler divergence between the posterior and prior (KLD). A well trained model should achieve a low reconstruction and small but non-trivial KL distance BIBREF27.,Experiments ::: Automatic Evaluation ::: Diversity.,"To measure the generation diversity, we calculate Dist-1, Dist-2, and Dist-3, the ratio of the number of distinct n-grams (unigrams, bigrams, and trigrams) over the total number of n-grams. A higher distinct n-grams ratio indicates more diverse generation.",What approach performs better in experiments global latent or sequence of fine-grained latent variables?,c69f4df4943a2ca4c10933683a02b179a5e76f64,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,,"PPL: SVT
Diversity: GVT
Embeddings Similarity: SVT
Human Evaluation: SVT","Compare to baseline models, the GVT achieves relatively lower reconstruction PPL, which suggests that the global latent variable contains rich latent information (e.g., topic) for response generation. Meanwhile, the sequential latent variables of the SVT encode fine-grained latent information and further improve the reconstruction PPL. FLOAT SELECTED: Table 1: Results of Variational Transformer compared to baselines on automatic and human evaluations.","Compare to baseline models, the GVT achieves relatively lower reconstruction PPL, which suggests that the global latent variable contains rich latent information (e.g., topic) for response generation. Meanwhile, the sequential latent variables of the SVT encode fine-grained latent information and further improve the reconstruction PPL. FLOAT SELECTED: Table 1: Results of Variational Transformer compared to baselines on automatic and human evaluations.",a88de75bc60cab3829d5fcfd51b41290f8c93e87,a0b403873302db7cada39008f04d01155ef68f4f,,,,,,,,,What baselines other than standard transformers are used in experiments?,6aed1122050b2d508dc1790c13cdbe38ff126089,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,,,"An attention-based sequence-to-sequence model with the emoji vector as additional input as discribed in MojiTalk BIBREF16. An RNN-based conditional variational autoencoder for dialogue response generation BIBREF16, which uses a multivariate Gaussian latent variable to model the response and concatenate it with the last hidden state of the encoder as the initial state of the decoder. KL annealing, early stopping strategy and bag-of-word auxiliary loss are applied during the training. We use the implementation released by BIBREF16.","An attention-based sequence-to-sequence model with the emoji vector as additional input as discribed in MojiTalk BIBREF16. CVAE.
An RNN-based conditional variational autoencoder for dialogue response generation BIBREF16, which uses a multivariate Gaussian latent variable to model the response and concatenate it with the last hidden state of the encoder as the initial state of the decoder. KL annealing, early stopping strategy and bag-of-word auxiliary loss are applied during the training. We use the implementation released by BIBREF16",2480a1548fe73160bbb792eda5a0f4baeeb2d64a,a0b403873302db7cada39008f04d01155ef68f4f,What three conversational datasets are used for evaluation?,8740c3000e740ac5c0bc8f329d908309f7ffeff6,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,,,"We evaluate the proposed models on three conversationet dataset such as MojiTalk BIBREF16, PersonaChat BIBREF11, Empathetic-Dialogues BIBREF26.","We evaluate the proposed models on three conversationet dataset such as MojiTalk BIBREF16, PersonaChat BIBREF11, Empathetic-Dialogues BIBREF26.",38d23527a2e40607267e6d7f46d7dc56468f9fb7,a0b403873302db7cada39008f04d01155ef68f4f,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3-Figure1-1.png,"Figure 1: The Global Variational Transformer. During training, The posterior latent variable z by the posterior network is passed to the decoder, while during testing, the target response is absent, and z is replaced by the prior latent variable. The word embeddings, positional encoding, softmax layer and meta vectors are ignored for simplicity",4-Figure2-1.png,"Figure 2: The Sequential Variational Transformer. During training, The posterior latent variables z by the posterior network are passed to the decoder, while during testing, the target response is absent, and z is replaced by the prior latent variables z. The word embeddings, positional encoding, softmax layer and meta vectors are ignored for simplicity",7-Table1-1.png,Table 1: Results of Variational Transformer compared to baselines on automatic and human evaluations.,8-Table2-1.png,Table 2: Generated responses from proposed models and baseline models. The reference responses (Ref) are given.,,,,,Experiments ::: Automatic Evaluation ::: Embeddings Similarity.,"This metric computes the cosine similarity between the sentence embedding of a generated sequence and that of a ground-truth response. In our experiments, we introduce two different ways to represent sentence embeddings. The first is $\textbf {EMB}_\textbf {FT}$ BIBREF28 that calculates the average of word embeddings in a sentence using FastText BIBREF29 which is trained with Common Crawl and Wikipedia data. We use FastText embeddings instead of other pre-trained word embeddings because it can handle out-of-vocabulary issue. However, representing a sentence by simply taking the average of word embeddings ignores the context information. Therefore, we propose to use a pre-trained language model BERT BIBREF25 to compute the contextualized sentence representation. Specifically, we use a pre-trained BERT to encode a generated sentence and a ground-truth response, and average the output representation of both to obtain the sentence embeddings. We denote such contextualized sentence embedding as $\textbf {EMB}_\textbf {BERT}$.",Experiments ::: Human Evaluation,"In the human evaluation, we prepare multiple-choice questions for human evaluators and the answers are the generation results from the five models (Seq2Seq, CVAE, Transformer, GVT, and SVT). we first randomly sample 100 dialogues and their corresponding responses from our models and the baselines. For each response, we assign three human annotators to select the most coherent (on topic) response to the context (multiple answers are allowed). In addition, annotators also need to choose the best response correlated to the given emoji label in Mojitalk and the most engaging response in PersonaChat and Empathetic-Dialogues. If there is no response that satisfies the evaluators, they can choose “all answers are bad"", which means none of the answer is chosen. We compute the rate that each model is chosen to quantify generation quality regarding to the human standard.",Results ::: Quantitative Analysis,"The automatic evaluation results are shown in Table TABREF35. Transformer-based models have significantly lower perplexity compared to RNN-based models which indicate that the global receptive field performed by multi-head self-attention boost the modeling capacity. However, deterministic Seq2Seq and Transformer models tends to generate generic responses which leads to a low diversity score. Meanwhile incorporating a stochastic latent variable into both models (CVAE and GVT) promote more diverse generation results and boost the diversity scores such as Dist-1, Dist-2, and Dist-3. Compare to baseline models, the GVT achieves relatively lower reconstruction PPL, which suggests that the global latent variable contains rich latent information (e.g., topic) for response generation. Meanwhile, the sequential latent variables of the SVT encode fine-grained latent information and further improve the reconstruction PPL. On the other hand, SVT achieves the highest score in terms of two semantic relevance-oriented metrics such as $\textbf {EMB}_\textbf {FT}$ and $\textbf {EMB}_\textbf {BERT}$ in MojiTalk dataset, while in the combined dataset of Persona and ED, we observe performance drop of SVT compare to other models. This is because both Persona and ED are well designed and have lower entropy than MojiTalk which collected from Twitter. We hypothesize that the sequential latent variables have no advantage in term of similarity to single, fixed ""gold response"" when model low entropy response. Indeed, in open domain dialogue response generation, automatic metric is not always aligned with the human judgement BIBREF28. In contrast, human evaluation result reported in Table TABREF35 demonstrates the generations of SVT are closer to the human standard in terms of coherence, invoked emotion and engagedness.",Results ::: Qualitative Analysis,"Table TABREF42 compares the generation of the proposed models with baselines given the same contexts. We observe that the Seq2Seq and vanilla transformer tend to generate generic and repetitive responses (e.g., i am not sure) in MojiTalk due to their deterministic structure fail to capture the variability in dialogue response. By incorporating stochastic latent variables, the CVAE and GVT can generate more diverse responses, but their responses are sometimes digressive (e.g., example 5). Interestingly, GVT and SVT generalize the topic beyong the context which make the dialogue more engaging (e.g., example 4). In general, SVT is able to generate more coherent and informative responses.",attention-based sequence-to-sequence model  CVAE,MojiTalk  PersonaChat  Empathetic-Dialogues,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Conclusion,"This paper introduces the Variational Transformer (VT), a variational self-attentive feed-forward sequence model that combines the global receptive field of a Transformer with the variational nature of a CVAE. We propose two types of the VT: 1) the Global Variational Transformer (GVT) which incorporates a global latent variable as additional input to the transformer decoder; and 2) the Sequential Variational Transformer (SVT) which generates latent variables for each position during decoding process. Quantitative and qualitative experimental results shows that our models outperform baselines in terms of diversity, semantic relevance, and human judgment. In future work, we will utilize the pre-training language models BIBREF30 as the back-bone to strengthen the language model of the VT for better generation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bootstrapping Generators from Noisy Data,"A core step in statistical data-to-text generation concerns learning correspondences between structured data representations (e.g., facts in a database) and associated texts. In this paper we aim to bootstrap generators from large scale datasets where the data (e.g., DBPedia facts) and related texts (e.g., Wikipedia abstracts) are loosely aligned. We tackle this challenging task by introducing a special-purpose content selection mechanism. We use multi-instance learning to automatically discover correspondences between data and text pairs and show how these can be used to enhance the content signal while training an encoder-decoder architecture. Experimental results demonstrate that models trained with content-specific objectives improve upon a vanilla encoder-decoder which solely relies on soft attention.",Introduction,"A core step in statistical data-to-text generation concerns learning correspondences between structured data representations (e.g., facts in a database) and paired texts BIBREF0 , BIBREF1 , BIBREF2 . These correspondences describe how data representations are expressed in natural language (content realisation) but also indicate which subset of the data is verbalised in the text (content selection). Although content selection is traditionally performed by domain experts, recent advances in generation using neural networks BIBREF3 , BIBREF4 have led to the use of large scale datasets containing loosely related data and text pairs. A prime example are online data sources like DBPedia BIBREF5 and Wikipedia and their associated texts which are often independently edited. Another example are sports databases and related textual resources. Wiseman et al. wiseman-shieber-rush:2017:EMNLP2017 recently define a generation task relating statistics of basketball games with commentaries and a blog written by fans. In this paper, we focus on short text generation from such loosely aligned data-text resources. We work with the biographical subset of the DBPedia and Wikipedia resources where the data corresponds to DBPedia facts and texts are Wikipedia abstracts about people. Figure 1 shows an example for the film-maker Robert Flaherty, the Wikipedia infobox, and the corresponding abstract. We wish to bootstrap a data-to-text generator that learns to verbalise properties about an entity from a loosely related example text. Given the set of properties in Figure ( 1 a) and the related text in Figure ( 1 b), we want to learn verbalisations for those properties that are mentioned in the text and produce a short description like the one in Figure ( 1 c). In common with previous work BIBREF6 , BIBREF7 , BIBREF8 our model draws on insights from neural machine translation BIBREF3 , BIBREF9 using an encoder-decoder architecture as its backbone. BIBREF7 introduce the task of generating biographies from Wikipedia data, however they focus on single sentence generation. We generalize the task to multi-sentence text, and highlight the limitations of the standard attention mechanism which is often used as a proxy for content selection. When exposed to sub-sequences that do not correspond to any facts in the input, the soft attention mechanism will still try to justify the sequence and somehow distribute the attention weights over the input representation BIBREF10 . The decoder will still memorise high frequency sub-sequences in spite of these not being supported by any facts in the input. We propose to alleviate these shortcomings via a specific content selection mechanism based on multi-instance learning (MIL; BIBREF11 , BIBREF11 ) which automatically discovers correspondences, namely alignments, between data and text pairs. These alignments are then used to modify the generation function during training. We experiment with two frameworks that allow to incorporate alignment information, namely multi-task learning (MTL; BIBREF12 , BIBREF12 ) and reinforcement learning (RL; BIBREF13 , BIBREF13 ). In both cases we define novel objective functions using the learnt alignments. Experimental results using automatic and human-based evaluation show that models trained with content-specific objectives improve upon vanilla encoder-decoder architectures which rely solely on soft attention. The remainder of this paper is organised as follows. We discuss related work in Section ""Related Work"" and describe the MIL-based content selection approach in Section ""Bidirectional Content Selection"" . We explain how the generator is trained in Section ""Generator Training"" and present evaluation experiments in Section ""Experimental Setup"" . Section ""Conclusions"" concludes the paper.",Related Work,"Previous attempts to exploit loosely aligned data and text corpora have mostly focused on extracting verbalisation spans for data units. Most approaches work in two stages: initially, data units are aligned with sentences from related corpora using some heuristics and subsequently extra content is discarded in order to retain only text spans verbalising the data. belz2010extracting obtain verbalisation spans using a measure of strength of association between data units and words, walter2013corpus extract textual patterns from paths in dependency trees while mrabet:webnlg16 rely on crowd-sourcing. Perez-Beltrachini and Gardent perezbeltrachini-gardent:2016:*SEM learn shared representations for data units and sentences reduced to subject-predicate-object triples with the aim of extracting verbalisations for knowledge base properties. Our work takes a step further, we not only induce data-to-text alignments but also learn generators that produce short texts verbalising a set of facts. Our work is closest to recent neural network models which learn generators from independently edited data and text resources. Most previous work BIBREF7 , BIBREF14 , BIBREF15 , BIBREF16 targets the generation of single sentence biographies from Wikipedia infoboxes, while wiseman-shieber-rush:2017:EMNLP2017 generate game summary documents from a database of basketball games where the input is always the same set of table fields. In contrast, in our scenario, the input data varies from one entity (e.g., athlete) to another (e.g., scientist) and properties might be present or not due to data incompleteness. Moreover, our generator is enhanced with a content selection mechanism based on multi-instance learning. MIL-based techniques have been previously applied to a variety of problems including image retrieval BIBREF17 , BIBREF18 , object detection BIBREF19 , BIBREF20 , text classification BIBREF21 , image captioning BIBREF22 , BIBREF23 , paraphrase detection BIBREF24 , and information extraction BIBREF25 . The application of MIL to content selection is novel to our knowledge. We show how to incorporate content selection into encoder-decoder architectures following training regimes based on multi-task learning and reinforcement learning. Multi-task learning aims to improve a main task by incorporating joint learning of one or more related auxiliary tasks. It has been applied with success to a variety of sequence-prediction tasks focusing mostly on morphosyntax. Examples include chunking, tagging BIBREF26 , BIBREF27 , BIBREF28 , BIBREF29 , name error detection BIBREF30 , and machine translation BIBREF31 . Reinforcement learning BIBREF13 has also seen popularity as a means of training neural networks to directly optimize a task-specific metric BIBREF4 or to inject task-specific knowledge BIBREF32 . We are not aware of any work that compares the two training methods directly. Furthermore, our reinforcement learning-based algorithm differs from previous text generation approaches BIBREF4 , BIBREF32 in that it is applied to documents rather than individual sentences.",Bidirectional Content Selection,"We consider loosely coupled data and text pairs where the data component is a set ${{\cal P}}$ of property-values $\lbrace p_1:v_1, \cdots , p_{|{{\cal P}}|}:v_{|{{\cal P}}|}\rbrace $ and the related text ${{\cal T}}$ is a sequence of sentences $(s_1, \cdots , s_{|{\cal T}|})$ . We define a mention span $\tau $ as a (possibly discontinuous) subsequence of ${{\cal T}}$ containing one or several words that verbalise one or more property-value from ${{\cal P}}$ . For instance, in Figure 1 , the mention span “married to Frances H. Flaherty” verbalises the property-value $\lbrace Spouse(s)
: Frances \; Johnson \; Hubbard\rbrace $ . In traditional supervised data to text generation tasks, data units (e.g., $p_i:v_i$ in our particular setting) are either covered by some mention span $\tau _j$ or do not have any mention span at all in ${{\cal T}}$ . The latter is a case of content selection where the generator will learn which properties to ignore when generating text from such data. In this work, we consider text components which are independently edited, and will unavoidably contain unaligned spans, i.e., text segments which do not correspond to any property-value in ${{\cal P}}$ . The phrase “from 1914” in the text in Figure ( 1 b) is such an example. Similarly, the last sentence, talks about Frances' awards and nominations and this information is not supported by the properties either. Our model checks content in both directions; it identifies which properties have a corresponding text span (data selection) and also foregrounds (un)aligned text spans (text selection). This knowledge is then used to discourage the generator from producing text not supported by facts in the property set ${{\cal P}}$ . We view a property set ${{\cal P}}$ and its loosely coupled text ${{\cal T}}$ as a coarse level, imperfect alignment. From this alignment signal, we want to discover a set of finer grained alignments indicating which mention spans in ${{\cal T}}$ align to which properties in ${{\cal P}}$ . For each pair $({\cal P},
{\cal T})$ , we learn an alignment set ${\cal A}({\cal P}, {\cal T})$ which contains property-value word pairs. For example, for the properties $spouse$ and $died$ in Figure 1 , we would like to derive the alignments in Table 1 . We formulate the task of discovering finer-grained word alignments as a multi-instance learning problem BIBREF11 . We assume that words from the text are positive labels for some property-values but we do not know which ones. For each data-text pair $({{\cal P}},
{{\cal T}})$ , we derive $|{\cal T}|$ pairs of the form $({{\cal P}},s)$ where $|{\cal T}|$ is the number of sentences in ${\cal T}$ . We encode property sets ${{\cal P}}$ and sentences $s$ into a common multi-modal $h$ -dimensional embedding space. While doing this, we discover finer grained alignments between words and property-values. The intuition is that by learning a high similarity score for a property set ${{\cal P}}$ and sentence pair $s$ , we will also learn the contribution of individual elements (i.e., words and property-values) to the overall similarity score. We will then use this individual contribution as a measure of word and property-value alignment. More concretely, we assume the pair is aligned (or unaligned) if this individual score is above (or below) a given threshold. Across examples like the one shown in Figure ( 1 a-b), we expect the model to learn an alignment between the text span “married to Frances H. Flaherty” and the property-value $|{\cal T}|$0 . In what follows we describe how we encode $({{\cal P}}, s)$ pairs and define the similarity function.",Generator Training,In this section we describe the base generation architecture and explain two alternative ways of using the alignments to guide the training of the model. One approach follows multi-task training where the generator learns to output a sequence of words but also to predict alignment labels for each word. The second approach relies on reinforcement learning for adjusting the probability distribution of word sequences learnt by a standard word prediction training algorithm.,Encoder-Decoder Base Generator,"We follow a standard attention based encoder-decoder architecture for our generator BIBREF3 , BIBREF33 . Given a set of properties $X$ as input, the model learns to predict an output word sequence $Y$ which is a verbalisation of (part of) the input. More precisely, the generation of sequence $Y$ is conditioned on input $X$ : $$P(Y|X) = \prod _{t=1}^{|Y|} P(y_t|y_{1:t-1}, X)$$   (Eq. 12)   The encoder module constitutes an intermediate representation of the input. For this, we use the property-set encoder described in Section ""Bidirectional Content Selection"" which outputs vector representations $\lbrace  \mathbf {p}_1, \cdots , \mathbf {p}_{|X|} \rbrace $ for a set of property-value pairs. The decoder uses an LSTM and a soft attention mechanism BIBREF33 to generate one word $y_t$ at a time conditioned on the previous output words and a context vector $c_t$ dynamically created: $$P(y_{t+1}|y_{1:t},X) = softmax(g(\mathbf {h}_t, c_t))$$   (Eq. 13)  where $g(\cdot )$ is a neural network with one hidden layer parametrised by $\mathbf {W}_o \in \mathbb {R}^{|V| \times d}$ , $|V|$ is the output vocabulary size and $d$ the hidden unit dimension, over $\mathbf {h}_t$ and $c_t$ composed as follows: $$g(\mathbf {h}_t, c_t) = \mathbf {W}_o \; tanh(\mathbf {W}_c [ c_t ; \mathbf {h}_t ] )$$   (Eq. 14)  where $\mathbf {W}_c \in \mathbb {R}^{d \times 2d}$ . $\mathbf {h}_t$ is the hidden state of the LSTM decoder which summarises $y_{1:t}$ : $$\mathbf {h}_t = \text{LSTM}(y_t, \mathbf {h}_{t-1})$$   (Eq. 15)  The dynamic context vector $c_t$ is the weighted sum of the hidden states of the input property set (Equation ( 16 )); and the weights $\alpha _{ti}$ are determined by a dot product attention mechanism: $$c_t = \sum _{i=1}^{|X|}\alpha _{ti} \, \mathbf {p}_i$$   (Eq. 16)  $$\alpha _{ti} = {\text{exp}(\mathbf {h}_{t} \, \mathchoice{\mathbin {\hbox{\scalebox {.5}{$\m@th \displaystyle \bullet $}}}}{}{}{}}{\mathbin {\hbox{\scalebox {.5}{$\m@th \textstyle \bullet $}}}}$$   (Eq. 17)   pi)i exp(ht pi ) We initialise the decoder with the averaged sum of the encoded input representations BIBREF34 . The model is trained to optimize negative log likelihood: $${\cal L}_{wNLL} = - \sum _{t=1}^{|Y|} log \, P(y_t|y_{1:t-1}, X)$$   (Eq. 18)  We extend this architecture to multi-sentence texts in a way similar to wiseman-shieber-rush:2017:EMNLP2017. We view the abstract as a single sequence, i.e., all sentences are concatenated. When training, we cut the abstracts in blocks of equal size and perform forward backward iterations for each block (this includes the back-propagation through the encoder). From one block iteration to the next, we initialise the decoder with the last state of the previous block. The block size is a hyperparameter tuned experimentally on the development set.",Predicting Alignment Labels,"The generation of the output sequence is conditioned on the previous words and the input. However, when certain sequences are very common, the language modelling conditional probability will prevail over the input conditioning. For instance, the phrase from 1914 in our running example is very common in contexts that talk about periods of marriage or club membership, and as a result, the language model will output this phrase often, even in cases where there are no supporting facts in the input. The intuition behind multi-task training BIBREF12 is that it will smooth the probabilities of frequent sequences when trying to simultaneously predict alignment labels. Using the set of alignments obtained by our content selection model, we associate each word in the training data with a binary label $a_t$ indicating whether it aligns with some property in the input set. Our auxiliary task is to predict $a_t$ given the sequence of previously predicted words and input $X$ : $$P(a_{t+1}|y_{1:t},X) = sigmoid(g^{\prime }(\mathbf {h}_t, c_t))$$   (Eq. 20)  $$g^{\prime }(\mathbf {h}_t, c_t) = \mathbf {v}_a \, \mathchoice{\mathbin {\hbox{\scalebox {.5}{$\m@th \displaystyle \bullet $}}}}{}{}{}$$   (Eq. 21)   tanh(Wc [ ct ; ht ] ) where $\mathbf {v}_a \in \mathbb {R}^{d}$ and the other operands are as defined in Equation ( 14 ). We optimise the following auxiliary objective function: $${\cal L}_{aln} = - \sum _{t=1}^{|Y|} log \, P(a_t|y_{1:t-1}, X)$$   (Eq. 22)  and the combined multi-task objective is the weighted sum of both word prediction and alignment prediction losses: $${\cal L}_{MTL} = \lambda \, {\cal L}_{wNLL} + (1 - \lambda ) \, {\cal L}_{aln}$$   (Eq. 23)  where $\lambda $ controls how much model training will focus on each task. As we will explain in Section ""Experimental Setup"" , we can anneal this value during training in favour of one objective or the other.",Reinforcement Learning Training,"Although the multi-task approach aims to smooth the target distribution, the training process is still driven by the imperfect target text. In other words, at each time step $t$ the algorithm feeds the previous word $w_{t-1}$ of the target text and evaluates the prediction against the target $w_t$ . Alternatively, we propose a training approach based on reinforcement learning ( BIBREF13 ) which allows us to define an objective function that does not fully rely on the target text but rather on a revised version of it. In our case, the set of alignments obtained by our content selection model provides a revision for the target text. The advantages of reinforcement learning are twofold: (a) it allows to exploit additional task-specific knowledge BIBREF32 during training, and (b) enables the exploration of other word sequences through sampling. Our setting differs from previous applications of RL BIBREF4 , BIBREF32 in that the reward function is not computed on the target text but rather on its alignments with the input. The encoder-decoder model is viewed as an agent whose action space is defined by the set of words in the target vocabulary. At each time step, the encoder-decoder takes action $\hat{y}_t$ with policy $P_{\pi }(\hat{y}_t|\hat{y}_{1:t-1}, X)$ defined by the probability in Equation ( 13 ). The agent terminates when it emits the End Of Sequence (EOS) token, at which point the sequence of all actions taken yields the output sequence $\hat{Y}=(\hat{y}_1, \cdots ,
\hat{y}_{|\hat{Y}|})$ . This sequence in our task is a short text describing the properties of a given entity. After producing the sequence of actions $\hat{Y}$ , the agent receives a reward $r(\hat{Y})$ and the policy is updated according to this reward. We define the reward function $r(\hat{Y})$ on the alignment set ${\cal A}(X,Y)$ . If the output action sequence $\hat{Y}$ is precise with respect to the set of alignments ${\cal A}(X,Y)$ , the agent will receive a high reward. Concretely, we define $r(\hat{Y})$ as follows: $$r(\hat{Y}) = \gamma ^{pr} \, r^{pr}(\hat{Y})$$   (Eq. 26)  where $\gamma ^{pr}$ adjusts the reward value $r^{pr}$ which is the unigram precision of the predicted sequence $\hat{Y}$ and the set of words in ${\cal A}(X,Y)$ . We use the REINFORCE algorithm BIBREF13 to learn an agent that maximises the reward function. As this is a gradient descent method, the training loss of a sequence is defined as the negative expected reward: $${\cal L}_{RL} = -\mathbb {E}_{(\hat{y}_1, \cdots , \hat{y}_{|\hat{Y}|})} \sim P_\pi (\text{·}|X)[r(\hat{y}_1, \cdots ,
\hat{y}_{|\hat{Y}|})] \nonumber $$   (Eq. 28)  where $P_\pi $ is the agent's policy, i.e., the word distribution produced by the encoder-decoder model (Equation ( 13 )) and $r(\text{·})$ is the reward function as defined in Equation ( 26 ). The gradient of ${\cal L}_{RL}$ is given by: $$\nabla {\cal L}_{RL} \approx \sum ^{|\hat{Y}|}_{t=1}\nabla \,
\text{log} \, P_{\pi }(\hat{y}_t|\hat{y}_{1:t-1},
X)[r(\hat{y}_{1:|\hat{Y}|})-b_t] \nonumber $$   (Eq. 29)  where $b_t$ is a baseline linear regression model used to reduce the variance of the gradients during training. $b_t$ predicts the future reward and is trained by minimizing mean squared error. The input to this predictor is the agent hidden state $\mathbf {h}_t$ , however we do not back-propagate the error to $\mathbf {h}_t$ . We refer the interested reader to BIBREF13 and BIBREF4 for more details. Rather than starting from a state given by a random policy, we initialise the agent with a policy learnt by pre-training with the negative log-likelihood objective BIBREF4 , BIBREF32 . The reinforcement learning objective is applied gradually in combination with the log-likelihood objective on each target block subsequence. Recall from Section ""Encoder-Decoder Base Generator"" that our document is segmented into blocks of equal size during training which we denote as MaxBlock. When training begins, only the last $\mho $ tokens are predicted by the agent while for the first $(\text{{\small \textsc {MaxBlock}}}-\mho )$ we still use the negative log-likelihood objective. The number of tokens $\mho $ predicted by the agent is incremented by $\mho $ units every 2 epochs. We set $\mho =3$ and the training ends when $(\text{{\small \textsc {MaxBlock}}}-\mho )=0$ . Since we evaluate the model's predictions at the block level, the reward function is also evaluated at the block level.",Results,"We compared the performance of an encoder-decoder model trained with the standard negative log-likelihood method (ED), against a model trained with multi-task learning (ED $_{\mathrm {MTL}}$ ) and reinforcement learning (ED $_{\mathrm {RL}}$ ). We also included a template baseline system (Templ) in our evaluation experiments. The template generator used hand-written rules to realise property-value pairs. As an approximation for content selection, we obtained the 50 more frequent property names from the training set and manually defined content ordering rules with the following criteria. We ordered personal life properties (e.g., $birth\_date$ or $occupation$ ) based on their most common order of mention in the Wikipedia abstracts. Profession dependent properties (e.g., $position$ or $genre$ ), were assigned an equal ordering but posterior to the personal properties. We manually lexicalised properties into single sentence templates to be concatenated to produce the final text. The template for the property $position$ and example verbalisation for the property-value ${position : defender}$ of the entity zanetti are “ $[$ NAME $]$ played as $[$ POSITION $]$ .” and “ Zanetti played as defender.” respectively.",Conclusions,"In this paper we focused on the task of bootstrapping generators from large-scale datasets consisting of DBPedia facts and related Wikipedia biography abstracts. We proposed to equip standard encoder-decoder models with an additional content selection mechanism based on multi-instance learning and developed two training regimes, one based on multi-task learning and the other on reinforcement learning. Overall, we find that the proposed content selection mechanism improves the accuracy and fluency of the generated texts. In the future, it would be interesting to investigate a more sophisticated representation of the input BIBREF34 . It would also make sense for the model to decode hierarchically, taking sequences of words and sentences into account BIBREF41 , BIBREF42 .",Acknowledgments,"We thank the NAACL reviewers for their constructive feedback. We also thank Xingxing Zhang, Li Dong and Stefanos Angelidis for useful discussions about implementation details. We gratefully acknowledge the financial support of the European Research Council (award number 681760).",,,,,,,,,,,,,,,,,,,,,,,What is the multi-instance learning?,736c74d2f61ac8d3ac31c45c6510a36c767a5d6d,two,unfamiliar,no,,50d8b4a941c26b89482c94ab324b5a274f9ced66,True,,,,,edfdcd0cf572621147af45b4c7b1f9207209fb36,604fb22404e2a9945333bfff0c82012d85b11bb4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2-Figure1-1.png,"Figure 1: Property-value pairs (a), related biographic abstract (b) for the Wikipedia entity Robert Flaherty, and model verbalisation in italics (c).",3-Table1-1.png,Table 1: Example of word-property alignments for the Wikipedia abstract and facts in Figure 1.,7-Table2-1.png,Table 2: Dataset statistics.,8-Table4-1.png,Table 4: Rankings shown as proportions and mean ranks given to systems by human subjects.,8-Table3-1.png,"Table 3: BLEU-4 results using the original Wikipedia abstract (Abstract) as reference and crowd-sourced revised abstracts (RevAbs) for template baseline (Templ), standard encoder-decoder model (ED), and our content-basedmodels trained with multi-task learning (EDMTL) and reinforcement learning (EDRL).",9-Table5-1.png,Table 5: Examples of system output.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Generating Narrative Text in a Switching Dynamical System,"Early work on narrative modeling used explicit plans and goals to generate stories, but the language generation itself was restricted and inflexible. Modern methods use language models for more robust generation, but often lack an explicit representation of the scaffolding and dynamics that guide a coherent narrative. This paper introduces a new model that integrates explicit narrative structure with neural language models, formalizing narrative modeling as a Switching Linear Dynamical System (SLDS). A SLDS is a dynamical system in which the latent dynamics of the system (i.e. how the state vector transforms over time) is controlled by top-level discrete switching variables. The switching variables represent narrative structure (e.g., sentiment or discourse states), while the latent state vector encodes information on the current state of the narrative. This probabilistic formulation allows us to control generation, and can be learned in a semi-supervised fashion using both labeled and unlabeled data. Additionally, we derive a Gibbs sampler for our model that can fill in arbitrary parts of the narrative, guided by the switching variables. Our filled-in (English language) narratives outperform several baselines on both automatic and human evaluations.",A Switching Dynamical System for Narrative Generation,"In this section, we give a brief overview of Switching Dynamical systems and how they can be used to capture both a scaffold of the narrative as well as the narrative dynamics. We then describe in detail the components of our model and its relation to existing models.",A Switching Dynamical System for Narrative Generation ::: Narrative Dynamics in a Dynamical System,"The specifics of the narrative (characters, setting, etc.), will differ between stories, but as BIBREF0 notes, the way they transition to the next point in the narrative (what we refer to as “narrative dynamics"") is often shared. Let's say that, as done often, we represent the `narrative specifics' at time step $i$ with a latent vector $Z_i$. A natural way to explicitly model how this state evolves over time that fits with the above observation is as a Linear Dynamical System: Where $A$ is a matrix, shared across all narratives, and $\Sigma $ is a noise term that takes into consideration idiosyncrasies different narratives will have. The fact that the shared transition matrix $A$ is linear means that narratives will have linearly analogous trajectories through time, despite having different details (comparable to stories with different settings but matching structures such as Ran/King Lear, Ulysses/Odyssey, etc). Of course, the fatal flaw of the model is that it assumes there exists only one transition matrix, and thus only one possible way to transition through a narrative!",A Switching Dynamical System for Narrative Generation ::: Narrative Scaffolds as Switching Variables,"A more fitting model would thus be a Switching Linear Dynamical System BIBREF1, BIBREF2, BIBREF3. In an SLDS, we assume there exists a set of $K$ different sets of dynamics, $\lbrace (A_1, \Sigma _1),...(A_K,\Sigma _K)\rbrace $. At time step $i+1$, one of these sets of dynamics is used. The one used depends on the value of a discrete variable at time step $i+1$ called the switching variable, $S_{i+1} \in \lbrace 1,...K\rbrace $: There is a switching variable $S_i$ associated with each time step. The switching variable value itself evolves over time by a prior Markov process, $P(S_{i+1} | S_{i})$. This top level chain of switching variables thus forms our narrative scaffold, indicating what transitions we must go through in the narrative, with the dynamics matrices indicating how they transition.",A Switching Dynamical System for Narrative Generation ::: Narrative Scaffold - Emotional Trajectory,"What the switching variables actually represent can be chosen by the user. Straightforward narrative scaffolds include event sequences BIBREF6, keywords BIBREF7, or latent template ids BIBREF8. More complex but potentially more informative scaffolds may be created using concepts such as story grammar non-terminals BIBREF9, BIBREF10, or character action taken throughout a story BIBREF11. In our work, we use the sentiment trajectory of the narrative as the scaffold. That is, each $S_i$ for a sentence indicates the overall coarse sentiment of the sentence (Positive, Negative, or Neutral). Though simple, the overall sentiment trajectory of a narrative is important in defining the high level `shape' of a narrative often shared among different narratives BIBREF12, BIBREF13. Furthermore, sentiment trajectory has been shown to be fairly useful in story understanding tasks BIBREF14, BIBREF15. We discuss in the conclusion future directions for using different types of scaffolds.",A Switching Dynamical System for Narrative Generation ::: The Full Model,"The final component of the model is a conditional language model that generates sentence $i$ conditioned on the current $Z_i$, and all previous sentences, $X_{:i}$. Generation continues until an <eos> is reached. This conditional language model may be parameterized as desired, but in this work, we parameterize it as an RNN neural network language model. The graphical model for our SLDS is pictured in Figure FIGREF8. The model consists of three sets of variables: (1) Switching variables $S_1,...,S_N$, (2) Latent state variables $Z_1,...,Z_N$ capturing the details of the narrative at sentence $i$, (3) The sentences themselves $X_1,...X_N$, where each sentence $X_i$ has $n_i$ words, $x^i_1,...x^i_{n_i}$. The joint over all variables factorizes as below into the following components ($X_{:i}$ stands for all sentence before $X_i$): ❶ Narrative Scaffold Planner: The factor $P(S_i | S_{i-1})$ is a transition matrix, which we calculate via count based statistics from training. It is fed in as prior knowledge and fixed. ❷ Narrative Dynamics Network: The factor $P(Z_i | Z_{i-1}, S_i)$ is determined like a switching linear dynamical system: which is equivalent to drawing $Z_i$ from a Normal distribution with mean $A_{S_i}Z_{i-1}$ and variance $B_{S_i}B_{S_i}^T$. ❸ Conditional Language model: The factor $P(X_i | Z_i, X_{:i})$ is parameterized by an RNN language model conditioned on the latent $Z_i$.",Learning and Posterior Inference,"Due to the conditionals parameterized by neural networks we use amortized variational inference in a manner similar to Variational AutoEncoders BIBREF16, both to learn an approximate posterior $q(S, Z | X)$ and to learn the generative model parameters by maximizing a lower bound on the data likelihood (ELBO). We assume that the approximate posterior factorizes as follows: Like in VAEs, computing these individual factors is done through a parameterized function called the inference or recognition network whose parameters are trained jointly with the generative model. In our case there are two forms for the factors in our posterior: (1) The first form, $q(S_i | \textbf {X}) = q_{S_i}$ is parameterized by a classifier that takes in the set of sentences $\mathbf {X}$ and outputs a categorical distribution over the switching variables. (2) The second form, $q(Z_i| Z_{i-1}, S_i, X_{:i}, X_{i}) = q_{Z_i}$ is realized by functions $f_{\mu }(Z_{i-1}, S_i, X_{:i}, X_{i})$ and $f_\sigma (Z_{i-1}, S_i, X_{:i}, X_{i})$ that output the mean and variance, respectively, of a Gaussian over $Z_i$. Borrowing terminology from VAEs, the approximate posterior (the factors given above) act as an `encoder', while the generative model from the previous section can be seen as the `decoder'. This type of training has been previously used in BIBREF17, BIBREF18, BIBREF19, BIBREF20, BIBREF21.",Learning and Posterior Inference ::: Lower bound formula & exact training algorithm,"As mentioned previously, we optimize all parameters (including the variational factor functions) by optimizing a lower bound on the data likelihood. The model may be trained either with supervision labels for the switching states (in our case, sentiment labels) or without supervised labels. If one is training without the sentiment labels, then the lower bound on the marginal likelihood (and thus our optimization objective) may be written as follows: The derivation for this objective is identical to that found in BIBREF18, BIBREF19, and simply relies on using properties of iterated expectations. All expectations are estimated with Monte Carlo samples. If training with the sentiment labels $S_1,...,S_N$, then the objective is similar (but without the sampling of the switching states), and is augmented with an additional supervision objective as done in BIBREF22: Final training procedure for a single narrative is: For each sentence (starting from the first), sample the switching state $S_i$ from $q(S_i | \textbf {X})$. For each sentence (starting from the first), sample the latent $Z_i$ from $q(Z_i | S_i, Z_{i-1}, X)$. Evaluate the data likelihood and KL term(s) with these samples. Take the gradients of the objective function w.r.t. all parameters, using the reparameterization trick for $q_{Z_i}$ BIBREF16 or the Gumbel-Softmax trick for $q_{S_i}$ BIBREF23, and optimize.",Interpolations via Gibbs Sampling,"One of the benefits of probabilistic formulation is the possibility (if an inference procedure can be found) of generating narratives with specific constraints, where the constraints may be specified as clamped variables in the model. In this section, we show how narratives may be generated conditioned on arbitrary bits and pieces of the narrative already filled in, using approximate Gibbs sampling. This allows one to, for example, interpolate a narrative given the first and the last sentence (similar to how earlier story generation systems were able to generate with a given end goal in mind). Some examples of these interpolations generated by our system can be found in Table TABREF37. We give the equations and summarize the algorithm in the next sections.",Interpolations via Gibbs Sampling ::: Conditionals for Gibbs Sampling,"For our Gibbs sampling algorithm we give the narrative scaffold (switching variables), $S_1,...,S_T \in \mathbf {S}$ and a set of observed sentences, $\mathbf {X^+}$. This may be any set of sentences (the first and last, just the second sentence, etc) as inputs to the system. We wish to find values for the unobserved sentences in set $\mathbf {X^-}$ by sampling from the distribution $P(\mathbf {X^-}, Z_1,...,Z_T | \mathbf {S},\mathbf {X^+})$. We perform this sampling via Gibbs sampling. Two different forms of conditionals need to be derived to do Gibbs sampling. One over some $Z_i$ conditioned on everything else, and one over some $X_i$ conditioned on everything else. By using the d-separation properties of the graph, and substituting the true posterior over $Z_{i}$ with our approximate posterior $q$, we can show the first distribution is approximately proportional to The last line is the product between a Gaussian density over $Z_{i+1}$ and $Z_{i}$, respectively. With some algebraic manipulations, one can show the last line is proportional to a single Gaussian PDF over $Z_i$: To find the second conditional, one can use the d-separation properties of the graph to find that it is proportional to: These two distributions are simply factors of our conditional language model, and both terms can thus be evaluated easily. In theory, one could use this fact to sample the original conditional via Metropolis-Hastings . Unfortunately, we found this approach to be much too slow for practical purposes. We observed that the simple heuristic of deterministically assigning $X_i$ to be the greedy decoded output of the conditional language model $P(X_{i} | X_{:i}, Z_{i})$ works well, as evidenced by the empirical results. We leave it for future work to research different conditional language model parameterizations that allow easy sampling from this conditional",Interpolations via Gibbs Sampling ::: Gibbs Sampling Interpolation Overview,"The variables in the Gibbs sampler are first initialized using some heuristics (see Supplemental Materials for details). After initialization, performing the interpolations with Gibbs sampling follows the below two step process: For each $Z_i$, sample a value $Z^\prime $ from equation $(1)$ and set $Z_i$ to $Z^\prime $. For each $X_i$ in $\mathbf {X}^-$, find a new value for $X_i$ by running greedy decoding using the conditional language model.",Training Details ::: Dataset and Preprocessing,"We use the ROCStories corpora introduced in BIBREF27. It contains 98,159 short commonsense stories in English as training, and 1,570 stories for validation and test each. Each story in the dataset has five-sentences and captures causal and temporal commonsense relations. We limit our vocabulary size to 16,983 based on a per-word frequency cutoff set to 5. For sentiment tags, we automatically tag the entirety of the corpus with the rule based sentiment tagger, Vader BIBREF28, and bucket the polarity scores of Vader into three tags: neutral, negative, and positive. These tags form the label set of the $S$ variables in our SLDS model. We tokenize the stories with Spacy tokenizer. Each sentences in the input narrative has an <eos> tag except for the S2S model discussed below.",Training Details ::: Switching Linear Dynamical System (SLDS),SLDS has RNN encoder and decoder networks with single layer GRU cells of hidden size 1024. Model uses an embedding size of 300. We train the model using Adam optimizer with the defaults used by PyTorch. We stop training the models when the validation loss does not decrease for 3 consecutive epochs. Training details remain same as above unless otherwise mentioned.,Training Details ::: Baselines,"Language Model (LM): We train a two layer recurrent neural language model with GRU cells of hidden size 512. Sequence-to-Sequence Attention Model (S2S): We train a two layer neural sequence to sequence model equipped with bi-linear attention function with GRU cells of hidden size 512. Sentiments tags for a narrative (1 for each sentence) are given as input to the model and the corresponding sentences are concatenated together as the output with only one <eos> tag at the end. This model is trained with a 0.1 dropout. This model is comparable to the static model of BIBREF7, and other recent works employing a notion of scaffolding into neural generation (albeit adapted for our setting). Linear Dynamical System (LDS): We also train a linear dynamical system as discussed in Section SECREF1 as one of our baselines for fair comparisons. Apart from having just a single transition matrix this model has the same architectural details as SLDS. Semi-Supervised SLDS (SLDS-X%): To gauge the usability of semi-supervision, we also train semi-supervised SLDS models with varying amount of labelled sentiment tags unlike the original model which uses 100% tagged data. We refer to these as SLDS-X%, where X is the % labelled data used for training: 1%, 10%, 25%, and 50%.",Evaluations,"As described above, our model is able to perform narrative interpolations via an approximate Gibbs sampling procedure. At the core of our evaluations is thus a fill-in-the-sentences task. We provide 1 or 2 sentences, and require the model to generate the rest of the narrative . We evaluate this via automatic evaluations as well as with crowd-sourced human evaluations. We also report perplexity to evaluate the models' ability to fit the data. Lastly, we look at whether the transitions learned by the SLDS models capture what they are intended to capture: does using the transition matrix associated with a sentiment tag (positive/negative/neutral) lead to a generated sentence with that sentiment?",Evaluations ::: Generating the Interpolations,"For the SLDS models, the interpolations are generated via the Gibbs sampling algorithm described earlier. In all experiments for the SLDS models we draw 50 samples (including burn in samples) and output the interpolation that maximizes the probability of the given sentence(s). Since the baselines do not have the means for doing interpolations, we simulate `interpolations' for the baselines; we draw 1000 samples using top k (with k=15) truncated sampling (conditioned on the given initial sentences, if available). We then output the sample that maximizes the probability of the clamped sentences around which we are interpolating the others. We allow the S2S access to the gold sentiment tags. To give a lower bound on the performance of the SLDS model, we do not provide it with gold tags. We instead provide the SLDS model with the semi-noisy tags that are output from $q(S_i | X)$.",Evaluations ::: Automatic Evaluation of Interpolations,"We automatically evaluate on four different types of interpolations (where different combinations of sentences are removed and the model is forced to regenerate them), We evaluate the generations with the ROUGE BIBREF29 and METEOR BIBREF30 metrics using the true sentences as targets. Table TABREF33 shows the automatic evaluation results from interpolations using our proposed models and baselines. The #Sent(s) column indicates which sentence(s) were removed, and then regenerated by the model. We gave the baselines a slight edge over SLDS because they pick the best out of 1000 samples while SLDS is only out of 50. The SLDS models see their largest gain over the baseline models when at least the first sentence is given as an input. The baseline models do better when the first and second sentence need to be imputed. This is likely due to the fact that having access to the earlier sentences allows a better initialization for the Gibbs sampler. Surprisingly, the semi-supervised variants of the SLDS models achieve higher scores. The reasons for this is discussed below in the Perplexity section.",Evaluations ::: Human Evaluation of Interpolations ::: Annotation Scheme,"As automatic evaluation metrics are not sufficient to assess the quality of any creative task such as narrative generation, we measure the quality of the generations through human evaluation of 200 stories on the Amazon Mechanical Turk platform. We provided Turkers with two generated narratives from two different models, each with five sentences. The first and last sentences were fed to each model as input, and the middle three sentences were generated. Each pair of narratives is graded by 3 users each with two tasks: (1) to rank on a scale of 0-3 each of the sentences except the first one on the basis of its coherency with the previous sentence(s) and (2) compare and rank the two narratives based on their overall coherency, ie how well the story connects the starting/ending sentences.",Evaluations ::: Human Evaluation of Interpolations ::: Human Evaluation Results,"Table TABREF41 reports the result of human evaluations of SLDS and baseline generations. We can observe that people preferred narratives generated by SLDS over the ones generated by baseline models (LM and S2S) as they found the former model more coherent, which is an important criteria for narrative generation. 51.3% of the time SLDS generates better narratives than the LM model while LM in turn does it only 35.0% of the times. 13.7% of the generations end up in tie. The mean sentence level coherence score for SLDS is around 12.5% larger than that of the LM, with a slightly lower standard deviation. We see similar results when compared against the S2S model.",Evaluations ::: Language Modeling Perplexity Score,"As our models are essentially language models, we evaluated their per-sentence negative log-likelihood and per-word perplexity scores, which can be viewed as an indirect measure of how well a system works as a generative model of narrative text. For the SLDS and LDS models these scores are approximations, an upper bound (the negative of the ELBO) to the actual values. For the other two models the scores are exact. A good model should assign low perplexity scores to its test set. In Table TABREF44 SLDS achieves the lowest scores, implying that it is able to model the data distribution well. In Table TABREF45 we also calculate the perplexity scores for the semi-supervised SLDS models to assess the effectiveness of semi-supervised training. Surprisingly, the models with less supervision scored better in terms of perplexity. One possibility for this might be the use of the soft Gumbel-Softmax in the semi-supervised models. The soft Gumbel-Softmax variant does not commit to using a single transition matrix at each time step (instead linearly combining them, weighted by the Softmax weights). This fact may permit the model greater flexibility in fitting the training data. While this leads to better scores in metrics such as perplexity or BLEU, it does leads to transitions that are worse in capturing the properties they should be capturing, as we shall see in the next section.",Evaluations ::: Evaluation of Transition Dynamics,"One matter of interest is whether or not the transitions are capturing what they are supposed to capture, appropriate sentiment. Since we used the sentiment tagger Vader for training tags, we again utilize it to evaluate whether using transitions of a certain sentiment actually leads the model to produce outputs with the given sentiment. To perform this evaluation, we give as input to our models (and the S2S baseline) the sentiment tags for a sentence and allow it to generate a sentence conditioned on these sentiment tags. We then tag the generated sentences with Vader and see if the sentiment tags match the originals. We calculate the F1 score across all sentiment tags and report the macro average. In Table TABREF47 we see that having labels is incredibly important for meaningful transitions. There is a large drop in F1 as the amount of labels given to the model is decreased. The SLDS model that is trained with 100% of the labels performs a little better than even S2S, despite not having direct access to the sentiment labels (SLDS only uses the sentiment labels to decide which transition to use while the S2S model uses attention directly on the sentiment labels).",Related Work,"Story/narrative generation has a rich history in the field of AI. Many early systems were based on structured formalisms for describing common narrative structures BIBREF9, BIBREF10, BIBREF31, many being inspired by the initial work of BIBREF0. There has been a swath of recent work that has looked to add some semblance of a `narrative scaffold' back into generation methods BIBREF32, BIBREF6, BIBREF7, BIBREF33. Many of these methods work as conditional LMs (conditioned directly on the scaffold). This line of work may be combined with our formalization as well, by conditioning the generation on the switching state as well, as done in the model of BIBREF4. Recent work by BIBREF34 has similar goals to ours in permitting more controlability in generation systems, developing a RL-based system that allows users to specify an end goal for a story (by specifying the event class that is desired to appear at the end). Their work differs from ours in that it does not deal with text directly, modeling only the sequences of events in the narrative. It may be possible to utilize this model as the scaffolding component in our model (utilizing their RL policy for the scaffold planner, rather than the simple Markovian distribution used here).",What metrics are used for evaluation?,41bff17f7d7e899c03b051e20ef01f0ebc5c8bb1,two,familiar,no,,18f4d5a2eb93a969d55361267e74aa0c4f6f82fe,False,,,"We automatically evaluate on four different types of interpolations (where different combinations of sentences are removed and the model is forced to regenerate them), We evaluate the generations with the ROUGE BIBREF29 and METEOR BIBREF30 metrics using the true sentences as targets. Table TABREF33 shows the automatic evaluation results from interpolations using our proposed models and baselines. The #Sent(s) column indicates which sentence(s) were removed, and then regenerated by the model. We gave the baselines a slight edge over SLDS because they pick the best out of 1000 samples while SLDS is only out of 50. The SLDS models see their largest gain over the baseline models when at least the first sentence is given as an input. The baseline models do better when the first and second sentence need to be imputed. This is likely due to the fact that having access to the earlier sentences allows a better initialization for the Gibbs sampler. Surprisingly, the semi-supervised variants of the SLDS models achieve higher scores. The reasons for this is discussed below in the Perplexity section.",We evaluate the generations with the ROUGE BIBREF29 and METEOR BIBREF30 metrics using the true sentences as targets.,baaff0164eecfb3d3f1c55a308952fbc3934408d,486a870694ba60f1a1e7e4ec13e328164cd4b43c,,,,,,,,,What baselines are used?,b03e8e9a0cd2a44a215082773c7338f2f3be412a,two,familiar,no,,18f4d5a2eb93a969d55361267e74aa0c4f6f82fe,False,,,"Language Model (LM): We train a two layer recurrent neural language model with GRU cells of hidden size 512. Sequence-to-Sequence Attention Model (S2S): We train a two layer neural sequence to sequence model equipped with bi-linear attention function with GRU cells of hidden size 512. Sentiments tags for a narrative (1 for each sentence) are given as input to the model and the corresponding sentences are concatenated together as the output with only one <eos> tag at the end. This model is trained with a 0.1 dropout. This model is comparable to the static model of BIBREF7, and other recent works employing a notion of scaffolding into neural generation (albeit adapted for our setting). Linear Dynamical System (LDS): We also train a linear dynamical system as discussed in Section SECREF1 as one of our baselines for fair comparisons. Apart from having just a single transition matrix this model has the same architectural details as SLDS. Semi-Supervised SLDS (SLDS-X%): To gauge the usability of semi-supervision, we also train semi-supervised SLDS models with varying amount of labelled sentiment tags unlike the original model which uses 100% tagged data. We refer to these as SLDS-X%, where X is the % labelled data used for training: 1%, 10%, 25%, and 50%.","Language Model (LM): We train a two layer recurrent neural language model with GRU cells of hidden size 512.

 Sequence-to-Sequence Attention Model (S2S): We train a two layer neural sequence to sequence model equipped with bi-linear attention function with GRU cells of hidden size 512.  Linear Dynamical System (LDS): We also train a linear dynamical system as discussed in Section SECREF1 as one of our baselines for fair comparisons. Semi-Supervised SLDS (SLDS-X%): To gauge the usability of semi-supervision, we also train semi-supervised SLDS models with varying amount of labelled sentiment tags unlike the original model which uses 100% tagged data. ",416fd6db64cd80c67c40d80464748a3a86b6f976,486a870694ba60f1a1e7e4ec13e328164cd4b43c,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1-Table1-1.png,"Table 1: A sample filled in narrative generated by our SLDS model given the first and last sentences as input (grayed out), the middle 3 sentences are imputed by our model (bold).",3-Figure1-1.png,"Figure 1: SLDS Generative model-Si is a discrete state (sentiment of a sentence in a multi-sentence narrative ). Zi is a continuous latent vector that is conditioned on to generate the ith sentence in the narrative , Xi. The dynamics of the narrative are completely captured in the dynamical system controlling the latent vector Z. How to transition from Zi to Zi+1 is determined by the state variable Si+1. Arrows fromXi toXi+2 have been left out for clarity.",7-Table2-1.png,"Table 2: F1 scores for ROUGE-1, 2, and L and METEOR (M) (default mode score) for randomly sampled 500 stories from the test set. #Sents(s) column represents the “fill in” sentence(s) that the models generated using Gibbs sampling. Our SLDS models pick the best of 50 samples, the baselines models pick the best of 1000 samples",8-Table3-1.png,Table 3: Sample interpolations from Gibbs sampling. Grayed out lines are provided as input and bold sentences are generated by SLDS.,8-Table4-1.png,Table 4: Human evaluation scores for filled-in narrative generation. Humans judged sentence coherence and chose which model filled in the most coherent narrative overall (13.7% and 13% tie for LM and S2S).,8-Table5-1.png,Table 5: NLL and PPL scores on the test set. Lower is better for both the metrics. Variance in NLL calculation is in the order of 10−3.,Conclusion and Future Work,"In this paper, we formulated the problem of narrative generation as a switching dynamical system. We showed how this formulation captures notions important in narrative generation, such as narrative dynamics and scaffolds. We developed an approximate Gibbs sampling algorithm for the model that permits the system to generate interpolations conditioned on arbitrary parts of the narrative, and evaluated these interpolations using both human and automatic evaluations. Though in this work we used sentiment tags for our scaffolds/switching variables, future work may look at utilizing different kinds of information to guide the generation of narratives. Utilizing the main predicate of a sentence as a scaffold would be a logical next step, and may prove more informative then the sentiment trajectory. A scaffold such as this can take on many more possible values then a sentiment tag, and as such, it may prove difficult to assign a set of dynamics to each value. Another avenue for future work would deal with this possible problem. One potential solution could be to associate each switching variable value with a (learned) vector in a probability simplex, and use this vector to combine a small set of “primitive"" dynamics matrices in order to get that value's associated set of dynamics.",,,,,,,a two layer recurrent neural language model with GRU cells of hidden size 512 a two layer neural sequence to sequence model equipped with bi-linear attention function with GRU cells of hidden size 512 a linear dynamical system semi-supervised SLDS models with varying amount of labelled sentiment tags,,8-Table6-1.png,Table 6: Approximate NLL and PPL scores for SLDS and semi-supervised SLDS on the test set.,9-Table7-1.png,Table 7: Macro F1 scores on sentiment classification task. Results for SLDS and SLDS-X% are averaged over 5 runs.,12-Table8-1.png,Table 8: Sample interpolations from Gibbs sampling. Grayed out lines are provided as input and bold sentences are generated by SLDS.,,,,,,ROUGE BIBREF29 and METEOR BIBREF30,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spatial Concept Acquisition for a Mobile Robot that Integrates Self-Localization and Unsupervised Word Discovery from Spoken Sentences,"In this paper, we propose a novel unsupervised learning method for the lexical acquisition of words related to places visited by robots, from human continuous speech signals. We address the problem of learning novel words by a robot that has no prior knowledge of these words except for a primitive acoustic model. Further, we propose a method that allows a robot to effectively use the learned words and their meanings for self-localization tasks. The proposed method is nonparametric Bayesian spatial concept acquisition method (SpCoA) that integrates the generative model for self-localization and the unsupervised word segmentation in uttered sentences via latent variables related to the spatial concept. We implemented the proposed method SpCoA on SIGVerse, which is a simulation environment, and TurtleBot2, which is a mobile robot in a real environment. Further, we conducted experiments for evaluating the performance of SpCoA. The experimental results showed that SpCoA enabled the robot to acquire the names of places from speech sentences. They also revealed that the robot could effectively utilize the acquired spatial concepts and reduce the uncertainty in self-localization.",Introduction,"Autonomous robots, such as service robots, operating in the human living environment with humans have to be able to perform various tasks and language communication. To this end, robots are required to acquire novel concepts and vocabulary on the basis of the information obtained from their sensors, e.g., laser sensors, microphones, and cameras, and recognize a variety of objects, places, and situations in an ambient environment. Above all, we consider it important for the robot to learn the names that humans associate with places in the environment and the spatial areas corresponding to these names; i.e., the robot has to be able to understand words related to places. Therefore, it is important to deal with considerable uncertainty, such as the robot's movement errors, sensor noise, and speech recognition errors. Several studies on language acquisition by robots have assumed that robots have no prior lexical knowledge. These studies differ from speech recognition studies based on a large vocabulary and natural language processing studies based on lexical, syntactic, and semantic knowledge BIBREF0 , BIBREF1 . Studies on language acquisition by robots also constitute a constructive approach to the human developmental process and the emergence of symbols. The objectives of this study were to build a robot that learns words related to places and efficiently utilizes this learned vocabulary in self-localization. Lexical acquisition related to places is expected to enable a robot to improve its spatial cognition. A schematic representation depicting the target task of this study is shown in Fig. FIGREF3 . This study assumes that a robot does not have any vocabularies in advance but can recognize syllables or phonemes. The robot then performs self-localization while moving around in the environment, as shown in Fig. FIGREF3 (a). An utterer speaks a sentence including the name of the place to the robot, as shown in Fig. FIGREF3 (b). For the purposes of this study, we need to consider the problems of self-localization and lexical acquisition simultaneously. When a robot learns novel words from utterances, it is difficult to determine segmentation boundaries and the identity of different phoneme sequences from the speech recognition results, which can lead to errors. First, let us consider the case of the lexical acquisition of an isolated word. For example, if a robot obtains the speech recognition results “aporu”, “epou”, and “aqpuru” (incorrect phoneme recognition of apple), it is difficult for the robot to determine whether they denote the same referent without prior knowledge. Second, let us consider a case of the lexical acquisition of the utterance of a sentence. For example, a robot obtains a speech recognition result, such as “thisizanaporu.” The robot has to necessarily segment a sentence into individual words, e.g., “this”, “iz”, “an”, and “aporu”. In addition, it is necessary for the robot to recognize words referring to the same referent, e.g., the fruit apple, from among the many segmented results that contain errors. In case of Fig. FIGREF3 (c), there is some possibility of learning names including phoneme errors, e.g., “afroqtabutibe,” because the robot does not have any lexical knowledge. On the other hand, when a robot performs online probabilistic self-localization, we assume that the robot uses sensor data and control data, e.g., values obtained using a range sensor and odometry. If the position of the robot on the global map is unclear, the difficulties associated with the identification of the self-position by only using local sensor information become problematic. In the case of global localization using local information, e.g., a range sensor, the problem that the hypothesis of self-position is present in multiple remote locations, frequently occurs, as shown in Fig. FIGREF3 (d). In order to solve the abovementioned problems, in this study, we adopted the following approach. An utterance is recognized as not a single phoneme sequence but a set of candidates of multiple phonemes. We attempt to suppress the variability in the speech recognition results by performing word discovery taking into account the multiple candidates of speech recognition. In addition, the names of places are learned by associating with words and positions. The lexical acquisition is complemented by using certain particular spatial information; i.e., this information is obtained by hearing utterances including the same word in the same place many times. Furthermore, in this study, we attempt to address the problem of the uncertainty of self-localization by improving the self-position errors by using a recognized utterance including the name of the current place and the acquired spatial concepts, as shown in Fig. FIGREF3 (e). In this paper, we propose nonparametric Bayesian spatial concept acquisition method (SpCoA) on basis of unsupervised word segmentation and a nonparametric Bayesian generative model that integrates self-localization and a clustering in both words and places. The main contributions of this paper are as follows: The remainder of this paper is organized as follows: In Section SECREF2 , previous studies on language acquisition and lexical acquisition relevant to our study are described. In Section SECREF3 , the proposed method SpCoA is presented. In Sections SECREF4 and SECREF5 , we discuss the effectiveness of SpCoA in the simulation and in the real environment. Section SECREF6 concludes this paper.",Lexical acquisition,"Most studies on lexical acquisition typically focus on lexicons about objects BIBREF0 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 . Many of these studies have not be able to address the lexical acquisition of words other than those related to objects, e.g., words about places. Roy et al. proposed a computational model that enables a robot to learn the names of objects from an object image and spontaneous infant-directed speech BIBREF0 . Their results showed that the model performed speech segmentation, word discovery, and visual categorization. Iwahashi et al. reported that a robot properly understands the situation and acquires the relationship of object behaviors and sentences BIBREF2 , BIBREF3 , BIBREF4 . Qu & Chai focused on the conjunction between speech and eye gaze and the use of domain knowledge in lexical acquisition BIBREF6 , BIBREF7 . They proposed an unsupervised learning method that automatically acquires novel words for an interactive system. Qu & Chai's method based on the IBM translation model BIBREF11 estimates the word-entity association probability. Nakamura et al. proposed a method to learn object concepts and word meanings from multimodal information and verbal information BIBREF9 . The method proposed in BIBREF9 is a categorization method based on multimodal latent Dirichlet allocation (MLDA) that enables the acquisition of object concepts from multimodal information, such as visual, auditory, and haptic information BIBREF12 . Araki et al. addressed the development of a method combining unsupervised word segmentation from uttered sentences by a nested Pitman-Yor language model (NPYLM) BIBREF13 and the learning of object concepts by MLDA BIBREF10 . However, the disadvantage of using NPYLM was that phoneme sequences with errors did not result in appropriate word segmentation. These studies did not address the lexical acquisition of the space and place that can also tolerate the uncertainty of phoneme recognition. However, for the introduction of robots into the human living environment, robots need to acquire a lexicon related to not only objects but also places. Our study focuses on the lexical acquisition related to places. Robots can adaptively learn the names of places in various human living environments by using SpCoA. We consider that the acquired names of places can be useful for various tasks, e.g., tasks with a movement of robots by the speech instruction.",Simultaneous learning of places and vocabulary,"The following studies have addressed lexical acquisition related to places. However, these studies could not utilize the learned language knowledge in other estimations such as the self-localization of a robot. Taguchi et al. proposed a method for the unsupervised learning of phoneme sequences and relationships between words and objects from various user utterances without any prior linguistic knowledge other than an acoustic model of phonemes BIBREF1 , BIBREF14 . Further, they proposed a method for the simultaneous categorization of self-position coordinates and lexical learning BIBREF15 . These experimental results showed that it was possible to learn the name of a place from utterances in some cases and to output words corresponding to places in a location that was not used for learning. Milford et al. proposed RatSLAM inspired by the biological knowledge of a pose cell of the hippocampus of rodents BIBREF16 . Milford et al. proposed a method that enables a robot to acquire spatial concepts by using RatSLAM BIBREF17 . Further, Lingodroids, mobile robots that learn a language through robot-to-robot communication, have been studied BIBREF18 , BIBREF19 , BIBREF20 . Here, a robot communicated the name of a place to other robots at various locations. Experimental results showed that two robots acquired the lexicon of places that they had in common. In BIBREF20 , the researchers showed that it was possible to learn temporal concepts in a manner analogous to the acquisition of spatial concepts. These studies reported that the robots created their own vocabulary. However, these studies did not consider the acquisition of a lexicon by human-to-robot speech interactions. Welke et al. proposed a method that acquires spatial representation by the integration of the representation of the continuous state space on the sensorimotor level and the discrete symbolic entities used in high-level reasoning BIBREF21 . This method estimates the probable spatial domain and word from the given objects by using the spatial lexical knowledge extracted from Google Corpus and the position information of the object. Their study is different from ours because their study did not consider lexicon learning from human speech. In the case of global localization, the hypothesis of self-position often remains in multiple remote places. In this case, there is some possibility of performing an incorrect estimation and increasing the estimation error. This problem exists during teaching tasks and self-localization after the lexical acquisition. The abovementioned studies could not deal with this problem. In this paper, we have proposed a method that enables a robot to perform more accurate self-localization by reducing the estimation error of the teaching time by using a smoothing method in the teaching task and by utilizing words acquired through the lexical acquisition. The strengths of this study are that learning of spatial concept and self-localization represented as one generative model and robots are able to utilize acquired lexicon to self-localization autonomously.",Spatial Concept Acquisition,"We propose nonparametric Bayesian spatial concept acquisition method (SpCoA) that integrates a nonparametric morphological analyzer for the lattice BIBREF22 , i.e., latticelm, a spatial clustering method, and Monte Carlo localization (MCL) BIBREF23 .",Generative model,"In our study, we define a position as a specific coordinate or a local point in the environment, and the position distribution as the spatial area of the environment. Further, we define a spatial concept as the names of places and the position distributions corresponding to these names. The model that was developed for spatial concept acquisition is a probabilistic generative model that integrates a self-localization with the simultaneous clustering of places and words. Fig. FIGREF13 shows the graphical model for spatial concept acquisition. Table TABREF14 shows each variable of the graphical model. The number of words in a sentence at time INLINEFORM0 is denoted as INLINEFORM1 . The generative model of the proposed method is defined as equation ( EQREF11 -). DISPLAYFORM0  Then, the probability distribution for equation () can be defined as follows: DISPLAYFORM0  The prior distribution configured by using the stick breaking process (SBP) BIBREF24 is denoted as INLINEFORM0 , the multinomial distribution as INLINEFORM1 , the Dirichlet distribution as INLINEFORM2 , the inverse–Wishart distribution as INLINEFORM3 , and the multivariate Gaussian (normal) distribution as INLINEFORM4 . The motion model and the sensor model of self-localization are denoted as INLINEFORM5 and INLINEFORM6 in equations () and (), respectively. This model can learn an appropriate number of spatial concepts, depending on the data, by using a nonparametric Bayesian approach. We use the SBP, which is one of the methods based on the Dirichlet process. In particular, this model can consider a theoretically infinite number of spatial concepts INLINEFORM0 and position distributions INLINEFORM1 . SBP computations are difficult because they generate an infinite number of parameters. In this study, we approximate a number of parameters by setting sufficiently large values, i.e., a weak-limit approximation BIBREF25 . It is possible to correlate a name with multiple places, e.g., “staircase” is in two different places, and a place with multiple names, e.g., “toilet” and “restroom” refer to the same place. Spatial concepts are represented by a word distribution of the names of the place INLINEFORM0 and several position distributions ( INLINEFORM1 , INLINEFORM2 ) indicated by a multinomial distribution INLINEFORM3 . In other words, this model is capable of relating the mixture of Gaussian distributions to a multinomial distribution of the names of places. It should be noted that the arrows connecting INLINEFORM4 to the surrounding nodes of the proposed graphical model differ from those of ordinal Gaussian mixture model (GMM). We assume that words obtained by the robot do not change its position, but that the position of the robot affects the distribution of words. Therefore, the proposed generative process assumes that the index of position distribution INLINEFORM5 , i.e., the category of the place, is generated from the position of the robot INLINEFORM6 . This change can be naturally introduced without any troubles by introducing equation ( EQREF12 ).",Overview of the proposed method SpCoA,"We assume that a robot performs self-localization by using control data and sensor data at all times. The procedure for the learning of spatial concepts is as follows: An utterer teaches a robot the names of places, as shown in Fig. FIGREF3 (b). Every time the robot arrives at a place that was a designated learning target, the utterer says a sentence, including the name of the current place. The robot performs speech recognition from the uttered speech signal data. Thus, the speech recognition system includes a word dictionary of only Japanese syllables. The speech recognition results are obtained in a lattice format. Word segmentation is performed by using the lattices of the speech recognition results. The robot learns spatial concepts from words obtained by word segmentation and robot positions obtained by self-localization for all teaching times. The details of the learning are given in SECREF23 . The procedure for self-localization utilizing spatial concepts is as follows: The words of the learned spatial concepts are registered to the word dictionary of the speech recognition system. When a robot obtains a speech signal, speech recognition is performed. Then, a word sequence as the 1-best speech recognition result is obtained. The robot modifies the self-localization from words obtained by speech recognition and the position likelihood obtained by spatial concepts. The details of self-localization are provided in SECREF35 . The proposed method can learn words related to places from the utterances of sentences. We use an unsupervised word segmentation method latticelm that can directly segment words from the lattices of the speech recognition results of the uttered sentences BIBREF22 . The lattice can represent to a compact the set of more promising hypotheses of a speech recognition result, such as N-best, in a directed graph format. Unsupervised word segmentation using the lattices of syllable recognition is expected to be able to reduce the variability and errors in phonemes as compared to NPYLM BIBREF13 , i.e., word segmentation using the 1-best speech recognition results. The self-localization method adopts MCL BIBREF23 , a method that is generally used as the localization of mobile robots for simultaneous localization and mapping (SLAM) BIBREF26 . We assume that a robot generates an environment map by using MCL-based SLAM such as FastSLAM BIBREF27 , BIBREF28 in advance, and then, performs localization by using the generated map. Then, the environment map of both an occupancy grid map and a landmark map is acceptable.",Learning of spatial concept,"Spatial concepts are learned from multiple teaching data, control data, and sensor data. The teaching data are a set of uttered sentences for all teaching times. Segmented words of an uttered sentence are converted into a bag-of-words (BoW) representation as a vector of the occurrence counts of words INLINEFORM0 . The set of the teaching times is denoted as INLINEFORM1 , and the number of teaching data items is denoted as INLINEFORM2 . The model parameters are denoted as INLINEFORM3 . The initial values of the model parameters can be set arbitrarily in accordance with a condition. Further, the sampling values of the model parameters from the following joint posterior distribution are obtained by performing Gibbs sampling. DISPLAYFORM0  where the hyperparameters of the model are denoted as INLINEFORM0 . The algorithm of the learning of spatial concepts is shown in Algorithm SECREF23 . The conditional posterior distribution of each element used for performing Gibbs sampling can be expressed as follows: An index INLINEFORM0 of the position distribution is sampled for each data INLINEFORM1 from a posterior distribution as follows: DISPLAYFORM0  An index INLINEFORM0 of the spatial concepts is sampled for each data item INLINEFORM1 from a posterior distribution as follows: DISPLAYFORM0  where INLINEFORM0 denotes a vector of the occurrence counts of words in the sentence at time INLINEFORM1 . A posterior distribution representing word probabilities of the name of place INLINEFORM2 is calculated as follows: DISPLAYFORM0  where variables with the subscript INLINEFORM0 denote the set of all teaching times. A word probability of the name of place INLINEFORM1 is sampled for each INLINEFORM2 as follows: DISPLAYFORM0  where INLINEFORM0 represents the posterior parameter and INLINEFORM1 denotes the BoW representation of all sentences of INLINEFORM2 in INLINEFORM3 . A posterior distribution representing the position distribution INLINEFORM4 is calculated as follows: DISPLAYFORM0  A position distribution INLINEFORM0 , INLINEFORM1 is sampled for each INLINEFORM2 as follows: DISPLAYFORM0  where INLINEFORM0 denotes the Gaussian–inverse–Wishart distribution; INLINEFORM1 , and INLINEFORM2 represent the posterior parameters; and INLINEFORM3 indicates the set of the teaching positions of INLINEFORM4 in INLINEFORM5 . A topic probability distribution INLINEFORM6 of spatial concepts is sampled as follows: DISPLAYFORM0  A posterior distribution representing the mixed weights INLINEFORM0 of the position distributions is calculated as follows: DISPLAYFORM0  A mixed weight INLINEFORM0 of the position distributions is sampled for each INLINEFORM1 as follows: DISPLAYFORM0  where INLINEFORM0 denotes a vector counting all the indices of the Gaussian distribution of INLINEFORM1 in INLINEFORM2 . Self-positions INLINEFORM0 are sampled by using a Monte Carlo fixed-lag smoother BIBREF29 in the learning phase. The smoother can estimate self-position INLINEFORM1 and not INLINEFORM2 , i.e., a sequential estimation from the given data INLINEFORM3 until time INLINEFORM4 , but it can estimate INLINEFORM5 , i.e., an estimation from the given data INLINEFORM6 until time INLINEFORM7 later than INLINEFORM8 INLINEFORM9 . In general, the smoothing method can provide a more accurate estimation than the MCL of online estimation. In contrast, if the self-position of a robot INLINEFORM10 is sampled like direct assignment sampling for each time INLINEFORM11 , the sampling of INLINEFORM12 is divided in the case with the teaching time INLINEFORM13 and another time INLINEFORM14 as follows: DISPLAYFORM0  [tb] Learning of spatial concepts [1] INLINEFORM0 , INLINEFORM1 Localization and speech recognition INLINEFORM2 to INLINEFORM3 INLINEFORM4 BIBREF29 the speech signal is observed INLINEFORM5 add INLINEFORM6 to INLINEFORM7 Registering the lattice add INLINEFORM8 to INLINEFORM9 Registering the teaching time Word segmentation using lattices INLINEFORM10 BIBREF22 Gibbs sampling Initialize parameters INLINEFORM11 , INLINEFORM12 , INLINEFORM13 INLINEFORM14 to INLINEFORM15 INLINEFORM16 ( EQREF25 ) INLINEFORM17 ( EQREF26 ) INLINEFORM18 ( EQREF28 ) INLINEFORM19 ( EQREF30 ) INLINEFORM20 ( EQREF31 ) INLINEFORM21 ( EQREF33 ) INLINEFORM22 to INLINEFORM23 INLINEFORM24 ( EQREF34 ) INLINEFORM25 ",Self-localization of after learning spatial concepts,"A robot that acquires spatial concepts can leverage spatial concepts to self-localization. The estimated model parameters INLINEFORM0 and a speech recognition sentence INLINEFORM1 at time INLINEFORM2 are given to the condition part of the probability formula of MCL as follows: DISPLAYFORM0  When the robot hears the name of a place spoken by the utterer, in addition to the likelihood of the sensor model of MCL, the likelihood of INLINEFORM0 with respect to a speech recognition sentence is calculated as follows: DISPLAYFORM0  The algorithm of self-localization utilizing spatial concepts is shown in Algorithm SECREF35 . The set of particles is denoted as INLINEFORM0 , the temporary set that stores the pairs of the particle INLINEFORM1 and the weight INLINEFORM2 , i.e., INLINEFORM3 , is denoted as INLINEFORM4 . The number of particles is INLINEFORM5 . The function INLINEFORM6 is a function that moves each particle from its previous state INLINEFORM7 to its current state INLINEFORM8 by using control data. The function INLINEFORM9 calculates the likelihood of each particle INLINEFORM10 using sensor data INLINEFORM11 . These functions are normally used in MCL. For further details, please refer to BIBREF26 . In this case, a speech recognition sentence INLINEFORM12 is obtained by the speech recognition system using a word dictionary containing all the learned words. [tb] Self-localization utilizing spatial concepts [1] INLINEFORM13 INLINEFORM14 INLINEFORM15 INLINEFORM16 to INLINEFORM17 INLINEFORM18 () INLINEFORM19 () the speech signal is observed INLINEFORM20 add INLINEFORM21 to INLINEFORM22 INLINEFORM23 to INLINEFORM24 draw INLINEFORM25 with probability INLINEFORM26 add INLINEFORM27 to INLINEFORM28 INLINEFORM29 ",Experiment I,"In this experiment, we validate the evidence of the proposed method (SpCoA) in an environment simulated on the simulator platform SIGVerse BIBREF30 , which enables the simulation of social interactions. The speech recognition is performed using the Japanese continuous speech recognition system Julius BIBREF31 , BIBREF32 . The set of 43 Japanese phonemes defined by Acoustical Society of Japan (ASJ)'s speech database committee is adopted by Julius BIBREF31 . The representation of these phonemes is also adopted in this study. The Julius system uses a word dictionary containing 115 Japanese syllables. The microphone attached on the robot is SHURE's PG27-USB. Further, an unsupervised morphological analyzer, a latticelm 0.4, is implemented BIBREF22 . In the experiment, we compare the following three types of word segmentation methods. A set of syllable sequences is given to the graphical model of SpCoA by each method. This set is used for the learning of spatial concepts as recognized uttered sentences INLINEFORM0 . The remainder of this section is organized as follows: In Section SECREF43 , the conditions and results of learning spatial concepts are described. The experiments performed using the learned spatial concepts are described in Section SECREF49 to SECREF64 . In Section SECREF49 , we evaluate the accuracy of the phoneme recognition and word segmentation for uttered sentences. In Section SECREF56 , we evaluate the clustering accuracy of the estimation results of index INLINEFORM0 of spatial concepts for each teaching utterance. In Section SECREF60 , we evaluate the accuracy of the acquisition of names of places. In Section SECREF64 , we show that spatial concepts can be utilized for effective self-localization.",Learning of spatial concepts,"We conduct this experiment of spatial concept acquisition in the environment prepared on SIGVerse. The experimental environment is shown in Fig. FIGREF45 . A mobile robot can move by performing forward, backward, right rotation, or left rotation movements on a two-dimensional plane. In this experiment, the robot can use an approximately correct map of the considered environment. The robot has a range sensor in front and performs self-localization on the basis of an occupancy grid map. The initial particles are defined by the true initial position of the robot. The number of particles is INLINEFORM0 . The lag value of the Monte Carlo fixed-lag smoothing is fixed at 100. The other parameters of this experiment are as follows: INLINEFORM0 , INLINEFORM1 , INLINEFORM2 , INLINEFORM3 , INLINEFORM4 , INLINEFORM5 , INLINEFORM6 , INLINEFORM7 , and INLINEFORM8 . The number of iterations used for Gibbs sampling is 100. This experiment does not include the direct assignment sampling of INLINEFORM9 in equation ( EQREF34 ), i.e., lines 22–24 of Algorithm SECREF23 are omitted, because we consider that the self-position can be obtained with sufficiently good accuracy by using the Monte Carlo smoothing. Eight places are selected as the learning targets, and eight types of place names are considered. Each uttered place name is shown in Fig. FIGREF45 . These utterances include the same name in different places, i.e., “teeburunoatari” (which means near the table in English), and different names in the same place, i.e., “kiqchiN” and “daidokoro” (which mean a kitchen in English). The other teaching names are “geNkaN” (which means an entrance or a doorway in English); “terebimae” (which means the front of the TV in English); “gomibako” (which means a trash box in English); “hoNdana” (which means a bookshelf in English); and “sofaamae” (which means the front of the sofa in English). The teaching utterances, including the 10 types of phrases, are spoken for a total of 90 times. The phrases in each uttered sentence are listed in Table TABREF46 . The learning results of spatial concepts obtained by using the proposed method are presented here. Fig. FIGREF47 shows the position distributions learned in the experimental environment. Fig. FIGREF47 (top) shows the word distributions of the names of places for each spatial concept, and Fig. FIGREF47 (bottom) shows the multinomial distributions of the indices of the position distributions. Consequently, the proposed method can learn the names of places corresponding to each place of the learning target. In the spatial concept of index INLINEFORM0 , the highest probability of words was “sofamae”, and the highest probability of the indices of the position distribution was INLINEFORM1 ; therefore, the name of a place “sofamae” was learned to correspond to the position distribution of INLINEFORM2 . In the spatial concept of index INLINEFORM3 , “kiqchi” and “daidokoro” were learned to correspond to the position distribution of INLINEFORM4 . Therefore, this result shows that multiple names can be learned for the same place. In the spatial concept of index INLINEFORM5 , “te” and “durunoatari” (one word in a normal situation) were learned to correspond to the position distributions of INLINEFORM6 and INLINEFORM7 . Therefore, this result shows that the same name can be learned for multiple places.",Phoneme recognition accuracy of uttered sentences,"We compared the performance of three types of word segmentation methods for all the considered uttered sentences. It was difficult to weigh the ambiguous syllable recognition and the unsupervised word segmentation separately. Therefore, this experiment considered the positions of a delimiter as a single letter. We calculated the matching rate of a phoneme string of a recognition result of each uttered sentence and the correct phoneme string of the teaching data that was suitably segmented into Japanese morphemes using MeCab, which is an off-the-shelf Japanese morphological analyzer that is widely used for natural language processing. The matching rate of the phoneme string was calculated by using the phoneme accuracy rate (PAR) as follows: DISPLAYFORM0  The numerator of equation ( EQREF52 ) is calculated by using the Levenshtein distance between the correct phoneme string and the recognition phoneme string. INLINEFORM0 denotes the number of substitutions; INLINEFORM1 , the number of deletions; and INLINEFORM2 , the number of insertions. INLINEFORM3 represents the number of phonemes of the correct phoneme string. Table TABREF54 shows the results of PAR. Table TABREF55 presents examples of the word segmentation results of the three considered methods. We found that the unsupervised morphological analyzer capable of using lattices improved the accuracy of phoneme recognition and word segmentation. Consequently, this result suggests that this word segmentation method considers the multiple hypothesis of speech recognition as a whole and reduces uncertainty such as variability in recognition by using the syllable recognition results in the lattice format.",Estimation accuracy of spatial concepts,"We compared the matching rate with the estimation results of index INLINEFORM0 of the spatial concepts of each teaching utterance and the classification results of the correct answer given by humans. The evaluation of this experiment used the adjusted Rand index (ARI) BIBREF33 . ARI is a measure of the degree of similarity between two clustering results. Further, we compared the proposed method with a method of word clustering without location information for the investigation of the effect of lexical acquisition using location information. In particular, a method of word clustering without location information used the Dirichlet process mixture (DPM) of the unigram model of an SBP representation. The parameters corresponding to those of the proposed method were the same as the parameters of the proposed method and were estimated using Gibbs sampling. Fig. FIGREF59 shows the results of the average of the ARI values of 10 trials of learning by Gibbs sampling. Here, we found that the proposed method showed the best score. These results and the results reported in Section SECREF49 suggest that learning by uttered sentences obtained by better phoneme recognition and better word segmentation produces a good result for the acquisition of spatial concepts. Furthermore, in a comparison of two clustering methods, we found that SpCoA was considerably better than DPM, a word clustering method without location information, irrespective of the word segmentation method used. The experimental results showed that it is possible to improve the estimation accuracy of spatial concepts and vocabulary by performing word clustering that considered location information.",Accuracy of acquired phoneme sequences representing the names of places,"We evaluated whether the names of places were properly learned for the considered teaching places. This experiment assumes a request for the best phoneme sequence INLINEFORM0 representing the self-position INLINEFORM1 for a robot. The robot moves close to each teaching place. The probability of a word INLINEFORM2 when the self-position INLINEFORM3 of the robot is given, INLINEFORM4 , can be obtained by using equation ( EQREF37 ). The word having the best probability was selected. We compared the PAR with the correct phoneme sequence and a selected name of the place. Because “kiqchiN” and “daidokoro” were taught for the same place, the word whose PAR was the higher score was adopted. Fig. FIGREF63 shows the results of PAR for the word considered the name of a place. SpCoA (latticelm), the proposed method using the results of unsupervised word segmentation on the basis of the speech recognition results in the lattice format, showed the best PAR score. In the 1-best and BoS methods, a part syllable sequence of the name of a place was more minutely segmented as shown in Table TABREF55 . Therefore, the robot could not learn the name of the teaching place as a coherent phoneme sequence. In contrast, the robot could learn the names of teaching places more accurately by using the proposed method.",Self-localization that utilizes acquired spatial concepts,"In this experiment, we validate that the robot can make efficient use of the acquired spatial concepts. We compare the estimation accuracy of localization for the proposed method (SpCoA MCL) and the conventional MCL. When a robot comes to the learning target, the utterer speaks out the sentence containing the name of the place once again for the robot. The moving trajectory of the robot and the uttered positions are the same in all the trials. In particular, the uttered sentence is “kokowa ** dayo”. When learning a task, this phrase is not used. The number of particles is INLINEFORM0 , and the initial particles are uniformly distributed in the considered environment. The robot performs a control operation for each time step. The estimation error in the localization is evaluated as follows: While running localization, we record the estimation error (equation ( EQREF66 )) on the INLINEFORM0 plane of the floor for each time step. DISPLAYFORM0  where INLINEFORM0 denote the true position coordinates of the robot as obtained from the simulator, and INLINEFORM1 , INLINEFORM2 represent the weighted mean values of localization coordinates. The normalized weight INLINEFORM3 is obtained from the sensor model in MCL as a likelihood. In the utterance time, this likelihood is multiplied by the value calculated using equation ( EQREF37 ). INLINEFORM4 , INLINEFORM5 denote the INLINEFORM6 -coordinate and the INLINEFORM7 -coordinate of index INLINEFORM8 of each particle at time INLINEFORM9 . After running the localization, we calculated the average of INLINEFORM10 . Further, we compared the estimation accuracy rate (EAR) of the global localization. In each trial, we calculated the proportion of time step in which the estimation error was less than 50 cm. Fig. FIGREF68 shows the results of the estimation error and the EAR for 10 trials of each method. All trials of SpCoA MCL (latticelm) and almost all trials of the method using 1-best NPYLM and BoS showed relatively small estimation errors. Results of the second trial of 1-best NPYLM and the fifth trial of BoS showed higher estimation errors. In these trials, many particles converged to other places instead of the place where the robot was, based on utterance information. Nevertheless, compared with those of the conventional MCL, the results obtained using spatial concepts showed an obvious improvement in the estimation accuracy. Consequently, spatial concepts acquired by using the proposed method proved to be very helpful in improving the localization accuracy.",Experiment II,"In this experiment, the effectiveness of the proposed method was tested by using an autonomous mobile robot TurtleBot 2 in a real environment. Fig. FIGREF70 shows TurtleBot 2 used in the experiments. Mapping and self-localization are performed by the robot operating system (ROS). The speech recognition system, the microphone, and the unsupervised morphological analyzer were the same as those described in Section SECREF4 .",Learning of spatial concepts in the real environment,"We conducted an experiment of the spatial concept acquisition in a real environment of an entire floor of a building. In this experiment, self-localization was performed using a map generated by SLAM. The initial particles are defined by the true initial position of the robot. The generated map in the real environment and the names of teaching places are shown in Fig. FIGREF73 . The number of teaching places was 19, and the number of teaching names was 16. The teaching utterances were performed for a total of 100 times. Fig. FIGREF75 shows the position distributions learned on the map. Table TABREF76 shows the five best elements of the multinomial distributions of the name of place INLINEFORM0 and the multinomial distributions of the indices of the position distribution INLINEFORM1 for each index of spatial concept INLINEFORM2 . Thus, we found that the proposed method can learn the names of places corresponding to the considered teaching places in the real environment. For example, in the spatial concept of index INLINEFORM0 , “torire” was learned to correspond to a position distribution of INLINEFORM1 . Similarly, “kidanokeN” corresponded to INLINEFORM2 in INLINEFORM3 , and “kaigihitsu” was corresponded to INLINEFORM4 in INLINEFORM5 . In the spatial concept of index INLINEFORM6 , a part of the syllable sequences was minutely segmented as “sohatsuke”, “N”, and “tani”, “guchi”. In this case, the robot was taught two types of names. These words were learned to correspond to the same position distribution of INLINEFORM7 . In INLINEFORM8 , “gomibako” showed a high probability, and it corresponded to three distributions of the position of INLINEFORM9 . The position distribution of INLINEFORM10 had the fourth highest probability in the spatial concept INLINEFORM11 . Therefore, “raqkukeN,” which had the fifth highest probability in the spatial concept INLINEFORM12 (and was expected to relate to the spatial concept INLINEFORM13 ), can be estimated as the word drawn from spatial concept INLINEFORM14 . However, in practice, this situation did not cause any severe problems because the spatial concept of the index INLINEFORM15 had the highest probabilities for the word “rapukeN” and the position distribution INLINEFORM16 than INLINEFORM17 . In the probabilistic model, the relative probability and the integrative information are important. When the robot listened to an utterance related to “raqkukeN,” it could make use of the spatial concept of index INLINEFORM18 for self-localization with a high probability, and appropriately updated its estimated self-location. We expected that the spatial concept of index INLINEFORM19 was learned as two separate spatial concepts. However, “watarirooka” and “kaidaNmae” were learned as the same spatial concept. Therefore, the multinomial distribution INLINEFORM20 showed a higher probability for the indices of the position distribution corresponding to the teaching places of both “watarirooka” and “kaidaNmae”. The proposed method adopts a nonparametric Bayesian method in which it is possible to form spatial concepts that allow many-to-many correspondences between names and places. In contrast, this can create ambiguity that classifies originally different spatial concepts into one spatial concept as a side effect. There is a possibility that the ambiguity of concepts such as INLINEFORM0 will have a negative effect on self-localization, even though the self-localization performance was (overall) clearly increased by employing the proposed method. The solution of this problem will be considered in future work. In terms of the PAR of uttered sentences, the evaluation value from the evaluation method used in Section SECREF49 is 0.83; this value is comparable to the result in Section SECREF49 . However, in terms of the PAR of the name of the place, the evaluation value from the evaluation method used in Section SECREF60 is 0.35, which is lower than that in Section SECREF60 . We consider that the increase in uncertainty in the real environment and the increase in the number of teaching words reduced the performance. We expect that this problem could be improved using further experience related to places, e.g., if the number of utterances per place is increased, and additional sensory information is provided.",Modification of localization by the acquired spatial concepts,"In this experiment, we verified the modification results of self-localization by using spatial concepts in global self-localization. This experiment used the learning results of spatial concepts presented in Section SECREF71 . The experimental procedures are shown below. The initial particles were uniformly distributed on the entire floor. The robot begins to move from a little distance away to the target place. When the robot reached the target place, the utterer spoke the sentence containing the name of the place for the robot. Upon obtaining the speech information, the robot modifies the self-localization on the basis of the acquired spatial concepts. The number of particles was the same as that mentioned in Section SECREF71 . Fig. FIGREF80 shows the results of the self-localization before (the top part of the figure) and after (the bottom part of the figure) the utterance for three places. The particle states are denoted by red arrows. The moving trajectory of the robot is indicated by a green dotted arrow. Figs. FIGREF80 (a), (b), and (c) show the results for the names of places “toire”, “souhatsukeN”, and “gomibako”. Further, three spatial concepts, i.e., those at INLINEFORM0 , were learned as “gomibako”. In this experiment, the utterer uttered to the robot when the robot came close to the place of INLINEFORM1 . In all the examples shown in the top part of the figure, the particles were dispersed in several places. In contrast, the number of particles near the true position of the robot showed an almost accurate increase in all the examples shown in the bottom part of the figure. Thus, we can conclude that the proposed method can modify self-localization by using spatial concepts.",Conclusion and Future Work,"In this paper, we discussed the spatial concept acquisition, lexical acquisition related to places, and self-localization using acquired spatial concepts. We proposed nonparametric Bayesian spatial concept acquisition method SpCoA that integrates latticelm BIBREF22 , a spatial clustering method, and MCL. We conducted experiments for evaluating the performance of SpCoA in a simulation and a real environment. SpCoA showed good results in all the experiments. In experiments of the learning of spatial concepts, the robot could form spatial concepts for the places of the learning targets from human continuous speech signals in both the room of the simulation environment and the entire floor of the real environment. Further, the unsupervised word segmentation method latticelm could reduce the variability and errors in the recognition of phonemes in all the utterances. SpCoA achieved more accurate lexical acquisition by performing word segmentation using the lattices of the speech recognition results. In the self-localization experiments, the robot could effectively utilize the acquired spatial concepts for recognizing self-position and reducing the estimation errors in self-localization. As a method that further improves the performance of the lexical acquisition, a mutual learning method was proposed by Nakamura et al. on the basis of the integration of the learning of object concepts with a language model BIBREF34 , BIBREF35 . Following a similar approach, Heymann et al. proposed a method that alternately and repeatedly updates phoneme recognition results and the language model by using unsupervised word segmentation BIBREF36 . As a result, they achieved robust lexical acquisition. In our study, we can expect to improve the accuracy of lexical acquisition for spatial concepts by estimating both the spatial concepts and the language model. Furthermore, as a future work, we consider it necessary for robots to learn spatial concepts online and to recognize whether the uttered word indicates the current place or destination. Furthermore, developing a method that simultaneously acquires spatial concepts and builds a map is one of our future objectives. We believe that the spatial concepts will have a positive effect on the mapping. We also intend to examine a method that associates the image and the landscape with spatial concepts and a method that estimates both spatial concepts and object concepts. [] Akira Taniguchi received his BE degree from Ritsumeikan University in 2013 and his ME degree from the Graduate School of Information Science and Engineering, Ritsumeikan University, in 2015. He is currently working toward his PhD degree at the Emergent System Lab, Ritsumeikan University, Japan. His research interests include language acquisition, concept acquisition, and symbol emergence in robotics. [] Tadahiro Taniguchi received the ME and PhD degrees from Kyoto University in 2003 and 2006, respectively. From April 2005 to March 2006, he was a Japan Society for the Promotion of Science (JSPS) research fellow (DC2) in the Department of Mechanical Engineering and Science, Graduate School of Engineering, Kyoto University. From April 2006 to March 2007, he was a JSPS research fellow (PD) in the same department. From April 2007 to March 2008, he was a JSPS research fellow in the Department of Systems Science, Graduate School of Informatics, Kyoto University. From April 2008 to March 2010, he was an assistant professor at the Department of Human and Computer Intelligence, Ritsumeikan University. Since April 2010, he has been an associate professor in the same department. He is currently engaged in research on machine learning, emergent systems, and semiotics. [] Tetsunari Inamura received the BE, MS and PhD degrees from the University of Tokyo, in 1995, 1997 and 2000, respectively. He was a Researcher of the CREST program, Japanese Science and Technology Cooperation, from 2000 to 2003, and then joined the Department of Mechano-Informatics, School of Information Science and Technology, University of Tokyo as a Lecturer, from 2003 to 2006. He is now an Associate Professor in the Principles of Informatics Research Division, National Institute of Informatics, and an Associate Professor in the Department of Informatics, School of Multidisciplinary Sciences, Graduate University for Advanced Studies (SOKENDAI). His research interests include imitation learning and symbol emergence on humanoid robots, development of interactive robots through virtual reality and so on.",,,,,,,How do they show that acquiring names of places helps self-localization?,23d0637f8ae72ae343556ab135eedc7f4cb58032,infinity,unfamiliar,no,,2cfd959e433f290bb50b55722370f0d22fe090b7,False,,,"Table TABREF54 shows the results of PAR. Table TABREF55 presents examples of the word segmentation results of the three considered methods. We found that the unsupervised morphological analyzer capable of using lattices improved the accuracy of phoneme recognition and word segmentation. Consequently, this result suggests that this word segmentation method considers the multiple hypothesis of speech recognition as a whole and reduces uncertainty such as variability in recognition by using the syllable recognition results in the lattice format.","We found that the unsupervised morphological analyzer capable of using lattices improved the accuracy of phoneme recognition and word segmentation. Consequently, this result suggests that this word segmentation method considers the multiple hypothesis of speech recognition as a whole and reduces uncertainty such as variability in recognition by using the syllable recognition results in the lattice format.",9eea9f7b9f483c773699694e80da81d9f5fb9388,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,How do they evaluate how their model acquired words?,21c104d14ba3db7fe2cd804a191f9e6258208235,infinity,unfamiliar,no,,2cfd959e433f290bb50b55722370f0d22fe090b7,False,,,"Accuracy of acquired phoneme sequences representing the names of places We evaluated whether the names of places were properly learned for the considered teaching places. This experiment assumes a request for the best phoneme sequence INLINEFORM0 representing the self-position INLINEFORM1 for a robot. The robot moves close to each teaching place. The probability of a word INLINEFORM2 when the self-position INLINEFORM3 of the robot is given, INLINEFORM4 , can be obtained by using equation ( EQREF37 ). The word having the best probability was selected. We compared the PAR with the correct phoneme sequence and a selected name of the place. Because “kiqchiN” and “daidokoro” were taught for the same place, the word whose PAR was the higher score was adopted. Fig. FIGREF63 shows the results of PAR for the word considered the name of a place. SpCoA (latticelm), the proposed method using the results of unsupervised word segmentation on the basis of the speech recognition results in the lattice format, showed the best PAR score. In the 1-best and BoS methods, a part syllable sequence of the name of a place was more minutely segmented as shown in Table TABREF55 . Therefore, the robot could not learn the name of the teaching place as a coherent phoneme sequence. In contrast, the robot could learn the names of teaching places more accurately by using the proposed method.","Accuracy of acquired phoneme sequences representing the names of places
We evaluated whether the names of places were properly learned for the considered teaching places. This experiment assumes a request for the best phoneme sequence INLINEFORM0 representing the self-position INLINEFORM1 for a robot. The robot moves close to each teaching place. The probability of a word INLINEFORM2 when the self-position INLINEFORM3 of the robot is given, INLINEFORM4 , can be obtained by using equation ( EQREF37 ). The word having the best probability was selected. We compared the PAR with the correct phoneme sequence and a selected name of the place. Because “kiqchiN” and “daidokoro” were taught for the same place, the word whose PAR was the higher score was adopted.

Fig. FIGREF63 shows the results of PAR for the word considered the name of a place. SpCoA (latticelm), the proposed method using the results of unsupervised word segmentation on the basis of the speech recognition results in the lattice format, showed the best PAR score. In the 1-best and BoS methods, a part syllable sequence of the name of a place was more minutely segmented as shown in Table TABREF55 . Therefore, the robot could not learn the name of the teaching place as a coherent phoneme sequence. In contrast, the robot could learn the names of teaching places more accurately by using the proposed method.",668d93e897fd33aa512ddb05558327e6dc536278,258ee4069f740c400c0049a2580945a1cc7f044c,Which method do they use for word segmentation?,d557752c4706b65dcdb7718272180c59d77fb7a7,infinity,unfamiliar,no,,2cfd959e433f290bb50b55722370f0d22fe090b7,False,,,"The proposed method can learn words related to places from the utterances of sentences. We use an unsupervised word segmentation method latticelm that can directly segment words from the lattices of the speech recognition results of the uttered sentences BIBREF22 . The lattice can represent to a compact the set of more promising hypotheses of a speech recognition result, such as N-best, in a directed graph format. Unsupervised word segmentation using the lattices of syllable recognition is expected to be able to reduce the variability and errors in phonemes as compared to NPYLM BIBREF13 , i.e., word segmentation using the 1-best speech recognition results.",We use an unsupervised word segmentation method latticelm that can directly segment words from the lattices of the speech recognition results of the uttered sentences BIBREF22 .,98c74c25f24acd3dd6220f4dd2310d4b84b2f823,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,Does their model start with any prior knowledge of words?,1bdf7e9f3f804930b2933ebd9207a3e000b27742,infinity,unfamiliar,no,,2cfd959e433f290bb50b55722370f0d22fe090b7,False,False,,"The objectives of this study were to build a robot that learns words related to places and efficiently utilizes this learned vocabulary in self-localization. Lexical acquisition related to places is expected to enable a robot to improve its spatial cognition. A schematic representation depicting the target task of this study is shown in Fig. FIGREF3 . This study assumes that a robot does not have any vocabularies in advance but can recognize syllables or phonemes. The robot then performs self-localization while moving around in the environment, as shown in Fig. FIGREF3 (a). An utterer speaks a sentence including the name of the place to the robot, as shown in Fig. FIGREF3 (b). For the purposes of this study, we need to consider the problems of self-localization and lexical acquisition simultaneously.",This study assumes that a robot does not have any vocabularies in advance but can recognize syllables or phonemes.,ac643eb23016caf26384e4a5522dd1d90369d26e,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1-Figure1-1.png,"Fig. 1. Schematic representation of the target task: (a) Learning targets are three places near the objects. (b) When an utterer and a robot came in front of the TV, the utterer spoke “Here is the front of TV.” The same holds true for the other places. (c) The robot performs word discovery from the uttered sentences and learns the related spatial concepts. Words are related to each place. (d) The robot is in front of TV actually. However, the hypothesis of the self-position of the robot is uncertain. Then, the robot asks a neighbor what the current place is. (e) By utilizing spatial concepts and an uttered sentence, the robot can narrow down the hypothesis of self-position.",4-Figure2-1.png,Fig. 2. Graphical model of the proposed method SpCoA,4-TableI-1.png,TABLE I EACH ELEMENT OF THE GRAPHICAL MODEL,7-Figure3-1.png,"Fig. 3. Environment to be used for learning and localization on SIGVerse: This is a pseudo-room in the simulated real world. There is a robot in the center of the room. The size of the room is 500 cm × 1,000 cm, and the size of the robot is 50 cm × 50 cm.",7-Figure4-1.png,Fig. 4. Learning result of the position distribution: A point group of each color to represent each position distribution is drawn on an map of the considered environment. The colors of the point groups are determined randomly. Each balloon shows the index number for each position distribution.,7-TableII-1.png,"TABLE II VARIOUS PHRASES OF EACH JAPANESE SENTENCE: “**” IS USED AS A PLACEHOLDER FOR THE NAME OF EACH PLACE. EXAMPLES OF THESE PHRASES ARE “** is here.”, “This place is **.”, “This place’s name is **.”, AND “Came to **.” IN ENGLISH.",,,,,,,,,PAR score,unsupervised word segmentation method latticelm,7-Figure5-1.png,Fig. 5. Learning result of the multinomial distributions of the names of places W (top); multinomial distributions of the index of the position distribution φl (bottom): All the words obtained during the experiment are shown.,8-TableIII-1.png,TABLE III COMPARISON OF THE PHONEME ACCURACY RATES OF UTTERED SENTENCES FOR DIFFERENT WORD SEGMENTATION METHODS,9-TableIV-1.png,TABLE IV EXAMPLES OF WORD SEGMENTATION RESULTS OF UTTERED SENTENCES. “|” DENOTES A WORD SEGMENT POINT.,9-Figure6-1.png,Fig. 6. Comparison of the accuracy rates of the estimation results of spatial concepts,9-Figure7-1.png,Fig. 7. PAR scores for the word considered the name of a place,,"unsupervised morphological analyzer capable of using lattices improved the accuracy of phoneme recognition and word segmentation Consequently, this result suggests that this word segmentation method considers the multiple hypothesis of speech recognition as a whole and reduces uncertainty such as variability in recognition",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,10-Figure10-1.png,Fig. 10. Teaching places and the names of places shown on the generated map. The teaching places included places having two names each and multiple places having the same names.,10-Figure8-1.png,Fig. 8. Results of estimation errors and EARs of self-localization,,,,,,,,10-Figure9-1.png,Fig. 9. Autonomous mobile robot TurtleBot 2: The robot is based on Yujin Robot Kobuki and Microsoft Kinect for its use as a range sensor.,11-Figure11-1.png,"Fig. 11. Learning result of each position distribution: A point group of each color denoting each position distribution was drawn on the map. The colors of the point groups were determined randomly. Further, each index number is denoted as it = k.",11-TableV-1.png,TABLE V LEARNING RESULT OF HIGH-PROBABILITY WORDS AND INDICES OF THE POSITION DISTRIBUTION FOR EACH SPATIAL CONCEPT,13-Figure12-1.png,"Fig. 12. States of particles: before the teaching utterance (top); after the teaching utterance (bottom). The uttered sentence is “kokowa ** dayo,” (which means “Here is **.”) “**” is the name of the place.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fortia-FBK at SemEval-2017 Task 5: Bullish or Bearish? Inferring Sentiment towards Brands from Financial News Headlines,"In this paper, we describe a methodology to infer Bullish or Bearish sentiment towards companies/brands. More specifically, our approach leverages affective lexica and word embeddings in combination with convolutional neural networks to infer the sentiment of financial news headlines towards a target company. Such architecture was used and evaluated in the context of the SemEval 2017 challenge (task 5, subtask 2), in which it obtained the best performance.",Introduction,"Real time information is key for decision making in highly technical domains such as finance. The explosive growth of financial technology industry (Fintech) continued in 2016, partially due to the current interest in the market for Artificial Intelligence-based technologies. Opinion-rich texts such as micro-blogging and news can have an important impact in the financial sector (e.g. raise or fall in stock value) or in the overall economy (e.g. the Greek public debt crisis). In such a context, having granular access to the opinions of an important part of the population is of key importance to any public and private actor in the field. In order to take advantage of this raw data, it is thus needed to develop machine learning methods allowing to convert unstructured text into information that can be managed and exploited. In this paper, we address the sentiment analysis problem applied to financial headlines, where the goal is, for a given news headline and target company, to infer its polarity score i.e. how positive (or negative) the sentence is with respect to the target company. Previous research BIBREF0 has highlighted the association between news items and market fluctiations; hence, in the financial domain, sentiment analysis can be used as a proxy for bullish (i.e. positive, upwards trend) or bearish (i.e. negative, downwards trend) attitude towards a specific financial actor, allowing to identify and monitor in real-time the sentiment associated with e.g. stocks or brands. Our contribution leverages pre-trained word embeddings (GloVe, trained on wikipedia+gigaword corpus), the DepecheMood affective lexicon, and convolutional neural networks.",Related Works,"While image and sound come with a natural high dimensional embedding, the issue of which is the best representation is still an open research problem in the context of natural language and text. It is beyond the scope of this paper to do a thorough overview of word representations, for this we refer the interest reader to the excellent review provided by BIBREF1 . Here, we will just introduce the main representations that are related to the proposed method.",Data,"The data consists of a set of financial news headlines, crawled from several online outlets such as Yahoo Finance, where each sentence contains one or more company names/brands. Each tuple (headline, company) is annotated with a sentiment score ranging from -1 (very negative, bearish) to 1 (very positive, bullish). The training/test sets provided contain 1142 and 491 annotated sentences, respectively. A sample instance is reported below: Headline: “Morrisons book second consecutive quarter of sales growth” Company name: “Morrisons” Sentiment score: 0.43",Method,"In Figure FIGREF5 , we can see the overall architecture of our model.",Sentence representation and preprocessing,"Minimal preprocessing was adopted in our approach: we replaced the target company's name with a fixed word <company> and numbers with <number>. The sentences were then tokenized using spaces as separator and keeping punctuation symbols as separate tokens. The words are represented as fixed length vectors INLINEFORM0 resulting from the concatenation of GloVe pre-trained embeddings and DepecheMood BIBREF19 lexicon representation. Since we cannot directly concatenate token-based embeddings (provided in GloVe) with the lemma#PoS-based representation available in DepecheMood, we proceeded to re-build the latter in token-based form, applying the exact same methodology albeit with two differences: we started from a larger dataset (51.9K news articles instead of 25.3K) and used a frequency cut-off, i.e. keeping only those tokens that appear at least 5 times in the corpus. These word-level representation are used as the first layer of our network. During training we allow the weights of the representation to be updated. We further add the VADER score for the sentence under analysis. The complete sentence representation is presented in Algorithm UID8 . InputInput OutputOutput The sentence embedding INLINEFORM0 INLINEFORM1   INLINEFORM0 in INLINEFORM1 INLINEFORM2 = [GloVe( INLINEFORM3 , INLINEFORM4 ), DepecheMood( INLINEFORM5 )] INLINEFORM6 Sentence representation",Architectural Details,"A 1D convolutional layer with filters of multiple sizes {2, 3, 4} is applied to the sequence of word embeddings. The filters are used to learn useful translation-invariant representations of the sequential input data. A global max-pooling is then applied across the sequence for each filter output. We apply the concatenation layer to the output of the global max-pooling and the output of VADER. The activation function used between layers is ReLU BIBREF24 except for the out layer where tanh is used to map the output into [-1, 1] range. Dropout BIBREF25 was used to avoid over-fitting to the training data: it prevents the co-adaptation of the neurones and it also provides an inexpensive way to average an exponential number of networks. In addition, we averaged the output of multiple networks with the same architecture but trained independently with different random seeds in order to reduce noise. The loss function used is the cosine distance between the predicted scores and the gold standard for each batch. Even though stochastic optimization methods like Adam BIBREF26 are usually applied to loss functions that are written as a sum of per-sample loss, which is not the case for the cosine, it converges to an acceptable solution. The loss can be written as : DISPLAYFORM0  where INLINEFORM0 and INLINEFORM1 are the predicted and true sentiment scores for batch INLINEFORM2 , respectively. The algorithm for training/testing our model is reported in Algorithm UID15 . InputInput OutputOutput ParameterParameters A set of trained models INLINEFORM0 , and the predictions INLINEFORM1 for the test set INLINEFORM2 The number INLINEFORM3 of models to train INLINEFORM4 see sec 3.1 INLINEFORM5 in INLINEFORM6 INLINEFORM7 ) see Alg. UID8 INLINEFORM8 INLINEFORM9 see Eq. EQREF16 INLINEFORM10 INLINEFORM11 INLINEFORM12 Training/Testing algorithm. To build our model, we set N=10.",Results,"In this section, we report the results obtained by our model according to challenge official evaluation metric, which is based cosine-similarity and described in BIBREF27 . Results are reported for three diverse configurations: (i) the full system; (ii) the system without using word embeddings (i.e. Glove and DepecheMood); and (iii) the system without using pre-processing. In Table TABREF17 we show model's performances on the challenge training data, in a 5-fold cross-validation setting. Further, the final performances obtained with our approach on the challenge test set are reported in Table TABREF18 . Consistently with the cross-validation performances shown earlier, we observe the beneficial impact of word-representations and basic pre-processing.",Conclusions,"In this paper, we presented the network architecture used for the Fortia-FBK submission to the Semeval-2017 Task 5, Subtask 2 challenge, with the goal of predicting positive (bullish) or negative (bearish) attitude towards a target brand from financial news headlines. The proposed system ranked 1st in such challenge. Our approach is based on 1d convolutions and uses fine-tuning of unsupervised word representations and a rule based sentiment model in its inputs. We showed that the use of pre-computed word representations allows to reduce over-fitting and to achieve significantly better generalization, while some basic pre-processing was needed to further improve the performance.",,,,,,,,,,,,,,,,,,,,,,,,,,,How do they incorporate lexicon into the neural network?,6dfad97356b6e82009ee442d7fd2b97b5dcabfe2,five,familiar,no,,5053f146237e8fc8859ed3984b5d3f02f39266b7,False,,,"The words are represented as fixed length vectors INLINEFORM0 resulting from the concatenation of GloVe pre-trained embeddings and DepecheMood BIBREF19 lexicon representation. Since we cannot directly concatenate token-based embeddings (provided in GloVe) with the lemma#PoS-based representation available in DepecheMood, we proceeded to re-build the latter in token-based form, applying the exact same methodology albeit with two differences: we started from a larger dataset (51.9K news articles instead of 25.3K) and used a frequency cut-off, i.e. keeping only those tokens that appear at least 5 times in the corpus.","The words are represented as fixed length vectors INLINEFORM0 resulting from the concatenation of GloVe pre-trained embeddings and DepecheMood BIBREF19 lexicon representation. Since we cannot directly concatenate token-based embeddings (provided in GloVe) with the lemma#PoS-based representation available in DepecheMood, we proceeded to re-build the latter in token-based form, applying the exact same methodology albeit with two differences: we started from a larger dataset (51.9K news articles instead of 25.3K) and used a frequency cut-off, i.e. keeping only those tokens that appear at least 5 times in the corpus.",44ba57260cac39df85bd420c161dbb2c274061d4,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,What is the source of their lexicon?,59a5959a6abfb81b114e7bfaa945301349d20f0f,five,familiar,no,,5053f146237e8fc8859ed3984b5d3f02f39266b7,False,,,"Our contribution leverages pre-trained word embeddings (GloVe, trained on wikipedia+gigaword corpus), the DepecheMood affective lexicon, and convolutional neural networks.","Our contribution leverages pre-trained word embeddings (GloVe, trained on wikipedia+gigaword corpus), the DepecheMood affective lexicon, and convolutional neural networks.",f5727a7370e2fe97ab505b044f66b3de01a5ab4a,258ee4069f740c400c0049a2580945a1cc7f044c,What was their performance?,e2e31ab279d3092418159dfd24760f0f0566e9d3,five,familiar,no,,5053f146237e8fc8859ed3984b5d3f02f39266b7,False,,,"In this section, we report the results obtained by our model according to challenge official evaluation metric, which is based cosine-similarity and described in BIBREF27 . Results are reported for three diverse configurations: (i) the full system; (ii) the system without using word embeddings (i.e. Glove and DepecheMood); and (iii) the system without using pre-processing. In Table TABREF17 we show model's performances on the challenge training data, in a 5-fold cross-validation setting. Further, the final performances obtained with our approach on the challenge test set are reported in Table TABREF18 . Consistently with the cross-validation performances shown earlier, we observe the beneficial impact of word-representations and basic pre-processing. FLOAT SELECTED: Table 1: Cross-validation results FLOAT SELECTED: Table 2: Final results","In Table TABREF17 we show model's performances on the challenge training data, in a 5-fold cross-validation setting.

Further, the final performances obtained with our approach on the challenge test set are reported in Table TABREF18 . Consistently with the cross-validation performances shown earlier, we observe the beneficial impact of word-representations and basic pre-processing. FLOAT SELECTED: Table 1: Cross-validation results FLOAT SELECTED: Table 2: Final results",7251417cb79948cb5178c8711df0aca458f026e9,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,How long is the dataset used for training?,6407dae0c095c2c8e15e6769f03925aa4f0e902e,five,familiar,no,,5053f146237e8fc8859ed3984b5d3f02f39266b7,True,,,,,75c4db9b4d01c01d07ad0fade06a8a02fd04837b,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,What embeddings do they use?,5a2f7e27efdedf3c43498ff0c32f808d406c42ec,five,familiar,no,,5053f146237e8fc8859ed3984b5d3f02f39266b7,False,GloVe,,,"Our contribution leverages pre-trained word embeddings (GloVe, trained on wikipedia+gigaword corpus), the DepecheMood affective lexicon, and convolutional neural networks.","Our contribution leverages pre-trained word embeddings (GloVe, trained on wikipedia+gigaword corpus), the DepecheMood affective lexicon, and convolutional neural networks.",527a86257787c039331092a3217b16a2f0847649,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3-Figure1-1.png,Figure 1: Network architecture,4-Table2-1.png,Table 2: Final results,4-Table1-1.png,Table 1: Cross-validation results,,,,,,,,,,,,,,,DepecheMood,beneficial impact of word-representations and basic pre-processing,,,,,,,,,,,,concatenation of GloVe pre-trained embeddings and DepecheMood BIBREF19 lexicon representation cannot directly concatenate  re-build the latter in token-based form,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Learning Word Embeddings from the Portuguese Twitter Stream: A Study of some Practical Aspects,"This paper describes a preliminary study for producing and distributing a large-scale database of embeddings from the Portuguese Twitter stream. We start by experimenting with a relatively small sample and focusing on three challenges: volume of training data, vocabulary size and intrinsic evaluation metrics. Using a single GPU, we were able to scale up vocabulary size from 2048 words embedded and 500K training examples to 32768 words over 10M training examples while keeping a stable validation loss and approximately linear trend on training time per epoch. We also observed that using less than 50\% of the available training examples for each vocabulary size might result in overfitting. Results on intrinsic evaluation show promising performance for a vocabulary size of 32768 words. Nevertheless, intrinsic evaluation metrics suffer from over-sensitivity to their corresponding cosine similarity thresholds, indicating that a wider range of metrics need to be developed to track progress.",Introduction,"Word embeddings have great practical importance since they can be used as pre-computed high-density features to ML models, significantly reducing the amount of training data required in a variety of NLP tasks. However, there are several inter-related challenges with computing and consistently distributing word embeddings concerning the: Not only the space of possibilities for each of these aspects is large, there are also challenges in performing a consistent large-scale evaluation of the resulting embeddings BIBREF0 . This makes systematic experimentation of alternative word-embedding configurations extremely difficult. In this work, we make progress in trying to find good combinations of some of the previous parameters. We focus specifically in the task of computing word embeddings for processing the Portuguese Twitter stream. User-generated content (such as twitter messages) tends to be populated by words that are specific to the medium, and that are constantly being added by users. These dynamics pose challenges to NLP systems, which have difficulties in dealing with out of vocabulary words. Therefore, learning a semantic representation for those words directly from the user-generated stream - and as the words arise - would allow us to keep up with the dynamics of the medium and reduce the cases for which we have no information about the words. Starting from our own implementation of a neural word embedding model, which should be seen as a flexible baseline model for further experimentation, our research tries to answer the following practical questions: By answering these questions based on a reasonably small sample of Twitter data (5M), we hope to find the best way to proceed and train embeddings for Twitter vocabulary using the much larger amount of Twitter data available (300M), but for which parameter experimentation would be unfeasible. This work can thus be seen as a preparatory study for a subsequent attempt to produce and distribute a large-scale database of embeddings for processing Portuguese Twitter data.",Related Work,"There are several approaches to generating word embeddings. One can build models that explicitly aim at generating word embeddings, such as Word2Vec or GloVe BIBREF1 , BIBREF2 , or one can extract such embeddings as by-products of more general models, which implicitly compute such word embeddings in the process of solving other language tasks. Word embeddings methods aim to represent words as real valued continuous vectors in a much lower dimensional space when compared to traditional bag-of-words models. Moreover, this low dimensional space is able to capture lexical and semantic properties of words. Co-occurrence statistics are the fundamental information that allows creating such representations. Two approaches exist for building word embeddings. One creates a low rank approximation of the word co-occurrence matrix, such as in the case of Latent Semantic Analysis BIBREF3 and GloVe BIBREF2 . The other approach consists in extracting internal representations from neural network models of text BIBREF4 , BIBREF5 , BIBREF1 . Levy and Goldberg BIBREF6 showed that the two approaches are closely related. Although, word embeddings research go back several decades, it was the recent developments of Deep Learning and the word2vec framework BIBREF1 that captured the attention of the NLP community. Moreover, Mikolov et al. BIBREF7 showed that embeddings trained using word2vec models (CBOW and Skip-gram) exhibit linear structure, allowing analogy questions of the form “man:woman::king:??.” and can boost performance of several text classification tasks. One of the issues of recent work in training word embeddings is the variability of experimental setups reported. For instance, in the paper describing GloVe BIBREF2 authors trained their model on five corpora of different sizes and built a vocabulary of 400K most frequent words. Mikolov et al. BIBREF7 trained with 82K vocabulary while Mikolov et al. BIBREF1 was trained with 3M vocabulary. Recently, Arora et al. BIBREF8 proposed a generative model for learning embeddings that tries to explain some theoretical justification for nonlinear models (e.g. word2vec and GloVe) and some hyper parameter choices. Authors evaluated their model using 68K vocabulary. SemEval 2016-Task 4: Sentiment Analysis in Twitter organizers report that participants either used general purpose pre-trained word embeddings, or trained from Tweet 2016 dataset or “from some sort of dataset” BIBREF9 . However, participants neither report the size of vocabulary used neither the possible effect it might have on the task specific results. Recently, Rodrigues et al. BIBREF10 created and distributed the first general purpose embeddings for Portuguese. Word2vec gensim implementation was used and authors report results with different values for the parameters of the framework. Furthermore, authors used experts to translate well established word embeddings test sets for Portuguese language, which they also made publicly available and we use some of those in this work.",Our Neural Word Embedding Model,"The neural word embedding model we use in our experiments is heavily inspired in the one described in BIBREF4 , but ours is one layer deeper and is set to solve a slightly different word prediction task. Given a sequence of 5 words - INLINEFORM0 INLINEFORM1 INLINEFORM2 INLINEFORM3 INLINEFORM4 , the task the model tries to perform is that of predicting the middle word, INLINEFORM5 , based on the two words on the left - INLINEFORM6 INLINEFORM7 - and the two words on the right - INLINEFORM8 INLINEFORM9 : INLINEFORM10 . This should produce embeddings that closely capture distributional similarity, so that words that belong to the same semantic class, or which are synonyms and antonyms of each other, will be embedded in “close” regions of the embedding hyper-space. Our neural model is composed of the following layers: All neural activations in the model are sigmoid functions. The model was implemented using the Syntagma library which relies on Keras BIBREF11 for model development, and we train the model using the built-in ADAM BIBREF12 optimizer with the default parameters.",Experimental Setup,"We are interested in assessing two aspects of the word embedding process. On one hand, we wish to evaluate the semantic quality of the produced embeddings. On the other, we want to quantify how much computational power and training data are required to train the embedding model as a function of the size of the vocabulary INLINEFORM0 we try to embed. These aspects have fundamental practical importance for deciding how we should attempt to produce the large-scale database of embeddings we will provide in the future. All resources developed in this work are publicly available. Apart from the size of the vocabulary to be processed ( INLINEFORM0 ), the hyperparamaters of the model that we could potentially explore are i) the dimensionality of the input word embeddings and ii) the dimensionality of the output word embeddings. As mentioned before, we set both to 64 bits after performing some quick manual experimentation. Full hyperparameter exploration is left for future work. Our experimental testbed comprises a desktop with a nvidia TITAN X (Pascal), Intel Core Quad i7 3770K 3.5Ghz, 32 GB DDR3 RAM and a 180GB SSD drive.",Training Data,"We randomly sampled 5M tweets from a corpus of 300M tweets collected from the Portuguese Twitter community BIBREF13 . The 5M comprise a total of 61.4M words (approx. 12 words per tweets in average). From those 5M tweets we generated a database containing 18.9M distinct 5-grams, along with their frequency counts. In this process, all text was down-cased. To help anonymizing the n-gram information, we substituted all the twitter handles by an artificial token “T_HANDLE"". We also substituted all HTTP links by the token “LINK"". We prepended two special tokens to complete the 5-grams generated from the first two words of the tweet, and we correspondingly appended two other special tokens to complete 5-grams centered around the two last tokens of the tweet. Tokenization was perform by trivially separating tokens by blank space. No linguistic pre-processing, such as for example separating punctuation from words, was made. We opted for not doing any pre-processing for not introducing any linguistic bias from another tool (tokenization of user generated content is not a trivial problem). The most direct consequence of not performing any linguistic pre-processing is that of increasing the vocabulary size and diluting token counts. However, in principle, and given enough data, the embedding model should be able to learn the correct embeddings for both actual words (e.g. “ronaldo"") and the words that have punctuation attached (e.g. “ronaldo!""). In practice, we believe that this can actually be an advantage for the downstream consumers of the embeddings, since they can also relax the requirements of their own tokenization stage. Overall, the dictionary thus produced contains approximately 1.3M distinct entries. Our dictionary was sorted by frequency, so the words with lowest index correspond to the most common words in the corpus. We used the information from the 5-gram database to generate all training data used in the experiments. For a fixed size INLINEFORM0 of the target vocabulary to be embedded (e.g. INLINEFORM1 = 2048), we scanned the database to obtain all possible 5-grams for which all tokens were among the top INLINEFORM2 words of the dictionary (i.e. the top INLINEFORM3 most frequent words in the corpus). Depending on INLINEFORM4 , different numbers of valid training 5-grams were found in the database: the larger INLINEFORM5 the more valid 5-grams would pass the filter. The number of examples collected for each of the values of INLINEFORM6 is shown in Table TABREF16 . Since one of the goals of our experiments is to understand the impact of using different amounts of training data, for each size of vocabulary to be embedded INLINEFORM0 we will run experiments training the models using 25%, 50%, 75% and 100% of the data available.",Metrics related with the Learning Process,"We tracked metrics related to the learning process itself, as a function of the vocabulary size to be embedded INLINEFORM0 and of the fraction of training data used (25%, 50%, 75% and 100%). For all possible configurations, we recorded the values of the training and validation loss (cross entropy) after each epoch. Tracking these metrics serves as a minimalistic sanity check: if the model is not able to solve the word prediction task with some degree of success (e.g. if we observe no substantial decay in the losses) then one should not expect the embeddings to capture any of the distributional information they are supposed to capture.",Tests and Gold-Standard Data for Intrinsic Evaluation,"Using the gold standard data (described below), we performed three types of tests: Class Membership Tests: embeddings corresponding two member of the same semantic class (e.g. “Months of the Year"", “Portuguese Cities"", “Smileys"") should be close, since they are supposed to be found in mostly the same contexts. Class Distinction Test: this is the reciprocal of the previous Class Membership test. Embeddings of elements of different classes should be different, since words of different classes ere expected to be found in significantly different contexts. Word Equivalence Test: embeddings corresponding to synonyms, antonyms, abbreviations (e.g. “porque"" abbreviated by “pq"") and partial references (e.g. “slb and benfica"") should be almost equal, since both alternatives are supposed to be used be interchangeable in all contexts (either maintaining or inverting the meaning). Therefore, in our tests, two words are considered: distinct if the cosine of the corresponding embeddings is lower than 0.70 (or 0.80). to belong to the same class if the cosine of their embeddings is higher than 0.70 (or 0.80). equivalent if the cosine of the embeddings is higher that 0.85 (or 0.95). We report results using different thresholds of cosine similarity as we noticed that cosine similarity is skewed to higher values in the embedding space, as observed in related work BIBREF14 , BIBREF15 . We used the following sources of data for testing Class Membership: AP+Battig data. This data was collected from the evaluation data provided by BIBREF10 . These correspond to 29 semantic classes. Twitter-Class - collected manually by the authors by checking top most frequent words in the dictionary and then expanding the classes. These include the following 6 sets (number of elements in brackets): smileys (13), months (12), countries (6), names (19), surnames (14) Portuguese cities (9). For the Class Distinction test, we pair each element of each of the gold standard classes, with all the other elements from other classes (removing duplicate pairs since ordering does not matter), and we generate pairs of words which are supposed belong to different classes. For Word Equivalence test, we manually collected equivalente pairs, focusing on abbreviations that are popular in Twitters (e.g. “qt"" INLINEFORM0 “quanto"" or “lx"" INLINEFORM1 “lisboa"" and on frequent acronyms (e.g. “slb"" INLINEFORM2 “benfica""). In total, we compiled 48 equivalence pairs. For all these tests we computed a coverage metric. Our embeddings do not necessarily contain information for all the words contained in each of these tests. So, for all tests, we compute a coverage metric that measures the fraction of the gold-standard pairs that could actually be tested using the different embeddings produced. Then, for all the test pairs actually covered, we obtain the success metrics for each of the 3 tests by computing the ratio of pairs we were able to correctly classified as i) being distinct (cosine INLINEFORM0 0.7 or 0.8), ii) belonging to the same class (cosine INLINEFORM1 0.7 or 0.8), and iii) being equivalent (cosine INLINEFORM2 0.85 or 0.95). It is worth making a final comment about the gold standard data. Although we do not expect this gold standard data to be sufficient for a wide-spectrum evaluation of the resulting embeddings, it should be enough for providing us clues regarding areas where the embedding process is capturing enough semantics, and where it is not. These should still provide valuable indications for planning how to produce the much larger database of word embeddings.",Results and Analysis,"We run the training process and performed the corresponding evaluation for 12 combinations of size of vocabulary to be embedded, and the volume of training data available that has been used. Table TABREF27 presents some overall statistics after training for 40 epochs. The average time per epoch increases first with the size of the vocabulary to embed INLINEFORM0 (because the model will have more parameters), and then, for each INLINEFORM1 , with the volume of training data. Using our testbed (Section SECREF4 ), the total time of learning in our experiments varied from a minimum of 160 seconds, with INLINEFORM2 = 2048 and 25% of data, to a maximum of 22.5 hours, with INLINEFORM3 = 32768 and using 100% of the training data available (extracted from 5M tweets). These numbers give us an approximate figure of how time consuming it would be to train embeddings from the complete Twitter corpus we have, consisting of 300M tweets. We now analyze the learning process itself. We plot the training set loss and validation set loss for the different values of INLINEFORM0 (Figure FIGREF28 left) with 40 epochs and using all the available data. As expected, the loss is reducing after each epoch, with validation loss, although being slightly higher, following the same trend. When using 100% we see no model overfitting. We can also observe that the higher is INLINEFORM1 the higher are the absolute values of the loss sets. This is not surprising because as the number of words to predict becomes higher the problem will tend to become harder. Also, because we keep the dimensionality of the embedding space constant (64 dimensions), it becomes increasingly hard to represent and differentiate larger vocabularies in the same hyper-volume. We believe this is a specially valuable indication for future experiments and for deciding the dimensionality of the final embeddings to distribute. On the right side of Figure FIGREF28 we show how the number of training (and validation) examples affects the loss. For a fixed INLINEFORM0 = 32768 we varied the amount of data used for training from 25% to 100%. Three trends are apparent. As we train with more data, we obtain better validation losses. This was expected. The second trend is that by using less than 50% of the data available the model tends to overfit the data, as indicated by the consistent increase in the validation loss after about 15 epochs (check dashed lines in right side of Figure FIGREF28 ). This suggests that for the future we should not try any drastic reduction of the training data to save training time. Finally, when not overfitting, the validation loss seems to stabilize after around 20 epochs. We observed no phase-transition effects (the model seems simple enough for not showing that type of behavior). This indicates we have a practical way of safely deciding when to stop training the model.",Intrinsic Evaluation,"Table TABREF30 presents results for the three different tests described in Section SECREF4 . The first (expected) result is that the coverage metrics increase with the size of the vocabulary being embedded, i.e., INLINEFORM0 . Because the Word Equivalence test set was specifically created for evaluating Twitter-based embedding, when embedding INLINEFORM1 = 32768 words we achieve almost 90% test coverage. On the other hand, for the Class Distinction test set - which was created by doing the cross product of the test cases of each class in Class Membership test set - we obtain very low coverage figures. This indicates that it is not always possible to re-use previously compiled gold-standard data, and that it will be important to compile gold-standard data directly from Twitter content if we want to perform a more precise evaluation. The effect of varying the cosine similarity decision threshold from 0.70 to 0.80 for Class Membership test shows that the percentage of classified as correct test cases drops significantly. However, the drop is more accentuated when training with only a portion of the available data. The differences of using two alternative thresholds values is even higher in the Word Equivalence test. The Word Equivalence test, in which we consider two words equivalent word if the cosine of the embedding vectors is higher than 0.95, revealed to be an extremely demanding test. Nevertheless, for INLINEFORM0 = 32768 the results are far superior, and for a much larger coverage, than for lower INLINEFORM1 . The same happens with the Class Membership test. On the other hand, the Class Distinction test shows a different trend for larger values of INLINEFORM0 = 32768 but the coverage for other values of INLINEFORM1 is so low that becomes difficult to hypothesize about the reduced values of True Negatives (TN) percentage obtained for the largest INLINEFORM2 . It would be necessary to confirm this behavior with even larger values of INLINEFORM3 . One might hypothesize that the ability to distinguish between classes requires larger thresholds when INLINEFORM4 is large. Also, we can speculate about the need of increasing the number of dimensions to be able to encapsulate different semantic information for so many words.",Further Analysis regarding Evaluation Metrics,"Despite already providing interesting practical clues for our goal of trying to embed a larger vocabulary using more of the training data we have available, these results also revealed that the intrinsic evaluation metrics we are using are overly sensitive to their corresponding cosine similarity thresholds. This sensitivity poses serious challenges for further systematic exploration of word embedding architectures and their corresponding hyper-parameters, which was also observed in other recent works BIBREF15 . By using these absolute thresholds as criteria for deciding similarity of words, we create a dependency between the evaluation metrics and the geometry of the embedded data. If we see the embedding data as a graph, this means that metrics will change if we apply scaling operations to certain parts of the graph, even if its structure (i.e. relative position of the embedded words) does not change. For most practical purposes (including training downstream ML models) absolute distances have little meaning. What is fundamental is that the resulting embeddings are able to capture topological information: similar words should be closer to each other than they are to words that are dissimilar to them (under the various criteria of similarity we care about), independently of the absolute distances involved. It is now clear that a key aspect for future work will be developing additional performance metrics based on topological properties. We are in line with recent work BIBREF16 , proposing to shift evaluation from absolute values to more exploratory evaluations focusing on weaknesses and strengths of the embeddings and not so much in generic scores. For example, one metric could consist in checking whether for any given word, all words that are known to belong to the same class are closer than any words belonging to different classes, independently of the actual cosine. Future work will necessarily include developing this type of metrics.",Conclusions,"Producing word embeddings from tweets is challenging due to the specificities of the vocabulary in the medium. We implemented a neural word embedding model that embeds words based on n-gram information extracted from a sample of the Portuguese Twitter stream, and which can be seen as a flexible baseline for further experiments in the field. Work reported in this paper is a preliminary study of trying to find parameters for training word embeddings from Twitter and adequate evaluation tests and gold-standard data. Results show that using less than 50% of the available training examples for each vocabulary size might result in overfitting. The resulting embeddings obtain an interesting performance on intrinsic evaluation tests when trained a vocabulary containing the 32768 most frequent words in a Twitter sample of relatively small size. Nevertheless, results exhibit a skewness in the cosine similarity scores that should be further explored in future work. More specifically, the Class Distinction test set revealed to be challenging and opens the door to evaluation of not only similarity between words but also dissimilarities between words of different semantic classes without using absolute score values. Therefore, a key area of future exploration has to do with better evaluation resources and metrics. We made some initial effort in this front. However, we believe that developing new intrinsic tests, agnostic to absolute values of metrics and concerned with topological aspects of the embedding space, and expanding gold-standard data with cases tailored for user-generated content, is of fundamental importance for the progress of this line of work. Furthermore, we plan to make public available word embeddings trained from a large sample of 300M tweets collected from the Portuguese Twitter stream. This will require experimenting producing embeddings with higher dimensionality (to avoid the cosine skewness effect) and training with even larger vocabularies. Also, there is room for experimenting with some of the hyper-parameters of the model itself (e.g. activation functions, dimensions of the layers), which we know have impact on final results.",,,,,,,,,,,,,,,,,,,,,What new metrics are suggested to track progress?,982979cb3c71770d8d7d2d1be8f92b66223dec85,five,unfamiliar,no,twitter,e8b24c3133e0bec0a6465e1f13acd3a5ed816b37,False,,,"It is now clear that a key aspect for future work will be developing additional performance metrics based on topological properties. We are in line with recent work BIBREF16 , proposing to shift evaluation from absolute values to more exploratory evaluations focusing on weaknesses and strengths of the embeddings and not so much in generic scores. For example, one metric could consist in checking whether for any given word, all words that are known to belong to the same class are closer than any words belonging to different classes, independently of the actual cosine. Future work will necessarily include developing this type of metrics.","We are in line with recent work BIBREF16 , proposing to shift evaluation from absolute values to more exploratory evaluations focusing on weaknesses and strengths of the embeddings and not so much in generic scores. For example, one metric could consist in checking whether for any given word, all words that are known to belong to the same class are closer than any words belonging to different classes, independently of the actual cosine.",45d149671cf9fe75e00db47b656f3903653915d7,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,What intrinsic evaluation metrics are used?,5ba6f7f235d0f5d1d01fd97dd5e4d5b0544fd212,five,unfamiliar,no,twitter,e8b24c3133e0bec0a6465e1f13acd3a5ed816b37,False,,,"Tests and Gold-Standard Data for Intrinsic Evaluation Using the gold standard data (described below), we performed three types of tests: Class Membership Tests: embeddings corresponding two member of the same semantic class (e.g. “Months of the Year"", “Portuguese Cities"", “Smileys"") should be close, since they are supposed to be found in mostly the same contexts. Class Distinction Test: this is the reciprocal of the previous Class Membership test. Embeddings of elements of different classes should be different, since words of different classes ere expected to be found in significantly different contexts. Word Equivalence Test: embeddings corresponding to synonyms, antonyms, abbreviations (e.g. “porque"" abbreviated by “pq"") and partial references (e.g. “slb and benfica"") should be almost equal, since both alternatives are supposed to be used be interchangeable in all contexts (either maintaining or inverting the meaning). Therefore, in our tests, two words are considered: distinct if the cosine of the corresponding embeddings is lower than 0.70 (or 0.80). to belong to the same class if the cosine of their embeddings is higher than 0.70 (or 0.80). equivalent if the cosine of the embeddings is higher that 0.85 (or 0.95).","Tests and Gold-Standard Data for Intrinsic Evaluation
Using the gold standard data (described below), we performed three types of tests:

Class Membership Tests: embeddings corresponding two member of the same semantic class (e.g. “Months of the Year"", “Portuguese Cities"", “Smileys"") should be close, since they are supposed to be found in mostly the same contexts.

Class Distinction Test: this is the reciprocal of the previous Class Membership test. Embeddings of elements of different classes should be different, since words of different classes ere expected to be found in significantly different contexts.

Word Equivalence Test: embeddings corresponding to synonyms, antonyms, abbreviations (e.g. “porque"" abbreviated by “pq"") and partial references (e.g. “slb and benfica"") should be almost equal, since both alternatives are supposed to be used be interchangeable in all contexts (either maintaining or inverting the meaning).

Therefore, in our tests, two words are considered:

distinct if the cosine of the corresponding embeddings is lower than 0.70 (or 0.80).

to belong to the same class if the cosine of their embeddings is higher than 0.70 (or 0.80).

equivalent if the cosine of the embeddings is higher that 0.85 (or 0.95).",5b95a3c6959abfd9d7011bf633e6275a25ac80e4,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,What experimental results suggest that using less than 50% of the available training examples might result in overfitting?,7ce7edd06925a943e32b59f3e7b5159ccb7acaf6,five,unfamiliar,no,twitter,e8b24c3133e0bec0a6465e1f13acd3a5ed816b37,False,,,"On the right side of Figure FIGREF28 we show how the number of training (and validation) examples affects the loss. For a fixed INLINEFORM0 = 32768 we varied the amount of data used for training from 25% to 100%. Three trends are apparent. As we train with more data, we obtain better validation losses. This was expected. The second trend is that by using less than 50% of the data available the model tends to overfit the data, as indicated by the consistent increase in the validation loss after about 15 epochs (check dashed lines in right side of Figure FIGREF28 ). This suggests that for the future we should not try any drastic reduction of the training data to save training time. Finally, when not overfitting, the validation loss seems to stabilize after around 20 epochs. We observed no phase-transition effects (the model seems simple enough for not showing that type of behavior). This indicates we have a practical way of safely deciding when to stop training the model.","The second trend is that by using less than 50% of the data available the model tends to overfit the data, as indicated by the consistent increase in the validation loss after about 15 epochs (check dashed lines in right side of Figure FIGREF28 ).",c420b34ca1e7a288443bbfb7b81a7fcbd3b002b2,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,6-Table1-1.png,Table 1. Number of 5-grams available for training for different sizes of target vocabulary |V |,8-Table2-1.png,Table 2. Overall statistics for 12 combinations of models learned varying |V | and volume of training data. Results observed after 40 training epochs.,8-Figure1-1.png,Fig. 1. Continuous line represents loss in the training data while dashed line represents loss in the validation data. Left side: effect of increasing |V | using 100% of training data. Right side: effect of varying the amount of training data used with |V | = 32768.,10-Table3-1.png,"Table 3. Evaluation of resulting embeddings using Class Membership, Class Distinction and Word Equivalence tests for different thresholds of cosine similarity.",,,,,,,,,,,,,Class Membership Tests Class Distinction Test Word Equivalence Test,consistent increase in the validation loss after about 15 epochs,,,,,,,,,,,," For example, one metric could consist in checking whether for any given word, all words that are known to belong to the same class are closer than any words belonging to different classes, independently of the actual cosine",False,coverage metric being distinct (cosine INLINEFORM0 0.7 or 0.8) belonging to the same class (cosine INLINEFORM1 0.7 or 0.8) being equivalent (cosine INLINEFORM2 0.85 or 0.95),,,"For all these tests we computed a coverage metric. Our embeddings do not necessarily contain information for all the words contained in each of these tests. So, for all tests, we compute a coverage metric that measures the fraction of the gold-standard pairs that could actually be tested using the different embeddings produced. Then, for all the test pairs actually covered, we obtain the success metrics for each of the 3 tests by computing the ratio of pairs we were able to correctly classified as i) being distinct (cosine INLINEFORM0 0.7 or 0.8), ii) belonging to the same class (cosine INLINEFORM1 0.7 or 0.8), and iii) being equivalent (cosine INLINEFORM2 0.85 or 0.95).","For all these tests we computed a coverage metric. Our embeddings do not necessarily contain information for all the words contained in each of these tests. So, for all tests, we compute a coverage metric that measures the fraction of the gold-standard pairs that could actually be tested using the different embeddings produced. Then, for all the test pairs actually covered, we obtain the success metrics for each of the 3 tests by computing the ratio of pairs we were able to correctly classified as i) being distinct (cosine INLINEFORM0 0.7 or 0.8), ii) belonging to the same class (cosine INLINEFORM1 0.7 or 0.8), and iii) being equivalent (cosine INLINEFORM2 0.85 or 0.95).",dac6a1aecadafa7ee208f8900a38ec11ad12fa2f,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Attentional Encoder Network for Targeted Sentiment Classification,"Targeted sentiment classification aims at determining the sentimental tendency towards specific targets. Most of the previous approaches model context and target words with RNN and attention. However, RNNs are difficult to parallelize and truncated backpropagation through time brings difficulty in remembering long-term patterns. To address this issue, this paper proposes an Attentional Encoder Network (AEN) which eschews recurrence and employs attention based encoders for the modeling between context and target. We raise the label unreliability issue and introduce label smoothing regularization. We also apply pre-trained BERT to this task and obtain new state-of-the-art results. Experiments and analysis demonstrate the effectiveness and lightweight of our model.",Introduction,"Targeted sentiment classification is a fine-grained sentiment analysis task, which aims at determining the sentiment polarities (e.g., negative, neutral, or positive) of a sentence over “opinion targets” that explicitly appear in the sentence. For example, given a sentence “I hated their service, but their food was great”, the sentiment polarities for the target “service” and “food” are negative and positive respectively. A target is usually an entity or an entity aspect. In recent years, neural network models are designed to automatically learn useful low-dimensional representations from targets and contexts and obtain promising results BIBREF0 , BIBREF1 . However, these neural network models are still in infancy to deal with the fine-grained targeted sentiment classification task. Attention mechanism, which has been successfully used in machine translation BIBREF2 , is incorporated to enforce the model to pay more attention to context words with closer semantic relations with the target. There are already some studies use attention to generate target-specific sentence representations BIBREF3 , BIBREF4 , BIBREF5 or to transform sentence representations according to target words BIBREF6 . However, these studies depend on complex recurrent neural networks (RNNs) as sequence encoder to compute hidden semantics of texts. The first problem with previous works is that the modeling of text relies on RNNs. RNNs, such as LSTM, are very expressive, but they are hard to parallelize and backpropagation through time (BPTT) requires large amounts of memory and computation. Moreover, essentially every training algorithm of RNN is the truncated BPTT, which affects the model's ability to capture dependencies over longer time scales BIBREF7 . Although LSTM can alleviate the vanishing gradient problem to a certain extent and thus maintain long distance information, this usually requires a large amount of training data. Another problem that previous studies ignore is the label unreliability issue, since neutral sentiment is a fuzzy sentimental state and brings difficulty for model learning. As far as we know, we are the first to raise the label unreliability issue in the targeted sentiment classification task. This paper propose an attention based model to solve the problems above. Specifically, our model eschews recurrence and employs attention as a competitive alternative to draw the introspective and interactive semantics between target and context words. To deal with the label unreliability issue, we employ a label smoothing regularization to encourage the model to be less confident with fuzzy labels. We also apply pre-trained BERT BIBREF8 to this task and show our model enhances the performance of basic BERT model. Experimental results on three benchmark datasets show that the proposed model achieves competitive performance and is a lightweight alternative of the best RNN based models. The main contributions of this work are presented as follows:",Related Work,"The research approach of the targeted sentiment classification task including traditional machine learning methods and neural networks methods. Traditional machine learning methods, including rule-based methods BIBREF9 and statistic-based methods BIBREF10 , mainly focus on extracting a set of features like sentiment lexicons features and bag-of-words features to train a sentiment classifier BIBREF11 . The performance of these methods highly depends on the effectiveness of the feature engineering works, which are labor intensive. In recent years, neural network methods are getting more and more attention as they do not need handcrafted features and can encode sentences with low-dimensional word vectors where rich semantic information stained. In order to incorporate target words into a model, Tang et al. tang2016effective propose TD-LSTM to extend LSTM by using two single-directional LSTM to model the left context and right context of the target word respectively. Tang et al. tang2016aspect design MemNet which consists of a multi-hop attention mechanism with an external memory to capture the importance of each context word concerning the given target. Multiple attention is paid to the memory represented by word embeddings to build higher semantic information. Wang et al. wang2016attention propose ATAE-LSTM which concatenates target embeddings with word representations and let targets participate in computing attention weights. Chen et al. chen2017recurrent propose RAM which adopts multiple-attention mechanism on the memory built with bidirectional LSTM and nonlinearly combines the attention results with gated recurrent units (GRUs). Ma et al. ma2017interactive propose IAN which learns the representations of the target and context with two attention networks interactively.",Proposed Methodology,"Given a context sequence INLINEFORM0 and a target sequence INLINEFORM1 , where INLINEFORM2 is a sub-sequence of INLINEFORM3 . The goal of this model is to predict the sentiment polarity of the sentence INLINEFORM4 over the target INLINEFORM5 . Figure FIGREF9 illustrates the overall architecture of the proposed Attentional Encoder Network (AEN), which mainly consists of an embedding layer, an attentional encoder layer, a target-specific attention layer, and an output layer. Embedding layer has two types: GloVe embedding and BERT embedding. Accordingly, the models are named AEN-GloVe and AEN-BERT.",Embedding Layer,"Let INLINEFORM0 to be the pre-trained GloVe BIBREF12 embedding matrix, where INLINEFORM1 is the dimension of word vectors and INLINEFORM2 is the vocabulary size. Then we map each word INLINEFORM3 to its corresponding embedding vector INLINEFORM4 , which is a column in the embedding matrix INLINEFORM5 . BERT embedding uses the pre-trained BERT to generate word vectors of sequence. In order to facilitate the training and fine-tuning of BERT model, we transform the given context and target to “[CLS] + context + [SEP]” and “[CLS] + target + [SEP]” respectively.",Attentional Encoder Layer,"The attentional encoder layer is a parallelizable and interactive alternative of LSTM and is applied to compute the hidden states of the input embeddings. This layer consists of two submodules: the Multi-Head Attention (MHA) and the Point-wise Convolution Transformation (PCT). Multi-Head Attention (MHA) is the attention that can perform multiple attention function in parallel. Different from Transformer BIBREF13 , we use Intra-MHA for introspective context words modeling and Inter-MHA for context-perceptive target words modeling, which is more lightweight and target is modeled according to a given context. An attention function maps a key sequence INLINEFORM0 and a query sequence INLINEFORM1 to an output sequence INLINEFORM2 : DISPLAYFORM0   where INLINEFORM0 denotes the alignment function which learns the semantic relevance between INLINEFORM1 and INLINEFORM2 : DISPLAYFORM0   where INLINEFORM0 are learnable weights. MHA can learn n_head different scores in parallel child spaces and is very powerful for alignments. The INLINEFORM0 outputs are concatenated and projected to the specified hidden dimension INLINEFORM1 , namely, DISPLAYFORM0   where “ INLINEFORM0 ” denotes vector concatenation, INLINEFORM1 , INLINEFORM2 is the output of the INLINEFORM3 -th head attention and INLINEFORM4 . Intra-MHA, or multi-head self-attention, is a special situation for typical attention mechanism that INLINEFORM0 . Given a context embedding INLINEFORM1 , we can get the introspective context representation INLINEFORM2 by: DISPLAYFORM0   The learned context representation INLINEFORM0 is aware of long-term dependencies. Inter-MHA is the generally used form of attention mechanism that INLINEFORM0 is different from INLINEFORM1 . Given a context embedding INLINEFORM2 and a target embedding INLINEFORM3 , we can get the context-perceptive target representation INLINEFORM4 by: DISPLAYFORM0  After this interactive procedure, each given target word INLINEFORM0 will have a composed representation selected from context embeddings INLINEFORM1 . Then we get the context-perceptive target words modeling INLINEFORM2 . A Point-wise Convolution T ransformation (PCT) can transform contextual information gathered by the MHA. Point-wise means that the kernel sizes are 1 and the same transformation is applied to every single token belonging to the input. Formally, given a input sequence INLINEFORM0 , PCT is defined as: DISPLAYFORM0   where INLINEFORM0 stands for the ELU activation, INLINEFORM1 is the convolution operator, INLINEFORM2 and INLINEFORM3 are the learnable weights of the two convolutional kernels, INLINEFORM4 and INLINEFORM5 are biases of the two convolutional kernels. Given INLINEFORM0 and INLINEFORM1 , PCTs are applied to get the output hidden states of the attentional encoder layer INLINEFORM2 and INLINEFORM3 by: DISPLAYFORM0 ",Target-specific Attention Layer,"After we obtain the introspective context representation INLINEFORM0 and the context-perceptive target representation INLINEFORM1 , we employ another MHA to obtain the target-specific context representation INLINEFORM2 by: DISPLAYFORM0   The multi-head attention function here also has its independent parameters.",Output Layer,"We get the final representations of the previous outputs by average pooling, concatenate them as the final comprehensive representation INLINEFORM0 , and use a full connected layer to project the concatenated vector into the space of the targeted INLINEFORM1 classes. DISPLAYFORM0   where INLINEFORM0 is the predicted sentiment polarity distribution, INLINEFORM1 and INLINEFORM2 are learnable parameters.",Regularization and Model Training,"Since neutral sentiment is a very fuzzy sentimental state, training samples which labeled neutral are unreliable. We employ a Label Smoothing Regularization (LSR) term in the loss function. which penalizes low entropy output distributions BIBREF14 . LSR can reduce overfitting by preventing a network from assigning the full probability to each training example during training, replaces the 0 and 1 targets for a classifier with smoothed values like 0.1 or 0.9. For a training sample INLINEFORM0 with the original ground-truth label distribution INLINEFORM1 , we replace INLINEFORM2 with DISPLAYFORM0   where INLINEFORM0 is the prior distribution over labels , and INLINEFORM1 is the smoothing parameter. In this paper, we set the prior label distribution to be uniform INLINEFORM2 . LSR is equivalent to the KL divergence between the prior label distribution INLINEFORM0 and the network's predicted distribution INLINEFORM1 . Formally, LSR term is defined as: DISPLAYFORM0  The objective function (loss function) to be optimized is the cross-entropy loss with INLINEFORM0 and INLINEFORM1 regularization, which is defined as: DISPLAYFORM0   where INLINEFORM0 is the ground truth represented as a one-hot vector, INLINEFORM1 is the predicted sentiment distribution vector given by the output layer, INLINEFORM2 is the coefficient for INLINEFORM3 regularization term, and INLINEFORM4 is the parameter set.",Datasets and Experimental Settings,"We conduct experiments on three datasets: SemEval 2014 Task 4 BIBREF15 dataset composed of Restaurant reviews and Laptop reviews, and ACL 14 Twitter dataset gathered by Dong et al. dong2014adaptive. These datasets are labeled with three sentiment polarities: positive, neutral and negative. Table TABREF31 shows the number of training and test instances in each category. Word embeddings in AEN-GloVe do not get updated in the learning process, but we fine-tune pre-trained BERT in AEN-BERT. Embedding dimension INLINEFORM0 is 300 for GloVe and is 768 for pre-trained BERT. Dimension of hidden states INLINEFORM1 is set to 300. The weights of our model are initialized with Glorot initialization BIBREF16 . During training, we set label smoothing parameter INLINEFORM2 to 0.2 BIBREF14 , the coefficient INLINEFORM3 of INLINEFORM4 regularization item is INLINEFORM5 and dropout rate is 0.1. Adam optimizer BIBREF17 is applied to update all the parameters. We adopt the Accuracy and Macro-F1 metrics to evaluate the performance of the model.",Model Comparisons,"In order to comprehensively evaluate and analysis the performance of AEN-GloVe, we list 7 baseline models and design 4 ablations of AEN-GloVe. We also design a basic BERT-based model to evaluate the performance of AEN-BERT.   Non-RNN based baselines:  INLINEFORM0 Feature-based SVM BIBREF18 is a traditional support vector machine based model with extensive feature engineering.  INLINEFORM0 Rec-NN BIBREF0 firstly uses rules to transform the dependency tree and put the opinion target at the root, and then learns the sentence representation toward target via semantic composition using Recursive NNs.  INLINEFORM0 MemNet BIBREF19 uses multi-hops of attention layers on the context word embeddings for sentence representation to explicitly captures the importance of each context word.   RNN based baselines:  INLINEFORM0 TD-LSTM BIBREF1 extends LSTM by using two LSTM networks to model the left context with target and the right context with target respectively. The left and right target-dependent representations are concatenated for predicting the sentiment polarity of the target.  INLINEFORM0 ATAE-LSTM BIBREF3 strengthens the effect of target embeddings, which appends the target embeddings with each word embeddings and use LSTM with attention to get the final representation for classification.  INLINEFORM0 IAN BIBREF4 learns the representations of the target and context with two LSTMs and attentions interactively, which generates the representations for targets and contexts with respect to each other.  INLINEFORM0 RAM BIBREF5 strengthens MemNet by representing memory with bidirectional LSTM and using a gated recurrent unit network to combine the multiple attention outputs for sentence representation.   AEN-GloVe ablations:  INLINEFORM0 AEN-GloVe w/o PCT ablates PCT module.  INLINEFORM0 AEN-GloVe w/o MHA ablates MHA module.  INLINEFORM0 AEN-GloVe w/o LSR ablates label smoothing regularization.  INLINEFORM0 AEN-GloVe-BiLSTM replaces the attentional encoder layer with two bidirectional LSTM.   Basic BERT-based model:  INLINEFORM0 BERT-SPC feeds sequence “[CLS] + context + [SEP] + target + [SEP]” into the basic BERT model for sentence pair classification task.",Main Results,"Table TABREF34 shows the performance comparison of AEN with other models. BERT-SPC and AEN-BERT obtain substantial accuracy improvements, which shows the power of pre-trained BERT on small-data task. The overall performance of AEN-BERT is better than BERT-SPC, which suggests that it is important to design a downstream network customized to a specific task. As the prior knowledge in the pre-trained BERT is not specific to any particular domain, further fine-tuning on the specific task is necessary for releasing the true power of BERT. The overall performance of TD-LSTM is not good since it only makes a rough treatment of the target words. ATAE-LSTM, IAN and RAM are attention based models, they stably exceed the TD-LSTM method on Restaurant and Laptop datasets. RAM is better than other RNN based models, but it does not perform well on Twitter dataset, which might because bidirectional LSTM is not good at modeling small and ungrammatical text. Feature-based SVM is still a competitive baseline, but relying on manually-designed features. Rec-NN gets the worst performances among all neural network baselines as dependency parsing is not guaranteed to work well on ungrammatical short texts such as tweets and comments. Like AEN, MemNet also eschews recurrence, but its overall performance is not good since it does not model the hidden semantic of embeddings, and the result of the last attention is essentially a linear combination of word embeddings.",Model Analysis,"As shown in Table TABREF34 , the performances of AEN-GloVe ablations are incomparable with AEN-GloVe in both accuracy and macro-F1 measure. This result shows that all of these discarded components are crucial for a good performance. Comparing the results of AEN-GloVe and AEN-GloVe w/o LSR, we observe that the accuracy of AEN-GloVe w/o LSR drops significantly on all three datasets. We could attribute this phenomenon to the unreliability of the training samples with neutral sentiment. The overall performance of AEN-GloVe and AEN-GloVe-BiLSTM is relatively close, AEN-GloVe performs better on the Restaurant dataset. More importantly, AEN-GloVe has fewer parameters and is easier to parallelize. To figure out whether the proposed AEN-GloVe is a lightweight alternative of recurrent models, we study the model size of each model on the Restaurant dataset. Statistical results are reported in Table TABREF37 . We implement all the compared models base on the same source code infrastructure, use the same hyperparameters, and run them on the same GPU . RNN-based and BERT-based models indeed have larger model size. ATAE-LSTM, IAN, RAM, and AEN-GloVe-BiLSTM are all attention based RNN models, memory optimization for these models will be more difficult as the encoded hidden states must be kept simultaneously in memory in order to perform attention mechanisms. MemNet has the lowest model size as it only has one shared attention layer and two linear layers, it does not calculate hidden states of word embeddings. AEN-GloVe's lightweight level ranks second, since it takes some more parameters than MemNet in modeling hidden states of sequences. As a comparison, the model size of AEN-GloVe-BiLSTM is more than twice that of AEN-GloVe, but does not bring any performance improvements.",Conclusion,"In this work, we propose an attentional encoder network for the targeted sentiment classification task. which employs attention based encoders for the modeling between context and target. We raise the the label unreliability issue add a label smoothing regularization to encourage the model to be less confident with fuzzy labels. We also apply pre-trained BERT to this task and obtain new state-of-the-art results. Experiments and analysis demonstrate the effectiveness and lightweight of the proposed model.",,,,,,,,,,,,,,,,,Do they use multi-attention heads?,9bffc9a9c527e938b2a95ba60c483a916dbd1f6b,five,familiar,no,,5053f146237e8fc8859ed3984b5d3f02f39266b7,False,True,,The attentional encoder layer is a parallelizable and interactive alternative of LSTM and is applied to compute the hidden states of the input embeddings. This layer consists of two submodules: the Multi-Head Attention (MHA) and the Point-wise Convolution Transformation (PCT).,This layer consists of two submodules: the Multi-Head Attention (MHA) and the Point-wise Convolution Transformation (PCT).,0064ff0d9e06a701f36bb4baabb7d086c3311fd6,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,How big is their model?,8434974090491a3c00eed4f22a878f0b70970713,five,familiar,no,,5053f146237e8fc8859ed3984b5d3f02f39266b7,False,,Proposed model has 1.16 million parameters and 11.04 MB.,"To figure out whether the proposed AEN-GloVe is a lightweight alternative of recurrent models, we study the model size of each model on the Restaurant dataset. Statistical results are reported in Table TABREF37 . We implement all the compared models base on the same source code infrastructure, use the same hyperparameters, and run them on the same GPU . FLOAT SELECTED: Table 3: Model sizes. Memory footprints are evaluated on the Restaurant dataset. Lowest 2 are in bold.",Statistical results are reported in Table TABREF37 . FLOAT SELECTED: Table 3: Model sizes. Memory footprints are evaluated on the Restaurant dataset. Lowest 2 are in bold.,dfb36457161c897a38f62432f6193613b02071e8,258ee4069f740c400c0049a2580945a1cc7f044c,How is their model different from BERT?,b67420da975689e47d3ea1c12b601851018c4071,five,familiar,no,,5053f146237e8fc8859ed3984b5d3f02f39266b7,False,,,"Figure FIGREF9 illustrates the overall architecture of the proposed Attentional Encoder Network (AEN), which mainly consists of an embedding layer, an attentional encoder layer, a target-specific attention layer, and an output layer. Embedding layer has two types: GloVe embedding and BERT embedding. Accordingly, the models are named AEN-GloVe and AEN-BERT.","Figure FIGREF9 illustrates the overall architecture of the proposed Attentional Encoder Network (AEN), which mainly consists of an embedding layer, an attentional encoder layer, a target-specific attention layer, and an output layer. Embedding layer has two types: GloVe embedding and BERT embedding. Accordingly, the models are named AEN-GloVe and AEN-BERT.",5cfeb55daf47a1b7845791e8c4a7ed3da8a2ccfd,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3-Figure1-1.png,Figure 1: Overall architecture of the proposed AEN.,5-Table1-1.png,Table 1: Statistics of the datasets.,6-Table2-1.png,Table 2: Main results. The results of baseline models are retrieved from published papers. Top 2 scores are in bold.,7-Table3-1.png,Table 3: Model sizes. Memory footprints are evaluated on the Restaurant dataset. Lowest 2 are in bold.,,,,,,,,,,,,,,"overall architecture of the proposed Attentional Encoder Network (AEN), which mainly consists of an embedding layer, an attentional encoder layer, a target-specific attention layer, and an output layer.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Dependency or Span, End-to-End Uniform Semantic Role Labeling","Semantic role labeling (SRL) aims to discover the predicateargument structure of a sentence. End-to-end SRL without syntactic input has received great attention. However, most of them focus on either span-based or dependency-based semantic representation form and only show specific model optimization respectively. Meanwhile, handling these two SRL tasks uniformly was less successful. This paper presents an end-to-end model for both dependency and span SRL with a unified argument representation to deal with two different types of argument annotations in a uniform fashion. Furthermore, we jointly predict all predicates and arguments, especially including long-term ignored predicate identification subtask. Our single model achieves new state-of-the-art results on both span (CoNLL 2005, 2012) and dependency (CoNLL 2008, 2009) SRL benchmarks.",Introduction,"The purpose of semantic role labeling (SRL) is to derive the meaning representation for a sentence, which is beneficial to a wide range of natural language processing (NLP) tasks BIBREF0 , BIBREF1 . SRL can be formed as four subtasks, including predicate detection, predicate disambiguation, argument identification and argument classification. For argument annotation, there are two formulizations. One is based on text spans, namely span-based SRL. The other is dependency-based SRL, which annotates the syntactic head of argument rather than entire argument span. Figure FIGREF1 shows example annotations. Great progress has been made in syntactic parsing BIBREF2 , BIBREF3 , BIBREF4 . Most traditional SRL methods rely heavily on syntactic features. To alleviate the inconvenience, recent works BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 propose end-to-end models for SRL, putting syntax aside and still achieving favorable results. However, these systems focus on either span or dependency SRL, which motivates us to explore a uniform approach. Both span and dependency are effective formal representations for semantics, though for a long time it has been kept unknown which form, span or dependency, would be better for the convenience and effectiveness of semantic machine learning and later applications. Furthermore, researchers are interested in two forms of SRL models that may benefit from each other rather than their separated development. This topic has been roughly discussed in BIBREF19 , who concluded that the (best) dependency SRL system at then clearly outperformed the span-based (best) system through gold syntactic structure transformation. However, BIBREF19 johansson2008EMNLP like all other traditional SRL models themselves had to adopt rich syntactic features, and their comparison was done between two systems in quite different building styles. Instead, this work will develop full syntax-agnostic SRL systems with the same fashion for both span and dependency representation, so that we can revisit this issue under a more solid empirical basis. In addition, most efforts focus on argument identification and classification since span and dependency SRL corpora have already marked predicate positions. Although no predicate identification is needed, it is not available in many downstream applications. Therefore, predicate identification should be carefully handled in a complete practical SRL system. To address this problem, BIBREF9 he2018jointly proposed an end-to-end approach for jointly predicting predicates and arguments for span SRL. Likewise, BIBREF11 cai2018full introduced an end-to-end model to naturally cover all predicate/argument identification and classification subtasks for dependency SRL. To jointly predict predicates and arguments, we present an end-to-end framework for both span and dependency SRL. Our model extends the span SRL model of BIBREF9 he2018jointly, directly regarding all words in a sentence as possible predicates, considering all spans or words as potential arguments and learning distributions over possible predicates. However, we differ by (1) introducing unified argument representation to handle two different types of SRL tasks, and (2) employing biaffine scorer to make decisions for predicate-argument relationship. The proposed models are evaluated on span SRL datasets: CoNLL 2005 and 2012 data, as well as the dependency SRL dataset of CoNLL 2008 and 2009 shared tasks. For span SRL, our single model outperforms the previous best results by 0.3% and 0.5% F INLINEFORM0 -score on CoNLL 2005 and 2012 test sets respectively. For dependency SRL, we achieve new state-of-the-art of 85.3% F INLINEFORM1 and 90.4% F INLINEFORM2 on CoNLL 2008 and 2009 benchmarks respectively.",Background,"SRL is pioneered by BIBREF20 gildea2002, which uses the PropBank conventions BIBREF21 . Conventionally, span SRL consists of two subtasks, argument identification and classification. The former identifies the arguments of a predicate, and the latter assigns them semantic role labels, namely, determining the relation between arguments and predicates. The PropBank defines a set of semantic roles to label arguments, falling into two categories: core and non-core roles. The core roles (A0-A5 and AA) indicate different semantics in predicate-argument structure, while the non-core roles are modifiers (AM-adj) where adj specifies the adjunct type, such as temporal (AM-TMP) and locative (AM-LOC) adjuncts. For example shown in Figure FIGREF1 , A0 is a proto-agent, representing the borrower. Slightly different from span SRL in argument annotation, dependency SRL labels the syntactic heads of arguments rather than phrasal arguments, which was popularized by CoNLL-2008 and CoNLL-2009 shared tasks BIBREF22 , BIBREF23 . Furthermore, when no predicate is given, two other indispensable subtasks of dependency SRL are predicate identification and disambiguation. One is to identify all predicates in a sentence, and the other is to determine the senses of predicates. As the example shown in Figure FIGREF1 , 01 indicates the first sense from the PropBank sense repository for predicate borrowed in the sentence.",Related Work,"The traditional approaches on SRL were mostly about designing hand-crafted feature templates and then employ linear classifiers such as BIBREF24 , BIBREF25 , BIBREF12 . Even though neural models were introduced, early work still paid more attention on syntactic features. For example, BIBREF14 Fitzgerald2015 integrated syntactic information into neural networks with embedded lexicalized features, while BIBREF15 roth2016 embedded syntactic dependency paths between predicates and arguments. Similarly, BIBREF16 marcheggianiEMNLP2017 leveraged the graph convolutional network to encode syntax for dependency SRL. Recently, BIBREF17 Strubell2018 presented a multi-task neural model to incorporate auxiliary syntactic information for SRL, BIBREF18 li2018unified adopted several kinds of syntactic encoder for syntax encoding while BIBREF10 he:2018Syntax used syntactic tree for argument pruning. However, using syntax may be quite inconvenient sometimes, recent studies thus have attempted to build SRL systems without or with little syntactic guideline. BIBREF5 zhou-xu2015 proposed the first syntax-agnostic model for span SRL using LSTM sequence labeling, while BIBREF7 he-acl2017 further enhanced their model using highway bidirectional LSTMs with constrained decoding. Later, BIBREF8 selfatt2018 presented a deep attentional neural network for applying self-attention to span SRL task. Likewise for dependency SRL, BIBREF6 marcheggiani2017 proposed a syntax-agnostic model with effective word representation and obtained favorable results. BIBREF11 cai2018full built a full end-to-end model with biaffine attention and outperformed the previous state-of-the-art. More recently, joint predicting both predicates and arguments has attracted extensive interest on account of the importance of predicate identification, including BIBREF7 , BIBREF17 , BIBREF9 , BIBREF11 and this work. In our preliminary experiments, we tried to integrate the self-attention into our model, but it does not provide any significant performance gain on span or dependency SRL, which is not consistent with the conclusion in BIBREF8 and lets us exclude it from this work. Generally, the above work is summarized in Table TABREF2 . Considering motivation, our work is most closely related to the work of BIBREF14 Fitzgerald2015, which also tackles span and dependency SRL in a uniform fashion. The essential difference is that their model employs the syntactic features and takes pre-identified predicates as inputs, while our model puts syntax aside and jointly learns and predicts predicates and arguments.",Overview,"Given a sentence INLINEFORM0 , we attempt to predict a set of predicate-argument-relation tuples INLINEFORM1 , where INLINEFORM2 is the set of all possible predicate tokens, INLINEFORM3 includes all the candidate argument spans or dependencies, and INLINEFORM6 is the set of the semantic roles. To simplify the task, we introduce a null label INLINEFORM7 to indicate no relation between arbitrary predicate-argument pair following BIBREF9 he2018jointly. As shown in Figure FIGREF5 , our uniform SRL model includes four main modules:  INLINEFORM0 token representation component to build token representation INLINEFORM1 from word INLINEFORM2 ,  INLINEFORM0 a BiHLSTM encoder that directly takes sequential inputs,  INLINEFORM0 predicate and argument representation module to learn candidate representations,  INLINEFORM0 a biaffine scorer which takes the candidate representations as input and predicts semantic roles.",Token Representation,"We follow the bi-directional LSTM-CNN architecture BIBREF26 , where convolutional neural networks (CNNs) encode characters inside a word INLINEFORM0 into character-level representation INLINEFORM1 then concatenated with its word-level INLINEFORM2 into context-independent representation. To further enhance the word representation, we leverage an external representation INLINEFORM3 from pretrained ELMo (Embeddings from Language Models) layers according to BIBREF27 ELMo. Eventually, the resulting token representation is concatenated as DISPLAYFORM0 ",Deep Encoder,"The encoder in our model adopts the bidirectional LSTM with highway connections (BiHLSTM) to contextualize the representation into task-specific representation: INLINEFORM0 , where the gated highway connections is used to alleviate the vanishing gradient problem when training very deep BiLSTMs.",Predicate and Argument Representation,"We employ contextualized representations for all candidate arguments and predicates. As referred in BIBREF2 , applying a multi-layer perceptron (MLP) to the recurrent output states before the classifier has the advantage of stripping away irrelevant information for the current decision. Therefore, to distinguish the currently considered predicate from its candidate arguments in SRL context, we add an MLP layer to contextualized representations for argument INLINEFORM0 and predicate INLINEFORM1 candidates specific representations respectively with ReLU BIBREF28 as its activation function: INLINEFORM2 INLINEFORM3  To perform uniform SRL, we introduce unified argument representation. For dependency SRL, we assume single word argument span by limiting the length of candidate argument to be 1, so our model uses the INLINEFORM0 as the final argument representation INLINEFORM1 directly. While for span SRL, we utilize the approach of span representation from BIBREF29 lee2017end. Each candidate span representation INLINEFORM2 is built by DISPLAYFORM0  where INLINEFORM0 and INLINEFORM1 are boundary representations, INLINEFORM2 indicates a span, INLINEFORM3 is a feature vector encoding the size of span, and INLINEFORM4 is the specific notion of headedness which is learned by attention mechanism BIBREF30 over words in each span (where INLINEFORM5 is the position inside span) as follows : INLINEFORM6 INLINEFORM7 ",Scorers,"For predicate and arguments, we introduce two unary scores on their candidates: INLINEFORM0 INLINEFORM1  For semantic role, we adopt a relation scorer with biaffine attention BIBREF2 : DISPLAYFORM0   where INLINEFORM0 and INLINEFORM1 respectively denote the weight matrix of the bi-linear and the linear terms and INLINEFORM2 is the bias item. The biaffine scorer differs from feed-forward networks scorer in bilinear transformation. Since SRL can be regarded as a classification task, the distribution of classes is uneven and the problem comes worse after the null labels are introduced. The output layer of the model normally includes a bias term designed to capture the prior probability of each class, with the rest of the model focusing on learning the likelihood of every classes occurring in data. The biaffine attention as Dozat and Manning (2017) in our model directly assigns a score for each specific semantic role and would be helpful for semantic role prediction. Actually, (He et al., 2018a) used a scorer as Equation (2), which is only a part of our scorer including both Equations ( EQREF14 ) and (). Therefore, our scorer would be more informative than previous models such as BIBREF9 .",Training Objective,"The model is trained to optimize the probability INLINEFORM0 of the predicate-argument-relation tuples INLINEFORM1 given the sentence INLINEFORM2 , which can be factorized as: DISPLAYFORM0   where INLINEFORM0 represents the model parameters, and INLINEFORM1 , is the score for the predicate-argument-relation tuple, including predicate score INLINEFORM2 , argument score INLINEFORM3 and relation score INLINEFORM4 . Our model adopts a biaffine scorer for semantic role label prediction, which is implemented as cross-entropy loss. Moreover, our model is trained to minimize the negative likehood of the golden structure INLINEFORM0 : INLINEFORM1 . The score of null labels are enforced into INLINEFORM2 . For predicates and arguments prediction, we train separated scorers ( INLINEFORM3 and INLINEFORM4 ) in parallel fed to the biaffine scorer for predicate and argument predication respectively, which helps to reduce the chance of error propagation.",Candidates Pruning,"The number of candidate arguments for a sentence of length INLINEFORM0 is INLINEFORM1 for span SRL, and INLINEFORM2 for dependency. As the model deals with INLINEFORM3 possible predicates, the computational complexity is INLINEFORM4 for span, INLINEFORM5 for dependency, which is too computationally expensive. To address this issue, we attempt to prune candidates using two beams for storing the candidate arguments and predicates with size INLINEFORM6 and INLINEFORM7 inspired by BIBREF9 he2018jointly, where INLINEFORM8 and INLINEFORM9 are two manually setting thresholds. First, the predicate and argument candidates are ranked according to their predicted score ( INLINEFORM10 and INLINEFORM11 ) respectively, and then we reduce the predicate and argument candidates with defined beams. Finally, we take the candidates from the beams to participate the label prediction. Such pruning will reduce the overall number of candidate tuples to INLINEFORM12 for both types of tasks. Furthermore, for span SRL, we set the maximum length of candidate arguments to INLINEFORM13 , which may decrease the number of candidate arguments to INLINEFORM14 .",SRL Constraints,"According to PropBank semantic convention, predicate-argument structure has to follow a few of global constraints BIBREF25 , BIBREF7 , we thus incorporate constraints on the output structure with a dynamic programing decoder during inference. These constraints are described as follows:  INLINEFORM0 Unique core roles (U): Each core role (A0-A5, AA) should appear at most once for each predicate.  INLINEFORM0 Continuation roles (C): A continuation role C-X can exist only when its base role X is realized before it.  INLINEFORM0 Reference roles (R): A reference role R-X can exist only when its base role X is realized (not necessarily before R-X).  INLINEFORM0 Non-overlapping (O): The semantic arguments for the same predicate do not overlap in span SRL. As C and R constraints lead to worse performance in our models from our preliminary experiments, we only enforce U and O constraints on span SRL and U constraints on dependency SRL.",Experiments,"Our models are evaluated on two PropBank-style SRL tasks: span and dependency. For span SRL, we test model on the common span SRL datasets from CoNLL-2005 BIBREF32 and CoNLL-2012 BIBREF31 shared tasks. For dependency SRL, we experiment on CoNLL 2008 BIBREF22 and 2009 BIBREF23 benchmarks. As for the predicate disambiguation in dependency SRL task, we follow the previous work BIBREF15 . We consider two SRL setups: end-to-end and pre-identified predicates. For the former setup, our system jointly predicts all the predicates and their arguments in one shot, which turns into CoNLL-2008 setting for dependency SRL. In order to compare with previous models, we also report results with pre-identified predicates, where predicates have been beforehand identified in corpora. Therefore, the experimental results fall into two categories: end-to-end results and results with pre-identified predicates.",Datasets,"CoNLL 2005 and 2012 The CoNLL-2005 shared task focused on verbal predicates only for English. The CoNLL-2005 dataset takes section 2-21 of Wall Street Journal (WSJ) data as training set, and section 24 as development set. The test set consists of section 23 of WSJ for in-domain evaluation together with 3 sections from Brown corpus for out-of-domain evaluation. The larger CoNLL-2012 dataset is extracted from OntoNotes v5.0 corpus, which contains both verbal and nominal predicates. CoNLL 2008 and 2009 CoNLL-2008 and the English part of CoNLL-2009 shared tasks use the same English corpus, which merges two treebanks, PropBank and NomBank. NomBank is a complement to PropBank with similar semantic convention for nominal predicate-argument structure annotation. Besides, the training, development and test splits of English data are identical to that of CoNLL-2005.",Setup,"In our experiments, the word embeddings are 300-dimensional GloVe vectors BIBREF33 . The character representations with dimension 8 randomly initialized. In the character CNN, the convolutions have window sizes of 3, 4, and 5, each consisting of 50 filters. Moreover, we use 3 stacked bidirectional LSTMs with 200 dimensional hidden states. The outputs of BiLSTM employs two 300-dimensional MLP layers with the ReLU as activation function. Besides, we use two 150-dimensional hidden MLP layers with ReLU to score predicates and arguments respectively. For candidates pruning, we follow the settings of BIBREF9 he2018jointly, modeling spans up to length INLINEFORM0 for span SRL and INLINEFORM1 for dependency SRL, using INLINEFORM2 for pruning predicates and INLINEFORM3 for pruning arguments. Training Details During training, we use the categorical cross-entropy as objective, with Adam optimizer BIBREF34 initial learning rate 0.001. We apply 0.5 dropout to the word embeddings and character CNN outputs and 0.2 dropout to all hidden layers and feature embeddings. In the LSTMs, we employ variational dropout masks that are shared across timesteps BIBREF35 , with 0.4 dropout rate. All models are trained for up to 600 epochs with batch size 40 on a single NVIDIA GeForce GTX 1080Ti GPU, which occupies 8 GB graphic memory and takes 12 to 36 hours.",End-to-end Results,"We present all results using the official evaluation script from the CoNLL-2005 and CoNLL-2009 shared tasks, and compare our model with previous state-of-the-art models. Span SRL Table TABREF15 shows results on CoNLL-2005 in-domain (WSJ) and out-of-domain (Brown) test sets, as well as the CoNLL-2012 test set (OntoNotes). The upper part of table presents results from single models. Our model outperforms the previous models with absolute improvements in F INLINEFORM0 -score of 0.3% on CoNLL-2005 benchmark. Besides, our single model performs even much better than all previous ensemble systems. Dependency SRL Table TABREF19 presents the results on CoNLL-2008. J & N (2008b) BIBREF36 was the highest ranked system in CoNLL-2008 shared task. We obtain comparable results with the recent state-of-the-art method BIBREF11 , and our model surpasses the model BIBREF10 by 2% in F INLINEFORM0 -score.",Results with Pre-identified Predicates,"To compare with to previous systems with pre-identified predicates, we report results from our models as well. Span SRL Table TABREF22 shows that our model outperforms all published systems, even the ensemble model BIBREF8 , achieving the best results of 87.7%, 80.5% and 86.0% in F INLINEFORM0 -score respectively. Dependency SRL Table TABREF29 compares the results of dependency SRL on CoNLL-2009 English data. Our single model gives a new state-of-the-art result of 90.4% F INLINEFORM0 on WSJ. For Brown data, the proposed syntax-agnostic model yields a performance gain of 1.7% F INLINEFORM1 over the syntax-aware model BIBREF18 .",Ablation,"To investigate the contributions of ELMo representations and biaffine scorer in our end-to-end model, we conduct a series of ablation studies on the CoNLL-2005 and CoNLL-2008 WSJ test sets, unless otherwise stated. Table TABREF31 compares F INLINEFORM0 scores of BIBREF9 he2018jointly and our model without ELMo representations. We observe that effect of ELMo is somewhat surprising, where removal of the ELMo dramatically declines the performance by 3.3-3.5 F INLINEFORM1 on CoNLL-2005 WSJ. However, our model gives quite stable performance for dependency SRL regardless of whether ELMo is concatenated or not. The results indicate that ELMo is more beneficial to span SRL. In order to better understand how the biaffine scorer influences our model performance, we train our model with different scoring functions. To ensure a fair comparison with the model BIBREF9 , we replace the biaffine scorer with their scoring functions implemented with feed-forward networks, and the results of removing biaffine scorer are also presented in Table TABREF31 . We can see 0.5% and 1.6% F INLINEFORM0 performance degradation on CoNLL 2005 and 2008 WSJ respectively. The comparison shows that the biaffine scorer is more effective for scoring the relations between predicates and arguments. Furthermore, these results show that biaffine attention mechanism is applicable to span SRL.",Dependency or Span?,"It is very hard to say which style of semantic formal representation, dependency or span, would be more convenient for machine learning as they adopt incomparable evaluation metric. Recent researches BIBREF37 have proposed to learn semantic parsers from multiple datasets in Framenet style semantics, while our goal is to compare the quality of different models in the span and dependency SRL for Propbank style semantics. Following BIBREF19 johansson2008EMNLP, we choose to directly compare their performance in terms of dependency-style metric through a transformation way. Using the head-finding algorithm in BIBREF19 which used gold-standard syntax, we may determine a set of head nodes for each span. This process will output an upper bound performance measure about the span conversion due to the use of gold syntax. We do not train new models for the conversion and the resulted comparison. Instead, we do the job on span-style CoNLL 2005 test set and dependency-style CoNLL 2009 test set (WSJ and Brown), considering these two test sets share the same text content. As the former only contains verbal predicate-argument structures, for the latter, we discard all nomial predicate-argument related results and predicate disambiguation results during performance statistics. Table TABREF33 shows the comparison. On a more strict setting basis, the results from our same model for span and dependency SRL verify the same conclusion of BIBREF19 johansson2008EMNLP, namely, dependency form is in a favor of machine learning effectiveness for SRL even compared to the conversion upper bound of span form.",Conclusion,"This paper presents an end-to-end neural model for both span and dependency SRL, which may jointly learn and predict all predicates and arguments. We extend existing model and introduce unified argument representation with biaffine scorer to the uniform SRL for both span and dependency representation forms. Our model achieves new state-of-the-art results on the CoNLL 2005, 2012 and CoNLL 2008, 2009 benchmarks. Our results show that span and dependency SRL can be effectively handled in a uniform fashion, which for the first time enables us to conveniently explore the useful connection between two types of semantic representation forms.",,,,,what were the baselines?,73bbe0b6457423f08d9297a0951381098bd89a2b,,,,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,"2008 Punyakanok et al. 
2009 Zhao et al. + ME 
2008 Toutanova et al. 
2010 Bjorkelund et al.  
2015 FitzGerald et al. 
2015 Zhou and Xu 
2016 Roth and Lapata 
2017 He et al. 
2017 Marcheggiani et al.
2017 Marcheggiani and Titov 
2018 Tan et al. 
2018 He et al. 
2018 Strubell et al. 
2018 Cai et al. 
2018 He et al. 
2018 Li et al. 
","Generally, the above work is summarized in Table TABREF2 . Considering motivation, our work is most closely related to the work of BIBREF14 Fitzgerald2015, which also tackles span and dependency SRL in a uniform fashion. The essential difference is that their model employs the syntactic features and takes pre-identified predicates as inputs, while our model puts syntax aside and jointly learns and predicts predicates and arguments. FLOAT SELECTED: Table 1: A chronicle of related work for span and dependency SRL. SA represents syntax-aware system (no + indicates syntaxagnostic system) and ST indicates sequence tagging model. F1 is the result of single model on official test set.","Generally, the above work is summarized in Table TABREF2 . Considering motivation, our work is most closely related to the work of BIBREF14 Fitzgerald2015, which also tackles span and dependency SRL in a uniform fashion. The essential difference is that their model employs the syntactic features and takes pre-identified predicates as inputs, while our model puts syntax aside and jointly learns and predicts predicates and arguments. FLOAT SELECTED: Table 1: A chronicle of related work for span and dependency SRL. SA represents syntax-aware system (no + indicates syntaxagnostic system) and ST indicates sequence tagging model. F1 is the result of single model on official test set.",5c9801d2dbcb7c4a1b5bb88fbe952c02bb1fd9d6,a0b403873302db7cada39008f04d01155ef68f4f,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1-Figure1-1.png,Figure 1: Examples of annotations in span (above) and dependency (below) SRL.,2-Table1-1.png,Table 1: A chronicle of related work for span and dependency SRL. SA represents syntax-aware system (no + indicates syntaxagnostic system) and ST indicates sequence tagging model. F1 is the result of single model on official test set.,4-Figure2-1.png,Figure 2: The framework of our end-to-end model for uniform SRL.,5-Table2-1.png,"Table 2: End-to-end span SRL results on CoNLL-2005 and CoNLL-2012 data, compared with previous systems in terms of precision (P), recall (R), F1-score. The CoNLL-2005 contains two test sets: WSJ (in-domain) and Brown (out-of-domain).",5-Table3-1.png,Table 3: Dependency SRL results on CoNLL-2008 test sets.,6-Table4-1.png,Table 4: Span SRL results with pre-identified predicates on CoNLL-2005 and CoNLL-2012 test sets.,,,,,,,,,,,7-Table5-1.png,Table 5: Dependency SRL results with pre-identified predicates on CoNLL-2009 English benchmark.,7-Table6-1.png,Table 6: Effectiveness of ELMo representations and biaffine scorer on the CoNLL 2005 and 2008 WSJ sets.,7-Table7-1.png,"Table 7: Dependency vs. Span-converted Dependency on CoNLL 2005, 2009 test sets with dependency evaluation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Language-Agnostic Syllabification with Neural Sequence Labeling,"The identification of syllables within phonetic sequences is known as syllabification. This task is thought to play an important role in natural language understanding, speech production, and the development of speech recognition systems. The concept of the syllable is cross-linguistic, though formal definitions are rarely agreed upon, even within a language. In response, data-driven syllabification methods have been developed to learn from syllabified examples. These methods often employ classical machine learning sequence labeling models. In recent years, recurrence-based neural networks have been shown to perform increasingly well for sequence labeling tasks such as named entity recognition (NER), part of speech (POS) tagging, and chunking. We present a novel approach to the syllabification problem which leverages modern neural network techniques. Our network is constructed with long short-term memory (LSTM) cells, a convolutional component, and a conditional random field (CRF) output layer. Existing syllabification approaches are rarely evaluated across multiple language families. To demonstrate cross-linguistic generalizability, we show that the network is competitive with state of the art systems in syllabifying English, Dutch, Italian, French, Manipuri, and Basque datasets.",Introduction,"Words can be considered compositions of syllables, which in turn are compositions of phones. Phones are units of sound producible by the human vocal apparatus. Syllables play an important role in prosody and are influential components of natural language understanding, speech production, and speech recognition systems. Text-to-speech (TTS) systems can rely heavily on automatically syllabified phone sequences BIBREF0. One prominent example is Festival, an open source TTS system that relies on a syllabification algorithm to organize speech production BIBREF1. Linguists have recognized since the late 1940s that the syllable is a hierarchical structure, present in most, if not all, languages (though there is some disagreement on this score. See, for example, BIBREF2). An optional consonant onset is followed by a rime, which may be further decomposed into a high sonority vowel nucleus followed by an optional consonant coda. All languages appear to have at least the single syllable vowel ($V$) and the two syllable vowel-consonant ($VC$) forms in their syllable inventories. For example, oh and so in English. Most languages supplement these with codas to form the $\lbrace V, CV, VC, CVC\rbrace $ syllable inventory. Sonority rises from the consonant onset to the vowel nucleus and falls toward the consonant coda, as in the English pig. The components of the syllable obey the phonotactic constraints of the language in which they occur, and therein lies the question that motivates this research. Phonologists agree that the human vocal apparatus produces speech sounds that form a sonority hierarchy, from highest to lowest: vowels, glides, liquids, nasals, and obstruents. Examples are, come, twist, lack, ring, and cat, respectively. English, and other languages with complex syllable inventories, supplement the basic forms in ways that are usually consistent with the sonority hierarchy, where usually is the operative word. Thus, English permits double consonant onsets, as in twist with a consonant lower in the hierarchy (t, an obstruent) followed by a consonant one higher in the hierarchy (w, a glide). So sonority rises to the vowel, i, falls to the fricative, s, an obstruent, and falls further to another obstruent, t, still lower in the hierarchy. Yet p and w do not form a double consonant onset in English, probably because English avoids grouping sounds that use the same articulators, the lips, in this instance. Constructing an automatic syllabifier could be the process of encoding all rules such as these in the language under investigation. Another approach, one more congenial to the rising tide of so-called usage-based linguists (e.g, BIBREF3), is to recognize that the regularities of language formulated as rules can be usefully expressed as probabilities BIBREF4, BIBREF5, BIBREF6. An automatic syllabifier is a computer program that, given a word as a sequence of phones, divides the word into its component syllables, where the syllables are legal in the language under investigation. Approaches take the form of dictionary-based look-up procedures, rule-based systems, data-driven systems, and hybrids thereof BIBREF7. Dictionary look-ups are limited to phone sequences previously seen and thus cannot handle new vocabulary BIBREF8. Rule-based approaches can process previously unseen phone sequences by encoding linguistic knowledge. Formalized language-specific rules are developed by hand, necessarily accompanied by many exceptions, such as the one noted in the previous paragraph. An important example is the syllabification package tsylb, developed at the National Institute of Standards and Technology (NIST), which is based on Daniel Kahn's 1979 MIT dissertation BIBREF9, BIBREF10. Language particularity is a stumbling block for rule-based and other formal approaches to language such as Optimality Theory (OT), however much they strive for universality. Thus, T.A. Hall argues that the OT approach to syllabification found in BIBREF11 is superior to previous OT research as well as to Kahn's rule-based work, because both postulate language-specific structures without cross-linguistic motivation. From Hall's perspective, previous systems do not capture important cross-linguistic features of the syllable. In a word, the earlier systems require kludges, an issue for both builders of automatic, language-agnostic syllabifiers and theoretical linguists like Hall. Data-driven syllabification methods, like the one to be presented in this paper, have the potential to function across languages and to process new, out of dictionary words. For languages that have transcribed syllable data, data-driven approaches often outperform rule-based ones. BIBREF12 used a combined support vector machine (SVM) and hidden Markov model (HMM) to maximize the classification margin between a correct and incorrect syllable boundary. BIBREF13 used segmental conditional random fields (SCRF). The SCRF hybrid method statistically leveraged general principles of syllabification such as legality, sonority and maximal onset. Many other HMM-based labeling structures exist, such as evolved phonetic categorization and high order n-gram models with back-off BIBREF14, BIBREF15. Data-driven models are evaluated by word accuracy against transcribed datasets. Commonly, only one language or languages of the same family are used. The CELEX lexical database from BIBREF16 contains syllabifications of phone sequences for English, Dutch, and German. These three languages fall into the West Germanic language family, so the phonologies of each are closely related. Evaluating a model solely on these three languages, the approach taken in BIBREF13 and others, does not adequately test a model's generalized ability to learn diverse syllable structures. In this paper, we present a neural network that can syllabify phone sequences without introducing any fixed principles or rules of syllabification. We show that this novel approach to syllabification is language-agnostic by evaluating it on datasets of six languages, five from two major language families, and one that appears to be unrelated to any existing language.",Method,"Syllabification can be considered a sequence labeling task where each label delineates the existence or absence of a syllable boundary. As such, syllabification has much in common with well-researched topics such as part-of-speech tagging, named-entity recognition, and chunking BIBREF17. Neural networks have recently outpaced more traditional methods in sequence labeling tasks. These neural-based approaches are taking the place of HMMs, maximum entropy Markov models (MEMM), and conditional random fields (CRF) BIBREF18. In the following section and in Fig. FIGREF1, we present a neural network architecture that leverages both recurrence and one-dimensional convolutions. Recurrence enables our model to read a sequence much like a human would; a sequence with elements $abcd$ would be read one element at a time, updating a latent understanding after reading each $a$, $b$, $c$, and finally $d$. One-dimensional convolutions extract a spatial relationship between sequential elements. The $abcd$ example sequence may then be read as $ab$, $bc$, $cd$. Explicitly recognizing this spatial relationship is beneficial in syllabification because a syllable is a local sub-sequence of phones within a word. The input to the model is a sequence of phones that together represent a word. We pad each phone sequence to a length of $n$ where $n$ is the length of the longest phone sequence. All inputs then take the form Each phone $p_i$ is mapped to a $d$-dimensional embedding vector $x_i$ resulting in where $x$ has a dimension of $d\times n$. Taken together, the phone embeddings represent the relationships between phones in a real-valued vector space. The embedding dimension $d$ is optimized as a model hyperparameter and has a large impact on overall model performance BIBREF19. As such, we carefully tune $d$ for the proposed Base model and reduce it for our Small model as described in Section SECREF24. The vector values of the phone embeddings are learned during each model training. Using learned embeddings enables the model to have a custom embedding space for each language that it is trained on. This is desirable because phonetic patterns differ from language to language. Also, learned embeddings allow the model to be trained using the input of any phonetic transcription. For example, one training of the model can use IPA and one can use SAMPA without needing to specify a mapping of one alphabet to another.",Method ::: Bidirectional LSTM,"Recurrent neural networks (RNNs) differ from standard feed-forward neural networks in their treatment of input order; each element is processed given the context of the input that came before. RNNs operate on sequential data and can take many forms. Our network leverages the long short-term memory (LSTM) cell which is a prominent RNN variant capable of capturing long-term sequential dependencies BIBREF20. The gated memory cells of LSTM are an improvement over the standard RNN because the standard RNN is often biased toward short-term dependencies BIBREF21, BIBREF22. At each time step, the LSTM cell determines what information is important to introduce, to keep, and to output. This is done using an input gate, a forget gate, and an output gate shown in Fig. FIGREF5. LSTM operates in a single direction through time. This can be a limitation when a time step has both past dependency and future dependency. For example, a consonant sound may be the coda of a syllable earlier in the sequence or the onset of a syllable later in the sequence. Thus, processing a phonetic sequence in both the forward and backwards directions provides an improved context for assigning syllable boundaries. A bidirectional LSTM (BiLSTM) is formed when an LSTM moving forward through time is concatenated with an LSTM moving backward through time BIBREF23. We use the LSTM network as follows. The $x$ vector is fed through the LSTM network which outputs a vector $\overrightarrow{h_i}$ for each time step $i$ from 0 to $n-1$. This is the forward LSTM. As we have access to the complete vector $x$, we can process a backward LSTM as well. This is done by computing a vector $\overleftarrow{h_i}$ for each time step $i$ from $n-1$ to 0. Finally, we concatenate the backward LSTM with the forward LSTM: Both $\overrightarrow{h_i}$ and $\overleftarrow{h_i}$ have a dimension of $l$, which is an optimized hyperparameter. The BiLSTM output $h$ thus has dimension $2l\times n$.",Method ::: CNN,"Convolutional neural networks (CNNs) are traditionally used in computer vision, but perform well in many text processing tasks that benefit from position-invariant abstractions BIBREF24, BIBREF25. These abstractions depend exclusively on local neighboring features rather than the position of features in a global structure. According to a comparative study by BIBREF26, BiLSTMs tend to outperform CNNs in sequential tasks such as POS tagging, but CNNs tend to outperform BiLSTMs in global relation detection tasks such as keyphrase matching for question answering. We use both the BiLSTM and the CNN in our network so that the strengths of each are incorporated. CNNs have been combined with BiLSTMs to perform state-of-the-art sequence tagging in both POS tagging and NER. BIBREF27 used BiLSTMs to process the word sequence while each word's character sequence was processed with CNNs to provide a second representation. In textual syllabification, the only input is the phone sequence. Both our BiLSTM and CNN components process the same input: the $x$ vector. We pad $x$ with $w-1$ $d$-dimensional zero vectors before $x_0$. A 1-dimensional convolutional filter of width $w$ processes a window $x_{i-w+1},...,x_i$ for all $i$ from 0 to $n-1$. To determine the output vector $c$, the convolutional filter performs a nonlinear weight and bias computation. Due to the padding of $x$, the resulting dimension of $c$ is $f\times n$ where $f$ is the number of filters used. A 1-dimensional max pooling is performed over $c$ with a stride of 1 which keeps the dimensionality unaltered. The pool size is an optimized hyperparameter that determines how many adjacent elements are used in the $max$ operation. The convolutional and max pooling components can be repeated to compute higher-level abstractions. As the convolutional and max pooling output is conformant to the BiLSTM output, we can concatenate them to create a combined vector with dimension $(2l+f)\times n$:",Method ::: Output: Conditional Random Field,"We introduce a time-distributed fully connected layer over vector $o$, taking $o$ from a dimension of $(2l+f)\times n$ down to a dimension of $2\times n$. We do this because there are two class labels: either a syllable boundary or no syllable boundary. The output of the model is a sequence When $y_i\equiv 0$, there is no syllable boundary predicted to follow the phone $p_i$. When $y_i\equiv 1$, there is a syllable boundary predicted to follow $p_i$. Intuitively, we seek an output sequence $y$ that gives the highest $p(y|o)$. One approach calculates the softmax for each $o_i$: The softmax normalizes each $o_i$ to a probability distribution over the two discrete class labels. We can then model $p(y|o)$ by multiplying the maximum of each $s_i$ together: When using the softmax, $p(y|o)$ is calculated under the limiting assumption that each $o_i$ is independent. To more accurately model $p(y|o)$, we replace the softmax classifier with a conditional random field (CRF) BIBREF28. Specifically, we use a linear-chain CRF which is a sequential model that leverages both past and future output tags to model the output probability. The linear-chain CRF can be considered a sequential generalization of logistic regression classifiers as well as a discriminative analogue of hidden Markov models because it models $p(y|o)$ directly instead of modeling $p(o|y)$ BIBREF29. Using sequence-level tag information with a CRF has been shown to improve tag accuracy in the related tasks of POS tagging, chunking, and NER BIBREF30, BIBREF31. We use a linear-chain CRF to model the conditional distribution directly: where $Z(o)$ is the normalization function and $\theta $ is a learned parameter vector scaled by the set of transition feature functions $f$.",Method ::: Training,"Training of the network parameters is performed using backpropagation. Using Keras, the backpropagation is automatically defined given the forward definition of the network. The defined loss function is sparse categorical cross entropy, in accordance with the real-valued probabilities given by the CRF output layer. Loss optimization is performed with the Adam optimizer BIBREF32. Adam was chosen because it adapts the learning rate on a parameter-to-parameter basis; strong convergence occurs at the end of optimization. Training is performed to a set number of epochs. Early stopping allows the network to conclude training if convergence is reached prior to reaching the epoch training limit BIBREF33.",Materials,The materials for this research comprises the software described above and several syllabified datasets.,Materials ::: Software,The implementation of our model was adapted from an open source code library designed for general-purpose sequence tagging and made available by BIBREF37. The modifications to this code include adding data preparation scripts and changing the model architecture to reflect the network architecture described above. Our code is made publicly available for future research at https://github.com/jacobkrantz/lstm-syllabify.,Materials ::: Datasets,"To produce a language-agnostic syllabifier, it is crucial to test syllabification accuracy across different language families and language groupings within families. We selected six evaluation languages: English, Dutch, Italian, French, Basque, and Manipuri. These represent two language families (Indo-European, Sino-Tibetan), a language isolate thought to be unrelated to any existing language (Basque), and two different subfamilies within the Indo-European family (West Germanic, Romance). The primary constraint was the availability of syllabified datasets for training and testing. Table TABREF17 presents details of each dataset. Among the six languages we evaluate with, both English and Dutch are notable for the availability of rich datasets of phonetic and syllabic transcriptions. These are found in the CELEX (Dutch Centre for Lexical Information) database BIBREF16. CELEX was built jointly by the University of Nijmegen, the Institute for Dutch Lexicology in Leiden, the Max Planck Institute for Psycholinguistics in Nijmegen, and the Institute for Perception Research in Eindhoven. CELEX is maintained by the Max Planck Institute for Psycholinguistics. The CELEX database contains information on orthography, phonology, morphology, syntax and word frequency. It also contains syllabified words in Dutch and English transcribed using SAM-PA, CELEX, CPA, and DISC notations. The first three are variations of the International Phonetic Alphabet (IPA), in that each uses a standard ASCII character to represent each IPA character. DISC is different than the other three in that it maps a distinct ASCII character to each phone in the sound systems of Dutch, English, and German BIBREF38. Different phonetic transcriptions are used in different datasets. Part of the strength of our proposed syllabifier is that every transcription can be used as-is without any additional modification to the syllabifier or the input sequences. The other datasets were hand-syllabified by linguists with the exception of the IIT-Guwahat dataset and the Festival dataset. Both IIT-Guwahat and Festival were initially syllabified with a naive algorithm and then each entry was confirmed or corrected by hand. For each dataset used to evaluate the proposed model, we compare our results with published accuracies of existing syllabification systems. Table TABREF21 shows the performance of well known and state of the art syllabifiers for each dataset. Liang's hyphenation algorithm is commonly known for its usage in . The patgen program was used to learn the rules of syllable boundaries BIBREF39. What we call Entropy CRF is a method particular to Manipuri; a rule-based component estimates the entropy of phones and phone clusters while a data-driven CRF component treats syllabification as a sequence modeling task BIBREF35.",Experiments,"Each dataset used to evaluate the model was split into three groups: training, development, and test. Each training epoch iterated over the training set to optimize the model parameters. The development set was used to tune the hyperparameters of the model, such as the batch size and the phone embedding dimension. The test set was exclusively used for reporting the model accuracy. The datasets were split randomly by percentages 80 (training), 10 (development), and 10 (test). For the English CELEX dataset of $89,402$ words, this resulted in $71,522$ words for training and $8,940$ words for each development and training. For each experiment, models were initialized with a random set of parameter weights. BIBREF37 showed that differences in random number generation produce statistically significant variances in the accuracy of LSTM-based models. Due to the stochastic nature of neural network training, we performed each experiment 20 times. We report model accuracy as a mean and standard deviation of these experiment repetitions.",Experiments ::: Data Cleaning,"Prior to splitting each dataset, a simple cleaning process had to be performed to remove unwanted entries. This cleaning involved removing all entries that had at least one other entry with the same word. It is important to note that two words being different does not necessitate a different pronunciation or syllabification. These entries with different words but same pronunciations were kept in the dataset. No other cleaning was needed for the datasets other than mapping the syllabified phone sequence to an input-target pair usable by our model for training and evaluation. This cleaning process contributes to the language-agnostic nature of this research. The simplicity of the cleaning process is enabled by the fact that the model is end to end; no external phonetic features are gathered, and any phonetic transcription can be accommodated in the training process.",Experiments ::: Hyperparameter Specification,"For all experiments, models were trained with a batch size of 64. A limit of 120 epochs was imposed with early stopping after 10 unimproved epochs. Dropout was used for the input connection to the BiLSTM layer at $25\%$ BIBREF41. The learned embeddings layer had dimension $d=300$. The LSTM outputs, $\overrightarrow{h_i}$ and $\overleftarrow{h_i}$, both had dimension $l=300$. The convolutional to max pooling component was repeated twice before concatenation with the BiLSTM output. 200 convolutional filters were used and each had a dimension of 3. Finally, when using the Adam optimizer, we scaled the gradient norm when it exceeded $1.0$ using the Keras clipnorm parameter. All training was performed on single GPU machines on Amazon Web Services (AWS) servers which provided more than enough compute power. The average training of a model on the English CELEX dataset took approximately 45 minutes to reach convergence.",Experiments ::: Results,"We tested three model versions against all datasets. The model we call Base is the BiLSTM-CNN-CRF model described in Section SECREF2 with the associated hyperparameters. Another model, Small, uses the same architecture as Base but reduces the number of convolutional layers to 1, the convolutional filters to 40, the LSTM dimension $l$ to 50, and the phone embedding size $d$ to 100. We also tested a Base-Softmax model, which replaces the CRF output of the Base model with a softmax. A comparison of the results of these three models can be seen in Table TABREF25. This comparison empirically motivates the CRF output because Base almost always outperforms Base-Softmax. Of these three models, the Base model performed the best with the exception of the French and Manipuri datasets. The differences in the French results can be considered negligible because the accuracies are all near $100\%$. The Small model performed best on Manipuri, which may suggest that reducing the number of parameters of the Base model leads to better accuracy on smaller datasets. When comparing our model with previous syllabifiers, we consider the Base model exclusively. In Table TABREF26, a side-by-side comparison of our Base model to a selection of published syllabifiers shows that Base is near state-of-the art performance on English CELEX. For the Dutch dataset, we report an accuracy of $99.47 \pm 0.04\%$, which improves on the previously best-known accuracy of $99.16\%$ from the HMM-SVM of BIBREF12. Best-known results are also obtained on the Italian, French, and Basque datasets. Our reported accuracy of $94.9 \pm 0.3\%$ on the Manipuri dataset is furthest from state of the art. We suspect this to be due to having limited amounts of training data; the $97.5\%$ accurate system from BIBREF35 supplemented their data-driven approach with rules of syllabification.",Discussion,"Examples from the outputs of the Base model can give us insight into what the model does well and what types of words it struggles with. The total number of sounds across languages is vast, but not infinite, as Ladefoged and Maddieson's The Sounds of the the World's Languages demonstrates BIBREF42. Different languages choose different inventories from the total producible by the human vocal apparatus. Within a language, sounds and patterns of sound vary widely in frequency, though with considerable regularity. This regularity has led a generation of linguists to attempt to uncover rules that describe not only syntax, but sound as well. Chomsky and Halle's The Sound Pattern of English is the classic effort, first appearing in 1968 BIBREF43. It is not surprising that the earliest attempts to produce automatic syllabifiers were based on just such rule collections. Nor is it surprising that the best-known rule-based syllabifier was inspired by a doctoral dissertation at MIT, Noam Chomsky's home institution for five decades. An alternative approach is to recognize that 1) rules can be reconceptualized as probabilities and 2) native speakers of a language have internalized those very probabilities. Nevertheless, where there is probability, there is ambiguity. With all of these caveats in mind, a few examples have been selected from our results to showcase the model as shown in Table TABREF27. The syllabification of misinterpretation illustrates the model's ability to process longer words. Containing 14 phones and 5 syllables, this word demonstrates that the model's pattern finding technique works well regardless of the location of phonetic and syllabic patterns in the word. The model can accurately handle prefixes, correctly syllabifying mis- as Table TABREF27 shows. Another word is achieved. Inflected languages, such as English, use morphemes to distinguish mood, tense, case, and number, among others. Thus, the verb achieve has several forms, or conjugates. The syllabifier correctly detected the stem and the past tense morpheme, ed. An odd aspect of the English CELEX dataset is the occurrence of entries, $22,393$ of which, that either have hyphens or are multiple entirely separate words, such as public-address systems. Because the phonetic representation does not denote hyphens or whitespace, the model has difficulties processing these words.",Conclusion,"We proposed a sequential neural network model that is capable of syllabifying phonetic sequences. This model is independent of any hand-crafted linguistic knowledge. We showed that this model performs at or near state of the art levels on a variety of datasets sampled from two Indo-European, one Sino-Tibetan, and an apparently family-less language. Specifically, the proposed model achieved accuracies higher than any other we could find on datasets from Dutch, Italian, French, and Basque languages and close to the best-reported accuracy for English and Manipuri. Evaluating the performance of the syllabifier across diverse languages provides strong evidence that the proposed model is language-agnostic.",Conclusion ::: Future Work,"With a language-agnostic syllabification system, any language can be syllabified given enough labeled training data. A problem is that many languages do not have large, labeled syllabification datasets. For example, we failed to find available and sufficient datasets in the Slavic languages of Russian and Serbian. This problem can be addressed either in a concentrated effort to create more labeled data or in the development of systems that require limited data.",Acknowledgment,This research was supported in part by a Gonzaga University McDonald Work Award by Robert and Claire McDonald and an Amazon Web Services (AWS) grant through the Cloud Credits for Research program.,,,,,,,,,What are the datasets used for the task?,ba56afe426906c4cfc414bca4c66ceb4a0a68121,two,familiar,no,italian,486a870694ba60f1a1e7e4ec13e328164cd4b43c,False,,"Datasets used are Celex (English, Dutch), Festival (Italian), OpenLexuque (French), IIT-Guwahati (Manipuri), E-Hitz (Basque)","FLOAT SELECTED: TABLE I DATASETS AND LANGUAGES USED FOR EVALUATION. AVERAGE PHONE AND SYLLABLE COUNTS ARE PER WORD. To produce a language-agnostic syllabifier, it is crucial to test syllabification accuracy across different language families and language groupings within families. We selected six evaluation languages: English, Dutch, Italian, French, Basque, and Manipuri. These represent two language families (Indo-European, Sino-Tibetan), a language isolate thought to be unrelated to any existing language (Basque), and two different subfamilies within the Indo-European family (West Germanic, Romance). The primary constraint was the availability of syllabified datasets for training and testing. Table TABREF17 presents details of each dataset. Among the six languages we evaluate with, both English and Dutch are notable for the availability of rich datasets of phonetic and syllabic transcriptions. These are found in the CELEX (Dutch Centre for Lexical Information) database BIBREF16. CELEX was built jointly by the University of Nijmegen, the Institute for Dutch Lexicology in Leiden, the Max Planck Institute for Psycholinguistics in Nijmegen, and the Institute for Perception Research in Eindhoven. CELEX is maintained by the Max Planck Institute for Psycholinguistics. The CELEX database contains information on orthography, phonology, morphology, syntax and word frequency. It also contains syllabified words in Dutch and English transcribed using SAM-PA, CELEX, CPA, and DISC notations. The first three are variations of the International Phonetic Alphabet (IPA), in that each uses a standard ASCII character to represent each IPA character. DISC is different than the other three in that it maps a distinct ASCII character to each phone in the sound systems of Dutch, English, and German BIBREF38. Different phonetic transcriptions are used in different datasets. Part of the strength of our proposed syllabifier is that every transcription can be used as-is without any additional modification to the syllabifier or the input sequences. The other datasets were hand-syllabified by linguists with the exception of the IIT-Guwahat dataset and the Festival dataset. Both IIT-Guwahat and Festival were initially syllabified with a naive algorithm and then each entry was confirmed or corrected by hand.","FLOAT SELECTED: TABLE I DATASETS AND LANGUAGES USED FOR EVALUATION. AVERAGE PHONE AND SYLLABLE COUNTS ARE PER WORD. We selected six evaluation languages: English, Dutch, Italian, French, Basque, and Manipuri. These represent two language families (Indo-European, Sino-Tibetan), a language isolate thought to be unrelated to any existing language (Basque), and two different subfamilies within the Indo-European family (West Germanic, Romance). The primary constraint was the availability of syllabified datasets for training and testing. Table TABREF17 presents details of each dataset.  These are found in the CELEX (Dutch Centre for Lexical Information) database BIBREF16. The other datasets were hand-syllabified by linguists with the exception of the IIT-Guwahat dataset and the Festival dataset.",db21ebb540520b9df2e5841a9b8f9947372f7cff,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,What is the accuracy of the model for the six languages tested?,14634943d96ea036725898ab2e652c2948bd33eb,two,familiar,no,italian,486a870694ba60f1a1e7e4ec13e328164cd4b43c,False,,"Authors report their best models have following accuracy: English CELEX (98.5%), Dutch CELEX (99.47%), Festival (99.990%), OpenLexique (100%), IIT-Guwahat (95.4%), E-Hitz (99.83%)","We tested three model versions against all datasets. The model we call Base is the BiLSTM-CNN-CRF model described in Section SECREF2 with the associated hyperparameters. Another model, Small, uses the same architecture as Base but reduces the number of convolutional layers to 1, the convolutional filters to 40, the LSTM dimension $l$ to 50, and the phone embedding size $d$ to 100. We also tested a Base-Softmax model, which replaces the CRF output of the Base model with a softmax. A comparison of the results of these three models can be seen in Table TABREF25. This comparison empirically motivates the CRF output because Base almost always outperforms Base-Softmax. Of these three models, the Base model performed the best with the exception of the French and Manipuri datasets. The differences in the French results can be considered negligible because the accuracies are all near $100\%$. The Small model performed best on Manipuri, which may suggest that reducing the number of parameters of the Base model leads to better accuracy on smaller datasets. FLOAT SELECTED: TABLE III THE ACCURACY OF OUR PROPOSED MODEL ON EACH EVALUATION DATASET. MODEL ACCURACY (%± σ) IS REPORTED ON A WORD LEVEL WHICH MEANS THE ENTIRE WORD MUST BE SYLLABIFIED CORRECTLY.",A comparison of the results of these three models can be seen in Table TABREF25. FLOAT SELECTED: TABLE III THE ACCURACY OF OUR PROPOSED MODEL ON EACH EVALUATION DATASET. MODEL ACCURACY (%± σ) IS REPORTED ON A WORD LEVEL WHICH MEANS THE ENTIRE WORD MUST BE SYLLABIFIED CORRECTLY.,578c09d9c48ae44789c3daed655f563b07680978,258ee4069f740c400c0049a2580945a1cc7f044c,Which models achieve state-of-the-art performances?,d71cb7f3aa585e256ca14eebdc358edfc3a9539c,two,familiar,no,italian,486a870694ba60f1a1e7e4ec13e328164cd4b43c,False,,"CELEX (Dutch and English) - SVM-HMM
Festival, E-Hitz and OpenLexique - Liang hyphenation
IIT-Guwahat - Entropy CRF","FLOAT SELECTED: TABLE II REPORTED ACCURACIES OF STATE OF THE ART AND SELECTED HIGH PERFORMING SYLLABIFIERS ON EACH EVALUATION DATASET. For each dataset used to evaluate the proposed model, we compare our results with published accuracies of existing syllabification systems. Table TABREF21 shows the performance of well known and state of the art syllabifiers for each dataset. Liang's hyphenation algorithm is commonly known for its usage in . The patgen program was used to learn the rules of syllable boundaries BIBREF39. What we call Entropy CRF is a method particular to Manipuri; a rule-based component estimates the entropy of phones and phone clusters while a data-driven CRF component treats syllabification as a sequence modeling task BIBREF35.","FLOAT SELECTED: TABLE II REPORTED ACCURACIES OF STATE OF THE ART AND SELECTED HIGH PERFORMING SYLLABIFIERS ON EACH EVALUATION DATASET. For each dataset used to evaluate the proposed model, we compare our results with published accuracies of existing syllabification systems. Table TABREF21 shows the performance of well known and state of the art syllabifiers for each dataset.",96aa576aa4fabc980fc5fae39e636d0ba1302db3,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,Is the LSTM bidirectional?,f6556d2a8b42b133eaa361f562745edbe56c0b51,two,familiar,no,italian,486a870694ba60f1a1e7e4ec13e328164cd4b43c,False,True,,"We use the LSTM network as follows. The $x$ vector is fed through the LSTM network which outputs a vector $\overrightarrow{h_i}$ for each time step $i$ from 0 to $n-1$. This is the forward LSTM. As we have access to the complete vector $x$, we can process a backward LSTM as well. This is done by computing a vector $\overleftarrow{h_i}$ for each time step $i$ from $n-1$ to 0. Finally, we concatenate the backward LSTM with the forward LSTM: Both $\overrightarrow{h_i}$ and $\overleftarrow{h_i}$ have a dimension of $l$, which is an optimized hyperparameter. The BiLSTM output $h$ thus has dimension $2l\times n$.","We use the LSTM network as follows. The $x$ vector is fed through the LSTM network which outputs a vector $\overrightarrow{h_i}$ for each time step $i$ from 0 to $n-1$. This is the forward LSTM. As we have access to the complete vector $x$, we can process a backward LSTM as well. This is done by computing a vector $\overleftarrow{h_i}$ for each time step $i$ from $n-1$ to 0. Finally, we concatenate the backward LSTM with the forward LSTM:

Both $\overrightarrow{h_i}$ and $\overleftarrow{h_i}$ have a dimension of $l$, which is an optimized hyperparameter. The BiLSTM output $h$ thus has dimension $2l\times n$.",1e47a36c5fe3891315ef86dca350f95a4f22122b,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2-Figure1-1.png,Fig. 1. Network diagram detailing the concatenation of the forward and backward LSTMs with the convolutional component.,3-Figure2-1.png,"Fig. 2. Diagram of the LSTM cell. ci and hi are the cell states and hidden states that propagate through time, respectively. xi is the input at time i and is concatenated with the previous hidden state. X represents element-wise multiplication and + is element-wise addition.",5-TableI-1.png,TABLE I DATASETS AND LANGUAGES USED FOR EVALUATION. AVERAGE PHONE AND SYLLABLE COUNTS ARE PER WORD.,5-TableII-1.png,TABLE II REPORTED ACCURACIES OF STATE OF THE ART AND SELECTED HIGH PERFORMING SYLLABIFIERS ON EACH EVALUATION DATASET.,6-TableIII-1.png,TABLE III THE ACCURACY OF OUR PROPOSED MODEL ON EACH EVALUATION DATASET. MODEL ACCURACY (%± σ) IS REPORTED ON A WORD LEVEL WHICH MEANS THE ENTIRE WORD MUST BE SYLLABIFIED CORRECTLY.,6-TableIV-1.png,"TABLE IV COMPARISON OF REPORTED ACCURACIES AGAINST THE ENGLISH CELEX DATASET. NOTE THAT HMM-SVM TRAINED ON 30K EXAMPLES, LEARNED EBG TRAINED ON 60K, AND HMM-GA TRAINED ON 54K.",,,,,,,,,,,7-TableV-1.png,TABLE V EXAMPLES OF GENERATED SYLLABIFICATIONS WHEN THE Base BILSTM-CNN-CRF MODEL IS TRAINED ON ENGLISH CELEX. Target IS THE SYLLABIFICATION GIVEN IN ENGLISH CELEX. PHONES ARE REPRESENTED IN THE DISC FORMAT AND CORRECT SYLLABIFICATIONS ARE IN BOLD.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Effective Modeling of Encoder-Decoder Architecture for Joint Entity and Relation Extraction,"A relation tuple consists of two entities and the relation between them, and often such tuples are found in unstructured text. There may be multiple relation tuples present in a text and they may share one or both entities among them. Extracting such relation tuples from a sentence is a difficult task and sharing of entities or overlapping entities among the tuples makes it more challenging. Most prior work adopted a pipeline approach where entities were identified first followed by finding the relations among them, thus missing the interaction among the relation tuples in a sentence. In this paper, we propose two approaches to use encoder-decoder architecture for jointly extracting entities and relations. In the first approach, we propose a representation scheme for relation tuples which enables the decoder to generate one word at a time like machine translation models and still finds all the tuples present in a sentence with full entity names of different length and with overlapping entities. Next, we propose a pointer network-based decoding approach where an entire tuple is generated at every time step. Experiments on the publicly available New York Times corpus show that our proposed approaches outperform previous work and achieve significantly higher F1 scores.",Introduction,"Distantly-supervised information extraction systems extract relation tuples with a set of pre-defined relations from text. Traditionally, researchers BIBREF0, BIBREF1, BIBREF2 use pipeline approaches where a named entity recognition (NER) system is used to identify the entities in a sentence and then a classifier is used to find the relation (or no relation) between them. However, due to the complete separation of entity detection and relation classification, these models miss the interaction between multiple relation tuples present in a sentence. Recently, several neural network-based models BIBREF3, BIBREF4 were proposed to jointly extract entities and relations from a sentence. These models used a parameter-sharing mechanism to extract the entities and relations in the same network. But they still find the relations after identifying all the entities and do not fully capture the interaction among multiple tuples. BIBREF5 (BIBREF5) proposed a joint extraction model based on neural sequence tagging scheme. But their model could not extract tuples with overlapping entities in a sentence as it could not assign more than one tag to a word. BIBREF6 (BIBREF6) proposed a neural encoder-decoder model for extracting relation tuples with overlapping entities. However, they used a copy mechanism to copy only the last token of the entities, thus this model could not extract the full entity names. Also, their best performing model used a separate decoder to extract each tuple which limited the power of their model. This model was trained with a fixed number of decoders and could not extract tuples beyond that number during inference. Encoder-decoder models are powerful models and they are successful in many NLP tasks such as machine translation, sentence generation from structured data, and open information extraction. In this paper, we explore how encoder-decoder models can be used effectively for extracting relation tuples from sentences. There are three major challenges in this task: (i) The model should be able to extract entities and relations together. (ii) It should be able to extract multiple tuples with overlapping entities. (iii) It should be able to extract exactly two entities of a tuple with their full names. To address these challenges, we propose two novel approaches using encoder-decoder architecture. We first propose a new representation scheme for relation tuples (Table TABREF1) such that it can represent multiple tuples with overlapping entities and different lengths of entities in a simple way. We employ an encoder-decoder model where the decoder extracts one word at a time like machine translation models. At the end of sequence generation, due to the unique representation of the tuples, we can extract the tuples from the sequence of words. Although this model performs quite well, generating one word at a time is somewhat unnatural for this task. Each tuple has exactly two entities and one relation, and each entity appears as a continuous text span in a sentence. The most effective way to identify them is to find their start and end location in the sentence. Each relation tuple can then be represented using five items: start and end location of the two entities and the relation between them (see Table TABREF1). Keeping this in mind, we propose a pointer network-based decoding framework. This decoder consists of two pointer networks which find the start and end location of the two entities in a sentence, and a classification network which identifies the relation between them. At every time step of the decoding, this decoder extracts an entire relation tuple, not just a word. Experiments on the New York Times (NYT) datasets show that our approaches work effectively for this task and achieve state-of-the-art performance. To summarize, the contributions of this paper are as follows: (1) We propose a new representation scheme for relation tuples such that an encoder-decoder model, which extracts one word at each time step, can still find multiple tuples with overlapping entities and tuples with multi-token entities from sentences. We also propose a masking-based copy mechanism to extract the entities from the source sentence only. (2) We propose a modification in the decoding framework with pointer networks to make the encoder-decoder model more suitable for this task. At every time step, this decoder extracts an entire relation tuple, not just a word. This new decoding framework helps in speeding up the training process and uses less resources (GPU memory). This will be an important factor when we move from sentence-level tuple extraction to document-level extraction. (3) Experiments on the NYT datasets show that our approaches outperform all the previous state-of-the-art models significantly and set a new benchmark on these datasets.",Task Description,"A relation tuple consists of two entities and a relation. Such tuples can be found in sentences where an entity is a text span in a sentence and a relation comes from a pre-defined set $R$. These tuples may share one or both entities among them. Based on this, we divide the sentences into three classes: (i) No Entity Overlap (NEO): A sentence in this class has one or more tuples, but they do not share any entities. (ii) Entity Pair Overlap (EPO): A sentence in this class has more than one tuple, and at least two tuples share both the entities in the same or reverse order. (iii) Single Entity Overlap (SEO): A sentence in this class has more than one tuple and at least two tuples share exactly one entity. It should be noted that a sentence can belong to both EPO and SEO classes. Our task is to extract all relation tuples present in a sentence.",Encoder-Decoder Architecture,"In this task, input to the system is a sequence of words, and output is a set of relation tuples. In our first approach, we represent each tuple as entity1 ; entity2 ; relation. We use `;' as a separator token to separate the tuple components. Multiple tuples are separated using the `$\vert $' token. We have included one example of such representation in Table TABREF1. Multiple relation tuples with overlapping entities and different lengths of entities can be represented in a simple way using these special tokens (; and $\vert $). During inference, after the end of sequence generation, relation tuples can be extracted easily using these special tokens. Due to this uniform representation scheme, where entity tokens, relation tokens, and special tokens are treated similarly, we use a shared vocabulary between the encoder and decoder which includes all of these tokens. The input sentence contains clue words for every relation which can help generate the relation tokens. We use two special tokens so that the model can distinguish between the beginning of a relation tuple and the beginning of a tuple component. To extract the relation tuples from a sentence using the encoder-decoder model, the model has to generate the entity tokens, find relation clue words and map them to the relation tokens, and generate the special tokens at appropriate time. Our experiments show that the encoder-decoder models can achieve this quite effectively.",Encoder-Decoder Architecture ::: Embedding Layer & Encoder,"We create a single vocabulary $V$ consisting of the source sentence tokens, relation names from relation set $R$, special separator tokens (`;', `$\vert $'), start-of-target-sequence token (SOS), end-of-target-sequence token (EOS), and unknown word token (UNK). Word-level embeddings are formed by two components: (1) pre-trained word vectors (2) character embedding-based feature vectors. We use a word embedding layer $\mathbf {E}_w \in \mathbb {R}^{\vert V \vert \times d_w}$ and a character embedding layer $\mathbf {E}_c \in \mathbb {R}^{\vert A \vert \times d_c}$, where $d_w$ is the dimension of word vectors, $A$ is the character alphabet of input sentence tokens, and $d_c$ is the dimension of character embedding vectors. Following BIBREF7 (BIBREF7), we use a convolutional neural network with max-pooling to extract a feature vector of size $d_f$ for every word. Word embeddings and character embedding-based feature vectors are concatenated ($\Vert $) to obtain the representation of the input tokens. A source sentence $\mathbf {S}$ is represented by vectors of its tokens $\mathbf {x}_1, \mathbf {x}_2,....,\mathbf {x}_n$, where $\mathbf {x}_i \in \mathbb {R}^{(d_w+d_f)}$ is the vector representation of the $i$th word and $n$ is the length of $\mathbf {S}$. These vectors $\mathbf {x}_i$ are passed to a bi-directional LSTM BIBREF8 (Bi-LSTM) to obtain the hidden representation $\mathbf {h}_i^E$. We set the hidden dimension of the forward and backward LSTM of the Bi-LSTM to be $d_h/2$ to obtain $\mathbf {h}_i^E \in \mathbb {R}^{d_h}$, where $d_h$ is the hidden dimension of the sequence generator LSTM of the decoder described below.",Encoder-Decoder Architecture ::: Word-level Decoder & Copy Mechanism,"A target sequence $\mathbf {T}$ is represented by only word embedding vectors of its tokens $\mathbf {y}_0, \mathbf {y}_1,....,\mathbf {y}_m$ where $\mathbf {y}_i \in \mathbb {R}^{d_w}$ is the embedding vector of the $i$th token and $m$ is the length of the target sequence. $\mathbf {y}_0$ and $\mathbf {y}_m$ represent the embedding vector of the SOS and EOS token respectively. The decoder generates one token at a time and stops when EOS is generated. We use an LSTM as the decoder and at time step $t$, the decoder takes the source sentence encoding ($\mathbf {e}_t \in \mathbb {R}^{d_h}$) and the previous target word embedding ($\mathbf {y}_{t-1}$) as the input and generates the hidden representation of the current token ($\mathbf {h}_t^D \in \mathbb {R}^{d_h}$). The sentence encoding vector $\mathbf {e}_t$ can be obtained using attention mechanism. $\mathbf {h}_t^D$ is projected to the vocabulary $V$ using a linear layer with weight matrix $\mathbf {W}_v \in \mathbb {R}^{\vert V \vert \times d_h}$ and bias vector $\mathbf {b}_v \in \mathbb {R}^{\vert V \vert }$ (projection layer). $\mathbf {o}_t$ represents the normalized scores of all the words in the embedding vocabulary at time step $t$. $\mathbf {h}_{t-1}^D$ is the previous hidden state of the LSTM. The projection layer of the decoder maps the decoder output to the entire vocabulary. During training, we use the gold label target tokens directly. However, during inference, the decoder may predict a token from the vocabulary which is not present in the current sentence or the set of relations or the special tokens. To prevent this, we use a masking technique while applying the softmax operation at the projection layer. We mask (exclude) all words of the vocabulary except the current source sentence tokens, relation tokens, separator tokens (`;', `$\vert $'), UNK, and EOS tokens in the softmax operation. To mask (exclude) some word from softmax, we set the corresponding value in $\hat{\mathbf {o}}_t$ at $-\infty $ and the corresponding softmax score will be zero. This ensures the copying of entities from the source sentence only. We include the UNK token in the softmax operation to make sure that the model generates new entities during inference. If the decoder predicts an UNK token, we replace it with the corresponding source word which has the highest attention score. During inference, after decoding is finished, we extract all tuples based on the special tokens, remove duplicate tuples and tuples in which both entities are the same or tuples where the relation token is not from the relation set. This model is referred to as WordDecoding (WDec) henceforth.",Encoder-Decoder Architecture ::: Pointer Network-Based Decoder,"In the second approach, we identify the entities in the sentence using their start and end locations. We remove the special tokens and relation names from the word vocabulary and word embeddings are used only at the encoder side along with character embeddings. We use an additional relation embedding matrix $\mathbf {E}_r \in \mathbb {R}^{\vert R \vert \times d_r}$ at the decoder side of our model, where $R$ is the set of relations and $d_r$ is the dimension of relation vectors. The relation set $R$ includes a special relation token EOS which indicates the end of the sequence. Relation tuples are represented as a sequence $T=y_0, y_1,....,y_m$, where $y_t$ is a tuple consisting of four indexes in the source sentence indicating the start and end location of the two entities and a relation between them (see Table TABREF1). $y_0$ is a dummy tuple that represents the start tuple of the sequence and $y_m$ functions as the end tuple of the sequence which has EOS as the relation (entities are ignored for this tuple). The decoder consists of an LSTM with hidden dimension $d_h$ to generate the sequence of tuples, two pointer networks to find the two entities, and a classification network to find the relation of a tuple. At time step $t$, the decoder takes the source sentence encoding ($\mathbf {e}_t \in \mathbb {R}^{d_h}$) and the representation of all previously generated tuples ($\mathbf {y}_{prev}=\sum _{j=0}^{t-1}\mathbf {y}_{j}$) as the input and generates the hidden representation of the current tuple, $\mathbf {h}_t^D \in \mathbb {R}^{d_h}$. The sentence encoding vector $\mathbf {e}_t$ is obtained using an attention mechanism as explained later. Relation tuples are a set and to prevent the decoder from generating the same tuple again, we pass the information about all previously generated tuples at each time step of decoding. $\mathbf {y}_j$ is the vector representation of the tuple predicted at time step $j < t$ and we use the zero vector ($\mathbf {y}_0=\overrightarrow{0}$) to represent the dummy tuple $y_0$. $\mathbf {h}_{t-1}^D$ is the hidden state of the LSTM at time step $t-1$.",Encoder-Decoder Architecture ::: Relation Tuple Extraction,"After obtaining the hidden representation of the current tuple $\mathbf {h}_t^D$, we first find the start and end pointers of the two entities in the source sentence. We concatenate the vector $\mathbf {h}_t^D$ with the hidden vectors $\mathbf {h}_i^E$ of the encoder and pass them to a Bi-LSTM layer with hidden dimension $d_p$ for forward and backward LSTM. The hidden vectors of this Bi-LSTM layer $\mathbf {h}_i^k \in \mathbb {R}^{2d_p}$ are passed to two feed-forward networks (FFN) with softmax to convert each hidden vector into two scalar values between 0 and 1. Softmax operation is applied across all the words in the input sentence. These two scalar values represent the probability of the corresponding source sentence token to be the start and end location of the first entity. This Bi-LSTM layer with the two feed-forward layers is the first pointer network which identifies the first entity of the current relation tuple. where $\mathbf {W}_s^1 \in \mathbb {R}^{1 \times 2d_p}$, $\mathbf {W}_e^1 \in \mathbb {R}^{1 \times 2d_p}$, ${b}_s^1$, and ${b}_e^1$ are the weights and bias parameters of the feed-forward layers. ${s}_i^1$, ${e}_i^1$ represent the normalized probabilities of the $i$th source word being the start and end token of the first entity of the predicted tuple. We use another pointer network to extract the second entity of the tuple. We concatenate the hidden vectors $\mathbf {h}_i^k$ with $\mathbf {h}_t^D$ and $\mathbf {h}_i^E$ and pass them to the second pointer network to obtain ${s}_i^2$ and ${e}_i^2$, which represent the normalized probabilities of the $i$th source word being the start and end of the second entity. These normalized probabilities are used to find the vector representation of the two entities, $\mathbf {a}_t^1$ and $\mathbf {a}_t^2$. We concatenate the entity vector representations $\mathbf {a}_t^1$ and $\mathbf {a}_t^2$ with $\mathbf {h}_t^D$ and pass it to a feed-forward network (FFN) with softmax to find the relation. This feed-forward layer has a weight matrix $\mathbf {W}_r \in \mathbb {R}^{\vert R \vert \times (8d_p + d_h)}$ and a bias vector $\mathbf {b}_r \in \mathbb {R}^{\vert R \vert }$. $\mathbf {r}_t$ represents the normalized probabilities of the relation at time step $t$. The relation embedding vector $\mathbf {z}_t$ is obtained using $\mathrm {argmax}$ of $\mathbf {r}_t$ and $\mathbf {E}_r$. $\mathbf {y}_t \in \mathbb {R}^{(8d_p + d_r)}$ is the vector representation of the tuple predicted at time step $t$. During training, we pass the embedding vector of the gold label relation in place of the predicted relation. So the $\mathrm {argmax}$ function does not affect the back-propagation during training. The decoder stops the sequence generation process when the predicted relation is EOS. This is the classification network of the decoder. During inference, we select the start and end location of the two entities such that the product of the four pointer probabilities is maximized keeping the constraints that the two entities do not overlap with each other and $1 \le b \le e \le n$ where $b$ and $e$ are the start and end location of the corresponding entities. We first choose the start and end location of entity 1 based on the maximum product of the corresponding start and end pointer probabilities. Then we find entity 2 in a similar way excluding the span of entity 1 to avoid overlap. The same procedure is repeated but this time we first find entity 2 followed by entity 1. We choose that pair of entities which gives the higher product of four pointer probabilities between these two choices. This model is referred to as PtrNetDecoding (PNDec) henceforth.",Encoder-Decoder Architecture ::: Attention Modeling,"We experimented with three different attention mechanisms for our word-level decoding model to obtain the source context vector $\mathbf {e}_t$: (1) Avg.: The context vector is obtained by averaging the hidden vectors of the encoder: $\mathbf {e}_t=\frac{1}{n}\sum _{i=1}^n \mathbf {h}_i^E$ (2) N-gram: The context vector is obtained by the N-gram attention mechanism of BIBREF9 (BIBREF9) with N=3. $\textnormal {a}_i^g=(\mathbf {h}_n^{E})^T \mathbf {V}^g \mathbf {w}_i^g$, $\alpha ^g = \mathrm {softmax}(\mathbf {a}^g)$ $\mathbf {e}_t=[\mathbf {h}_n^E \Vert \sum _{g=1}^N \mathbf {W}^g (\sum _{i=1}^{\vert G^g \vert } \alpha _i^g \mathbf {w}_i^g)$] Here, $\mathbf {h}_n^E$ is the last hidden state of the encoder, $g \in \lbrace 1, 2, 3\rbrace $ refers to the word gram combination, $G^g$ is the sequence of g-gram word representations for the input sentence, $\mathbf {w}_i^g$ is the $i$th g-gram vector (2-gram and 3-gram representations are obtained by average pooling), $\alpha _i^g$ is the normalized attention score for the $i$th g-gram vector, $\mathbf {W} \in \mathbb {R}^{d_h \times d_h}$ and $\mathbf {V} \in \mathbb {R}^{d_h \times d_h}$ are trainable parameters. (3) Single: The context vector is obtained by the attention mechanism proposed by BIBREF10 (BIBREF10). This attention mechanism gives the best performance with the word-level decoding model. $\mathbf {u}_t^i = \mathbf {W}_{u} \mathbf {h}_i^E, \quad \mathbf {q}_t^i = \mathbf {W}_{q} \mathbf {h}_{t-1}^D + \mathbf {b}_{q}$, $\textnormal {a}_t^i = \mathbf {v}_a \tanh (\mathbf {q}_t^i + \mathbf {u}_t^i), \quad \alpha _t = \mathrm {softmax}(\mathbf {a}_t)$, $\mathbf {e}_t = \sum _{i=1}^n \alpha _t^i \mathbf {h}_i^E$ where $\mathbf {W}_u \in \mathbb {R}^{d_h \times d_h}$, $\mathbf {W}_q \in \mathbb {R}^{d_h \times d_h}$, and $\mathbf {v}_a \in \mathbb {R}^{d_h}$ are all trainable attention parameters and $\mathbf {b}_q \in \mathbb {R}^{d_h}$ is a bias vector. $\alpha _t^i$ is the normalized attention score of the $i$th source word at the decoding time step $t$. For our pointer network-based decoding model, we use three variants of the single attention model. First, we use $\mathbf {h}_{t-1}^D$ to calculate $\mathbf {q}_t^i$ in the attention mechanism. Next, we use $\mathbf {y}_{prev}$ to calculate $\mathbf {q}_t^i$, where $\mathbf {W}_q \in \mathbb {R}^{(8d_p + d_r) \times d_h}$. In the final variant, we obtain the attentive context vector by concatenating the two attentive vectors obtained using $\mathbf {h}_{t-1}^D$ and $\mathbf {y}_{prev}$. This gives the best performance with the pointer network-based decoding model. These variants are referred to as $\mathrm {dec_{hid}}$, $\mathrm {tup_{prev}}$, and $\mathrm {combo}$ in Table TABREF17.",Encoder-Decoder Architecture ::: Loss Function,"We minimize the negative log-likelihood loss of the generated words for word-level decoding ($\mathcal {L}_{word}$) and minimize the sum of negative log-likelihood loss of relation classification and the four pointer locations for pointer network-based decoding ($\mathcal {L}_{ptr}$). $v_t^b$ is the softmax score of the target word at time step $t$ for the word-level decoding model. $r$, $s$, and $e$ are the softmax score of the corresponding true relation label, true start and end pointer location of an entity. $b$, $t$, and $c$ refer to the $b$th training instance, $t$th time step of decoding, and the two entities of a tuple respectively. $B$ and $T$ are the batch size and maximum time step of the decoder respectively.",Experiments ::: Datasets,"We focus on the task of extracting multiple tuples with overlapping entities from sentences. We choose the New York Times (NYT) corpus for our experiments. This corpus has multiple versions, and we choose the following two versions as their test dataset has significantly larger number of instances of multiple relation tuples with overlapping entities. (i) The first version is used by BIBREF6 (BIBREF6) (mentioned as NYT in their paper) and has 24 relations. We name this version as NYT24. (ii) The second version is used by BIBREF11 (BIBREF11) (mentioned as NYT10 in their paper) and has 29 relations. We name this version as NYT29. We select 10% of the original training data and use it as the validation dataset. The remaining 90% is used for training. We include statistics of the training and test datasets in Table TABREF11.",Experiments ::: Parameter Settings,"We run the Word2Vec BIBREF12 tool on the NYT corpus to initialize the word embeddings. The character embeddings and relation embeddings are initialized randomly. All embeddings are updated during training. We set the word embedding dimension $d_w=300$, relation embedding dimension $d_r=300$, character embedding dimension $d_c=50$, and character-based word feature dimension $d_f=50$. To extract the character-based word feature vector, we set the CNN filter width at 3 and the maximum length of a word at 10. The hidden dimension $d_h$ of the decoder LSTM cell is set at 300 and the hidden dimension of the forward and the backward LSTM of the encoder is set at 150. The hidden dimension of the forward and backward LSTM of the pointer networks is set at $d_p=300$. The model is trained with mini-batch size of 32 and the network parameters are optimized using Adam BIBREF13. Dropout layers with a dropout rate fixed at $0.3$ are used in our network to avoid overfitting.",Experiments ::: Baselines and Evaluation Metrics,"We compare our model with the following state-of-the-art joint entity and relation extraction models: (1) SPTree BIBREF4: This is an end-to-end neural entity and relation extraction model using sequence LSTM and Tree LSTM. Sequence LSTM is used to identify all the entities first and then Tree LSTM is used to find the relation between all pairs of entities. (2) Tagging BIBREF5: This is a neural sequence tagging model which jointly extracts the entities and relations using an LSTM encoder and an LSTM decoder. They used a Cartesian product of entity tags and relation tags to encode the entity and relation information together. This model does not work when tuples have overlapping entities. (3) CopyR BIBREF6: This model uses an encoder-decoder approach for joint extraction of entities and relations. It copies only the last token of an entity from the source sentence. Their best performing multi-decoder model is trained with a fixed number of decoders where each decoder extracts one tuple. (4) HRL BIBREF11: This model uses a reinforcement learning (RL) algorithm with two levels of hierarchy for tuple extraction. A high-level RL finds the relation and a low-level RL identifies the two entities using a sequence tagging approach. This sequence tagging approach cannot always ensure extraction of exactly two entities. (5) GraphR BIBREF14: This model considers each token in a sentence as a node in a graph, and edges connecting the nodes as relations between them. They use graph convolution network (GCN) to predict the relations of every edge and then filter out some of the relations. (6) N-gram Attention BIBREF9: This model uses an encoder-decoder approach with N-gram attention mechanism for knowledge-base completion using distantly supervised data. The encoder uses the source tokens as its vocabulary and the decoder uses the entire Wikidata BIBREF15 entity IDs and relation IDs as its vocabulary. The encoder takes the source sentence as input and the decoder outputs the two entity IDs and relation ID for every tuple. During training, it uses the mapping of entity names and their Wikidata IDs of the entire Wikidata for proper alignment. Our task of extracting relation tuples with the raw entity names from a sentence is more challenging since entity names are not of fixed length. Our more generic approach is also helpful for extracting new entities which are not present in the existing knowledge bases such as Wikidata. We use their N-gram attention mechanism in our model to compare its performance with other attention models (Table TABREF17). We use the same evaluation method used by BIBREF11 (BIBREF11) in their experiments. We consider the extracted tuples as a set and remove the duplicate tuples. An extracted tuple is considered as correct if the corresponding full entity names are correct and the relation is also correct. We report precision, recall, and F1 score for comparison.",Experiments ::: Experimental Results,"Among the baselines, HRL achieves significantly higher F1 scores on the two datasets. We run their model and our models five times and report the median results in Table TABREF15. Scores of other baselines in Table TABREF15 are taken from previous published papers BIBREF6, BIBREF11, BIBREF14. Our WordDecoding (WDec) model achieves F1 scores that are $3.9\%$ and $4.1\%$ higher than HRL on the NYT29 and NYT24 datasets respectively. Similarly, our PtrNetDecoding (PNDec) model achieves F1 scores that are $3.0\%$ and $1.3\%$ higher than HRL on the NYT29 and NYT24 datasets respectively. We perform a statistical significance test (t-test) under a bootstrap pairing between HRL and our models and see that the higher F1 scores achieved by our models are statistically significant ($p < 0.001$). Next, we combine the outputs of five runs of our models and five runs of HRL to build ensemble models. For a test instance, we include those tuples which are extracted in the majority ($\ge 3$) of the five runs. This ensemble mechanism increases the precision significantly on both datasets with a small improvement in recall as well. In the ensemble scenario, compared to HRL, WDec achieves $4.2\%$ and $3.5\%$ higher F1 scores and PNDec achieves $4.2\%$ and $2.9\%$ higher F1 scores on the NYT29 and NYT24 datasets respectively.",Analysis and Discussion ::: Ablation Studies,"We include the performance of different attention mechanisms with our WordDecoding model, effects of our masking-based copy mechanism, and ablation results of three variants of the single attention mechanism with our PtrNetDecoding model in Table TABREF17. WordDecoding with single attention achieves the highest F1 score on both datasets. We also see that our copy mechanism improves F1 scores by around 4–7% in each attention mechanism with both datasets. PtrNetDecoding achieves the highest F1 scores when we combine the two attention mechanisms with respect to the previous hidden vector of the decoder LSTM ($\mathbf {h}_{t-1}^D$) and representation of all previously extracted tuples ($\mathbf {y}_{prev}$).",Analysis and Discussion ::: Performance Analysis,"From Table TABREF15, we see that CopyR, HRL, and our models achieve significantly higher F1 scores on the NYT24 dataset than the NYT29 dataset. Both datasets have a similar set of relations and similar texts (NYT). So task-wise both datasets should pose a similar challenge. However, the F1 scores suggest that the NYT24 dataset is easier than NYT29. The reason is that NYT24 has around 72.0% of overlapping tuples between the training and test data (% of test tuples that appear in the training data with different source sentences). In contrast, NYT29 has only 41.7% of overlapping tuples. Due to the memorization power of deep neural networks, it can achieve much higher F1 score on NYT24. The difference between the F1 scores of WordDecoding and PtrNetDecoding on NYT24 is marginally higher than NYT29, since WordDecoding has more trainable parameters (about 27 million) than PtrNetDecoding (about 24.5 million) and NYT24 has very high tuple overlap. However, their ensemble versions achieve closer F1 scores on both datasets. Despite achieving marginally lower F1 scores, the pointer network-based model can be considered more intuitive and suitable for this task. WordDecoding may not extract the special tokens and relation tokens at the right time steps, which is critical for finding the tuples from the generated sequence of words. PtrNetDecoding always extracts two entities of varying length and a relation for every tuple. We also observe that PtrNetDecoding is more than two times faster and takes one-third of the GPU memory of WordDecoding during training and inference. This speedup and smaller memory consumption are achieved due to the fewer number of decoding steps of PtrNetDecoding compared to WordDecoding. PtrNetDecoding extracts an entire tuple at each time step, whereas WordDecoding extracts just one word at each time step and so requires eight time steps on average to extract a tuple (assuming that the average length of an entity is two). The softmax operation at the projection layer of WordDecoding is applied across the entire vocabulary and the vocabulary size can be large (more than 40,000 for our datasets). In case of PtrNetDecoding, the softmax operation is applied across the sentence length (maximum of 100 in our experiments) and across the relation set (24 and 29 for our datasets). The costly softmax operation and the higher number of decoding time steps significantly increase the training and inference time for WordDecoding. The encoder-decoder model proposed by BIBREF9 (BIBREF9) faces a similar softmax-related problem as their target vocabulary contains the entire Wikidata entity IDs and relation IDs which is in the millions. HRL, which uses a deep reinforcement learning algorithm, takes around 8x more time to train than PtrNetDecoding with a similar GPU configuration. The speedup and smaller memory consumption will be useful when we move from sentence-level extraction to document-level extraction, since document length is much higher than sentence length and a document contains a higher number of tuples.",Analysis and Discussion ::: Error Analysis,"The relation tuples extracted by a joint model can be erroneous for multiple reasons such as: (i) extracted entities are wrong; (ii) extracted relations are wrong; (iii) pairings of entities with relations are wrong. To see the effects of the first two reasons, we analyze the performance of HRL and our models on entity generation and relation generation separately. For entity generation, we only consider those entities which are part of some tuple. For relation generation, we only consider the relations of the tuples. We include the performance of our two models and HRL on entity generation and relation generation in Table TABREF20. Our proposed models perform better than HRL on both tasks. Comparing our two models, PtrNetDecoding performs better than WordDecoding on both tasks, although WordDecoding achieves higher F1 scores in tuple extraction. This suggests that PtrNetDecoding makes more errors while pairing the entities with relations. We further analyze the outputs of our models and HRL to determine the errors due to ordering of entities (Order), mismatch of the first entity (Ent1), and mismatch of the second entity (Ent2) in Table TABREF21. WordDecoding generates fewer errors than the other two models in all the categories and thus achieves the highest F1 scores on both datasets.",Related Work,"Traditionally, researchers BIBREF0, BIBREF1, BIBREF2, BIBREF16, BIBREF17, BIBREF18, BIBREF19, BIBREF20, BIBREF21, BIBREF22, BIBREF23, BIBREF24, BIBREF25 used a pipeline approach for relation tuple extraction where relations were identified using a classification network after all entities were detected. BIBREF26 (BIBREF26) used an encoder-decoder model to extract multiple relations present between two given entities. Recently, some researchers BIBREF3, BIBREF4, BIBREF27, BIBREF28 tried to bring these two tasks closer together by sharing their parameters and optimizing them together. BIBREF5 (BIBREF5) used a sequence tagging scheme to jointly extract the entities and relations. BIBREF6 (BIBREF6) proposed an encoder-decoder model with copy mechanism to extract relation tuples with overlapping entities. BIBREF11 (BIBREF11) proposed a joint extraction model based on reinforcement learning (RL). BIBREF14 (BIBREF14) used a graph convolution network (GCN) where they treated each token in a sentence as a node in a graph and edges were considered as relations. BIBREF9 (BIBREF9) used an N-gram attention mechanism with an encoder-decoder model for completion of knowledge bases using distant supervised data. Encoder-decoder models have been used for many NLP applications such as neural machine translation BIBREF29, BIBREF10, BIBREF30, sentence generation from structured data BIBREF31, BIBREF32, and open information extraction BIBREF33, BIBREF34. Pointer networks BIBREF35 have been used to extract a text span from text for tasks such as question answering BIBREF36, BIBREF37. For the first time, we use pointer networks with an encoder-decoder model to extract relation tuples from sentences.",Conclusion,"Extracting relation tuples from sentences is a challenging task due to different length of entities, the presence of multiple tuples, and overlapping of entities among tuples. In this paper, we propose two novel approaches using encoder-decoder architecture to address this task. Experiments on the New York Times (NYT) corpus show that our proposed models achieve significantly improved new state-of-the-art F1 scores. As future work, we would like to explore our proposed models for a document-level tuple extraction task.",Acknowledgments,We would like to thank the anonymous reviewers for their valuable and constructive comments on this paper.,,,,,"Are there datasets with relation tuples annotated, how big are datasets available?",735f58e28d84ee92024a36bc348cfac2ee114409,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,True,,"We focus on the task of extracting multiple tuples with overlapping entities from sentences. We choose the New York Times (NYT) corpus for our experiments. This corpus has multiple versions, and we choose the following two versions as their test dataset has significantly larger number of instances of multiple relation tuples with overlapping entities. (i) The first version is used by BIBREF6 (BIBREF6) (mentioned as NYT in their paper) and has 24 relations. We name this version as NYT24. (ii) The second version is used by BIBREF11 (BIBREF11) (mentioned as NYT10 in their paper) and has 29 relations. We name this version as NYT29. We select 10% of the original training data and use it as the validation dataset. The remaining 90% is used for training. We include statistics of the training and test datasets in Table TABREF11.","This corpus has multiple versions, and we choose the following two versions as their test dataset has significantly larger number of instances of multiple relation tuples with overlapping entities. (i) The first version is used by BIBREF6 (BIBREF6) (mentioned as NYT in their paper) and has 24 relations. We name this version as NYT24. (ii) The second version is used by BIBREF11 (BIBREF11) (mentioned as NYT10 in their paper) and has 29 relations.",3fecd676405aba7cae9cf8b1a94afc80c85cfd53,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,Which one of two proposed approaches performed better in experiments?,710fa8b3e74ee63d2acc20af19f95f7702b7ce5e,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,,,"Among the baselines, HRL achieves significantly higher F1 scores on the two datasets. We run their model and our models five times and report the median results in Table TABREF15. Scores of other baselines in Table TABREF15 are taken from previous published papers BIBREF6, BIBREF11, BIBREF14. Our WordDecoding (WDec) model achieves F1 scores that are $3.9\%$ and $4.1\%$ higher than HRL on the NYT29 and NYT24 datasets respectively. Similarly, our PtrNetDecoding (PNDec) model achieves F1 scores that are $3.0\%$ and $1.3\%$ higher than HRL on the NYT29 and NYT24 datasets respectively. We perform a statistical significance test (t-test) under a bootstrap pairing between HRL and our models and see that the higher F1 scores achieved by our models are statistically significant ($p < 0.001$). Next, we combine the outputs of five runs of our models and five runs of HRL to build ensemble models. For a test instance, we include those tuples which are extracted in the majority ($\ge 3$) of the five runs. This ensemble mechanism increases the precision significantly on both datasets with a small improvement in recall as well. In the ensemble scenario, compared to HRL, WDec achieves $4.2\%$ and $3.5\%$ higher F1 scores and PNDec achieves $4.2\%$ and $2.9\%$ higher F1 scores on the NYT29 and NYT24 datasets respectively.","Our WordDecoding (WDec) model achieves F1 scores that are $3.9\%$ and $4.1\%$ higher than HRL on the NYT29 and NYT24 datasets respectively. Similarly, our PtrNetDecoding (PNDec) model achieves F1 scores that are $3.0\%$ and $1.3\%$ higher than HRL on the NYT29 and NYT24 datasets respectively.",c84efada9376d6cca3b26f0747032136ff633762,258ee4069f740c400c0049a2580945a1cc7f044c,What is previous work authors reffer to?,56123dd42cf5c77fc9a88fc311ed2e1eb672126e,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,,,"We compare our model with the following state-of-the-art joint entity and relation extraction models: (1) SPTree BIBREF4: This is an end-to-end neural entity and relation extraction model using sequence LSTM and Tree LSTM. Sequence LSTM is used to identify all the entities first and then Tree LSTM is used to find the relation between all pairs of entities. (2) Tagging BIBREF5: This is a neural sequence tagging model which jointly extracts the entities and relations using an LSTM encoder and an LSTM decoder. They used a Cartesian product of entity tags and relation tags to encode the entity and relation information together. This model does not work when tuples have overlapping entities. (3) CopyR BIBREF6: This model uses an encoder-decoder approach for joint extraction of entities and relations. It copies only the last token of an entity from the source sentence. Their best performing multi-decoder model is trained with a fixed number of decoders where each decoder extracts one tuple. (4) HRL BIBREF11: This model uses a reinforcement learning (RL) algorithm with two levels of hierarchy for tuple extraction. A high-level RL finds the relation and a low-level RL identifies the two entities using a sequence tagging approach. This sequence tagging approach cannot always ensure extraction of exactly two entities. (5) GraphR BIBREF14: This model considers each token in a sentence as a node in a graph, and edges connecting the nodes as relations between them. They use graph convolution network (GCN) to predict the relations of every edge and then filter out some of the relations. (6) N-gram Attention BIBREF9: This model uses an encoder-decoder approach with N-gram attention mechanism for knowledge-base completion using distantly supervised data. The encoder uses the source tokens as its vocabulary and the decoder uses the entire Wikidata BIBREF15 entity IDs and relation IDs as its vocabulary. The encoder takes the source sentence as input and the decoder outputs the two entity IDs and relation ID for every tuple. During training, it uses the mapping of entity names and their Wikidata IDs of the entire Wikidata for proper alignment. Our task of extracting relation tuples with the raw entity names from a sentence is more challenging since entity names are not of fixed length. Our more generic approach is also helpful for extracting new entities which are not present in the existing knowledge bases such as Wikidata. We use their N-gram attention mechanism in our model to compare its performance with other attention models (Table TABREF17).","We compare our model with the following state-of-the-art joint entity and relation extraction models:

(1) SPTree BIBREF4: This is an end-to-end neural entity and relation extraction model using sequence LSTM and Tree LSTM. (2) Tagging BIBREF5: This is a neural sequence tagging model which jointly extracts the entities and relations using an LSTM encoder and an LSTM decoder. (3) CopyR BIBREF6: This model uses an encoder-decoder approach for joint extraction of entities and relations. (4) HRL BIBREF11: This model uses a reinforcement learning (RL) algorithm with two levels of hierarchy for tuple extraction. (5) GraphR BIBREF14: This model considers each token in a sentence as a node in a graph, and edges connecting the nodes as relations between them. (6) N-gram Attention BIBREF9: This model uses an encoder-decoder approach with N-gram attention mechanism for knowledge-base completion using distantly supervised data.",5437ff59df4863dd3efe426a80a54c419f65d206,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,How higher are F1 scores compared to previous work?,1898f999626f9a6da637bd8b4857e5eddf2fc729,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,,,"Among the baselines, HRL achieves significantly higher F1 scores on the two datasets. We run their model and our models five times and report the median results in Table TABREF15. Scores of other baselines in Table TABREF15 are taken from previous published papers BIBREF6, BIBREF11, BIBREF14. Our WordDecoding (WDec) model achieves F1 scores that are $3.9\%$ and $4.1\%$ higher than HRL on the NYT29 and NYT24 datasets respectively. Similarly, our PtrNetDecoding (PNDec) model achieves F1 scores that are $3.0\%$ and $1.3\%$ higher than HRL on the NYT29 and NYT24 datasets respectively. We perform a statistical significance test (t-test) under a bootstrap pairing between HRL and our models and see that the higher F1 scores achieved by our models are statistically significant ($p < 0.001$). Next, we combine the outputs of five runs of our models and five runs of HRL to build ensemble models. For a test instance, we include those tuples which are extracted in the majority ($\ge 3$) of the five runs. This ensemble mechanism increases the precision significantly on both datasets with a small improvement in recall as well. In the ensemble scenario, compared to HRL, WDec achieves $4.2\%$ and $3.5\%$ higher F1 scores and PNDec achieves $4.2\%$ and $2.9\%$ higher F1 scores on the NYT29 and NYT24 datasets respectively.","Our WordDecoding (WDec) model achieves F1 scores that are $3.9\%$ and $4.1\%$ higher than HRL on the NYT29 and NYT24 datasets respectively. Similarly, our PtrNetDecoding (PNDec) model achieves F1 scores that are $3.0\%$ and $1.3\%$ higher than HRL on the NYT29 and NYT24 datasets respectively.",4343a22d92f83bdd38de26e2a06dbdf561f4271f,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,,"Among the baselines, HRL achieves significantly higher F1 scores on the two datasets. We run their model and our models five times and report the median results in Table TABREF15. Scores of other baselines in Table TABREF15 are taken from previous published papers BIBREF6, BIBREF11, BIBREF14. Our WordDecoding (WDec) model achieves F1 scores that are $3.9\%$ and $4.1\%$ higher than HRL on the NYT29 and NYT24 datasets respectively. Similarly, our PtrNetDecoding (PNDec) model achieves F1 scores that are $3.0\%$ and $1.3\%$ higher than HRL on the NYT29 and NYT24 datasets respectively. We perform a statistical significance test (t-test) under a bootstrap pairing between HRL and our models and see that the higher F1 scores achieved by our models are statistically significant ($p < 0.001$). Next, we combine the outputs of five runs of our models and five runs of HRL to build ensemble models. For a test instance, we include those tuples which are extracted in the majority ($\ge 3$) of the five runs. This ensemble mechanism increases the precision significantly on both datasets with a small improvement in recall as well. In the ensemble scenario, compared to HRL, WDec achieves $4.2\%$ and $3.5\%$ higher F1 scores and PNDec achieves $4.2\%$ and $2.9\%$ higher F1 scores on the NYT29 and NYT24 datasets respectively.","Among the baselines, HRL achieves significantly higher F1 scores on the two datasets. Our WordDecoding (WDec) model achieves F1 scores that are $3.9\%$ and $4.1\%$ higher than HRL on the NYT29 and NYT24 datasets respectively. In the ensemble scenario, compared to HRL, WDec achieves $4.2\%$ and $3.5\%$ higher F1 scores and PNDec achieves $4.2\%$ and $2.9\%$ higher F1 scores on the NYT29 and NYT24 datasets respectively.",585bf4e35871a8977533917335a6687244450f46,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2-Table1-1.png,Table 1: Relation tuple representation for encoder-decoder models.,3-Figure1-1.png,Figure 1: The architecture of an encoder-decoder model (left) and a pointer network-based decoder block (right).,5-Table2-1.png,Table 2: Statistics of train/test split of the two datasets.,6-Table4-1.png,Table 4: Ablation of attention mechanisms with WordDecoding (WDec) and PtrNetDecoding (PNDec) model.,6-Table3-1.png,Table 3: Performance comparison on the two datasets.,7-Table5-1.png,Table 5: Comparison on entity and relation generation tasks.,,,,,,,,,WordDecoding (WDec) model,SPTree Tagging CopyR HRL GraphR N-gram Attention,7-Table6-1.png,Table 6: % errors for wrong ordering and entity mismatch.,,,,,,,,,,,,,,,,,,,WordDecoding (WDec) model achieves F1 scores that are $3.9\%$ and $4.1\%$ higher than HRL on the NYT29 and NYT24 datasets respectively PtrNetDecoding (PNDec) model achieves F1 scores that are $3.0\%$ and $1.3\%$ higher than HRL on the NYT29 and NYT24 datasets respectively,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Our WordDecoding (WDec) model achieves F1 scores that are $3.9\%$ and $4.1\%$ higher than HRL on the NYT29 and NYT24 datasets respectively In the ensemble scenario, compared to HRL, WDec achieves $4.2\%$ and $3.5\%$ higher F1 scores",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A New Corpus for Low-Resourced Sindhi Language with Word Embeddings,"Representing words and phrases into dense vectors of real numbers which encode semantic and syntactic properties is a vital constituent in natural language processing (NLP). The success of neural network (NN) models in NLP largely rely on such dense word representations learned on the large unlabeled corpus. Sindhi is one of the rich morphological language, spoken by large population in Pakistan and India lacks corpora which plays an essential role of a test-bed for generating word embeddings and developing language independent NLP systems. In this paper, a large corpus of more than 61 million words is developed for low-resourced Sindhi language for training neural word embeddings. The corpus is acquired from multiple web-resources using web-scrappy. Due to the unavailability of open source preprocessing tools for Sindhi, the prepossessing of such large corpus becomes a challenging problem specially cleaning of noisy data extracted from web resources. Therefore, a preprocessing pipeline is employed for the filtration of noisy text. Afterwards, the cleaned vocabulary is utilized for training Sindhi word embeddings with state-of-the-art GloVe, Skip-Gram (SG), and Continuous Bag of Words (CBoW) word2vec algorithms. The intrinsic evaluation approach of cosine similarity matrix and WordSim-353 are employed for the evaluation of generated Sindhi word embeddings. Moreover, we compare the proposed word embeddings with recently revealed Sindhi fastText (SdfastText) word representations. Our intrinsic evaluation results demonstrate the high quality of our generated Sindhi word embeddings using SG, CBoW, and GloVe as compare to SdfastText word representations.",Introduction,"Sindhi is a rich morphological, mutltiscript, and multidilectal language. It belongs to the Indo-Aryan language family BIBREF0, with significant cultural and historical background. Presently, it is recognized as is an official language BIBREF1 in Sindh province of Pakistan, also being taught as a compulsory subject in Schools and colleges. Sindhi is also recognized as one of the national languages in India. Ulhasnagar, Rajasthan, Gujarat, and Maharashtra are the largest Indian regions of Sindhi native speakers. It is also spoken in other countries except for Pakistan and India, where native Sindhi speakers have migrated, such as America, Canada, Hong Kong, British, Singapore, Tanzania, Philippines, Kenya, Uganda, and South, and East Africa. Sindhi has rich morphological structure BIBREF2 due to a large number of homogeneous words. Historically, it was written in multiple writing systems, which differ from each other in terms of orthography and morphology. The Persian-Arabic is the standard script of Sindhi, which was officially accepted in 1852 by the British government. However, the Sindhi-Devanagari is also a popular writing system in India being written in left to right direction like the Hindi language. Formerly, Khudabadi, Gujrati, Landa, Khojki, and Gurumukhi were also adopted as its writing systems. Even though, Sindhi has great historical and literal background, presently spoken by nearly 75 million people BIBREF1. The research on SNLP was coined in 2002, however, IT grabbed research attention after the development of its Unicode system BIBREF3. But still, Sindhi stands among the low-resourced languages due to the scarcity of core language processing resources of the raw and annotated corpus, which can be utilized for training robust word embeddings or the use of machine learning algorithms. Since the development of annotated datasets requires time and human resources. The Language Resources (LRs) are fundamental elements for the development of high quality NLP systems based on automatic or NN based approaches. The LRs include written or spoken corpora, lexicons, and annotated corpora for specific computational purposes. The development of such resources has received great research interest for the digitization of human languages BIBREF4. Many world languages are rich in such language processing resources integrated in their software tools including English BIBREF5 BIBREF6, Chinese BIBREF7 and other languages BIBREF8 BIBREF9. The Sindhi language lacks the basic computational resources BIBREF10 of a large text corpus, which can be utilized for training robust word embeddings and developing language independent NLP applications including semantic analysis, sentiment analysis, parts of the speech tagging, named entity recognition, machine translation BIBREF11, multitasking BIBREF12, BIBREF13. Presently Sindhi Persian-Arabic is frequently used for online communication, newspapers, public institutions in Pakistan, and India BIBREF1. But little work has been carried out for the development of LRs such as raw corpus BIBREF14, BIBREF15, annotated corpus BIBREF16, BIBREF17, BIBREF1, BIBREF18. In the best of our knowledge, Sindhi lacks the large unlabelled corpus which can be utilized for generating and evaluating word embeddings for Statistical Sindhi Language Processing (SSLP). One way to to break out this loop is to learn word embeddings from unlabelled corpora, which can be utilized to bootstrap other downstream NLP tasks. The word embedding is a new term of semantic vector space BIBREF19, distributed representations BIBREF20, and distributed semantic models. It is a language modeling approach BIBREF21 used for the mapping of words and phrases into $n$-dimensional dense vectors of real numbers that effectively capture the semantic and syntactic relationship with neighboring words in a geometric way BIBREF22 BIBREF23. Such as “Einstein” and “Scientist” would have greater similarity compared with “Einstein” and “doctor.” In this way, word embeddings accomplish the important linguistic concept of “a word is characterized by the company it keeps"". More recently NN based models yield state-of-the-art performance in multiple NLP tasks BIBREF24 BIBREF25 with the word embeddings. One of the advantages of such techniques is they use unsupervised approaches for learning representations and do not require annotated corpus which is rare for low-resourced Sindhi language. Such representions can be trained on large unannotated corpora, and then generated representations can be used in the NLP tasks which uses a small amount of labelled data. In this paper, we address the problems of corpus construction by collecting a large corpus of more than 61 million words from multiple web resources using the web-scrappy framework. After the collection of the corpus, we carefully preprocessed for the filtration of noisy text, e.g., the HTML tags and vocabulary of the English language. The statistical analysis is also presented for the letter, word frequencies and identification of stop-words. Finally, the corpus is utilized to generate Sindhi word embeddings using state-of-the-art GloVe BIBREF26 SG and CBoW BIBREF27 BIBREF20 BIBREF24 algorithms. The popular intrinsic evaluation method BIBREF20 BIBREF28 BIBREF29 of calculating cosine similarity between word vectors and WordSim353 BIBREF30 are employed to measure the performance of the learned Sindhi word embeddings. We translated English WordSim353 word pairs into Sindhi using bilingual English to Sindhi dictionary. The intrinsic approach typically involves a pre-selected set of query terms BIBREF23 and semantically related target words, which we refer to as query words. Furthermore, we also compare the proposed word embeddings with recently revealed Sindhi fastText (SdfastText) BIBREF25 word representations. To the best of our knowledge, this is the first comprehensive work on the development of large corpus and generating word embeddings along with systematic evaluation for low-resourced Sindhi Persian-Arabic. The synopsis of our novel contributions is listed as follows: We present a large corpus of more than 61 million words obtained from multiple web resources and reveal a list of Sindhi stop words. We develop a text cleaning pipeline for the preprocessing of the raw corpus. Generate word embeddings using GloVe, CBoW, and SG Word2Vec algorithms also evaluate and compare them using the intrinsic evaluation approaches of cosine similarity matrix and WordSim353. We are the first to evaluate SdfastText word representations and compare them with our proposed Sindhi word embeddings. The remaining sections of the paper are organized as; Section SECREF2 presents the literature survey regarding computational resources, Sindhi corpus construction, and word embedding models. Afterwards, Section SECREF3 presents the employed methodology, Section SECREF4 consist of statistical analysis of the developed corpus. Section SECREF5 present the experimental setup. The intrinsic evaluation results along with comparison are given in Section SECREF6. The discussion and future work are given in Section SECREF7, and lastly, Section SECREF8 presents the conclusion.",Related work,"The natural language resources refer to a set of language data and descriptions BIBREF31 in machine readable form, used for building, improving, and evaluating NLP algorithms or softwares. Such resources include written or spoken corpora, lexicons, and annotated corpora for specific computational purposes. Many world languages are rich in such language processing resources integrated in the software tools including NLTK for English BIBREF5, Stanford CoreNLP BIBREF6, LTP for Chinese BIBREF7, TectoMT for German, Russian, Arabic BIBREF8 and multilingual toolkit BIBREF9. But Sindhi language is at an early stage for the development of such resources and software tools. The corpus construction for NLP mainly involves important steps of acquisition, preprocessing, and tokenization. Initially, BIBREF14 discussed the morphological structure and challenges concerned with the corpus development along with orthographical and morphological features in the Persian-Arabic script. The raw and annotated corpus BIBREF1 for Sindhi Persian-Arabic is a good supplement towards the development of resources, including raw and annotated datasets for parts of speech tagging, morphological analysis, transliteration between Sindhi Persian-Arabic and Sindhi-Devanagari, and machine translation system. But the corpus is acquired only form Wikipedia-dumps. A survey-based study BIBREF4 provides all the progress made in the Sindhi Natural Language Processing (SNLP) with the complete gist of adopted techniques, developed tools and available resources which show that work on resource development on Sindhi needs more sophisticated efforts. The raw corpus is utilized for word segmentation BIBREF32 of Sindhi Persian-Arabic. More recently, an initiative towards the development of resources is taken BIBREF16 by open sourcing annotated dataset of Sindhi Persian-Arabic obtained from news and social blogs. The existing and proposed work is presented in Table TABREF9 on the corpus development, word segmentation, and word embeddings, respectively. The power of word embeddings in NLP was empirically estimated by proposing a neural language model BIBREF21 and multitask learning BIBREF12, but recently usage of word embeddings in deep neural algorithms has become integral element BIBREF33 for performance acceleration in deep NLP applications. The CBoW and SG BIBREF27 BIBREF20 popular word2vec neural architectures yielded high quality vector representations in lower computational cost with integration of character-level learning on large corpora in terms of semantic and syntactic word similarity later extended BIBREF33 BIBREF24. Both approaches produce state-of-the-art accuracy with fast training performance, better representations of less frequent words and efficient representation of phrases as well. BIBREF34 proposed NN based approach for generating morphemic-level word embeddings, which surpassed all the existing embedding models in intrinsic evaluation. A count-based GloVe model BIBREF26 also yielded state-of-the-art results in an intrinsic evaluation and downstream NLP tasks. The performance of Word embeddings is evaluated using intrinsic BIBREF23 BIBREF29 and extrinsic evaluation BIBREF28 methods. The performance of word embeddings can be measured with intrinsic and extrinsic evaluation approaches. The intrinsic approach is used to measure the internal quality of word embeddings such as querying nearest neighboring words and calculating the semantic or syntactic similarity between similar word pairs. A method of direct comparison for intrinsic evaluation of word embeddings measures the neighborhood of a query word in vector space. The key advantage of that method is to reduce bias and create insight to find data-driven relevance judgment. An extrinsic evaluation approach is used to evaluate the performance in downstream NLP tasks, such as parts-of-speech tagging or named-entity recognition BIBREF23, but the Sindhi language lacks annotated corpus for such type of evaluation. Moreover, extrinsic evaluation is time consuming and difficult to interpret. Therefore, we opt intrinsic evaluation method BIBREF28 to get a quick insight into the quality of proposed Sindhi word embeddings by measuring the cosine distance between similar words and using WordSim353 dataset. A study reveals that the choice of optimized hyper-parameters BIBREF35 has a great impact on the quality of pretrained word embeddings as compare to desing a novel algorithm. Therefore, we optimized the hyperparameters for generating robust Sindhi word embeddings using CBoW, SG and GloVe models. The embedding visualization is also useful to visualize the similarity of word clusters. Therefore, we use t-SNE BIBREF36 dimensionality reduction algorithm for compressing high dimensional embedding into 2-dimensional $x$,$y$ coordinate pairs with PCA BIBREF37. The PCA is useful to combine input features by dropping the least important features while retaining the most valuable features.",Methodology,"This section presents the employed methodology in detail for corpus acquisition, preprocessing, statistical analysis, and generating Sindhi word embeddings.",Methodology ::: Task description,"We initiate this work from scratch by collecting large corpus from multiple web resources. After preprocessing and statistical analysis of the corpus, we generate Sindhi word embeddings with state-of-the-art CBoW, SG, and GloVe algorithms. The generated word embeddings are evaluated using the intrinsic evaluation approaches of cosine similarity between nearest neighbors, word pairs, and WordSim-353 for distributional semantic similarity. Moreover, we use t-SNE with PCA for the comparison of the distance between similar words via visualization.",Methodology ::: Corpus acquisition,"The corpus is a collection of human language text BIBREF31 built with a specific purpose. However, the statistical analysis of the corpus provides quantitative, reusable data, and an opportunity to examine intuitions and ideas about language. Therefore, the corpus has great importance for the study of written language to examine the text. In fact, realizing the necessity of large text corpus for Sindhi, we started this research by collecting raw corpus from multiple web resource using web-scrappy framwork for extraction of news columns of daily Kawish and Awami Awaz Sindhi newspapers, Wikipedia dumps, short stories and sports news from Wichaar social blog, news from Focus Word press blog, historical writings, novels, stories, books from Sindh Salamat literary websites, novels, history and religious books from Sindhi Adabi Board and tweets regarding news and sports are collected from twitter.",Methodology ::: Preprocessing,"The preprocessing of text corpus obtained from multiple web resources is a challenging task specially it becomes more complicated when working on low-resourced language like Sindhi due to the lack of open-source preprocessing tools such as NLTK BIBREF5 for English. Therefore, we design a preprocessing pipeline depicted in Figure FIGREF22 for the filtration of unwanted data and vocabulary of other languages such as English to prepare input for word embeddings. Whereas, the involved preprocessing steps are described in detail below the Figure FIGREF22. Moreover, we reveal the list of Sindhi stop words BIBREF38 which is labor intensive and requires human judgment as well. Hence, the most frequent and least important words are classified as stop words with the help of a Sindhi linguistic expert. The partial list of Sindhi stop words is given in TABREF61. We use python programming language for designing the preprocessing pipeline using regex and string functions. Input: The collected text documents were concatenated for the input in UTF-8 format. Replacement symbols: The punctuation marks of a full stop, hyphen, apostrophe, comma, quotation, and exclamation marks replaced with white space for authentic tokenization because without replacing these symbols with white space the words were found joined with their next or previous corresponding words. Filtration of noisy data: The text acquisition from web resources contain a huge amount of noisy data. Therefore, we filtered out unimportant data such as the rest of the punctuation marks, special characters, HTML tags, all types of numeric entities, email, and web addresses. Normalization: In this step, We tokenize the corpus then normalize to lower-case for the filtration of multiple white spaces, English vocabulary, and duplicate words. The stop words were only filtered out for preparing input for GloVe. However, the sub-sampling approach in CBoW and SG can discard most frequent or stop words automatically.",Methodology ::: Word embedding models,"The NN based approaches have produced state-of-the-art performance in NLP with the usage of robust word embedings generated from the large unlabelled corpus. Therefore, word embeddings have become the main component for setting up new benchmarks in NLP using deep learning approaches. Most recently, the use cases of word embeddings are not only limited to boost statistical NLP applications but can also be used to develop language resources such as automatic construction of WordNet BIBREF39 using the unsupervised approach. The word embedding can be precisely defined as the encoding of vocabulary $V$ into $N$ and the word $w$ from $V$ to vector $\overrightarrow{w} $ into $N$-dimensional embedding space. They can be broadly categorized into predictive and count based methods, being generated by employing co-occurrence statistics, NN algorithms, and probabilistic models. The GloVe BIBREF26 algorithm treats each word as a single entity in the corpus and generates a vector of each word. However, CBoW and SG BIBREF27 BIBREF20, later extended BIBREF33 BIBREF24, well-known as word2vec rely on simple two layered NN architecture which uses linear activation function in hidden layer and softmax in the output layer. The work2vec model treats each word as a bag-of-character n-gram.",Methodology ::: GloVe,"The GloVe is a log-bilinear regression model BIBREF26 which combines two methods of local context window and global matrix factorization for training word embeddings of a given vocabulary in an unsupervised way. It weights the contexts using the harmonic function, for example, a context word four tokens away from an occurrence will be counted as $\frac{1}{4}$. The Glove’s implementation represents word $w \in V_{w}$ and context $c \in V_{c}$ in $D$-dimensional vectors $\overrightarrow{w}$ and $\overrightarrow{c}$ in a following way, Where, $b^{\overrightarrow{w}}$ is row vector $\left|V_{w}\right|$ and $b^{\overrightarrow{c}}$ is $\left|V_{c}\right|$ is column vector.",Methodology ::: Continuous bag-of-words,"The standard CBoW is the inverse of SG BIBREF27 model, which predicts input word on behalf of the context. The length of input in the CBoW model depends on the setting of context window size which determines the distance to the left and right of the target word. Hence the context is a window that contain neighboring words such as by giving $w=\left\lbrace w_{1}, w_{2}, \dots \dots w_{t}\right\rbrace $ a sequence of words $T$, the objective of the CBoW is to maximize the probability of given neighboring words such as, Where, $c_{t}$ is context of $t^{\text{th}}$ word for example with window $w_{t-c}, \ldots w_{t-1}, w_{t+1}, \ldots w_{t+c}$ of size $2 c$.",Methodology ::: Skip gram,"The SG model predicts surrounding words by giving input word BIBREF20 with training objective of learning good word embeddings that efficiently predict the neighboring words. The goal of skip-gram is to maximize average log-probability of words $w=\left\lbrace w_{1}, w_{2}, \dots \dots w_{t}\right\rbrace $ across the entire training corpus, Where, $c_{t}$ denotes the context of words indices set of nearby $w_{t}$ words in the training corpus.",Methodology ::: Hyperparameters ::: Sub-sampling,"Th sub-sampling BIBREF20 approach is useful to dilute most frequent or stop words, also accelerates learning rate, and increases accuracy for learning rare word vectors. Numerous words in English, e.g., ‘the’, ‘you’, ’that’ do not have more importance, but these words appear very frequently in the text. However, considering all the words equally would also lead to over-fitting problem of model parameters BIBREF24 on the frequent word embeddings and under-fitting on the rest. Therefore, it is useful to count the imbalance between rare and repeated words. The sub-sampling technique randomly removes most frequent words with some threshold $t$ and probability $p$ of words and frequency $f$ of words in the corpus. Where each word$w_{i}$ is discarded with computed probability in training phase, $f(w_i )$ is frequency of word $w_{i}$ and $t>0$ are parameters.",Methodology ::: Hyperparameters ::: Dynamic context window,"The traditional word embedding models usually use a fixed size of a context window. For instance, if the window size ws=6, then the target word apart from 6 tokens will be treated similarity as the next word. The scheme is used to assign more weight to closer words, as closer words are generally considered to be more important to the meaning of the target word. The CBoW, SG and GloVe models employ this weighting scheme. The GloVe model weights the contexts using a harmonic function, for example, a context word four tokens away from an occurrence will be counted as $\frac{1}{4}$. However, CBoW and SG implementation equally consider the contexts by dividing the ws with the distance from target word, e.g. ws=6 will weigh its context by $\frac{6}{6} \frac{5}{6} \frac{4}{6} \frac{3}{6} \frac{2}{6} \frac{1}{6}$.",Methodology ::: Hyperparameters ::: Sub-word model,"The sub-word model BIBREF24 can learn the internal structure of words by sharing the character representations across words. In that way, the vector for each word is made of the sum of those character $n-gram$. Such as, a vector of a word “table” is a sum of $n-gram$ vectors by setting the letter $n-gram$ size $min=3$ to $max=6$ as, $<ta, tab, tabl, table, table>, abl, able, able>, ble, ble>, le>$, we can get all sub-words of ""table"" with minimum length of $minn=3$ and maximum length of $maxn=6$. The $<$ and $>$ symbols are used to separate prefix and suffix words from other character sequences. In this way, the sub-word model utilizes the principles of morphology, which improves the quality of infrequent word representations. In addition to character $n-grams$, the input word $w$ is also included in the set of character $n-gram$, to learn the representation of each word. We obtain scoring function using a input dictionary of $n-grams$ with size $K$ by giving word $w$ , where $K_{w} \subset \lbrace 1, \ldots , K\rbrace $. A word representation $Z_{k}$ is associated to each $n-gram$ $Z$. Hence, each word is represented by the sum of character $n-gram$ representations, where, $s$ is the scoring function in the following equation,",Methodology ::: Hyperparameters ::: Position-dependent weights,"The position-dependent weighting approach BIBREF40 is used to avoid direct encoding of representations for words and their positions which can lead to over-fitting problem. The approach learns positional representations in contextual word representations and used to reweight word embedding. Thus, it captures good contextual representations at lower computational cost, Where, $p$ is individual position in context window associated with $d_{p}$ vector. Afterwards the context vector reweighted by their positional vectors is average of context words. The relative positional set is $P$ in context window and $v_{C}$ is context vector of $w_{t}$ respectively.",Methodology ::: Hyperparameters ::: Shifted point-wise mutual information,"The use sparse Shifted Positive Point-wise Mutual Information (SPPMI) BIBREF41 word-context matrix in learning word representations improves results on two word similarity tasks. The CBoW and SG have $k$ (number of negatives) BIBREF27 BIBREF20 hyperparameter, which affects the value that both models try to optimize for each $(w, c): P M I(w, c)-\log k$. Parameter $k$ has two functions of better estimation of negative examples, and it performs as before observing the probability of positive examples (actual occurrence of $w,c$).",Methodology ::: Hyperparameters ::: Deleting rare words,"Before creating a context window, the automatic deletion of rare words also leads to performance gain in CBoW, SG and GloVe models, which further increases the actual size of context windows.",Methodology ::: Evaluation methods,The intrinsic evaluation is based on semantic similarity BIBREF23 in word embeddings. The word similarity measure approach states BIBREF35 that the words are similar if they appear in the similar context. We measure word similarity of proposed Sindhi word embeddings using dot product method and WordSim353.,Methodology ::: Evaluation methods ::: Cosine similarity,"The cosine similarity between two non-zero vectors is a popular measure that calculates the cosine of the angle between them which can be derived by using the Euclidean dot product method. The dot product is a multiplication of each component from both vectors added together. The result of a dot product between two vectors isn’t another vector but a single value or a scalar. The dot product for two vectors can be defined as: $\overrightarrow{a}=\left(a_{1}, a_{2}, a_{3}, \dots , a_{n}\right)$ and $\overrightarrow{b}=\left({b}_{1}, {b}_{2}, {b}_{3}, \ldots , {b}_{n}\right)$ where $a_{n}$ and $b_{n}$ are the components of the vector and $n$ is dimension of vectors such as, However, the cosine of two non-zero vectors can be derived by using the Euclidean dot product formula, Given $a_{i}$ two vectors of attributes $a$ and $b$, the cosine similarity, $\cos ({\theta })$, is represented using a dot product and magnitude as, where $a_{i}$ and $b_{i}$ are components of vector $\overrightarrow{a}$ and $\overrightarrow{b}$, respectively.",Methodology ::: Evaluation methods ::: WordSim353,"The WordSim353 BIBREF42 is popular for the evaluation of lexical similarity and relatedness. The similarity score is assigned with 13 to 16 human subjects with semantic relations BIBREF30 for 353 English noun pairs. Due to the lack of annotated datasets in the Sindhi language, we translated WordSim353 using English to Sindhi bilingual dictionary for the evaluation of our proposed Sindhi word embeddings and SdfastText. We use the Spearman correlation coefficient for the semantic and syntactic similarity comparison which is used to used to discover the strength of linear or nonlinear relationships if there are no repeated data values. A perfect Spearman’s correlation of $+1$ or $-1$ discovers the strength of a link between two sets of data (word-pairs) when observations are monotonically increasing or decreasing functions of each other in a following way, where $r_s$ is the rank correlation coefficient, $n$ denote the number of observations, and $d^i$ is the rank difference between $i^{th}$ observations.",Statistical analysis of corpus,"The large corpus acquired from multiple resources is rich in vocabulary. We present the complete statistics of collected corpus (see Table TABREF52) with number of sentences, words and unique tokens.",Statistical analysis of corpus ::: Letter occurrences,"The frequency of letter occurrences in human language is not arbitrarily organized but follow some specific rules which enable us to describe some linguistic regularities. The Zipf’s law BIBREF43 suggests that if the frequency of letter or word occurrence ranked in descending order such as, Where, $F_{r}$ is the letter frequency of rth rank, $a$ and $b$ are parameters of input text. The comparative letter frequency in the corpus is the total number of occurrences of a letter divided by the total number of letters present in the corpus. The letter frequencies in our developed corpus are depicted in Figure FIGREF55; however, the corpus contains 187,620,276 total number of the character set. Sindhi Persian-Arabic alphabet consists of 52 letters but in the vocabulary 59 letters are detected, additional seven letters are modified uni-grams and standalone honorific symbols.",How does proposed word embeddings compare to Sindhi fastText word representations?,5b6aec1b88c9832075cd343f59158078a91f3597,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,,"Proposed SG model vs SINDHI FASTTEXT:
Average cosine similarity score: 0.650 vs 0.388
Average semantic relatedness similarity score between countries and their capitals: 0.663 vs 0.391","Generally, closer words are considered more important to a word’s meaning. The word embeddings models have the ability to capture the lexical relations between words. Identifying such relationship that connects words is important in NLP applications. We measure that semantic relationship by calculating the dot product of two vectors using Eq. DISPLAY_FORM48. The high cosine similarity score denotes the closer words in the embedding matrix, while less cosine similarity score means the higher distance between word pairs. We present the cosine similarity score of different semantically or syntactically related word pairs taken from the vocabulary in Table TABREF77 along with English translation, which shows the average similarity of 0.632, 0.650, 0.591 yields by CBoW, SG and GloVe respectively. The SG model achieved a high average similarity score of 0.650 followed by CBoW with a 0.632 average similarity score. The GloVe also achieved a considerable average score of 0.591 respectively. However, the average similarity score of SdfastText is 0.388 and the word pair Microsoft-Bill Gates is not available in the vocabulary of SdfastText. This shows that along with performance, the vocabulary in SdfastText is also limited as compared to our proposed word embeddings. Moreover, the average semantic relatedness similarity score between countries and their capitals is shown in Table TABREF78 with English translation, where SG also yields the best average score of 0.663 followed by CBoW with 0.611 similarity score. The GloVe also yields better semantic relatedness of 0.576 and the SdfastText yield an average score of 0.391. The first query word China-Beijing is not available the vocabulary of SdfastText. However, the similarity score between Afghanistan-Kabul is lower in our proposed CBoW, SG, GloVe models because the word Kabul is the name of the capital of Afghanistan as well as it frequently appears as an adjective in Sindhi text which means able.","The SG model achieved a high average similarity score of 0.650 followed by CBoW with a 0.632 average similarity score. The GloVe also achieved a considerable average score of 0.591 respectively. However, the average similarity score of SdfastText is 0.388 and the word pair Microsoft-Bill Gates is not available in the vocabulary of SdfastText. Moreover, the average semantic relatedness similarity score between countries and their capitals is shown in Table TABREF78 with English translation, where SG also yields the best average score of 0.663 followed by CBoW with 0.611 similarity score. The GloVe also yields better semantic relatedness of 0.576 and the SdfastText yield an average score of 0.391.",ff8fd9518421abfced12a1541e4f26b5185fc32c,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,Are trained word embeddings used for any other NLP task?,a6717e334c53ebbb87e5ef878a77ef46866e3aed,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,False,,"In this era of the information age, the existence of LRs plays a vital role in the digital survival of natural languages because the NLP tools are used to process a flow of un-structured data from disparate sources. It is imperative to mention that presently, Sindhi Persian-Arabic is frequently used in online communication, newspapers, public institutions in Pakistan and India. Due to the growing use of Sindhi on web platforms, the need for its LRs is also increasing for the development of language technology tools. But little work has been carried out for the development of resources which is not sufficient to design a language independent or machine learning algorithms. The present work is a first comprehensive initiative on resource development along with their evaluation for statistical Sindhi language processing. More recently, the NN based approaches have produced a state-of-the-art performance in NLP by exploiting unsupervised word embeddings learned from the large unlabelled corpus. Such word embeddings have also motivated the work on low-resourced languages. Our work mainly consists of novel contributions of resource development along with comprehensive evaluation for the utilization of NN based approaches in SNLP applications. The large corpus obtained from multiple web resources is utilized for the training of word embeddings using SG, CBoW and Glove models. The intrinsic evaluation along with comparative results demonstrates that the proposed Sindhi word embeddings have accurately captured the semantic information as compare to recently revealed SdfastText word vectors. The SG yield best results in nearest neighbors, word pair relationship and semantic similarity. The performance of CBoW is also close to SG in all the evaluation matrices. The GloVe also yields better word representations; however SG and CBoW models surpass the GloVe model in all evaluation matrices. Hyperparameter optimization is as important as designing a new algorithm. The choice of optimal parameters is a key aspect of performance gain in learning robust word embeddings. Moreover, We analysed that the size of the corpus and careful preprocessing steps have a large impact on the quality of word embeddings. However, in algorithmic perspective, the character-level learning approach in SG and CBoW improves the quality of representation learning, and overall window size, learning rate, number of epochs are the core parameters that largely influence the performance of word embeddings models. Ultimately, the new corpus of low-resourced Sindhi language, list of stop words and pretrained word embeddings along with empirical evaluation, will be a good supplement for future research in SSLP applications. In the future, we aim to use the corpus for annotation projects such as parts-of-speech tagging, named entity recognition. The proposed word embeddings will be refined further by creating custom benchmarks and the extrinsic evaluation approach will be employed for the performance analysis of proposed word embeddings. Moreover, we will also utilize the corpus using Bi-directional Encoder Representation Transformer BIBREF13 for learning deep contextualized Sindhi word representations. Furthermore, the generated word embeddings will be utilized for the automatic construction of Sindhi WordNet.","In the future, we aim to use the corpus for annotation projects such as parts-of-speech tagging, named entity recognition. Furthermore, the generated word embeddings will be utilized for the automatic construction of Sindhi WordNet.",80d7f5da1461b4437290ddc0e2474bd1cd298e64,258ee4069f740c400c0049a2580945a1cc7f044c,How many uniue words are in the dataset?,a1064307a19cd7add32163a70b6623278a557946,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,,908456 unique words are available in collected corpus.,"The large corpus acquired from multiple resources is rich in vocabulary. We present the complete statistics of collected corpus (see Table TABREF52) with number of sentences, words and unique tokens. FLOAT SELECTED: Table 2: Complete statistics of collected corpus from multiple resources.","The large corpus acquired from multiple resources is rich in vocabulary. We present the complete statistics of collected corpus (see Table TABREF52) with number of sentences, words and unique tokens. FLOAT SELECTED: Table 2: Complete statistics of collected corpus from multiple resources.",6d40c2912577783189a8fe21a2a3f6b5d1f11cea,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,"How is the data collected, which web resources were used?",8cb9006bcbd2f390aadc6b70d54ee98c674e45cc,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,,,"The corpus is a collection of human language text BIBREF31 built with a specific purpose. However, the statistical analysis of the corpus provides quantitative, reusable data, and an opportunity to examine intuitions and ideas about language. Therefore, the corpus has great importance for the study of written language to examine the text. In fact, realizing the necessity of large text corpus for Sindhi, we started this research by collecting raw corpus from multiple web resource using web-scrappy framwork for extraction of news columns of daily Kawish and Awami Awaz Sindhi newspapers, Wikipedia dumps, short stories and sports news from Wichaar social blog, news from Focus Word press blog, historical writings, novels, stories, books from Sindh Salamat literary websites, novels, history and religious books from Sindhi Adabi Board and tweets regarding news and sports are collected from twitter.","In fact, realizing the necessity of large text corpus for Sindhi, we started this research by collecting raw corpus from multiple web resource using web-scrappy framwork for extraction of news columns of daily Kawish and Awami Awaz Sindhi newspapers, Wikipedia dumps, short stories and sports news from Wichaar social blog, news from Focus Word press blog, historical writings, novels, stories, books from Sindh Salamat literary websites, novels, history and religious books from Sindhi Adabi Board and tweets regarding news and sports are collected from twitter.",0e1c5eb88cfe7910e0f9f0990a926496818ae6cb,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4-Table1-1.png,Table 1: Comparison of existing and proposed work on Sindhi corpus construction and word embeddings.,5-Figure1-1.png,Figure 1: Employed preprocessing pipeline for text cleaning,8-Table2-1.png,Table 2: Complete statistics of collected corpus from multiple resources.,9-Figure2-1.png,Figure 2: Frequency distribution of letter occurrences,10-Table3-1.png,"Table 3: Length of letter n-grams in words, distinct words, frequency and percentage in corpus.",11-Table4-1.png,Table 4: Partial list of most frequent Sindhi stop words along with frequency in the developed corpus.,Statistical analysis of corpus ::: Letter n-grams frequency,"We denote the combination of letter occurrences in a word as n-grams, where each letter is a gram in a word. The letter n-gram frequency is carefully analyzed in order to find the length of words which is essential to develop NLP systems, including learning of word embeddings such as choosing the minimum or maximum length of sub-word for character-level representation learning BIBREF24. We calculate the letter n-grams in words along with their percentage in the developed corpus (see Table TABREF57). The bi-gram words are most frequent, mostly consists of stop words and secondly, 4-gram words have a higher frequency.",Statistical analysis of corpus ::: Word Frequencies,"The word frequency count is an observation of word occurrences in the text. The commonly used words are considered to be with higher frequency, such as the word “the"" in English. Similarly, the frequency of rarely used words to be lower. Such frequencies can be calculated at character or word-level. We calculate word frequencies by counting a word $w$ occurrence in the corpus $c$, such as, Where the frequency of $w$ is the sum of every occurrence $k$ of $w$ in $c$.",Statistical analysis of corpus ::: Stop words,"The most frequent and least important words in NLP are often classified as stop words. The removal of such words can boost the performance of the NLP model BIBREF38, such as sentiment analysis and text classification. But the construction of such words list is time consuming and requires user decisions. Firstly, we determined Sindhi stop words by counting their term frequencies using Eq. DISPLAY_FORM59, and secondly, by analysing their grammatical status with the help of Sindhi linguistic expert because all the frequent words are not stop words (see Figure FIGREF62). After determining the importance of such words with the help of human judgment, we placed them in the list of stop words. The total number of detected stop words is 340 in our developed corpus. The partial list of most frequent Sindhi stop words is depicted in Table TABREF61 along with their frequency. The filtration of stop words is an essential preprocessing step for learning GloVe BIBREF26 word embeddings; therefore, we filtered out stop words for preparing input for the GloVe model. However, the sub-sampling approach BIBREF33 BIBREF24 is used to discard such most frequent words in CBoW and SG models.",Experiments and results,"Hyperparameter optimization BIBREF23is more important than designing a novel algorithm. We carefully choose to optimize the dictionary and algorithm-based parameters of CBoW, SG and GloVe algorithms. Hence, we conducted a large number of experiments for training and evaluation until the optimization of most suitable hyperparameters depicted in Table TABREF64 and discussed in Section SECREF63. The choice of optimized hyperparameters is based on The high cosine similarity score in retrieving nearest neighboring words, the semantic, syntactic similarity between word pairs, WordSim353, and visualization of the distance between twenty nearest neighbours using t-SNE respectively. All the experiments are conducted on GTX 1080-TITAN GPU.",,,12-Figure3-1.png,Figure 3: Most frequent words after filtration of stop words,12-Table5-1.png,"Table 5: Optimized parameters for CBoW, SG and GloVe models.",14-Table6-1.png,Table 6: Eight nearest neighboring words of each query word with English translation.,15-Table7-1.png,Table 7: Word pair relationship using cosine similarity (higher is better).,15-Table8-1.png,Table 8: Cosine similarity score between country and capital.,,,,,,,,,,,"daily Kawish and Awami Awaz Sindhi newspapers Wikipedia dumps short stories and sports news from Wichaar social blog news from Focus Word press blog historical writings, novels, stories, books from Sindh Salamat literary website novels, history and religious books from Sindhi Adabi Board  tweets regarding news and sports are collected from twitter",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,16-Table9-1.png,Table 9: Comparison of semantic and syntactic accuracy of proposed word embeddings using WordSim-353 dataset on 300−D embedding choosing various window size (ws).,17-Figure4-1.png,Figure 4: Visualization of Sindhi CBoW word embeddings,,,,,,,,17-Figure5-1.png,Figure 5: Visualization of Sindhi SG word embeddings,17-Figure6-1.png,Figure 6: visualization of Sindhi GloVe word embeddings,18-Figure7-1.png,Figure 7: Visualization of SdfastText word embeddings,,,,,Experiments and results ::: Hyperparameter optimization,"The state-of-the-art SG, CBoW BIBREF27 BIBREF33 BIBREF20 BIBREF24 and Glove BIBREF26 word embedding algorithms are evaluated by parameter tuning for development of Sindhi word embeddings. These parameters can be categories into dictionary and algorithm based, respectively. The integration of character n-gram in learning word representations is an ideal method especially for rich morphological languages because this approach has the ability to compute rare and misspelled words. Sindhi is also a rich morphological language. Therefore more robust embeddings became possible to train with the hyperparameter optimization of SG, CBoW and GloVe algorithms. We tuned and evaluated the hyperparameters of three algorithms individually which are discussed as follows: Number of Epochs: Generally, more epochs on the corpus often produce better results but more epochs take long training time. Therefore, we evaluate 10, 20, 30 and 40 epochs for each word embedding model, and 40 epochs constantly produce good results. Learning rate (lr): We tried lr of $0.05$, $0.1$, and $0.25$, the optimal lr $(0.25)$ gives the better results for training all the embedding models. Dimensions ($D$): We evaluate and compare the quality of $100-D$, $200-D$, and $300-D$ using WordSim353 on different $ws$, and the optimal $300-D$ are evaluated with cosine similarity matrix for querying nearest neighboring words and calculating the similarity between word pairs. The embedding dimensions have little affect on the quality of the intrinsic evaluation process. However, the selection of embedding dimensions might have more impact on the accuracy in certain downstream NLP applications. The lower embedding dimensions are faster to train and evaluate. Character n-grams: The selection of minimum (minn) and the maximum (maxn) length of character $n-grams$ is an important parameter for learning character-level representations of words in CBoW and SG models. Therefore, the n-grams from $3-9$ were tested to analyse the impact on the accuracy of embedding. We optimized the length of character n-grams from $minn=2$ and $maxn=7$ by keeping in view the word frequencies depicted in Table TABREF57. Window size (ws): The large ws means considering more context words and similarly less ws means to limit the size of context words. By changing the size of the dynamic context window, we tried the ws of 3, 5, 7 the optimal ws=7 yield consistently better performance. Negative Sampling (NS): : The more negative examples yield better results, but more negatives take long training time. We tried 10, 20, and 30 negative examples for CBoW and SG. The best negative examples of 20 for CBoW and SG significantly yield better performance in average training time. Minimum word count (minw): We evaluated the range of minimum word counts from 1 to 8 and analyzed that the size of input vocabulary is decreasing at a large scale by ignoring more words similarly the vocabulary size was increasing by considering rare words. Therefore, by ignoring words with a frequency of less than 4 in CBoW, SG, and GloVe consistently yields better results with the vocabulary of 200,000 words. Loss function (ls): we use hierarchical softmax (hs) for CBoW, negative sampling (ns) for SG and default loss function for GloVe BIBREF26. The recommended verbosity level, number of buckets, sampling threshold, number of threads are used for training CBoW, SG BIBREF24, and GloVe BIBREF26.",Word similarity comparison of Word Embeddings ::: Nearest neighboring words,"The cosine similarity matrix BIBREF35 is a popular approach to compute the relationship between all embedding dimensions of their distinct relevance to query word. The words with similar context get high cosine similarity and geometrical relatedness to Euclidean distance, which is a common and primary method to measure the distance between a set of words and nearest neighbors. Each word contains the most similar top eight nearest neighboring words determined by the highest cosine similarity score using Eq. DISPLAY_FORM48. We present the English translation of both query and retrieved words also discuss with their English meaning for ease of relevance judgment between the query and retrieved words.To take a closer look at the semantic and syntactic relationship captured in the proposed word embeddings, Table TABREF74 shows the top eight nearest neighboring words of five different query words Friday, Spring, Cricket, Red, Scientist taken from the vocabulary. As the first query word Friday returns the names of days Saturday, Sunday, Monday, Tuesday, Wednesday, Thursday in an unordered sequence. The SdfastText returns five names of days Sunday, Thursday, Monday, Tuesday and Wednesday respectively. The GloVe model also returns five names of days. However, CBoW and SG gave six names of days except Wednesday along with different writing forms of query word Friday being written in the Sindhi language which shows that CBoW and SG return more relevant words as compare to SdfastText and GloVe. The CBoW returned Add and GloVe returns Honorary words which are little similar to the querry word but SdfastText resulted two irrelevant words Kameeso (N) which is a name (N) of person in Sindhi and Phrase is a combination of three Sindhi words which are not tokenized properly. Similarly, nearest neighbors of second query word Spring are retrieved accurately as names and seasons and semantically related to query word Spring by CBoW, SG and Glove but SdfastText returned four irrelevant words of Dilbahar (N), Pharase, Ashbahar (N) and Farzana (N) out of eight. The third query word is Cricket, the name of a popular game. The first retrieved word in CBoW is Kabadi (N) that is a popular national game in Pakistan. Including Kabadi (N) all the returned words by CBoW, SG and GloVe are related to Cricket game or names of other games. But the first word in SdfastText contains a punctuation mark in retrieved word Gone.Cricket that are two words joined with a punctuation mark (.), which shows the tokenization error in preprocessing step, sixth retrieved word Misspelled is a combination of three words not related to query word, and Played, Being played are also irrelevant and stop words. Moreover, fourth query word Red gave results that contain names of closely related to query word and different forms of query word written in the Sindhi language. The last returned word Unknown by SdfastText is irrelevant and not found in the Sindhi dictionary for translation. The last query word Scientist also contains semantically related words by CBoW, SG, and GloVe, but the first Urdu word given by SdfasText belongs to the Urdu language which means that the vocabulary may also contain words of other languages. Another unknown word returned by SdfastText does not have any meaning in the Sindhi dictionary. More interesting observations in the presented results are the diacritized words retrieved from our proposed word embeddings and The authentic tokenization in the preprocessing step presented in Figure FIGREF22. However, SdfastText has returned tri-gram words of Phrase in query words Friday, Spring, a Misspelled word in Cricket and Scientist query words. Hence, the overall performance of our proposed SG, CBoW, and GloVe demonstrate high semantic relatedness in retrieving the top eight nearest neighbor words.",Word similarity comparison of Word Embeddings ::: Word pair relationship,"Generally, closer words are considered more important to a word’s meaning. The word embeddings models have the ability to capture the lexical relations between words. Identifying such relationship that connects words is important in NLP applications. We measure that semantic relationship by calculating the dot product of two vectors using Eq. DISPLAY_FORM48. The high cosine similarity score denotes the closer words in the embedding matrix, while less cosine similarity score means the higher distance between word pairs. We present the cosine similarity score of different semantically or syntactically related word pairs taken from the vocabulary in Table TABREF77 along with English translation, which shows the average similarity of 0.632, 0.650, 0.591 yields by CBoW, SG and GloVe respectively. The SG model achieved a high average similarity score of 0.650 followed by CBoW with a 0.632 average similarity score. The GloVe also achieved a considerable average score of 0.591 respectively. However, the average similarity score of SdfastText is 0.388 and the word pair Microsoft-Bill Gates is not available in the vocabulary of SdfastText. This shows that along with performance, the vocabulary in SdfastText is also limited as compared to our proposed word embeddings. Moreover, the average semantic relatedness similarity score between countries and their capitals is shown in Table TABREF78 with English translation, where SG also yields the best average score of 0.663 followed by CBoW with 0.611 similarity score. The GloVe also yields better semantic relatedness of 0.576 and the SdfastText yield an average score of 0.391. The first query word China-Beijing is not available the vocabulary of SdfastText. However, the similarity score between Afghanistan-Kabul is lower in our proposed CBoW, SG, GloVe models because the word Kabul is the name of the capital of Afghanistan as well as it frequently appears as an adjective in Sindhi text which means able.",Word similarity comparison of Word Embeddings ::: Comparison with WordSim353,"We evaluate the performance of our proposed word embeddings using the WordSim353 dataset by translation English word pairs to Sindhi. Due to vocabulary differences between English and Sindhi, we were unable to find the authentic meaning of six terms, so we left these terms untranslated. So our final Sindhi WordSim353 consists of 347 word pairs. Table TABREF80 shows the Spearman correlation results using Eq. DISPLAY_FORM51 on different dimensional embeddings on the translated WordSim353. The Table TABREF80 presents complete results with the different ws for CBoW, SG and GloVe in which the ws=7 subsequently yield better performance than ws of 3 and 5, respectively. The SG model outperforms CBoW and GloVe in semantic and syntactic similarity by achieving the performance of 0.629 with ws=7. In comparison with English BIBREF27 achieved the average semantic and syntactic similarity of 0.637, 0.656 with CBoW and SG, respectively. Therefore, despite the challenges in translation from English to Sindhi, our proposed Sindhi word embeddings have efficiently captured the semantic and syntactic relationship.",Word similarity comparison of Word Embeddings ::: Visualization,"We use t-Distributed Stochastic Neighboring (t-SNE) dimensionality BIBREF36 reduction algorithm with PCA BIBREF37 for exploratory embeddings analysis in 2-dimensional map. The t-SNE is a non-linear dimensionality reduction algorithm for visualization of high dimensional datasets. It starts the probability calculation of similar word clusters in high-dimensional space and calculates the probability of similar points in the corresponding low-dimensional space. The purpose of t-SNE for visualization of word embeddings is to keep similar words close together in 2-dimensional $x,y$ coordinate pairs while maximizing the distance between dissimilar words. The t-SNE has a perplexity (PPL) tunable parameter used to balance the data points at both the local and global levels. We visualize the embeddings using PPL=20 on 5000-iterations of 300-D models. We use the same query words (see Table TABREF74) by retrieving the top 20 nearest neighboring word clusters for a better understanding of the distance between similar words. Every query word has a distinct color for the clear visualization of a similar group of words. The closer word clusters show the high similarity between the query and retrieved word clusters. The word clusters in SG (see Fig. FIGREF83) are closer to their group of semantically related words. Secondly, the CBoW model depicted in Fig. FIGREF82 and GloVe Fig. FIGREF84 also show the better cluster formation of words than SdfastText Fig. FIGREF85, respectively.",Discussion and future work,"In this era of the information age, the existence of LRs plays a vital role in the digital survival of natural languages because the NLP tools are used to process a flow of un-structured data from disparate sources. It is imperative to mention that presently, Sindhi Persian-Arabic is frequently used in online communication, newspapers, public institutions in Pakistan and India. Due to the growing use of Sindhi on web platforms, the need for its LRs is also increasing for the development of language technology tools. But little work has been carried out for the development of resources which is not sufficient to design a language independent or machine learning algorithms. The present work is a first comprehensive initiative on resource development along with their evaluation for statistical Sindhi language processing. More recently, the NN based approaches have produced a state-of-the-art performance in NLP by exploiting unsupervised word embeddings learned from the large unlabelled corpus. Such word embeddings have also motivated the work on low-resourced languages. Our work mainly consists of novel contributions of resource development along with comprehensive evaluation for the utilization of NN based approaches in SNLP applications. The large corpus obtained from multiple web resources is utilized for the training of word embeddings using SG, CBoW and Glove models. The intrinsic evaluation along with comparative results demonstrates that the proposed Sindhi word embeddings have accurately captured the semantic information as compare to recently revealed SdfastText word vectors. The SG yield best results in nearest neighbors, word pair relationship and semantic similarity. The performance of CBoW is also close to SG in all the evaluation matrices. The GloVe also yields better word representations; however SG and CBoW models surpass the GloVe model in all evaluation matrices. Hyperparameter optimization is as important as designing a new algorithm. The choice of optimal parameters is a key aspect of performance gain in learning robust word embeddings. Moreover, We analysed that the size of the corpus and careful preprocessing steps have a large impact on the quality of word embeddings. However, in algorithmic perspective, the character-level learning approach in SG and CBoW improves the quality of representation learning, and overall window size, learning rate, number of epochs are the core parameters that largely influence the performance of word embeddings models. Ultimately, the new corpus of low-resourced Sindhi language, list of stop words and pretrained word embeddings along with empirical evaluation, will be a good supplement for future research in SSLP applications. In the future, we aim to use the corpus for annotation projects such as parts-of-speech tagging, named entity recognition. The proposed word embeddings will be refined further by creating custom benchmarks and the extrinsic evaluation approach will be employed for the performance analysis of proposed word embeddings. Moreover, we will also utilize the corpus using Bi-directional Encoder Representation Transformer BIBREF13 for learning deep contextualized Sindhi word representations. Furthermore, the generated word embeddings will be utilized for the automatic construction of Sindhi WordNet.",Conclusion,"In this paper, we mainly present three novel contributions of large corpus development contains large vocabulary of more than 61 million tokens, 908,456 unique words. Secondly, the list of Sindhi stop words is constructed by finding their high frequency and least importance with the help of Sindhi linguistic expert. Thirdly, the unsupervised Sindhi word embeddings are generated using state-of-the-art CBoW, SG and GloVe algorithms and evaluated using popular intrinsic evaluation approaches of cosine similarity matrix and WordSim353 for the first time in Sindhi language processing. We translate English WordSim353 using the English-Sindhi bilingual dictionary, which will also be a good resource for the evaluation of Sindhi word embeddings. Moreover, the proposed word embeddings are also compared with recently revealed SdfastText word representations. Our empirical results demonstrate that our proposed Sindhi word embeddings have captured high semantic relatedness in nearest neighboring words, word pair relationship, country, and capital and WordSim353. The SG yields the best performance than CBoW and GloVe models subsequently. However, the performance of GloVe is low on the same vocabulary because of character-level learning of word representations and sub-sampling approaches in SG and CBoW. Our proposed Sindhi word embeddings have surpassed SdfastText in the intrinsic evaluation matrix. Also, the vocabulary of SdfastText is limited because they are trained on a small Wikipedia corpus of Sindhi Persian-Arabic. We will further investigate the extrinsic performance of proposed word embeddings on the Sindhi text classification task in the future. The proposed resources along with systematic evaluation will be a sophisticated addition to the computational resources for statistical Sindhi language processing.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Empirical Gaussian priors for cross-lingual transfer learning,"Sequence model learning algorithms typically maximize log-likelihood minus the norm of the model (or minimize Hamming loss + norm). In cross-lingual part-of-speech (POS) tagging, our target language training data consists of sequences of sentences with word-by-word labels projected from translations in $k$ languages for which we have labeled data, via word alignments. Our training data is therefore very noisy, and if Rademacher complexity is high, learning algorithms are prone to overfit. Norm-based regularization assumes a constant width and zero mean prior. We instead propose to use the $k$ source language models to estimate the parameters of a Gaussian prior for learning new POS taggers. This leads to significantly better performance in multi-source transfer set-ups. We also present a drop-out version that injects (empirical) Gaussian noise during online learning. Finally, we note that using empirical Gaussian priors leads to much lower Rademacher complexity, and is superior to optimally weighted model interpolation.",Cross-lingual transfer learning of sequence models,"The people of the world speak about 6,900 different languages. Open-source off-the-shelf natural language processing (NLP) toolboxes like OpenNLP and CoreNLP cover only 6–7 languages, and we have sufficient labeled training data for inducing models for about 20–30 languages. In other words, supervised sequence learning algorithms are not sufficient to induce POS models for but a small minority of the world's languages. What can we do for all the languages for which no training data is available? Unsupervised POS induction algorithms have methodological problems (in-sample evaluation, community-wide hyper-parameter tuning, etc.), and performance is prohibitive of downstream applications. Some work on unsupervised POS tagging has assumed other resources such as tag dictionaries BIBREF0 , but such resources are also only available for a limited number of languages. In our experiments, we assume that no training data or tag dictionaries are available. Our only assumption is a bit of text translated into multiple languages, specifically, fragments of the Bible. We will use Bible data for annotation projection, as well as for learning cross-lingual word embeddings (§3). Unsupervised learning with typologically informed priors BIBREF1 is an interesting approach to unsupervised POS induction that is more applicable to low-resource languages. Our work is related to this work, but we learn informed priors rather than stipulate them and combine these priors with annotation projection (learning from noisy labels) rather than unsupervised learning. Annotation projection refers to transferring annotation from one or more source languages to the target language (for which no labeled data is otherwise available), typically through word alignments. In our experiments below, we use an unsupervised word alignment algorithm to align $15\times 12$ language pairs. For 15 languages, we have predicted POS tags for each word in our multi-parallel corpus. For each word in one of our 12 target language training datasets, we thus have up to 15 votes for each word token, possibly weighted by the confidence of the word alignment algorithm. In this paper, we simply use the majority votes. This is the set-up assumed throughout in this paper (see §3 for more details):",Empirical Gaussian priors,"We will apply empirical Gaussian priors to linear-chain conditional random fields (CRFs; BIBREF3 ) and averaged structured perceptrons BIBREF4 . Linear-chain CRFs are trained by maximising the conditional log-likelihood of labeled sequences $LL(\mathbf {w},\mathcal {D})=\sum _{\langle \mathbf {x},\mathbf {y}\rangle \in \mathcal {D}}\log P(\mathbf {y}|\mathbf {x})$ with $\mathbf {w}\in \mathbb {R}^m$ and $\mathcal {D}$ a dataset consisting of sequences of discrete input symbols $\mathbf {x}=x_1,\ldots ,x_n$ associated with sequences of discrete labels $\mathbf {y}=y_1,\ldots ,y_n$ . L $k$ -regularized CRFs maximize $LL(\mathbf {w},\mathcal {D})-|\mathbf {w}|^k$ with typically $k\in \lbrace 0,1,2,\infty \rbrace $ , which all introduce costant-width, zero-mean regularizers. We refer to L $k$ -regularized CRFs as L2-CRF. L $k$ regularizers are parametric priors where the only parameter is the width of the bounding shape. The L2-regularizer is a Gaussian prior with zero mean, for example. The regularised log-likelihood with a Gaussian prior is $\mathbf {w}\in \mathbb {R}^m$0 . For practical reasons, hyper-parameters $\mathbf {w}\in \mathbb {R}^m$1 and $\mathbf {w}\in \mathbb {R}^m$2 are typically assumed to be constant for all values of $\mathbf {w}\in \mathbb {R}^m$3 . This also holds for recent work on parametric noise injection, e.g., BIBREF5 . If these parameters are assumed to be constant, the above objective becomes equivalent to L2-regularization. However, you can also try to learn these parameters. In empirical Bayes BIBREF6 , the parameters are learned from $\mathbf {w}\in \mathbb {R}^m$4 itself. BIBREF7 suggest learning the parameters from a validation set. In our set-up, we do not assume that we can learn the priors from training data (which is noisy) or validation data (which is generally not available in cross-lingual learning scenarios). Instead we estimate these parameters directly from source language models. When we estimate Gaussian priors from source language models, we will learn which features are invariant across languages, and which are not. We thereby introduce an ellipsoid regularizer whose centre is the average source model. In our experiments, we consider both the case where variance is assumed to be constant – which we call L2-regularization with priors (L2-Prior) — and the case where both variances and means are learned – which we call empirical Gaussian priors (EmpGauss). L2-Prior is the L2-CRF objective with $\sigma ^2_j=C$ with $C$ a regularization parameter, and $\mu _j=\hat{\mu _j}$ the average value of the corresponding parameter in the observed source models. EmpGauss replaces the above objective with $LL(\lambda )+\sum _j\log \frac{1}{\sigma \sqrt{2\pi }}e^{-\frac{(\lambda _j-\mu _j)^2}{2\sigma ^2}}$ , which, assuming model parameters are mutually independent, is the same as jointly optimising model probability and likelihood of the data. Note that minimizing the squared weights is equivalent to maximizing the log probability of the weights under a zero-mean Gaussian prior, and in the same way, this is equivalent to minimising the above objective with empirically estimated parameters $\hat{\mu _j}$ and $\sigma {\mu _j}$ . In other words, empirical Gaussian priors are bounding ellipsoids on the hypothesis space with learned widths and centres. Also, note that in single-source cross-lingual transfer learning, observed variance is zero, and we therefore replace this with a regularization parameter $C$ shared with the baseline. In the single-source set-up, L2-Prior is thus equivalent to EmpGauss. We use L-BFGS to maximize our baseline L2-regularized objectives, as well as our empirical Gaussian prior objectives.",Empirical Gaussian noise injection,"We also introduce a drop-out variant of empirical Gaussian priors. Our point of departure is average structured perceptron. We implement empirical Gaussian noise injection with Gaussians $\langle (\mu _1,\sigma _1),\ldots , (\mu _m,\sigma _m)\rangle $ for $m$ features as follows. We initialise our model parameters with the means $\mu _j$ . For every instance we pass over, we draw a corruption vector $\mathbf {g}$ of random values $v_i$ from the corresponding Gaussians $(1,\sigma _i)$ . We inject the noise in $\mathbf {g}$ by taking pairwise multiplications of $\mathbf {g}$ and our feature representations of the input sequence with the relevant label sequences. Note that this drop-out algorithm is parameter-free, but of course we could easily throw in a hyper-parameter controlling the degree of regularization. We give the algorithm in Algorithm 1. [1] $T=\lbrace  \langle \mathbf {x}^1,\mathbf {y}^1\rangle ,\ldots ,\langle \mathbf {x}_n,\mathbf {y}_n\rangle \rbrace \text{~w.~}\mathbf {x}_i=\langle v_1,\ldots \rangle \text{ and }v_k=\langle f_1,\ldots ,f_m\rangle , \mathbf {w}^0=\langle w_1:\hat{\mu _1},\ldots , w_m:\hat{\mu _m}\rangle $ $i\le I\times |T|$ $j\le n$ $\mathbf {g}\leftarrow \mathbf {sample}(\mathcal {N}(1,\sigma _1),\ldots ,\mathcal {N}(1,\sigma _m))$ $\hat{\mathbf {y}}\leftarrow \arg \max _{\mathbf {y}}\mathbf {w}^i \cdot \mathbf {g}$ $\mathbf {w}^{i+1}\leftarrow \mathbf {w}^i+\Phi (\mathbf {x}_j,\mathbf {y}_j)\cdot \mathbf {g}-\Phi (\mathbf {x}_j,\hat{\mathbf {y}})\cdot \mathbf {g}$ Averaged structured perceptron with empirical Gaussian noise ",Observations,"We make the following additional observations: (i) Following the procedure in BIBREF11 , we can compute the Rademacher complexity of our models, i.e., their ability to learn noise in the labels (overfit). Sampling POS tags randomly from a uniform distribution, chance complexity is 0.083. With small sample sizes, L2-CRFs actually begin to learn patterns with Rademacher complexity rising to 0.086, whereas both L2-Prior and EmpGauss never learn a better fit than chance. (ii) BIBREF2 present a simple approach to explicitly studying bias-variance trade-offs during learning. They draw subsamples of $l< m$ training data points $\mathcal {D}_1, \ldots , \mathcal {D}_k$ and use a validation dataset of $m^{\prime }$ data points to define the integrated variance of our methods. Again, we see that using empirical Gaussian priors lead to less integrated variance. (iii) An empirical Gaussian prior effectively limits us to hypotheses in $\mathcal {H}$ in a ellipsoid around the average source model. When inference is exact, and our loss function is convex, we learn the model with the smallest loss on the training data within this ellipsoid. Model interpolation of (some weighting of) the average source model and the unregularized target model can potentially result in the same model, but since model interpolation is limited to the hyperplane connecting the two models, the probability of this to happen is infinitely small ( $\frac{1}{\infty }$ ). Since for any effective regularization parameter value (such that the regularized model is different from the unregularized model), the empirical Gaussian prior can be expected to have the same Rademacher complexity as model interpolation, we conclude that using empirical Gaussian priors is superior to model interpolation (and data concatenation).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,What languages did they experiment with?,e67d2266476abd157fc8c396b3dfb70cb343471e,five,familiar,no,transfer,50d8b4a941c26b89482c94ab324b5a274f9ced66,True,,,,,242af0098d4f61a8d54d7f9225e27a11d1652f95,5d0eb97e8e840e171f73b7642c2c89dd3984157b,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Logician: A Unified End-to-End Neural Approach for Open-Domain Information Extraction,"In this paper, we consider the problem of open information extraction (OIE) for extracting entity and relation level intermediate structures from sentences in open-domain. We focus on four types of valuable intermediate structures (Relation, Attribute, Description, and Concept), and propose a unified knowledge expression form, SAOKE, to express them. We publicly release a data set which contains more than forty thousand sentences and the corresponding facts in the SAOKE format labeled by crowd-sourcing. To our knowledge, this is the largest publicly available human labeled data set for open information extraction tasks. Using this labeled SAOKE data set, we train an end-to-end neural model using the sequenceto-sequence paradigm, called Logician, to transform sentences into facts. For each sentence, different to existing algorithms which generally focus on extracting each single fact without concerning other possible facts, Logician performs a global optimization over all possible involved facts, in which facts not only compete with each other to attract the attention of words, but also cooperate to share words. An experimental study on various types of open domain relation extraction tasks reveals the consistent superiority of Logician to other states-of-the-art algorithms. The experiments verify the reasonableness of SAOKE format, the valuableness of SAOKE data set, the effectiveness of the proposed Logician model, and the feasibility of the methodology to apply end-to-end learning paradigm on supervised data sets for the challenging tasks of open information extraction.",Introduction,"Semantic applications typically work on the basis of intermediate structures derived from sentences. Traditional word-level intermediate structures, such as POS-tags, dependency trees and semantic role labels, have been widely applied. Recently, entity and relation level intermediate structures attract increasingly more attentions. In general, knowledge based applications require entity and relation level information. For instance, in BIBREF0 , the lexicalized dependency path between two entity mentions was taken as the surface pattern facts. In distant supervision BIBREF1 , the word sequence and dependency path between two entity mentions were taken as evidence of certain relation. In Probase BIBREF2 , candidates of taxonomies were extracted by Hearst patterns BIBREF3 . The surface patterns of relations extracted by Open Information Extraction (OIE) systems BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 worked as the source of question answering systems BIBREF9 , BIBREF10 . In addition, entity and relation level intermediate structures have been proven effective in many other tasks such as text summarization BIBREF11 , BIBREF12 , BIBREF13 , text comprehension, word similarity, word analogy BIBREF14 , and more. The task of entity/relation level mediate structure extraction studies how facts about entities and relations are expressed by natural language in sentences, and then expresses these facts in an intermediate (and convenient) format. Although entity/relation level intermediate structures have been utilized in many applications, the study of learning these structures is still in an early stage. Firstly, the problem of extracting different types of entity/relation level intermediate structures has not been considered in a unified fashion. Applications generally need to construct their own handcrafted heuristics to extract required entity/relation level intermediate structures, rather than consulting a commonly available NLP component, as they do for word level intermediate structures. Open IE-v4 system (http://knowitall.github.io/openie/) attempted to build such components by developing two sub-systems, with each extracting one type of intermediate structures, i.e., SRLIE BIBREF15 for verb based relations, and ReNoun BIBREF16 , BIBREF17 for nominal attributes. However, important information about descriptive tags for entities and concept-instance relations between entities were not considered. Secondly, existing solutions to the task either used pattern matching technique BIBREF2 , BIBREF4 , BIBREF6 , BIBREF7 , or were trained in a self-supervised manner on the data set automatically generated by heuristic patterns or info-box matching BIBREF7 , BIBREF4 , BIBREF8 . It is well-understood that pattern matching typically does not generalize well and the automatically generated samples may contain lots of noises. This paper aims at tackling some of the well-known challenging problems in OIE systems, in a supervised end-to-end deep learning paradigm. Our contribution can be summarized as three major components: SAOKE format, SAOKE data set, and Logician. Symbol Aided Open Knowledge Expression (SAOKE) is a knowledge expression form with several desirable properties: (i) SAOKE is literally honest and open-domain. Following the philosophy of OIE systems, SAOKE uses words in the original sentence to express knowledge. (ii) SAOKE provides a unified view over four common types of knowledge: relation, attribute, description and concept. (iii) SAOKE is an accurate expression. With the aid of symbolic system, SAOKE is able to accurately express facts with separated relation phrases, missing information, hidden information, etc. SAOKE Data Set is a human annotated data set containing 48,248 Chinese sentences and corresponding facts in the SAOKE form. We publish the data set for research purpose. To the best of our knowledge, this is the largest publicly available human annotated data set for open-domain information extraction tasks. Logician is a supervised end-to-end neural learning algorithm which transforms natural language sentences into facts in the SAOKE form. Logician is trained under the attention-based sequence-to-sequence paradigm, with three mechanisms: restricted copy mechanism to ensure literally honestness, coverage mechanism to alleviate the under extraction and over extraction problem, and gated dependency attention mechanism to incorporate dependency information. Experimental results on four types of open information extraction tasks reveal the superiority of the Logician algorithm. Our work will demonstrate that SAOKE format is suitable for expressing various types of knowledge and is friendly to end-to-end learning algorithms. Particularly, we will focus on showing that the supervised end-to-end learning is promising for OIE tasks, to extract entity and relation level intermediate structures. The rest of this paper is organized as follows. Section ""SAOKE Format:  Symbol Aided Open Knowledge Expression"" presents the details of SAOKE. Section ""SAOKE Data Set"" describes the human labeled SAOKE data set. Section ""Logician"" describes the Logician algorithm and Section ""Empirical Evaluation"" evaluates the Logician algorithm and compares its performance with the state-of-the-art algorithms on four OIE tasks. Section ""Related Works"" discusses the related work and Section ""Conclusion"" concludes the paper.",SAOKE Format:  Symbol Aided Open Knowledge Expression,"When reading a sentence in natural language, humans are able to recognize the facts involved in the sentence and accurately express them. In this paper, Symbolic Aided Open Knowledge Expression (SAOKE) is proposed as the form for honestly recording these facts. SAOKE expresses the primary information of sentences in n-ary tuples $(subject,predicate,object_{1},\cdots ,object_{N})$ , and (in this paper) neglects some auxiliary information. In the design of SAOKE, we take four requirements into consideration: completeness, accurateness, atomicity and compactness.",Completeness,"After having analyzed a large number of sentences, we observe that the majority of facts can be classified into the following classes: Relation: Verb/preposition based n-ary relations between entity mentions BIBREF15 , BIBREF6 ; Attribute:Nominal attributes for entity mentions BIBREF16 , BIBREF17 ; Description: Descriptive phrases of entity mentions BIBREF18 ; Concept: Hyponymy and synonym relations among concepts and instances BIBREF19 . SAOKE is designed to express all these four types of facts. Table 1 presents an example sentence and the involved facts of these four classes in the SAOKE form. We should mention that the sentences and facts in English are directly translated from the corresponding Chinese sentences and facts, and the facts in English may not be the desired outputs of OIE algorithms for those English sentences due to the differences between Chinese and English languages.",Accurateness,"SAOKE adopts the ideology of “literally honest”. That is, as much as possible, it uses the words in the original sentences to express the facts. SAOKE follows the philosophy of OIE systems to express various relations without relying on any predefined schema system. There are, however, exceptional situations which are beyond the expression ability of this format. Extra symbols will be introduced to handle these situations, which are explained as follows. Separated relation phrase: In some languages such as Chinese, relation phrases may be divided into several parts residing in discontinued locations of the sentences. To accurately express these relation phrases, we add placeholders ( $X$ , $Y$ , $Z$ , etc) to build continuous and complete expressions. UTF8gbsn “深受X影响” (“deeply influenced by X” in English) in the example of Table 1 is an instance of relation phrase after such processing. Abbreviated expression: We explicitly express the information in abbreviated expressions by introducing symbolic predicates. For example, the expression of “Person (birth date - death date)” is transformed into facts: (Person, BIRTH, birth date) (Person, DEATH, death date), and the synonym fact involved in “NBA (National Basketball Association)” is expressed in the form of (NBA, = , National Basketball Association) . Hidden information: Description of an entity and hyponymy relation between entities are in general expressed implicitly in sentences, and are expressed by symbolic predicates “DESC” and “ISA” respectively, as in Table 1 . Another source of hidden information is the address expression. For example, UTF8gbsn “法国巴黎” (“Paris, France” in English) implies the fact UTF8gbsn (巴黎, LOC, 法国) ((Paris, LOC, France) in English), where the symbol “LOC” means “location”. Missing information: A sentence may not tell us the exact relation between two entities, or the exact subject/objects of a relation, which are required to be inferred from the context. We use placeholders like “ $X,Y,Z$ ” to denote the missing subjects/objects, and “ $P$ ” to denote the missing predicates.",Atomicity,"Atomicity is introduced to eliminate the ambiguity of knowledge expressions. In SAOKE format, each fact is required to be atomic, which means that: (i) it is self-contained for an accurate expression; (ii) it cannot be decomposed into multiple valid facts. We provide examples in Table 2 to help understand these two criteria. Note that the second criterion implies that any logical connections (including nested expressions) between facts are neglected (e.g., the third case in Table 2 ). This problem of expression relations between facts will be considered in the future version of SAOKE.",Compactness,"Natural language may express several facts in a compact form. For example, in a sentence UTF8gbsn “李白爱饮酒作诗” (“Li Bai loved to drink and write poetry” in English ), according to atomicity, two facts should be extracted: UTF8gbsn (李白, 爱, 饮酒)(李白, 爱, 作诗) ( (Li Bai, loved to, drink)(Li Bai, loved to, write poetry) in English ). In this situation, SAOKE adopts a compact expression to merge these two facts into one expression: UTF8gbsn (李白, 爱, [饮酒|作诗]) ( (Li Bai, loved to, [drink| write poetry]) in English ). The compactness of expressions is introduced to fulfill, but not to violate the rule of “literally honest”. SAOKE does not allow merging facts if facts are not expressed compactly in original sentences. By this means, the differences between the sentences and the corresponding knowledge expressions are reduced, which may help reduce the complexity of learning from data in SAOKE form. With the above designs, SAOKE is able to express various kinds of facts, with each historically considered by different open information extraction algorithms, for example, verb based relations in SRLIE BIBREF15 and nominal attributes in ReNoun BIBREF16 , BIBREF17 , descriptive phrases for entities in EntityTagger BIBREF18 , and hypernyms in HypeNet BIBREF19 . SAOKE introduces the atomicity to eliminate the ambiguity of knowledge expressions, and achieves better accuracy and compactness with the aid of the symbolic expressions.",SAOKE Data Set,"We randomly collect sentences from Baidu Baike (http://baike.baidu.com), and send those sentences to a crowd sourcing company to label the involved facts. The workers are trained with labeling examples and tested with exams. Then the workers with high exam scores are asked to read and understand the facts in the sentences, and express the facts in the SAOKE format. During the procedure, one sentence is only labeled by one worker. Finally, more than forty thousand sentences with about one hundred thousand facts are returned to us. The manual evaluation results on 100 randomly selected sentences show that the fact level precision and recall is 89.5% and 92.2% respectively. Table 3 shows the proportions of four types of facts (described in Section ""SAOKE Data Set"" ) contained in the data set. Note that the facts with missing predicates represented by “P” are classified into “Unknown”. We publicize the data set at https://ai.baidu.com/broad/subordinate?dataset=saoke. Prior to the SAOKE data set, an annotated data set for OIE tasks with 3,200 sentences in 2 domains was released in BIBREF20 to evaluate OIE algorithms, in which the data set was said BIBREF20 “13 times larger than the previous largest annotated Open IE corpus”. The SAOKE data set is 16 times larger than the data set in BIBREF20 . To the best of our knowledge, SAOKE data set is the largest publicly available human labeled data set for OIE tasks. Furthermore, the data set released in BIBREF20 was generated from a QA-SRL data set BIBREF21 , which indicates that the data set only contains facts that can be discovered by SRL (Semantic Role Labeling) algorithms, and thus is biased, whereas the SAOKE data set is not biased to an algorithm. Finally, the SAOKE data set contains sentences and facts from a large number of domains.",Logician,"Given a sentence $S$ and a set of expected facts (with all the possible types of facts) $\mathbb {F}=\lbrace F_{1},\cdots ,F_{n}\rbrace $ in SAOKE form, we join all the facts in the order that annotators wrote them into a char sequence $F$ as the expected output. We build Logician under the attention-based sequence-to-sequence learning paradigm, to transform $S$ into $F$ , together with the restricted copy mechanism, the coverage mechanism and the gated dependency mechanism.",Attention based Sequence-to-sequence Learning ,"The attention-based sequence-to-sequence learning BIBREF22 have been successfully applied to the task of generating text and patterns. Given an input sentence $S=[w_{1}^{S},\cdots ,w_{N_{S}}^{S}]$ , the target sequence $F=[w_{1}^{F},\cdots ,w_{N_{F}}^{F}]$ and a vocabulary $V$ (including the symbols introduced in Section ""SAOKE Format:  Symbol Aided Open Knowledge Expression"" and the OOV (out of vocabulary) tag ) with size $N_{v}$ , the words $w_{i}^{S}$ and $w_{j}^{F}$ can be represented as one-hot vectors $v_{i}^{S}$ and $v_{j}^{F}$ with dimension $N_{v}$ , and transformed into $N_{e}$ -dimensional distributed representation vectors by an embedding transform $F=[w_{1}^{F},\cdots ,w_{N_{F}}^{F}]$0 and $F=[w_{1}^{F},\cdots ,w_{N_{F}}^{F}]$1 respectively, where $F=[w_{1}^{F},\cdots ,w_{N_{F}}^{F}]$2 . Then the sequence of $F=[w_{1}^{F},\cdots ,w_{N_{F}}^{F}]$3 is transformed into a sequence of $F=[w_{1}^{F},\cdots ,w_{N_{F}}^{F}]$4 -dimensional hidden states $F=[w_{1}^{F},\cdots ,w_{N_{F}}^{F}]$5 using bi-directional GRU (Gated Recurrent Units) network BIBREF23 , and the sequence of $F=[w_{1}^{F},\cdots ,w_{N_{F}}^{F}]$6 is transformed into a sequence of $F=[w_{1}^{F},\cdots ,w_{N_{F}}^{F}]$7 -dimensional hidden states $F=[w_{1}^{F},\cdots ,w_{N_{F}}^{F}]$8 using GRU network. For each position $t$ in the target sequence, the decoder learns a dynamic context vector $c_{t}$ to focus attention on specific location $l$ in the input hidden states $H^{S}$ , then computes the probability of generated words by $p(w_{t}^{F}|\lbrace w_{1}^{F},\cdots ,w_{t-1}^{F}\rbrace ,c_{t})=g(h_{t-1}^{F},s_{t},c_{t})$ , where $s_{t}$ is the hidden state of the GRU decoder, $g$ is the word selection model (details could be found in BIBREF22 ), and $c_{t}$ is computed as $c_{t}=\sum _{j=1}^{N_{S}}\alpha _{tj}h_{j},$ where $\alpha _{tj}=\frac{\exp (e_{tj})}{\sum _{k=1}^{N_{S}}\exp (e_{tk})}$ and $c_{t}$0 is the alignment model to measure the strength of focus on the $c_{t}$1 -th location. $c_{t}$2 , $c_{t}$3 , and $c_{t}$4 are weight matrices.",Restricted Copy Mechanism,"The word selection model employed in BIBREF22 selects words from the whole vocabulary $V$ , which evidently violates the “literal honest” requirement of SAOKE. We propose a restricted version of copy mechanism BIBREF24 as the word selection model for Logician: We collect the symbols introduced in Section ""SAOKE Format:  Symbol Aided Open Knowledge Expression"" into a keyword set $K=\lbrace $ “ $ISA$ ”, “ $DESC$ ”, “ $LOC$ ”, “ $BIRTH$ ”, “ $DEATH$ ”, “ $=$ ”, “ $($ ”, “)”, “ $\$$ ”,“ $[$ ”, “ $ISA$0 ”, “ $ISA$1 ”, “ $ISA$2 ”, “ $ISA$3 ”, “ $ISA$4 ”, “ $ISA$5 ” $ISA$6 where “ $ISA$7 ” is the separator of elements of fact tuples. “ $ISA$8 ”, “ $ISA$9 ”, “ $DESC$0 ”, “ $DESC$1 ” are placeholders . When the decoder is considering generating a word $DESC$2 , it can choose $DESC$3 from either $DESC$4 or $DESC$5 .  $$p(w_{t}^{F}|w_{t-1}^{F},s_{t},c_{t})=p_{X}(w_{t}^{F}|w_{t-1}^{F},s_{t},c_{t})+p_{K}(w_{t}^{F}|w_{t-1}^{F},s_{t},c_{t}),$$   (Eq. 15)  where $p_{X}$ is the probability of copying from $S$ and $p_{K}$ is the probability of selecting from $K$ . Since $S\cap K=\phi $ and there are no unknown words in this problem setting, we compute $p_{X}$ and $p_{K}$ in a simpler way than that in BIBREF24 , as follows: $
p_{X}(w_{t}^{F}=w_{j}^{S}) & = & \frac{1}{Z}\exp (\sigma ((h_{j}^{S})^{T}W_{c})s_{t}),\\
p_{K}(w_{t}^{F}=k_{i}) & = & \frac{1}{Z}\exp (v_{i}^{T}W_{o}s_{t}),
$  where the (generic) $Z$ is the normalization term, $k_{i}$ is one of keywords, $v_{i}$ is the one-hot indicator vector for $k_{i}$ , $W_{o}\in \mathbb {R}^{(|K|\times N_{h})}$ , $W_{c}\in \mathbb {R}^{(N_{h}\times N_{h})}$ , and $\sigma $ is a nonlinear activation function.",Coverage Mechanism,"In practice, Logician may forget to extract some facts (under-extraction) or extract the same fact many times (over-extraction). We incorporate the coverage mechanism BIBREF25 into Logician to alleviate these problems. Formally, when the decoder considers generating a word $w_{t}^{F}$ , a coverage vector $m_{j}^{t}$ is introduced for each word $w_{j}^{S}$ , and updated as follows: $
m_{j}^{t} & = & \mu (m_{j}^{t-1},\alpha _{tj},h_{j}^{S},s_{t-1})=(1-z_{i})\circ m_{j}^{t-1}+z_{j}\circ \tilde{m}_{j}^{t},\\
\tilde{m}_{j}^{t} & = & \tanh (W_{h}h_{j}^{S}+u_{\alpha }\alpha _{tj}+W_{s}s_{t-1}+U_{m}[r_{i}\circ m_{j}^{t-1}]),
$  where $\circ $ is the element-wise multiplication operator. The update gate $z_{j}$ and the reset gate $r_{j}$ are defined as, respectively, $
z_{j} & = & \sigma (W_{h}^{z}h_{j}^{S}+u_{\alpha }^{z}\alpha _{tj}+W_{s}^{z}s_{t-1}+U_{m}^{z}m_{j}^{t-1}),\\
r_{j} & = & \sigma (W_{h}^{r}h_{j}^{S}+u_{\alpha }^{r}\alpha _{tj}+W_{s}^{r}s_{t-1}+U_{m}^{r}m_{j}^{t-1}),
$  where $\sigma $ is a logistic sigmoid function. The coverage vector $m_{j}^{t}$ contains the information about the historical attention focused on $w_{j}^{S}$ , and is helpful for deciding whether $w_{j}^{S}$ should be extracted or not. The alignment model is updated as follows BIBREF25 : $
e_{tj}=a(s_{t-1},h_{j}^{S},m_{j}^{t-1})=v_{a}^{T}\tanh (W_{a}s_{t-1}+U_{a}h_{j}^{S}+V_{a}m_{j}^{t-1}),
$  where $V_{a}\in \mathbb {R}^{(N_{h}\times N_{h})}$ .",Gated Dependency Attention,"The semantic relationship between candidate words and the previously decoded word is valuable to guide the decoder to select the correct word. We introduce the gated dependency attention mechanism to utilize such guidance. For a sentence $S$ , we extract the dependency tree using NLP tools such as CoreNLP BIBREF26 for English and LTP BIBREF27 for Chinese, and convert the tree into a graph by adding reversed edges with a revised labels (for example, adding $w_{j}^{S}\xrightarrow{}w_{i}^{S}$ for edge $w_{i}^{S}\xrightarrow{}w_{j}^{S}$ in the dependency tree). Then for each pair of words $(w_{i}^{S},w_{j}^{S})$ , the shortest path with labels $L=[w_{1}^{L},\cdots ,w_{N_{L}}^{L}]$ in the graph is computed and mapped into a sequence of $N_{e}$ -dimensional distributed representation vectors $[l_{1},\cdots ,l_{N_{L}}]$ by the embedding operation. One can employ RNN network to convert this sequence of vectors into a feature vector, but RNN operation is time-consuming. We simply concatenate vectors in short paths ( $N_{L}\le $ 3) into a $3N_{e}$ dimensional vector and feed the vector into a two-layer feed forward neural network to generate an $N_{h}$ -dimensional feature vector $w_{j}^{S}\xrightarrow{}w_{i}^{S}$0 . For long paths with $w_{j}^{S}\xrightarrow{}w_{i}^{S}$1 , $w_{j}^{S}\xrightarrow{}w_{i}^{S}$2 is set to a zero vector. We define dependency attention vector $w_{j}^{S}\xrightarrow{}w_{i}^{S}$3 , where $w_{j}^{S}\xrightarrow{}w_{i}^{S}$4 is the sharpened probability $w_{j}^{S}\xrightarrow{}w_{i}^{S}$5 defined in Equation ( 15 ). If $w_{j}^{S}\xrightarrow{}w_{i}^{S}$6 , $w_{j}^{S}\xrightarrow{}w_{i}^{S}$7 represents the semantic relationship between $w_{j}^{S}\xrightarrow{}w_{i}^{S}$8 and $w_{j}^{S}\xrightarrow{}w_{i}^{S}$9 . If $w_{i}^{S}\xrightarrow{}w_{j}^{S}$0 , then $w_{i}^{S}\xrightarrow{}w_{j}^{S}$1 is close to zero. To correctly guide the decoder, we need to gate $w_{i}^{S}\xrightarrow{}w_{j}^{S}$2 to remember the previous attention vector sometimes (for example, when $w_{i}^{S}\xrightarrow{}w_{j}^{S}$3 is selected), and to forget it sometimes (for example, when a new fact is started). Finally, we define $w_{i}^{S}\xrightarrow{}w_{j}^{S}$4 $w_{i}^{S}\xrightarrow{}w_{j}^{S}$5 ) as the gated dependency attention vector, where $w_{i}^{S}\xrightarrow{}w_{j}^{S}$6 is the GRU gated function, and update the alignment model as follows: $w_{i}^{S}\xrightarrow{}w_{j}^{S}$7  where $D_{a}\in \mathbb {R}^{(N_{h}\times N_{h})}$ .",Post processing,"For each sequence generated by Logician, we parse it into a set of facts, remove tuples with illegal format or duplicated tuples. The resultant set is taken as the output of the Logician.",Experimental Design ,"We first measure the utility of various components in Logician to select the optimal model, and then compare this model to the state-of-the-art methods in four types of information extraction tasks: verb/preposition-based relation, nominal attribute, descriptive phrase and hyponymy relation. The SAOKE data set is split into training set, validating set and testing set with ratios of 80%, 10%, 10%, respectively. For all algorithms involved in the experiments, the training set can be used to train the model, the validating set can be used to select an optimal model, and the testing set is used to evaluate the performance. For each instance pair $(S,F)$ in the test set, where $S$ is the input sentence and $F$ is the formatted string of ground truth of facts, we parse $F$ into a set of tuples $\mathbb {F}=\lbrace F_{i}\rbrace _{j=1}^{M}$ . Given an open information extraction algorithm, it reads $S$ and produces a set of tuples $\mathbb {G}=\lbrace G_{i}\rbrace _{j=1}^{N}$ . To evaluate how well the $\mathbb {G}$ approximates $\mathbb {F}$ , we need to match each $G_{i}$ to a ground truth fact $S$0 and check whether $S$1 tells the same fact as $S$2 . To conduct the match, we compute the similarity between each predicted fact in $S$3 and each ground truth fact in $S$4 , then find the optimal matching to maximize the sum of matched similarities by solving a linear assignment problem BIBREF28 . In the procedure, the similarity between two facts is defined as $S$5  where $G_{i}(l)$ and $F_{j}(l)$ denote the $l$ -th element of tuple $G_{i}$ and $F_{j}$ respectively, $\mathbf {g}(\cdot ,\cdot )$ denotes the gestalt pattern matching BIBREF29 measure for two strings and $\mathbf {n}(\text{$\cdot $)}$ returns the length of the tuple. Given a matched pair of $G_{i}$ and $F_{j}$ , we propose an automatic approach to judge whether they tell the same fact. They are judged as telling the same fact if one of the following two conditions is satisfied:  $\mathbf {n}(G_{i})=\mathbf {n}(F_{j})$ , and $\mathbf {g}(G_{i}(l),F_{j}(l))\ge 0.85,l=1,\cdots ,\mathbf {n}(G_{i})$ ;  $\mathbf {n}(G_{i})=\mathbf {n}(F_{j})$ , and $\mathbf {g}(\mathcal {S}(G_{i}),\mathcal {S}(F_{j})\ge 0.85$ ; where $\mathcal {S}$ is a function formatting a fact into a string by filling the arguments into the placeholders of the predicate. With the automatic judgment, the precision ( $P$ ), recall ( $R$ ) and $F_{1}$ -score over a test set can be computed. By defining a confidence measure and ordering the facts by their confidences, a precision-recall curve can be drawn to illustrate the overall performance of the algorithm. For Logician, the confidence of a fact is computed as the average of log probabilities over all words in that fact. Beyond the automatic judgment, human evaluation is also employed. Given an algorithm and the corresponding fact confidence measure, we find a threshold that produces approximately 10% recall (measured by automatic judgment) on the validation set of SAOKE data set. A certain number of sentences (200 for verb/preposition based relation extraction task, and 1000 for other three tasks) are randomly chosen from the testing set of SAOKE data set, and the facts extracted from these sentences are filtered with that threshold. Then we invite three volunteers to manually refine the labeled set of facts for each sentence and vote to decide whether each filtered fact is correctly involved in the sentence. The standard precision, recall and $F_{1}$ -score are reported as the human evaluation results. For each instance pair $(S,F)$ in the training set of SAOKE data set, we split $S$ and $F$ into words using LTP toolset BIBREF27 , and words appearing in more than 2 sentences are added to the vocabulary. By adding the OOV (out of vocabulary) tag, we finally obtain a vocabulary $V$ with size $N_{V}=65,293$ . The dimension of all embedding vectors is set to $N_{e}=200$ , and the dimension of hidden states is set to $N_{h}=256$ . We use a three-layer bi-directional GRU with dimension 128 to encode $\lbrace x_{i}\rbrace _{i=1}^{N_{S}}$ into hidden states $\lbrace h_{i}^{S}\rbrace _{i=1}^{N_{S}}$ , and a two-layer GRU with hidden-dimension 256 to encode the sequence of $\lbrace y_{j}\rbrace _{j=1}^{N_{F}}$ into hidden states $S$0 . Finally, the Logician network is constructed as stated in Section ""Logician"" . The Logician is then trained using stochastic gradient descent (SGD) with RMSPROP BIBREF30 strategy for 20 epochs with batch size 10 on the training set of SAOKE data set. The model with best $S$1 -score by automatic judgment on the validation set is selected as the trained model. When the model is trained, given a sentence, we employ the greedy search procedure to produce the fact sequences.",Evaluating Components' Utilities,"In this section, we analyze the effects of components involved in Logician: restricted copy, coverage, and gated dependency. Since the restricted copy mechanism is the essential requirement of Logician in order to achieve the goal of literally honest, we take the Logician with only copy mechanism (denoted by $Copy$ ) as the baseline, and analyze the effeteness of coverage mechanism (denoted by $Copy+Coverage$ ), gated dependency mechanism (denoted by $Copy+GatedDep$ ) and both (denoted by $All$ ). Furthermore, there is another option of whether or not to involve shallow semantic information such as POS-tag and NER-tag into the model. For models involving such information, the POS-tag and NER-tag of each word in sentence $S$ are annotated using LTP. For each word in $F$ that is not any keyword in $K$ , the POS-tag and NER-tag are copied from the corresponding original word in $S$ . For each keyword in $K$ , a unique POS-tag and a unique NER-tag are assigned to it. Finally, for each word in $S$ or $Copy+Coverage$0 , the POS-tag and NER-tag are mapped into $Copy+Coverage$1 -dimensional distributed representation vectors and are concatenated into $Copy+Coverage$2 or $Copy+Coverage$3 to attend the training. All models are trained using the same settings described in above section, and the default output facts (without any confidence filtering) are evaluated by the automatic judgment. The results are reported in Table 4 . From the results, we can see that the model involving all the components and shallow tag information archives the best performance. We use that model to attend the comparisons with existing approaches.",Comparison with Existing Approaches,"In the task of extracting verb/preposition based facts, we compare our Logician with the following state-of-the-art Chinese OIE algorithms: SRLIE: our implementation of SRLIE BIBREF15 for the Chinese language, which first uses LTP tool set to extract the semantic role labels, and converts the results into fact tuples using heuristic rules. The confidence of each fact is computed as the ratio of the number of words in the fact to the number of words in the shortest fragment of source sentence that contains all words in the fact. ZORE : the Chinese Open Relation Extraction system BIBREF31 , which builds a set of patterns by bootstrapping based on dependency parsing results, and uses the patterns to extract relations. We used the program provided by the author of ZORE system BIBREF31 to generate the extraction results in XML format, and developed an algorithm to transform the facts into n-ary tuples, where auxiliary information extracted by ZORE is removed. The confidence measure for ZORE is the same as that for SRLIE. SRL $_{\text{SAOKE}}$ : our implementation of the states-of-the-art SRL algorithm proposed in BIBREF32 with modifications to fit OIE tasks. $\text{SRL}_{\text{SAOKE}}$ extracts facts in two steps: (i) Predicate head word detection: detects head word for predicate of each possible fact, where head word of a predicate is the last word in the predicate depending on words outside the predicate in the dependency tree. (ii) Element phrase detection: For each detected head word, detects the subject phrase, predicate phrase and object phrases by tagging the sentence with an extended BIOE tagging scheme, which tags the word neighboring the separation point of the phrase by “M” to cope with the separated phrase. We modify the code provided by the author of BIBREF32 to implement above strategy, and then train a model with the same parameter setting in BIBREF32 on the training set of SAOKE data set. The confidence measure for $\text{SRL}_{\text{SAOKE}}$ is computed as the average of log probabilities over all tags of words in facts. Note that $\text{SRL}_{\text{SAOKE}}$ can extract both verb/preposition based relation and nominal attributes, but in this section, we only evaluate the results of the former type of facts. The precision-recall curves of Logician and above three comparison algorithms are shown in Figure 1 , and the human evaluation results are shown in the first section of Table 5 . The state-of-the-art nominal attribute extraction method is ReNoun BIBREF16 , BIBREF17 . However, it relies on a pre-constructed English attribute schema system BIBREF33 which is not available for Chinese, so it is not an available baseline for Chinese. Since $\text{SRL}_{\text{SAOKE}}$ can extract nominal attributes, we compare Logician with $\text{SRL}_{\text{SAOKE}}$ on this task. The precision-recall curves of Logician and $\text{SRL}_{\text{SAOKE}}$ on the nominal attribute extraction task are shown in Figure 1 , and the human evaluation results are shown in the second section of Table 5 . Descriptive phrase extraction has been considered in BIBREF18 , in which domain names are required to develop patterns to extract candidates for descriptive phrases, so this method is not applicable to open domain tasks. We develop a baseline algorithm (called Semantic Dependency Description Extractor, SDDE) to extract descriptive phrase. It extracts semantic dependency relation between words using LTP toolset, and for each noun $w_n$ which is the parent of some semantic “Desc” relations, identifies a noun phrase $N$ with $w_n$ as its heading word, assembles a descriptive phrase $D$ containing all words with “Desc” relation to $w_n$ , and finally outputs the fact “( $N$ , $DESC$ , $D$ )”. The confidence of fact in SDDE is computed as the ratio of the number of adverbs and adjectives in $D$ to the number of words in $D$ . The precision-recall curves of Logician and SDDE on the descriptive phrase extraction task are shown in Figure 1 , and the human evaluation results are shown in the third section of Table 5 . HypeNet BIBREF19 is the state-of-the-art algorithm recommended for hyponymy extraction BIBREF34 , which judges whether hyponymy relation exists between two given words. To make it capable of judging hyponymy relation between two phrases, we replace the word embedding vector component in HypeNet by an LSTM network. Two modified HypeNet models are built using different training data sets: (i) $\text{HypeNet}_{\text{Phrase}}$ : using the pairs of phrases with ISA relation in the training set of SAOKE data set (9,407 pairs after the compact expression expansion); (ii) $\text{HypeNet}_{\text{Phrase}}^{\text{Extra}}$ : besides the training set for $\text{HypeNet}_{\text{Phrase}}$ , adding two Chinese hyponymy data sets (1.4 million pair of words in total in hyponymy relation): Tongyici Cilin (Extended) (CilinE for short) BIBREF27 and cleaned Wikipedia Category data BIBREF35 . In both cases, the sentences from both Chinese Wikipedia pages and training set of SAOKE data set are taken as the background corpus for the HypeNet algorithm. In the testing phase, the trained models are used to predict whether the hyponymy relation exists for each pair of noun phrases/words in sentences of the testing set of SAOKE data set. The confidence of a judgment is the predicted probability of the existence of hyponymy relation. The precision-recall curves of Logician, $\text{HypeNet}_{\text{Phrase}}$ and $\text{HypeNet}_{\text{Phrase}}^{\text{Extra}}$ are shown in Figure 1 , and the human evaluation results in the fourth section of Table 5 .",Results Analysis,"The experimental results reveal that, Logician outperforms the comparison methods with large margin in first three tasks. For hyponymy detection tasks, Logician overwhelms the $\text{HypeNet}_{\text{Phrase}}$ using the same training data, and produces comparable results to $\text{HypeNet}_{\text{Phrase}}^{\text{Extra}}$ with much less training data. Table 6 exhibits several example sentences and the facts extracted by these algorithms. The poor performance of pattern-based methods is plausibly due to the noise in SAOKE data set. The sentences in SAOKE data set are randomly selected from a web encyclopedia, with free and casual writing style, are thus more noisy than the training data of NLP toolset used by these methods. In this situation, the NLP toolset may produce poor results, so do the pattern-based methods. Models learned from the SAOKE data set archive much better performance. Nevertheless, $\text{SRL}_{\text{SAOKE}}$ extracts each fact without knowing whether a candidate word has been used in other facts, which results in the misleading overlap of the word UTF8gbsn“学” (“Learn” in English) between two facts in the first case of Table 6 . Similarly, $\text{HypeNet}_{\text{Phrase}}$ and $\text{HypeNet}_{\text{Phrase}}^{\text{Extra}}$ focus on the semantic vectors of pairs of phrases and their dependency paths in the background corpus. They extract each fact independently from other facts and hence do not know whether there have been any other relations extracted about these two phrases. In other words, for those comparison methods, an important source of information is neglected and a global optimization for all facts involved in sentences is absent. On the contrary, Logician performs global optimization over the facts involved in each sentence by the sequence-to-sequence learning paradigm with the help of the coverage mechanism, in which facts compete each other to attract the attention of words, but also cooperate to share words. Valuable information is shared between these multiple tasks, which makes Logician consistently superior to other algorithms in these tasks. Furthermore, $\text{SRL}_{\text{SAOKE}}$ and $\text{HypeNet}$ methods suffer from the OOV problem, such as unfamiliar words/phrases like the person name and school name in the last case of Table 6 . In this situation they may fail to produce a reasonable result. Logician is able to cope with unfamiliar words/phrases by exploiting the context information using deep RNN network with the help of copy mechanism.",Extraction Error Analysis of Logician,"We do a preliminary analysis for the results produced by the Logician model. The most notable problem is that it is unable to recall some facts for long or complex sentences. The last case in Table 6 exhibits such situation, where the fact UTF8gbsn(蔡竞,ISA,经济学博士)((Cai Jing, ISA, Ph. D. in economics) in English) is not recalled. This phenomenon indicates that the coverage mechanism may lose effectiveness in this situation. The second class of error is incomplete extraction, as exhibited in the third case in Table 6 . Due to the incomplete extraction, the left parts may interfere the generation of other facts, and result in nonsense results, which is the third class of error. We believe it is helpful to introduce extra rewards into the learning procedure of Logician to overcome these problems. For example, the reward could be the amount of remaining information left after the fact extraction, or the completeness of extracted facts. Developing such rewards and reinforcement learning algorithms using those rewards to refine Logician belongs to our future works.",Knowledge Expressions,"Tuple is the most common knowledge expression format for OIE systems to express n-ary relation between subject and objects. Beyond such information, ClausIE BIBREF36 extracts extra information in the tuples: a complement, and one or more adverbials, and OLLIE BIBREF6 extracts additional context information. SAOKE is able to express n-ary relations, and can be easily extended to support the knowledge extracted by ClausIE, but needs to be redesigned to support context information, which belongs to our future work. However, there is a fundamental difference between SAOKE and tuples in traditional OIE systems. In traditional OIE systems, knowledge expression is generally not directly related to the extraction algorithm. It is a tool to reorganize the extracted knowledge into a form for further easy reading/storing/computing. However, SAOKE is proposed to act as the direct learning target of the end-to-end Logician model. In such end-to-end framework, knowledge representation is the core of the system, which decides what information would be extracted and how complex the learning algorithm would be. To our knowledge, SAOKE is the first attempt to design a knowledge expression friendly to the end-to-end learning algorithm for OIE tasks. Efforts are still needed to make SAOKE more powerful in order to express more complex knowledge such as events.",Relation Extraction,"Relation extraction is the task to identify semantic connections between entities. Major existing relation extraction algorithms can be classified into two classes: closed-domain and open-domain. Closed-domain algorithms are learnt to identify a fixed and finite set of relations, using supervised methods BIBREF37 , BIBREF38 , BIBREF39 , BIBREF40 or weakly supervised methods BIBREF1 , BIBREF41 , while the open-domain algorithms, represented by aforementioned OIE systems, discover open-domain relations without predefined schema. Beyond these two classes, methods like universal schema BIBREF42 are able to learn from both data with fixed and finite set of relations, such as relations in Freebase, and data with open-domain surface relations produced by heuristic patterns or OIE systems. Logician can be used as an OIE system to extract open-domain relation between entities, and act as sub-systems for knowledge base construction/completion with the help of schema mapping BIBREF43 . Compared with existing OIE systems, which are pattern-based or self-supervised by labeling samples using patterns BIBREF13 , to our knowledge Logician is the first model trained in a supervised end-to-end approach for OIE task, which has exhibited powerful ability in our experiments. There are some neural based end-to-end systems BIBREF39 , BIBREF40 , BIBREF41 proposed for relation extraction, but they all aim to solve the close-domain problem. However, Logician is not limited to relation extraction task. First, Logician extracts more information beyond relations. Second, Logician focuses on examining how natural languages express facts BIBREF5 , and producing helpful intermediate structures for high level tasks.",Language to Logic ,"Efforts had been made to map natural language sentences into logical form. Some approaches such as BIBREF44 , BIBREF45 , BIBREF46 , BIBREF47 learn the mapping under the supervision of manually labeled logical forms, while others BIBREF48 , BIBREF49 are indirectly supervised by distant information, system rewards, etc. However, all previous works rely on a pre-defined, domain specific logical system, which limits their ability to learn facts out of the pre-defined logical system. Logician can be viewed as a system that maps language to natural logic, in which the majority of information is expressed by natural phrase. Other than systems mentioned above which aim at execution using the logical form, Logician focuses on understanding how the fact and logic are expressed by natural language. Further mapping to domain-specific logical system or even executor can be built on the basis of Logician's output, and we believe that, with the help of Logician, the work would be easier and the overall performance of the system may be improved.",What open relation extraction tasks did they experiment on?,cdf65116a7c50edddcb115e9afd86b2b6accb8ad,,familiar,no,information extraction,50d8b4a941c26b89482c94ab324b5a274f9ced66,False,,,"We first measure the utility of various components in Logician to select the optimal model, and then compare this model to the state-of-the-art methods in four types of information extraction tasks: verb/preposition-based relation, nominal attribute, descriptive phrase and hyponymy relation. The SAOKE data set is split into training set, validating set and testing set with ratios of 80%, 10%, 10%, respectively. For all algorithms involved in the experiments, the training set can be used to train the model, the validating set can be used to select an optimal model, and the testing set is used to evaluate the performance.","We first measure the utility of various components in Logician to select the optimal model, and then compare this model to the state-of-the-art methods in four types of information extraction tasks: verb/preposition-based relation, nominal attribute, descriptive phrase and hyponymy relation.",b356c8dda1b4dbc3c76273f601f952e8ae1c74e0,f320efb1fbb744616e420aaf8da0f9622b75b2ed,,,,,,,,,How is Logician different from traditional seq2seq models?,c8031c1629d270dedc3b0c16dcb7410524ff1bab,,familiar,no,information extraction,50d8b4a941c26b89482c94ab324b5a274f9ced66,False,,,"Logician is a supervised end-to-end neural learning algorithm which transforms natural language sentences into facts in the SAOKE form. Logician is trained under the attention-based sequence-to-sequence paradigm, with three mechanisms: restricted copy mechanism to ensure literally honestness, coverage mechanism to alleviate the under extraction and over extraction problem, and gated dependency attention mechanism to incorporate dependency information. Experimental results on four types of open information extraction tasks reveal the superiority of the Logician algorithm."," Logician is trained under the attention-based sequence-to-sequence paradigm, with three mechanisms: restricted copy mechanism to ensure literally honestness, coverage mechanism to alleviate the under extraction and over extraction problem, and gated dependency attention mechanism to incorporate dependency information. ",1dabe8cfa60ed4a5a9d045cf21ccf20ff0ace3ec,f320efb1fbb744616e420aaf8da0f9622b75b2ed,What's the size of the previous largest OpenIE dataset?,8c0e8a312b85c4ffdffabeef0d29df1ef8ff7fb2,,familiar,no,information extraction,50d8b4a941c26b89482c94ab324b5a274f9ced66,False,,,"Prior to the SAOKE data set, an annotated data set for OIE tasks with 3,200 sentences in 2 domains was released in BIBREF20 to evaluate OIE algorithms, in which the data set was said BIBREF20 “13 times larger than the previous largest annotated Open IE corpus”. The SAOKE data set is 16 times larger than the data set in BIBREF20 . To the best of our knowledge, SAOKE data set is the largest publicly available human labeled data set for OIE tasks. Furthermore, the data set released in BIBREF20 was generated from a QA-SRL data set BIBREF21 , which indicates that the data set only contains facts that can be discovered by SRL (Semantic Role Labeling) algorithms, and thus is biased, whereas the SAOKE data set is not biased to an algorithm. Finally, the SAOKE data set contains sentences and facts from a large number of domains.","Prior to the SAOKE data set, an annotated data set for OIE tasks with 3,200 sentences in 2 domains was released in BIBREF20 to evaluate OIE algorithms, in which the data set was said BIBREF20 “13 times larger than the previous largest annotated Open IE corpus”.",dc81816a4447546e52f5646f4598ada0d676a99f,f320efb1fbb744616e420aaf8da0f9622b75b2ed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3-Table1-1.png,Table 1: Expected facts of an example sentence.,3-Table2-1.png,Table 2: Example sentence and corresponding wrong/correct facts.,3-Table3-1.png,Table 3: Ratios of facts of each type in SAOKE data set.,6-Table4-1.png,Table 4: Analysis of Components involved in Logician.,6-Figure1-1.png,Figure 1: Performance comparison on four types of information extraction tasks.,7-Table5-1.png,Table 5: Human evaluation results on four types of information extraction tasks.,Facts to Language,"The problem of generating sentences from a set of facts has attracted a lot of attentions BIBREF50 , BIBREF51 , BIBREF52 , BIBREF53 . These models focus on facts with a predefined schema from a specific problem domain, such as people biographies and basketball game records, but could not work on open domain. The SAOKE data set provides an opportunity to extend the ability of these models into open domain.",Duality between Knowledge and Language,"As mentioned in above sections, the SAOKE data set provides examples of dual mapping between facts and sentences. Duality has been verified to be useful to promote the performance of agents in many NLP tasks, such as back-and-forth translation BIBREF54 , and question-answering BIBREF55 . It is a promising approach to use the duality between knowledge and language to improve the performance of Logician.",Conclusion,"In this paper, we consider the open information extraction (OIE) problem for a variety of types of facts in a unified view. Our solution consists of three components: SAOKE format, SAOKE data set, and Logician. SAOKE form is designed to express different types of facts in a unified manner. We publicly release the largest manually labeled data set for OIE tasks in SAOKE form. Using the labeled SAOKE data set, we train an end-to-end neural sequence-to-sequence model, called Logician, to transform sentences in natural language into facts. The experiments reveal the superiority of Logician in various open-domain information extraction tasks to the state-of-the-art algorithms. Regarding future work, there are at least three promising directions. Firstly, one can investigate knowledge expression methods to extend SAOKE to express more complex knowledge, for tasks such as event extraction. Secondly, one can develop novel learning strategies to improve the performance of Logician and adapt the algorithm to the extended future version of SAOKE. Thirdly, one can extend SAOKE format and Logician algorithm in other languages.",,,"restricted copy mechanism to ensure literally honestness, coverage mechanism to alleviate the under extraction and over extraction problem, and gated dependency attention mechanism to incorporate dependency information","3,200 sentences",8-Table6-1.png,Table 6: Example of extraction from a sentence for each task.,,,,,,,,,,"verb/preposition-based relation, nominal attribute, descriptive phrase and hyponymy relation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
#MeToo on Campus: Studying College Sexual Assault at Scale Using Data Reported on Social Media,"Recently, the emergence of the #MeToo trend on social media has empowered thousands of people to share their own sexual harassment experiences. This viral trend, in conjunction with the massive personal information and content available on Twitter, presents a promising opportunity to extract data driven insights to complement the ongoing survey based studies about sexual harassment in college. In this paper, we analyze the influence of the #MeToo trend on a pool of college followers. The results show that the majority of topics embedded in those #MeToo tweets detail sexual harassment stories, and there exists a significant correlation between the prevalence of this trend and official reports on several major geographical regions. Furthermore, we discover the outstanding sentiments of the #MeToo tweets using deep semantic meaning representations and their implications on the affected users experiencing different types of sexual harassment. We hope this study can raise further awareness regarding sexual misconduct in academia.",Introduction,"Sexual harassment is defined as ""bullying or coercion of a sexual nature, or the unwelcome or inappropriate promise of rewards in exchange for sexual favors."" In fact, it is an ongoing problem in the U.S., especially within the higher education community. According to the National Sexual Violence Resource Center (NSRVC), one in five women and one in sixteen men are sexually assaulted while they are attending college. In addition to the prevalence of campus sexual harassment, it has been shown to have detrimental effects on student's well-being, including health-related disorders and psychological distress BIBREF0, BIBREF1. However, these studies on college sexual misconduct usually collect data based on questionnaires from a small sample of the college population, which might not be sufficiently substantial to capture the big picture of sexual harassment risk of the entire student body. Alternatively, social media opens up new opportunities to gather a larger and more comprehensive amount of data and mitigate the risk of false or inaccurate narratives from the studied subjects. On October 15 of 2017, prominent Hollywood actress Alyssa Milano, by accusing Oscar-winning film producer, Harvey Weinstein, for multiple sexual impropriety attempts on herself and many other women in the film industry, ignited the ""MeToo"" trend on social media that called for women and men to share their own sexual harassment experience. According to CNN, over 1.7 million users had used the hash-tag in 85 countries. Benefiting from the tremendous amount of data supplied by this trend and the existing state-of-the-art semantic parser and generative statistical models, we propose a new approach to characterizing sexual harassment by mining the tweets from college users with the hash-tag #metoo on Twitter. Our main contributions are several folds. We investigate campus sexual harassment using a big-data approach by collecting data from Twitter. We employ traditional topic modeling and linear regression methods on a new dataset to highlight patterns of the ongoing troubling social behaviors at both institutional and individual levels. We propose a novel approach to combining domain-general deep semantic parsing and sentiment analysis to dissect personal narratives.",Related Work,"Previous works for sexual misconduct in academia and workplace dated back to last few decades, when researchers studied the existence, as well as psychometric and demographic insights regarding this social issue, based on survey and official data BIBREF2, BIBREF3, BIBREF4. However, these methods of gathering data are limited in scale and might be influenced by the psychological and cognitive tendencies of respondents not to provide faithful answers BIBREF5. The ubiquity of social media has motivated various research on widely-debated social topics such as gang violence, hate code, or presidential election using Twitter data BIBREF6, BIBREF7, BIBREF8, BIBREF9. Recently, researchers have taken the earliest steps to understand sexual harassment using textual data on Twitter. Using machine learning techniques, Modrek and Chakalov (2019) built predictive models for the identification and categorization of lexical items pertaining to sexual abuse, while analysis on semantic contents remains untouched BIBREF10. Despite the absence of Twitter data, Field et al. (2019) did a study more related to ours as they approach to the subject geared more towards linguistics tasks such as event, entity and sentiment analysis BIBREF11. Their work on event-entity extraction and contextual sentiment analysis has provided many useful insights, which enable us to tap into the potential of our Twitter dataset. There are several novelties in our approach to the #MeToo problem. Our target population is restricted to college followers on Twitter, with the goal to explore people's sentiment towards the sexual harassment they experienced and its implication on the society's awareness and perception of the issue. Moreover, the focus on the sexual harassment reality in colleges calls for an analysis on the metadata of this demographics to reveal meaningful knowledge of their distinctive characteristics BIBREF12.",Dataset ::: Data Collection,"In this study, we limit the sample size to the followers identified as English speakers in the U.S. News Top 200 National Universities. We utilize the Jefferson-Henrique script, a web scraper designed for Twitter to retrieve a total of over 300,000 #MeToo tweets from October 15th, when Alyssa Milano posted the inceptive #MeToo tweet, to November 15th of 2017 to cover a period of a month when the trend was on the rise and attracting mass concerns. Since the lists of the followers of the studied colleges might overlap and many Twitter users tend to reiterate other's tweets, simply putting all the data collected together could create a major redundancy problem. We extract unique users and tweets from the combined result set to generate a dataset of about 60,000 unique tweets, pertaining to 51,104 unique users.",Dataset ::: Text Preprocessing,"We pre-process the Twitter textual data to ensure that its lexical items are to a high degree lexically comparable to those of natural language. This is done by performing sentiment-aware tokenization, spell correction, word normalization, segmentation (for splitting hashtags) and annotation. The implemented tokenizer with SentiWordnet corpus BIBREF13 is able to avoid splitting expressions or words that should be kept intact (as one token), and identify most emoticons, emojis, expressions such as dates, currencies, acronyms, censored words (e.g. s**t), etc. In addition, we perform modifications on the extracted tokens. For spelling correction, we compose a dictionary for the most commonly seen abbreviations, censored words and elongated words (for emphasis, e.g. ""reallyyy""). The Viterbi algorithm is used for word segmentation, with word statistics (unigrams and bigrams) computed from the NLTK English Corpus to obtain the most probable segmentation posteriors from the unigrams and bigrams probabilities. Moreover, all texts are lower-cased, and URLs, emails and mentioned usernames are replaced with common designated tags so that they would not need to be annotated by the semantic parser.",Dataset ::: College Metadata,"The meta-statistics on the college demographics regarding enrollment, geographical location, private/public categorization and male-to-female ratio are obtained. Furthermore, we acquire the Campus Safety and Security Survey dataset from the official U.S. Department of Education website and use rape-related cases statistic as an attribute to complete the data for our linear regression model. The number of such reported cases by these 200 colleges in 2015 amounts to 2,939.",Methodology ::: Regression Analysis,"We examine other features regarding the characteristics of the studied colleges, which might be significant factors of sexual harassment. Four factual attributes pertaining to the 200 colleges are extracted from the U.S. News Statistics, which consists of Undergraduate Enrollment, Male/Female Ratio, Private/Public, and Region (Northeast, South, West, and Midwest). We also use the normalized rape-related cases count (number of cases reported per student enrolled) from the stated government resource as another attribute to examine the proximity of our dataset to the official one. This feature vector is then fitted in a linear regression to predict the normalized #metoo users count (number of unique users who posted #MeToo tweets per student enrolled) for each individual college.",Methodology ::: Labeling Sexual Harassment,"Per our topic modeling results, we decide to look deeper into the narratives of #MeToo users who reveal their personal stories. We examine 6,760 tweets from the most relevant topic of our LDA model, and categorize them based on the following metrics: harassment types (verbal, physical, and visual abuse) and context (peer-to-peer, school employee or work employer, and third-parties). These labels are based on definitions by the U.S. Dept. of Education BIBREF14.",Methodology ::: Topic Modeling on #MeToo Tweets,"In order to understand the latent topics of those #MeToo tweets for college followers, we first utilize Latent Dirichlet Allocation (LDA) to label universal topics demonstrated by the users. We determine the optimal topic number by selecting the one with the highest coherence score. Since certain words frequently appear in those #MeToo tweets (e.g., sexual harassment, men, women, story, etc.), we transform our corpus using TF-IDF, a term-weighting scheme that discounts the influence of common terms.",Methodology ::: Semantic Parsing with TRIPS,"Learning deep meaning representations, which enables the preservation of rich semantic content of entities, meaning ambiguity resolution and partial relational understanding of texts, is one of the challenges that the TRIPS parser BIBREF15 is tasked to tackle. This kind of meaning is represented by TRIPS Logical Form (LF), which is a graph-based representation that serves as the interface between structural analysis of text (i.e., parse) and the subsequent use of the information to produce knowledge. The LF graphs are obtained by using the semantic types, roles and rule-based relations defined by the TRIPS Ontology BIBREF15 at its core in combination with various linguistic techniques such as Dialogue Act Identification, Dependency Parsing, Named Entity Recognition, and Crowd-sourced Lexicon (Wordnet). Figure 1 illustrates an example of the TRIPS LF graph depicting the meaning of the sentence ""He harassed me,"" where the event described though the speech act TELL (i.e. telling a story) is the verb predicate HARASS, which is caused by the agent HE and influences the affected (also called ""theme"" in traditional literature) ME. As seen from the previously discussed example, the action-agent-affected relational structure is applicable to even the simplest sentences used for storytelling, and it is in fact very common for humans to encounter in both spoken and written languages. This makes it well suited for event extraction from short texts, useful for analyzing tweets with Twitter's 280 character limit. Therefore, our implementation of TRIPS parser is particularly tailored for identifying the verb predicates in tweets and their corresponding agent-affected arguments (with $82.4\%$ F1 score), so that we can have a solid ground for further analysis.",Methodology ::: Connotation Frames and Sentiment Analysis,"In order to develop an interpretable analysis that focuses on sentiment scores pertaining to the entities and events mentioned in the narratives, as well as the perceptions of readers on such events, we draw from existing literature on connotation frames: a set of verbs annotated according to what they imply about semantically dependent entities. Connotation frames, first introduced by Rashkin, Singh, and Choi (2016), provides a framework for analyzing nuanced dimensions in text by combining polarity annotations with frame semantics (Fillmore 1982). More specifically, verbs are annotated across various dimensions and perspectives so that a verb might elicit a positive sentiment for its subject (i.e. sympathy) but imply a negative effect for its object. We target the sentiments towards the entities and verb predicates through a pre-collected set of 950 verbs that have been annotated for these traits, which can be more clearly demonstrated through the example ""He harassed me."": ${Sentiment(\textrm {verb}) -}$: something negative happened to the writer. $Sentiment(\textrm {affected}) -$: the writer (affected) most likely feels negative about the event. $Perspective(\textrm {affected} \rightarrow \textrm {agent})-$: the writer most likely has negative feelings towards the agent as a result of the event. $Perspective(\textrm {reader} \rightarrow \textrm {affected})-$: the reader most likely view the agent as the antagonist. $Perspective(\textrm {affected} \rightarrow \textrm {affected})+$: the reader most likely feels sympathetic towards the writer. In addition to extracting sentiment scores from the pre-annotated corpus, we also need to predict sentiment scores of unknown verbs. To achieve this task, we rely on the 200-dimensional GloVe word embeddings BIBREF16, pretrained on their Twitter dataset, to compute the scores of the nearest neighboring synonyms contained in the annotated verb set and normalize their weighted sum to get the resulting sentiment (Equation 1). where $\mathcal {I}=\mathbf {1_{w \in \mathcal {A}}}$ is the indicator function for whether verb predicate $w$ is in the annotation set $\mathcal {A}$, $\gamma (w)$ is the set of nearest neighbors $e$'s of verb $w$. Because our predictive model computes event-entity sentiment scores and generates verb predicate knowledge simultaneously, it is sensitive to data initialization. Therefore, we train the model iteratively on a number of random initialization to achieve the best results.",Experimental Results ::: Topical Themes of #MeToo Tweets,"The results of LDA on #MeToo tweets of college users (Table 1) fall into the same pattern as the research of Modrek and Chakalov (2019), which suggests that a large portion of #MeToo tweets on Twitter focuses on sharing personal traumatic stories about sexual harassment BIBREF10. In fact, in our top 5 topics, Topics 1 and 5 mainly depict gruesome stories and childhood or college time experience. This finding seems to support the validity of the Twitter sample of Modrek and Chakalov (2019), where 11% discloses personal sexual harassment memories and 5.8% of them was in formative years BIBREF10. These users also shows multiple emotions toward this movement, such as compassion (topic 2), determination (topic 3), and hope (topic 4). We will further examine the emotion features in the latter results.",Experimental Results ::: Regression Result,"Observing the results of the linear regression in Table 2, we find the normalized governmental reported cases count and regional feature to be statistically significant on the sexual harassment rate in the Twitter data ($p-value<0.05$). Specifically, the change in the number of reported cases constitutes a considerable change in the number of #MeToo users on Twitter as p-value is extremely small at $5.7e-13$. This corresponds to the research by Napolitano (2014) regarding the ""Yes means yes"" movement in higher education institutes in recent years, as even with some limitations and inconsistency, the sexual assault reporting system is gradually becoming more rigorous BIBREF17. Meanwhile, attending colleges in the Northeast, West and South regions increases the possibility of posting about sexual harassment (positive coefficients), over the Midwest region. This finding is interesting and warrants further scrutiny.",Experimental Results ::: Event-Entity Sentiment Analysis,"We discover that approximately half of users who detailed their sexual harassment experiences with the #MeToo hashtag suffered from physical aggression. Also, more than half of them claimed to encounter the perpetrators outside the college and work environment. The sentimental score for the affected entities and the verb of cases pertaining to faculty are strictly negative, suggesting that academic personnel's actions might be described as more damaging to the students' mental health. This finding resonates a recent research by Cantapulo et al. regarding the potential hazard of sexual harassment conducts by university faculties using data from federal investigation and relevant social science literature BIBREF18. Furthermore, many in this group tend to mention their respective age, typically between 5 and 20 (24% of the studied subset). This observation reveals an alarming number of child and teenager sexual abuse, indicating that although college students are not as prone to sexual harassment from their peers and teachers, they might still be traumatized by their childhood experiences. In addition, although verbal abuse experiences accounts for a large proportion of the tweets, it is challenging to gain sentiment insights into them, as the majority of them contains insinuations and sarcasms regarding sexual harassment. This explains why the sentiment scores of the events and entities are very close to neutral.",Experimental Results ::: Limitations and Ethical Implications,"Our dataset is taken from only a sample of a specific set of colleges, and different samples might yield different results. Our method of identifying college students is simple, and might not reflect the whole student population. Furthermore, the majority of posts on Twitter are short texts (under 50 words). This factor, according to previous research, might hamper the performance of the LDA results, despite the use of the TF-IDF scheme BIBREF19. Furthermore, while the main goal of this paper is to shed lights to the ongoing problems in the academia and contribute to the future sociological study using big data analysis, our dataset might be misused for detrimental purposes. Also, data regarding sexual harassment is sensitive in nature, and might have unanticipated effects on those addressed users.",Conclusion,"In this study, we discover a novel correlation between the number of college users who participate in the #MeToo movement and the number of official reported cases from the government data. This is a positive sign suggesting that the higher education system is moving into a right direction to effectively utilize Title IV, a portion of the Education Amendments Act of 1972, which requests colleges to submit their sexual misconduct reports to the officials and protect the victims. In addition, we capture several geographic and behavioral characteristics of the #MeToo users related to sexual assault such as region, reaction and narrative content following the trend, as well as sentiment and social interactions, some of which are supported by various literature on sexual harassment. Importantly, our semantic analysis reveals interesting patterns of the assaulting cases. We believe our methodologies on defining these #MeToo users and their features will be applicable to further studies on this and other alarming social issues. Furthermore, we find that the social media-driven approach is highly useful in facilitating crime-related sociology research on a large scale and spectrum. Moreover, since social networks appeal to a broad audience, especially those outside academia, studies using these resources are highly useful for raising awareness in the community on concurrent social problems. Last but not least, many other aspects of the text data from social media, which could provide many interesting insights on sexual harassment, remain largely untouched. In the future, we intend to explore more sophisticated language features and implement more supervised models with advanced neural network parsing and classification. We believe that with our current dataset, an extension to take advantage of cutting-edge linguistic techniques will be the next step to address the previously unanswered questions and uncover deeper meanings of the tweets on sexual harassment.",,,,,,,,,,,,,Which major geographical regions are studied?,5dc1aca619323ea0d4717d1f825606b2b7c21f01,five,unfamiliar,no,twitter,e8b24c3133e0bec0a6465e1f13acd3a5ed816b37,False,,"Northeast U.S, South U.S., West U.S. and Midwest U.S.","We examine other features regarding the characteristics of the studied colleges, which might be significant factors of sexual harassment. Four factual attributes pertaining to the 200 colleges are extracted from the U.S. News Statistics, which consists of Undergraduate Enrollment, Male/Female Ratio, Private/Public, and Region (Northeast, South, West, and Midwest). We also use the normalized rape-related cases count (number of cases reported per student enrolled) from the stated government resource as another attribute to examine the proximity of our dataset to the official one. This feature vector is then fitted in a linear regression to predict the normalized #metoo users count (number of unique users who posted #MeToo tweets per student enrolled) for each individual college.","Four factual attributes pertaining to the 200 colleges are extracted from the U.S. News Statistics, which consists of Undergraduate Enrollment, Male/Female Ratio, Private/Public, and Region (Northeast, South, West, and Midwest). ",5a280f6f2ee2fb369c1b5ff5a59c638763efefd6,c1018a31c3272ce74964a3280069f62f314a1a58,,,,,,,,,How strong is the correlation between the prevalence of the #MeToo movement and official reports [of sexual harassment]?,dd5c9a370652f6550b4fd13e2ac317eaf90973a8,five,unfamiliar,no,twitter,e8b24c3133e0bec0a6465e1f13acd3a5ed816b37,False,,0.9098 correlation,"FLOAT SELECTED: Table 2: Linear regression results. We examine other features regarding the characteristics of the studied colleges, which might be significant factors of sexual harassment. Four factual attributes pertaining to the 200 colleges are extracted from the U.S. News Statistics, which consists of Undergraduate Enrollment, Male/Female Ratio, Private/Public, and Region (Northeast, South, West, and Midwest). We also use the normalized rape-related cases count (number of cases reported per student enrolled) from the stated government resource as another attribute to examine the proximity of our dataset to the official one. This feature vector is then fitted in a linear regression to predict the normalized #metoo users count (number of unique users who posted #MeToo tweets per student enrolled) for each individual college.",FLOAT SELECTED: Table 2: Linear regression results. We also use the normalized rape-related cases count (number of cases reported per student enrolled) from the stated government resource as another attribute to examine the proximity of our dataset to the official one. This feature vector is then fitted in a linear regression to predict the normalized #metoo users count (number of unique users who posted #MeToo tweets per student enrolled) for each individual college.,757e950d7571ea1e8002f26ac4cd4dbeacb0d2e2,c1018a31c3272ce74964a3280069f62f314a1a58,How are the topics embedded in the #MeToo tweets extracted?,39c78924df095c92e058ffa5a779de597e8c43f4,five,unfamiliar,no,twitter,e8b24c3133e0bec0a6465e1f13acd3a5ed816b37,False,,Using Latent Dirichlet Allocation on TF-IDF transformed from the corpus,"In order to understand the latent topics of those #MeToo tweets for college followers, we first utilize Latent Dirichlet Allocation (LDA) to label universal topics demonstrated by the users. We determine the optimal topic number by selecting the one with the highest coherence score. Since certain words frequently appear in those #MeToo tweets (e.g., sexual harassment, men, women, story, etc.), we transform our corpus using TF-IDF, a term-weighting scheme that discounts the influence of common terms.","In order to understand the latent topics of those #MeToo tweets for college followers, we first utilize Latent Dirichlet Allocation (LDA) to label universal topics demonstrated by the users. Since certain words frequently appear in those #MeToo tweets (e.g., sexual harassment, men, women, story, etc.), we transform our corpus using TF-IDF, a term-weighting scheme that discounts the influence of common terms.",98539e4210985c0822dfbc0d0d9368a4e68d383f,c1018a31c3272ce74964a3280069f62f314a1a58,,,,,,,,How many tweets are explored in this paper?,a95188a0f35d3cb3ca70ae1527d57ac61710afa3,,,,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,,"In this study, we limit the sample size to the followers identified as English speakers in the U.S. News Top 200 National Universities. We utilize the Jefferson-Henrique script, a web scraper designed for Twitter to retrieve a total of over 300,000 #MeToo tweets from October 15th, when Alyssa Milano posted the inceptive #MeToo tweet, to November 15th of 2017 to cover a period of a month when the trend was on the rise and attracting mass concerns. Since the lists of the followers of the studied colleges might overlap and many Twitter users tend to reiterate other's tweets, simply putting all the data collected together could create a major redundancy problem. We extract unique users and tweets from the combined result set to generate a dataset of about 60,000 unique tweets, pertaining to 51,104 unique users.","We extract unique users and tweets from the combined result set to generate a dataset of about 60,000 unique tweets, pertaining to 51,104 unique users.",9588e59be6b2ecf4cbc538327f00b6c4f31bdd1c,c1018a31c3272ce74964a3280069f62f314a1a58,,,,,,,,Which geographical regions correlate to the trend?,a1557ec0f3deb1e4cd1e68f4880dcecda55656dd,,,,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,,"Northeast U.S., West U.S. and South U.S.","Observing the results of the linear regression in Table 2, we find the normalized governmental reported cases count and regional feature to be statistically significant on the sexual harassment rate in the Twitter data ($p-value<0.05$). Specifically, the change in the number of reported cases constitutes a considerable change in the number of #MeToo users on Twitter as p-value is extremely small at $5.7e-13$. This corresponds to the research by Napolitano (2014) regarding the ""Yes means yes"" movement in higher education institutes in recent years, as even with some limitations and inconsistency, the sexual assault reporting system is gradually becoming more rigorous BIBREF17. Meanwhile, attending colleges in the Northeast, West and South regions increases the possibility of posting about sexual harassment (positive coefficients), over the Midwest region. This finding is interesting and warrants further scrutiny.","Meanwhile, attending colleges in the Northeast, West and South regions increases the possibility of posting about sexual harassment (positive coefficients), over the Midwest region. ",5fc94f529a5e1285bdc11328ada2235724b988ba,c1018a31c3272ce74964a3280069f62f314a1a58,How many followers did they analyze?,096f5c59f43f49cab1ef37126341c78f272c0e26,,,,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,,"In this study, we limit the sample size to the followers identified as English speakers in the U.S. News Top 200 National Universities. We utilize the Jefferson-Henrique script, a web scraper designed for Twitter to retrieve a total of over 300,000 #MeToo tweets from October 15th, when Alyssa Milano posted the inceptive #MeToo tweet, to November 15th of 2017 to cover a period of a month when the trend was on the rise and attracting mass concerns. Since the lists of the followers of the studied colleges might overlap and many Twitter users tend to reiterate other's tweets, simply putting all the data collected together could create a major redundancy problem. We extract unique users and tweets from the combined result set to generate a dataset of about 60,000 unique tweets, pertaining to 51,104 unique users.","We extract unique users and tweets from the combined result set to generate a dataset of about 60,000 unique tweets, pertaining to 51,104 unique users.",29250e7e5d792b0bcf905985619d7a451f8ee23d,c1018a31c3272ce74964a3280069f62f314a1a58,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3-Figure1-1.png,"Figure 1: The meaning representation of the example sentence ”He harassed me.” in TRIPS LF, the Ontology types of the words are indicated by ”:*” and the role-argument relations between them are denoted by named arcs.",4-Table1-1.png,"Table 1: Top 5 topics from all #MeToo Tweets from 51,104 college followers.",4-Table2-1.png,Table 2: Linear regression results.,4-Table3-1.png,Table 3: Semantic sentiment results.,,,,,,,,,,,,,,,,,,,,,,,,,"51,104",,,,,,,,,,"60,000 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Learning Open Information Extraction of Implicit Relations from Reading Comprehension Datasets,"The relationship between two entities in a sentence is often implied by word order and common sense, rather than an explicit predicate. For example, it is evident that""Fed chair Powell indicates rate hike""implies (Powell, is a, Fed chair) and (Powell, works for, Fed). These tuples are just as significant as the explicit-predicate tuple (Powell, indicates, rate hike), but have much lower recall under traditional Open Information Extraction (OpenIE) systems. Implicit tuples are our term for this type of extraction where the relation is not present in the input sentence. There is very little OpenIE training data available relative to other NLP tasks and none focused on implicit relations. We develop an open source, parse-based tool for converting large reading comprehension datasets to OpenIE datasets and release a dataset 35x larger than previously available by sentence count. A baseline neural model trained on this data outperforms previous methods on the implicit extraction task.",Introduction,"Open Information Extraction (OpenIE) is the NLP task of generating (subject, relation, object) tuples from unstructured text e.g. “Fed chair Powell indicates rate hike” outputs (Powell, indicates, rate hike). The modifier open is used to contrast IE research in which the relation belongs to a fixed set. OpenIE has been shown to be useful for several downstream applications such as knowledge base construction BIBREF0 , textual entailment BIBREF1 , and other natural language understanding tasks BIBREF2 . In our previous example an extraction was missing: (Powell, works for, Fed). Implicit extractions are our term for this type of tuple where the relation (“works for” in this example) is not contained in the input sentence. In both colloquial and formal language, many relations are evident without being explicitly stated. However, despite their pervasiveness, there has not been prior work targeted at implicit predicates in the general case. Implicit information extractors for some specific implicit relations such as noun-mediated relations, numerical relations, and others BIBREF3 , BIBREF4 , BIBREF5 have been researched. While specific extractors are important, there are a multiplicity of implicit relation types and it would be intractable to categorize and design extractors for each one. Past general OpenIE systems have been plagued by low recall on implicit relations BIBREF6 . In OpenIE's original application – web-scale knowledge base construction – this low recall is tolerable because facts are often restated in many ways BIBREF7 . However, in downstream NLU applications an implied relationship may be significant and only stated once BIBREF2 . The contribution of this work is twofold. In Section 4, we introduce our parse-based conversion tool and convert two large reading comprehension datasets into implicit OpenIE datasets. In Section 5 and 6, we train a simple neural model on this data and compare to previous systems on precision-recall curves using a new gold test set for implicit tuples.",Problem Statement,"We suggest that OpenIE research focus on producing implicit relations where the predicate is not contained in the input span. Formally, we define implicit tuples as (subject, relation, object) tuples that: These “implicit” or “common sense” tuples reproduce the relation explicitly, which may be important for downstream NLU applications using OpenIE as an intermediate schema. For example, in Figure 1, the input sentence tells us that the Norsemen swore fealty to Charles III under “their leader Rollo”. From this our model outputs (The Norse leader, was, Rollo) despite the relation never being contained in the input sentence. Our definition of implicit tuples corresponds to the “frequently occurring recall errors” identified in previous OpenIE systems BIBREF6 : noun-mediated, sentence-level inference, long sentence, nominalization, noisy informal, and PP-attachment. We use the term implicit tuple to collectively refer to all of these situations where the predicate is absent or very obfuscated.",Traditional Methods,"Due to space constraints, see Niklaus et al. Survey for a survey of of non-neural methods. Of these, several works have focused on pattern-based implicit information extractors for noun-mediated relations, numerical relations, and others BIBREF3 , BIBREF4 , BIBREF5 . In this work we compare to OpenIE-4 , ClausIE BIBREF8 , ReVerb BIBREF9 , OLLIE BIBREF10 , Stanford OpenIE BIBREF11 , and PropS BIBREF12 .",Neural Network Methods,"Stanovsky et al. SupervisedOIE frame OpenIE as a BIO-tagging problem and train an LSTM to tag an input sentence. Tuples can be derived from the tagger, input, and BIO CFG parser. This method outperforms traditional systems, though the tagging scheme inherently constrains the relations to be part of the input sentence, prohibiting implicit relation extraction. Cui et al. NeuralOpenIE bootstrap (sentence, tuple) pairs from OpenIE-4 and train a standard seq2seq with attention model using OpenNMT-py BIBREF13 . The system is inhibited by its synthetic training data which is bootstrapped from a rule-based system.",Dataset Conversion Methods,"Due to the lack of large datasets for OpenIE, previous works have focused on generating datasets from other tasks. These have included QA-SRL datasets BIBREF14 and QAMR datasets BIBREF6 . These methods are limited by the size of the source training data which are an order of magnitude smaller than existing reading comprehension datasets.",Dataset Conversion Method,"Span-based Question-Answer datasets are a type of reading comprehension dataset where each entry consists of a short passage, a question about the passage, and an answer contained in the passage. The datasets used in this work are the Stanford Question Answering Dataset (SQuADv1.1) BIBREF15 and NewsQA BIBREF16 . These QA datasets were built to require reasoning beyond simple pattern-recognition, which is exactly what we desire for implicit OpenIE. Our goal is to convert the QA schema to OpenIE, as was successfully done for NLI BIBREF17 . The repository of software and converted datasets is available at http://toAppear.",QA Pairs to OpenIE Tuples,"We started by examining SQuAD and noticing that each answer, $A$ , corresponds to either the subject, relation, or object in an implicit extraction. The corresponding question, $Q$ , contains the other two parts, i.e. either the (1) subject and relation, (2) subject and object, or (3) relation and object. Which two pieces the question contains depends on the type of question. For example, “who was... factoid” type questions contain the relation (“was”) and object (the factoid), which means that the answer is the subject. In Figure 1, “Who was Rollo” is recognized as a who was question and caught by the whoParse() parser. Similarly, a question in the form of “When did person do action” expresses a subject and a relation, with the answer containing the object. For example, “When did Einstein emigrate to the US“ and answer 1933, would convert to (Einstein, when did emigrate to the US, 1933). In cases like these the relation might not be grammatically ideal, but nevertheless captures the meaning of the input sentence. In order to identify generic patterns, we build our parse-based tool on top of a dependency parser BIBREF18 . It uses fifteen rules, with the proper rule being identified and run based on the question type. The rule then uses its pre-specified pattern to parse the input QA pair and output a tuple. These fifteen rules are certainly not exhaustive, but cover around eighty percent of the inputs. The tool ignores questions greater than 60 characters and complex questions it cannot parse, leaving a dataset smaller than the original (see Table 1). Each rule is on average forty lines of code that traverses a dependency parse tree according to its pre-specified pattern, extracting the matching spans at each step. A master function parse() determines which rule to apply based on the question type which is categorized by nsubj presence, and the type of question (who/what/etc.). Most questions contain an nsubj which makes the parse task easier, as this will also be the subject of the tuple. We allow the master parse() method try multiple rules. It first tries very specific rules (e.g. a parser for how questions where no subject is identified), then falls down to more generic rules. If no output is returned after all the methods are tried we throw the QA pair out. Otherwise, we find the appropriate sentence in the passage based on the index.",Sentence Alignment,"Following QA to tuple conversion, the tuple must be aligned with a sentence in the input passage. We segment the passage into sentences using periods as delimiters. The sentence containing the answer is taken as the input sentence for the tuple. Outputted sentences predominantly align with their tuple, but some exhibit partial misalignment in the case of some multi-sentence reasoning questions. 13.6% of questions require multi-sentence reasoning, so this is an upper bound on the number of partially misaligned tuples/sentences BIBREF15 . While there may be heuristics that can be used to check alignment, we didn't find a significant number of these misalignments and so left them in the corpus. Figure 1 demonstrates the conversion process.",Tuple Examination,"Examining a random subset of one hundred generated tuples in the combined dataset we find 12 noun-mediated, 33 sentence-level inference, 11 long sentence, 7 nominzalization, 0 noisy informal, 3 pp-attachment, 24 explicit, and 10 partially misaligned. With 66% implicit relations, this dataset shows promise in improving OpenIE's recall on implicit relations.",Our model,"Our implicit OpenIE extractor is implemented as a sequence to sequence model with attention BIBREF19 . We use a 2-Layer LSTM Encoder/Decoder with 500 parameters, general attention, SGD optimizer with adaptive learning rate, and 0.33 dropout BIBREF20 . The training objective is to maximize the likelihood of the output tuple given the input sentence. In the case of a sentence having multiple extractions, it appears in the dataset once for each output tuple. At test time, beam search is used for decoding to produce the top-10 outputs and an associated log likelihood value for each tuple (used to generate the precision-recall curves in Section 7).",Evaluation,We make use of the evaluation tool developed by Stanovsky and Dagan benchmark to test the precision and recall of our model against previous methods. We make two changes to the tool as described below.,Creating a Gold Dataset,"The test corpus contained no implicit data, so we re-annotate 300 tuples from the CoNLL-2009 English training data to use as gold data. Both authors worked on different sentence sets then pruned the other set to ensure only implicit relations remained. We note that this is a different dataset than our training data so should be a good test of generalizability; the training data consists of Wikipedia and news articles, while the test data resembles corporate press release headlines.",Matching function for implicit tuples,"We implement a new matching function (i.e. the function that decides if a generated tuple matches a gold tuple). The included matching functions used BoW overlap or BLEU, which aren't appropriate for implicit relations; our goal is to assess whether the meaning of the predicted tuple matches the gold, not the only tokens. For example, the if the gold relation is “is employed by” we want to accept “works for”. Thus, we instead compute the cosine similarity of the subject, relation, and object embeddings to our gold tuple. All three must be above a threshold to evaluate as a match. The sequence embeddings are computed by taking the average of the GloVe embeddings of each word (i.e. BoW embedding) BIBREF21 .",Results,"The results on our implicit corpus are shown in Figure 2 (our method in blue). For continuity with prior work, we also compare our model on the origional corpus but using our new matching function in Figure 3. Our model outperforms at every point in the implicit-tuples PR curve, accomplishing our goal of increasing recall on implicit relations. Our system performs poorly on explicit tuples, as we would expect considering our training data. We tried creating a multi-task model, but found the model either learned to produce implit or explicit tuples. Creating a multi-task network would be ideal, though it is sufficient for production systems to use both systems in tandem.",Conclusion,"We created a large training corpus for implicit OpenIE extractors based on SQuAD and NewsQA, trained a baseline on this dataset, and presented promising results on implicit extraction. We see this as part of a larger body of work in text-representation schemes which aim to represent meaning in a more structured form than free text. Implicit information extraction goes further than traditional OpenIE to elicit relations not contained in the original free text. This allows maximally-shortened tuples where common sense relations are made explicit. Our model should improve further as more QA datasets are released and converted to OpenIE data using our conversion tool.",,,,,,,,,,,,,How much better does this baseline neural model do?,e7329c403af26b7e6eef8b60ba6fefbe40ccf8ce,,familiar,no,information extraction,50d8b4a941c26b89482c94ab324b5a274f9ced66,False,,"The model outperforms at every point in the
implicit-tuples PR curve reaching almost 0.8 in recall",FLOAT SELECTED: Figure 2: PR curve on our implicit tuples dataset.,FLOAT SELECTED: Figure 2: PR curve on our implicit tuples dataset.,26df92ca3004b2f750fbff14cd0d2b5a611fdbee,5d0eb97e8e840e171f73b7642c2c89dd3984157b,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3-Table1-1.png,Table 1: Dataset statistics.,3-Figure1-1.png,Figure 1: Tuple conversion and alignment process flow.,4-Figure2-1.png,Figure 2: PR curve on our implicit tuples dataset.,4-Figure3-1.png,Figure 3: PR curve on the explicit tuples dataset.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The SIGMORPHON 2019 Shared Task: Morphological Analysis in Context and Cross-Lingual Transfer for Inflection,"The SIGMORPHON 2019 shared task on cross-lingual transfer and contextual analysis in morphology examined transfer learning of inflection between 100 language pairs, as well as contextual lemmatization and morphosyntactic description in 66 languages. The first task evolves past years' inflection tasks by examining transfer of morphological inflection knowledge from a high-resource language to a low-resource language. This year also presents a new second challenge on lemmatization and morphological feature analysis in context. All submissions featured a neural component and built on either this year's strong baselines or highly ranked systems from previous years' shared tasks. Every participating team improved in accuracy over the baselines for the inflection task (though not Levenshtein distance), and every team in the contextual analysis task improved on both state-of-the-art neural and non-neural baselines.",Introduction,"While producing a sentence, humans combine various types of knowledge to produce fluent output—various shades of meaning are expressed through word selection and tone, while the language is made to conform to underlying structural rules via syntax and morphology. Native speakers are often quick to identify disfluency, even if the meaning of a sentence is mostly clear. Automatic systems must also consider these constraints when constructing or processing language. Strong enough language models can often reconstruct common syntactic structures, but are insufficient to properly model morphology. Many languages implement large inflectional paradigms that mark both function and content words with a varying levels of morphosyntactic information. For instance, Romanian verb forms inflect for person, number, tense, mood, and voice; meanwhile, Archi verbs can take on thousands of forms BIBREF0. Such complex paradigms produce large inventories of words, all of which must be producible by a realistic system, even though a large percentage of them will never be observed over billions of lines of linguistic input. Compounding the issue, good inflectional systems often require large amounts of supervised training data, which is infeasible in many of the world's languages. This year's shared task is concentrated on encouraging the construction of strong morphological systems that perform two related but different inflectional tasks. The first task asks participants to create morphological inflectors for a large number of under-resourced languages, encouraging systems that use highly-resourced, related languages as a cross-lingual training signal. The second task welcomes submissions that invert this operation in light of contextual information: Given an unannotated sentence, lemmatize each word, and tag them with a morphosyntactic description. Both of these tasks extend upon previous morphological competitions, and the best submitted systems now represent the state of the art in their respective tasks.",Tasks and Evaluation ::: Task 1: Cross-lingual transfer for morphological inflection,"Annotated resources for the world's languages are not distributed equally—some languages simply have more as they have more native speakers willing and able to annotate more data. We explore how to transfer knowledge from high-resource languages that are genetically related to low-resource languages. The first task iterates on last year's main task: morphological inflection BIBREF1. Instead of giving some number of training examples in the language of interest, we provided only a limited number in that language. To accompany it, we provided a larger number of examples in either a related or unrelated language. Each test example asked participants to produce some other inflected form when given a lemma and a bundle of morphosyntactic features as input. The goal, thus, is to perform morphological inflection in the low-resource language, having hopefully exploited some similarity to the high-resource language. Models which perform well here can aid downstream tasks like machine translation in low-resource settings. All datasets were resampled from UniMorph, which makes them distinct from past years. The mode of the task is inspired by BIBREF2, who fine-tune a model pre-trained on a high-resource language to perform well on a low-resource language. We do not, though, require that models be trained by fine-tuning. Joint modeling or any number of methods may be explored instead.",Tasks and Evaluation ::: Task 1: Cross-lingual transfer for morphological inflection ::: Example,"The model will have access to type-level data in a low-resource target language, plus a high-resource source language. We give an example here of Asturian as the target language with Spanish as the source language. ",Tasks and Evaluation ::: Task 1: Cross-lingual transfer for morphological inflection ::: Evaluation,We score the output of each system in terms of its predictions' exact-match accuracy and the average Levenshtein distance between the predictions and their corresponding true forms.,Tasks and Evaluation ::: Task 2: Morphological analysis in context,"Although inflection of words in a context-agnostic manner is a useful evaluation of the morphological quality of a system, people do not learn morphology in isolation. In 2018, the second task of the CoNLL–SIGMORPHON Shared Task BIBREF1 required submitting systems to complete an inflectional cloze task BIBREF3 given only the sentential context and the desired lemma – an example of the problem is given in the following lines: A successful system would predict the plural form “dogs”. Likewise, a Spanish word form ayuda may be a feminine noun or a third-person verb form, which must be disambiguated by context.  This year's task extends the second task from last year. Rather than inflect a single word in context, the task is to provide a complete morphological tagging of a sentence: for each word, a successful system will need to lemmatize and tag it with a morphsyntactic description (MSD). width= Context is critical—depending on the sentence, identical word forms realize a large number of potential inflectional categories, which will in turn influence lemmatization decisions. If the sentence were instead “The barking dogs kept us up all night”, “barking” is now an adjective, and its lemma is also “barking”.",Data ::: Data for Task 1 ::: Language pairs,"We presented data in 100 language pairs spanning 79 unique languages. Data for all but four languages (Basque, Kurmanji, Murrinhpatha, and Sorani) are extracted from English Wiktionary, a large multi-lingual crowd-sourced dictionary with morphological paradigms for many lemmata. 20 of the 100 language pairs are either distantly related or unrelated; this allows speculation into the relative importance of data quantity and linguistic relatedness.",Data ::: Data for Task 1 ::: Data format,"For each language, the basic data consists of triples of the form (lemma, feature bundle, inflected form), as in tab:sub1data. The first feature in the bundle always specifies the core part of speech (e.g., verb). For each language pair, separate files contain the high- and low-resource training examples. All features in the bundle are coded according to the UniMorph Schema, a cross-linguistically consistent universal morphological feature set BIBREF8, BIBREF9.",Data ::: Data for Task 1 ::: Extraction from Wiktionary,"For each of the Wiktionary languages, Wiktionary provides a number of tables, each of which specifies the full inflectional paradigm for a particular lemma. As in the previous iteration, tables were extracted using a template annotation procedure described in BIBREF10.",Data ::: Data for Task 1 ::: Sampling data splits,"From each language's collection of paradigms, we sampled the training, development, and test sets as in 2018. Crucially, while the data were sampled in the same fashion, the datasets are distinct from those used for the 2018 shared task. Our first step was to construct probability distributions over the (lemma, feature bundle, inflected form) triples in our full dataset. For each triple, we counted how many tokens the inflected form has in the February 2017 dump of Wikipedia for that language. To distribute the counts of an observed form over all the triples that have this token as its form, we follow the method used in the previous shared task BIBREF1, training a neural network on unambiguous forms to estimate the distribution over all, even ambiguous, forms. We then sampled 12,000 triples without replacement from this distribution. The first 100 were taken as training data for low-resource settings. The first 10,000 were used as high-resource training sets. As these sets are nested, the highest-count triples tend to appear in the smaller training sets. The final 2000 triples were randomly shuffled and then split in half to obtain development and test sets of 1000 forms each. The final shuffling was performed to ensure that the development set is similar to the test set. By contrast, the development and test sets tend to contain lower-count triples than the training set.",Data ::: Data for Task 1 ::: Other modifications,"We further adopted some changes to increase compatibility. Namely, we corrected some annotation errors created while scraping Wiktionary for the 2018 task, and we standardized Romanian t-cedilla and t-comma to t-comma. (The same was done with s-cedilla and s-comma.)",Data ::: Data for Task 2,"Our data for task 2 come from the Universal Dependencies treebanks BIBREF11, which provides pre-defined training, development, and test splits and annotations in a unified annotation schema for morphosyntax and dependency relationships. Unlike the 2018 cloze task which used UD data, we require no manual data preparation and are able to leverage all 107 monolingual treebanks. As is typical, data are presented in CoNLL-U format, although we modify the morphological feature and lemma fields.",Data ::: Data for Task 2 ::: Data conversion,"The morphological annotations for the 2019 shared task were converted to the UniMorph schema BIBREF10 according to BIBREF12, who provide a deterministic mapping that increases agreement across languages. This also moves the part of speech into the bundle of morphological features. We do not attempt to individually correct any errors in the UD source material. Further, some languages received additional pre-processing. In the Finnish data, we removed morpheme boundaries that were present in the lemmata (e.g., puhe#kieli $\mapsto $ puhekieli `spoken+language'). Russian lemmata in the GSD treebank were presented in all uppercase; to match the 2018 shared task, we lowercased these. In development and test data, all fields except for form and index within the sentence were struck.",Baselines ::: Task 1 Baseline,"We include four neural sequence-to-sequence models mapping lemma into inflected word forms: soft attention BIBREF13, non-monotonic hard attention BIBREF14, monotonic hard attention and a variant with offset-based transition distribution BIBREF15. Neural sequence-to-sequence models with soft attention BIBREF13 have dominated previous SIGMORPHON shared tasks BIBREF16. BIBREF14 instead models the alignment between characters in the lemma and the inflected word form explicitly with hard attention and learns this alignment and transduction jointly. BIBREF15 shows that enforcing strict monotonicity with hard attention is beneficial in tasks such as morphological inflection where the transduction is mostly monotonic. The encoder is a biLSTM while the decoder is a left-to-right LSTM. All models use multiplicative attention and have roughly the same number of parameters. In the model, a morphological tag is fed to the decoder along with target character embeddings to guide the decoding. During the training of the hard attention model, dynamic programming is applied to marginalize all latent alignments exactly.",Baselines ::: Task 2 Baselines ::: Non-neural,"BIBREF17: The Lemming model is a log-linear model that performs joint morphological tagging and lemmatization. The model is globally normalized with the use of a second order linear-chain CRF. To efficiently calculate the partition function, the choice of lemmata are pruned with the use of pre-extracted edit trees.",Baselines ::: Task 2 Baselines ::: Neural,"BIBREF18: This is a state-of-the-art neural model that also performs joint morphological tagging and lemmatization, but also accounts for the exposure bias with the application of maximum likelihood (MLE). The model stitches the tagger and lemmatizer together with the use of jackknifing BIBREF19 to expose the lemmatizer to the errors made by the tagger model during training. The morphological tagger is based on a character-level biLSTM embedder that produces the embedding for a word, and a word-level biLSTM tagger that predicts a morphological tag sequence for each word in the sentence. The lemmatizer is a neural sequence-to-sequence model BIBREF15 that uses the decoded morphological tag sequence from the tagger as an additional attribute. The model uses hard monotonic attention instead of standard soft attention, along with a dynamic programming based training scheme.",Results,"The SIGMORPHON 2019 shared task received 30 submissions—14 for task 1 and 16 for task 2—from 23 teams. In addition, the organizers' baseline systems were evaluated.",Results ::: Task 1 Results,"Five teams participated in the first Task, with a variety of methods aimed at leveraging the cross-lingual data to improve system performance. The University of Alberta (UAlberta) performed a focused investigation on four language pairs, training cognate-projection systems from external cognate lists. Two methods were considered: one which trained a high-resource neural encoder-decoder, and projected the test data into the HRL, and one that projected the HRL data into the LRL, and trained a combined system. Results demonstrated that certain language pairs may be amenable to such methods. The Tuebingen University submission (Tuebingen) aligned source and target to learn a set of edit-actions with both linear and neural classifiers that independently learned to predict action sequences for each morphological category. Adding in the cross-lingual data only led to modest gains. AX-Semantics combined the low- and high-resource data to train an encoder-decoder seq2seq model; optionally also implementing domain adaptation methods to focus later epochs on the target language. The CMU submission first attends over a decoupled representation of the desired morphological sequence before using the updated decoder state to attend over the character sequence of the lemma. Secondly, in order to reduce the bias of the decoder's language model, they hallucinate two types of data that encourage common affixes and character copying. Simply allowing the model to learn to copy characters for several epochs significantly out-performs the task baseline, while further improvements are obtained through fine-tuning. Making use of an adversarial language discriminator, cross lingual gains are highly-correlated to linguistic similarity, while augmenting the data with hallucinated forms and multiple related target language further improves the model. The system from IT-IST also attends separately to tags and lemmas, using a gating mechanism to interpolate the importance of the individual attentions. By combining the gated dual-head attention with a SparseMax activation function, they are able to jointly learn stem and affix modifications, improving significantly over the baseline system. The relative system performance is described in tab:sub2team, which shows the average per-language accuracy of each system. The table reflects the fact that some teams submitted more than one system (e.g. Tuebingen-1 & Tuebingen-2 in the table).",Results ::: Task 2 Results,"Nine teams submitted system papers for Task 2, with several interesting modifications to either the baseline or other prior work that led to modest improvements. Charles-Saarland achieved the highest overall tagging accuracy by leveraging multi-lingual BERT embeddings fine-tuned on a concatenation of all available languages, effectively transporting the cross-lingual objective of Task 1 into Task 2. Lemmas and tags are decoded separately (with a joint encoder and separate attention); Lemmas are a sequence of edit-actions, while tags are calculated jointly. (There is no splitting of tags into features; tags are atomic.) CBNU instead lemmatize using a transformer network, while performing tagging with a multilayer perceptron with biaffine attention. Input words are first lemmatized, and then pipelined to the tagger, which produces atomic tag sequences (i.e., no splitting of features). The team from Istanbul Technical University (ITU) jointly produces lemmatic edit-actions and morphological tags via a two level encoder (first word embeddings, and then context embeddings) and separate decoders. Their system slightly improves over the baseline lemmatization, but significantly improves tagging accuracy. The team from the University of Groningen (RUG) also uses separate decoders for lemmatization and tagging, but uses ELMo to initialize the contextual embeddings, leading to large gains in performance. Furthermore, joint training on related languages further improves results. CMU approaches tagging differently than the multi-task decoding we've seen so far (baseline is used for lemmatization). Making use of a hierarchical CRF that first predicts POS (that is subsequently looped back into the encoder), they then seek to predict each feature separately. In particular, predicting POS separately greatly improves results. An attempt to leverage gold typological information led to little gain in the results; experiments suggest that the system is already learning the pertinent information. The team from Ohio State University (OHIOSTATE) concentrates on predicting tags; the baseline lemmatizer is used for lemmatization. To that end, they make use of a dual decoder that first predicts features given only the word embedding as input; the predictions are fed to a GRU seq2seq, which then predicts the sequence of tags. The UNT HiLT+Ling team investigates a low-resource setting of the tagging, by using parallel Bible data to learn a translation matrix between English and the target language, learning morphological tags through analogy with English. The UFAL-Prague team extends their submission from the UD shared task (multi-layer LSTM), replacing the pretrained embeddings with BERT, to great success (first in lemmatization, 2nd in tagging). Although they predict complete tags, they use the individual features to regularize the decoder. Small gains are also obtained from joining multi-lingual corpora and ensembling. CUNI–Malta performs lemmatization as operations over edit actions with LSTM and ReLU. Tagging is a bidirectional LSTM augmented by the edit actions (i.e., two-stage decoding), predicting features separately. The Edinburgh system is a character-based LSTM encoder-decoder with attention, implemented in OpenNMT. It can be seen as an extension of the contextual lemmatization system Lematus BIBREF20 to include morphological tagging, or alternatively as an adaptation of the morphological re-inflection system MED BIBREF21 to incorporate context and perform analysis rather than re-inflection. Like these systems it uses a completely generic encoder-decoder architecture with no specific adaptation to the morphological processing task other than the form of the input. In the submitted version of the system, the input is split into short chunks corresponding to the target word plus one word of context on either side, and the system is trained to output the corresponding lemmas and tags for each three-word chunk. Several teams relied on external resources to improve their lemmatization and feature analysis. Several teams made use of pre-trained embeddings. CHARLES-SAARLAND-2 and UFALPRAGUE-1 used pretrained contextual embeddings (BERT) provided by Google BIBREF22. CBNU-1 used a mix of pre-trained embeddings from the CoNLL 2017 shared task and fastText. Further, some teams trained their own embeddings to aid performance.",Future Directions,"In general, the application of typology to natural language processing BIBREF23, BIBREF24 provides an interesting avenue for multilinguality. Further, our shared task was designed to only leverage a single helper language, though many may exist with lexical or morphological overlap with the target language. Techniques like those of BIBREF25 may aid in designing universal inflection architectures. Neither task this year included unannotated monolingual corpora. Using such data is well-motivated from an L1-learning point of view, and may affect the performance of low-resource data settings. In the case of inflection an interesting future topic could involve departing from orthographic representation and using more IPA-like representations, i.e. transductions over pronunciations. Different languages, in particular those with idiosyncratic orthographies, may offer new challenges in this respect. Only one team tried to learn inflection in a multilingual setting—i.e. to use all training data to train one model. Such transfer learning is an interesting avenue of future research, but evaluation could be difficult. Whether any cross-language transfer is actually being learned vs. whether having more data better biases the networks to copy strings is an evaluation step to disentangle. Creating new data sets that accurately reflect learner exposure (whether L1 or L2) is also an important consideration in the design of future shared tasks. One pertinent facet of this is information about inflectional categories—often the inflectional information is insufficiently prescribed by the lemma, as with the Romanian verbal inflection classes or nominal gender in German. As we move toward multilingual models for morphology, it becomes important to understand which representations are critical or irrelevant for adapting to new languages; this may be probed in the style of BIBREF27, and it can be used as a first step toward designing systems that avoid catastrophic forgetting as they learn to inflect new languages BIBREF28. Future directions for Task 2 include exploring cross-lingual analysis—in stride with both Task 1 and BIBREF29—and leveraging these analyses in downstream tasks.",Conclusions,"The SIGMORPHON 2019 shared task provided a type-level evaluation on 100 language pairs in 79 languages and a token-level evaluation on 107 treebanks in 66 languages, of systems for inflection and analysis. On task 1 (low-resource inflection with cross-lingual transfer), 14 systems were submitted, while on task 2 (lemmatization and morphological feature analysis), 16 systems were submitted. All used neural network models, completing a trend in past years' shared tasks and other recent work on morphology. In task 1, gains from cross-lingual training were generally modest, with gains positively correlating with the linguistic similarity of the two languages. In the second task, several methods were implemented by multiple groups, with the most successful systems implementing variations of multi-headed attention, multi-level encoding, multiple decoders, and ELMo and BERT contextual embeddings. We have released the training, development, and test sets, and expect these datasets to provide a useful benchmark for future research into learning of inflectional morphology and string-to-string transduction.",Acknowledgments,MS has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No 771113).,What were the non-neural baselines used for the task?,b65b1c366c8bcf544f1be5710ae1efc6d2b1e2f1,two,unfamiliar,no,morphology,486a870694ba60f1a1e7e4ec13e328164cd4b43c,False,,The Lemming model in BIBREF17,"BIBREF17: The Lemming model is a log-linear model that performs joint morphological tagging and lemmatization. The model is globally normalized with the use of a second order linear-chain CRF. To efficiently calculate the partition function, the choice of lemmata are pruned with the use of pre-extracted edit trees.",BIBREF17: The Lemming model is a log-linear model that performs joint morphological tagging and lemmatization. ,012a77e1bbdaa410ad83a28a87526db74bd1e353,486a870694ba60f1a1e7e4ec13e328164cd4b43c,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2-Table1-1.png,Table 1: Sample language pair and data format for Task 1,4-Table2-1.png,"Table 2: Task 1 Team Scores, averaged across all Languages; * indicates submissions were only applied to a subset of languages, making scores incomparable. † indicates that additional resources were used for training.",5-Table3-1.png,Table 3: Task 1 Accuracy scores,6-Table4-1.png,Table 4: Task 1 Levenshtein scores,8-Table5-1.png,"Table 5: Task 2 Team Scores, averaged across all treebanks; * indicates submissions were only applied to a subset of languages, making scores incomparable. † indicates that additional external resources were used for training, and ‡ indicates that training data were shared across languages or treebanks.",9-Table6-1.png,Table 6: Task 2 Lemma Accuracy scores,,,,,,,,,,,10-Table7-1.png,Table 7: Task 2 Lemma Levenshtein scores,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Discriminative Acoustic Word Embeddings: Recurrent Neural Network-Based Approaches,"Acoustic word embeddings --- fixed-dimensional vector representations of variable-length spoken word segments --- have begun to be considered for tasks such as speech recognition and query-by-example search. Such embeddings can be learned discriminatively so that they are similar for speech segments corresponding to the same word, while being dissimilar for segments corresponding to different words. Recent work has found that acoustic word embeddings can outperform dynamic time warping on query-by-example search and related word discrimination tasks. However, the space of embedding models and training approaches is still relatively unexplored. In this paper we present new discriminative embedding models based on recurrent neural networks (RNNs). We consider training losses that have been successful in prior work, in particular a cross entropy loss for word classification and a contrastive loss that explicitly aims to separate same-word and different-word pairs in a""Siamese network""training setting. We find that both classifier-based and Siamese RNN embeddings improve over previously reported results on a word discrimination task, with Siamese RNNs outperforming classification models. In addition, we present analyses of the learned embeddings and the effects of variables such as dimensionality and network structure.",Introduction,"Many speech processing tasks – such as automatic speech recognition or spoken term detection – hinge on associating segments of speech signals with word labels. In most systems developed for such tasks, words are broken down into sub-word units such as phones, and models are built for the individual units. An alternative, which has been considered by some researchers, is to consider each entire word segment as a single unit, without assigning parts of it to sub-word units. One motivation for the use of whole-word approaches is that they avoid the need for sub-word models. This is helpful since, despite decades of work on sub-word modeling BIBREF0 , BIBREF1 , it still poses significant challenges. For example, speech processing systems are still hampered by differences in conversational pronunciations BIBREF2 . A second motivation is that considering whole words at once allows us to consider a more flexible set of features and reason over longer time spans. Whole-word approaches typically involve, at some level, template matching. For example, in template-based speech recognition BIBREF3 , BIBREF4 , word scores are computed from dynamic time warping (DTW) distances between an observed segment and training segments of the hypothesized word. In query-by-example search, putative matches are typically found by measuring the DTW distance between the query and segments of the search database BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 . In other words, whole-word approaches often boil down to making decisions about whether two segments are examples of the same word or not. An alternative to DTW that has begun to be explored is the use of acoustic word embeddings (AWEs), or vector representations of spoken word segments. AWEs are representations that can be learned from data, ideally such that the embeddings of two segments corresponding to the same word are close, while embeddings of segments corresponding to different words are far apart. Once word segments are represented via fixed-dimensional embeddings, computing distances is as simple as measuring a cosine or Euclidean distance between two vectors. There has been some, thus far limited, work on acoustic word embeddings, focused on a number of embedding models, training approaches, and tasks BIBREF9 , BIBREF10 , BIBREF11 , BIBREF12 , BIBREF13 , BIBREF14 , BIBREF15 , BIBREF16 . In this paper we explore new embedding models based on recurrent neural networks (RNNs), applied to a word discrimination task related to query-by-example search. RNNs are a natural model class for acoustic word embeddings, since they can handle arbitrary-length sequences. We compare several types of RNN-based embeddings and analyze their properties. Compared to prior embeddings tested on the same task, our best models achieve sizable improvements in average precision.",Related work,"We next briefly describe the most closely related prior work. Maas et al. BIBREF9 and Bengio and Heigold BIBREF10 used acoustic word embeddings, based on convolutional neural networks (CNNs), to generate scores for word segments in automatic speech recognition. Maas et al. trained CNNs to predict (continuous-valued) embeddings of the word labels, and used the resulting embeddings to define feature functions in a segmental conditional random field BIBREF17 rescoring system. Bengio and Heigold also developed CNN-based embeddings for lattice rescoring, but with a contrastive loss to separate embeddings of a given word from embeddings of other words. Levin et al. BIBREF11 developed unsupervised embeddings based on representing each word as a vector of DTW distances to a collection of reference word segments. This representation was subsequently used in several applications: a segmental approach for query-by-example search BIBREF12 , lexical clustering BIBREF18 , and unsupervised speech recognition BIBREF19 . Voinea et al. BIBREF15 developed a representation also based on templates, in their case phone templates, designed to be invariant to specific transformations, and showed their robustness on digit classification. Kamper et al. BIBREF13 compared several types of acoustic word embeddings for a word discrimination task related to query-by-example search, finding that embeddings based on convolutional neural networks (CNNs) trained with a contrastive loss outperformed the reference vector approach of Levin et al. BIBREF11 as well as several other CNN and DNN embeddings and DTW using several feature types. There have now been a number of approaches compared on this same task and data BIBREF11 , BIBREF20 , BIBREF21 , BIBREF22 . For a direct comparison with this prior work, in this paper we use the same task and some of the same training losses as Kamper et al., but develop new embedding models based on RNNs. The only prior work of which we are aware using RNNs for acoustic word embeddings is that of Chen et al. BIBREF16 and Chung et al. BIBREF14 . Chen et al. learned a long short-term memory (LSTM) RNN for word classification and used the resulting hidden state vectors as a word embedding in a query-by-example task. The setting was quite specific, however, with a small number of queries and speaker-dependent training. Chung et al. BIBREF14 worked in an unsupervised setting and trained single-layer RNN autoencoders to produce embeddings for a word discrimination task. In this paper we focus on the supervised setting, and compare a variety of RNN-based structures trained with different losses. ",Approach," An acoustic word embedding is a function that takes as input a speech segment corresponding to a word, INLINEFORM0 , where each INLINEFORM1 is a vector of frame-level acoustic features, and outputs a fixed-dimensional vector representing the segment, INLINEFORM2 . The basic embedding model structure we use is shown in Fig. FIGREF1 . The model consists of a deep RNN with some number INLINEFORM3 of stacked layers, whose final hidden state vector is passed as input to a set of INLINEFORM4 of fully connected layers; the output of the final fully connected layer is the embedding INLINEFORM5 . The RNN hidden state at each time frame can be viewed as a representation of the input seen thus far, and its value in the last time frame INLINEFORM0 could itself serve as the final word embedding. The fully connected layers are added to account for the fact that some additional transformation may improve the representation. For example, the hidden state may need to be larger than the desired word embedding dimension, in order to be able to ""remember"" all of the needed intermediate information. Some of that information may not be needed in the final embedding. In addition, the information maintained in the hidden state may not necessarily be discriminative; some additional linear or non-linear transformation may help to learn a discriminative embedding. Within this class of embedding models, we focus on Long Short-Term Memory (LSTM) networks BIBREF23 and Gated Recurrent Unit (GRU) networks BIBREF24 . These are both types of RNNs that include a mechanism for selectively retaining or discarding information at each time frame when updating the hidden state, in order to better utilize long-term context. Both of these RNN variants have been used successfully in speech recognition BIBREF25 , BIBREF26 , BIBREF27 , BIBREF28 . In an LSTM RNN, at each time frame both the hidden state INLINEFORM0 and an associated “cell memory"" vector INLINEFORM1 , are updated and passed on to the next time frame. In other words, each forward edge in Figure FIGREF1 can be viewed as carrying both the cell memory and hidden state vectors. The updates are modulated by the values of several gating vectors, which control the degree to which the cell memory and hidden state are updated in light of new information in the current frame. For a single-layer LSTM network, the updates are as follows:  INLINEFORM0  where INLINEFORM0 , and INLINEFORM1 are all vectors of the same dimensionality, INLINEFORM2 , and INLINEFORM3 are learned weight matrices of the appropriate sizes, INLINEFORM4 and INLINEFORM5 are learned bias vectors, INLINEFORM6 is a componentwise logistic activation, and INLINEFORM7 refers to the Hadamard (componentwise) product. Similarly, in a GRU network, at each time step a GRU cell determines what components of old information are retained, overwritten, or modified in light of the next step in the input sequence. The output from a GRU cell is only the hidden state vector. A GRU cell uses a reset gate INLINEFORM0 and an update gate INLINEFORM1 as described below for a single-layer network: INLINEFORM2  where INLINEFORM0 , and INLINEFORM1 are all the same dimensionality, INLINEFORM2 , and INLINEFORM3 are learned weight matrices of the appropriate size, and INLINEFORM4 , INLINEFORM5 and INLINEFORM6 are learned bias vectors. All of the above equations refer to single-layer networks. In a deep network, with multiple stacked layers, the same update equations are used in each layer, with the state, cell, and gate vectors replaced by layer-specific vectors INLINEFORM0 and so on for layer INLINEFORM1 . For all but the first layer, the input INLINEFORM2 is replaced by the hidden state vector from the previous layer INLINEFORM3 . For the fully connected layers, we use rectified linear unit (ReLU) BIBREF29 activation, except for the final layer which depends on the form of supervision and loss used in training. ",Training,"We train the RNN-based embedding models using a set of pre-segmented spoken words. We use two main training approaches, inspired by prior work but with some differences in the details. As in BIBREF13 , BIBREF10 , our first approach is to use the word labels of the training segments and train the networks to classify the word. In this case, the final layer of INLINEFORM0 is a log-softmax layer. Here we are limited to the subset of the training set that has a sufficient number of segments per word to train a good classifier, and the output dimensionality is equal to the number of words (but see BIBREF13 for a study of varying the dimensionality in such a classifier-based embedding model by introducing a bottleneck layer). This model is trained end-to-end and is optimized with a cross entropy loss. Although labeled data is necessarily limited, the hope is that the learned models will be useful even when applied to spoken examples of words not previously seen in the training data. For words not seen in training, the embeddings should correspond to some measure of similarity of the word to the training words, measured via the posterior probabilities of the previously seen words. In the experiments below, we examine this assumption by analyzing performance on words that appear in the training data compared to those that do not. The second training approach, based on earlier work of Kamper et al. BIBREF13 , is to train ""Siamese"" networks BIBREF30 . In this approach, full supervision is not needed; rather, we use weak supervision in the form of pairs of segments labeled as same or different. The base model remains the same as before—an RNN followed by a set of fully connected layers—but the final layer is no longer a softmax but rather a linear activation layer of arbitrary size. In order to learn the parameters, we simultaneously feed three word segments through three copies of our model (i.e. three networks with shared weights). One input segment is an “anchor"", INLINEFORM0 , the second is another segment with the same word label, INLINEFORM1 , and the third is a segment corresponding to a different word label, INLINEFORM2 . Then, the network is trained using a “cos-hinge"" loss:  DISPLAYFORM0  where INLINEFORM0 is the cosine distance between INLINEFORM1 . Unlike cross entropy training, here we directly aim to optimize relative (cosine) distance between same and different word pairs. For tasks such as query-by-example search, this training loss better respects our end objective, and can use more data since neither fully labeled data nor any minimum number of examples of each word should be needed. ",EXPERIMENTS," Our end goal is to improve performance on downstream tasks requiring accurate word discrimination. In this paper we use an intermediate task that more directly tests whether same- and different-word pairs have the expected relationship. and that allows us to compare to a variety of prior work. Specifically, we use the word discrimination task of Carlin et al. BIBREF20 , which is similar to a query-by-example task where the word segmentations are known. The evaluation consists of determining, for each pair of evaluation segments, whether they are examples of the same or different words, and measuring performance via the average precision (AP). We do this by measuring the cosine similarity between their acoustic word embeddings and declaring them to be the same if the distance is below a threshold. By sweeping the threshold, we obtain a precision-recall curve from which we compute the AP. The data used for this task is drawn from the Switchboard conversational English corpus BIBREF31 . The word segments range from 50 to 200 frames in length. The acoustic features in each frame (the input to the word embedding models INLINEFORM0 ) are 39-dimensional MFCCs+ INLINEFORM1 + INLINEFORM2 . We use the same train, development, and test partitions as in prior work BIBREF13 , BIBREF11 , and the same acoustic features as in BIBREF13 , for as direct a comparison as possible. The train set contains approximately 10k example segments, while dev and test each contain approximately 11k segments (corresponding to about 60M pairs for computing the dev/test AP). As in BIBREF13 , when training the classification-based embeddings, we use a subset of the training set containing all word types with a minimum of 3 occurrences, reducing the training set size to approximately 9k segments. When training the Siamese networks, the training data consists of all of the same-word pairs in the full training set (approximately 100k pairs). For each such training pair, we randomly sample a third example belonging to a different word type, as required for the INLINEFORM0 loss. ",Classification network details,"Our classifier-based embeddings use LSTM or GRU networks with 2–4 stacked layers and 1–3 fully connected layers. The final embedding dimensionality is equal to the number of unique word labels in the training set, which is 1061. The recurrent hidden state dimensionality is fixed at 512 and dropout BIBREF32 between stacked recurrent layers is used with probability INLINEFORM0 . The fully connected hidden layer dimensionality is fixed at 1024. Rectified linear unit (ReLU) non-linearities and dropout with INLINEFORM1 are used between fully-connected layers. However, between the final recurrent hidden state output and the first fully-connected layer no non-linearity or dropout is applied. These settings were determined through experiments on the development set. The classifier network is trained with a cross entropy loss and optimized using stochastic gradient descent (SGD) with Nesterov momentum BIBREF33 . The learning rate is initialized at 0.1 and is reduced by a factor of 10 according to the following heuristic: If 99% of the current epoch's average batch loss is greater than the running average of batch losses over the last 3 epochs, this is considered a plateau; if there are 3 consecutive plateau epochs, then the learning rate is reduced. Training stops when reducing the learning rate no longer improves dev set AP. Then, the model from the epoch corresponding to the the best dev set AP is chosen. Several other optimizers—Adagrad BIBREF34 , Adadelta BIBREF35 , and Adam BIBREF36 —were explored in initial experiments on the dev set, but all reported results were obtained using SGD with Nesterov momentum. ",Siamese network details,"For experiments with Siamese networks, we initialize (warm-start) the networks with the tuned classification network, removing the final log-softmax layer and replacing it with a linear layer of size equal to the desired embedding dimensionality. We explored embeddings with dimensionalities between 8 and 2048. We use a margin of 0.4 in the cos-hinge loss. In training the Siamese networks, each training mini-batch consists of INLINEFORM0 triplets. INLINEFORM1 triplets are of the form INLINEFORM2 where INLINEFORM3 and INLINEFORM4 are examples of the same class (a pair from the 100k same-word pair set) and INLINEFORM5 is a randomly sampled example from a different class. Then, for each of these INLINEFORM6 triplets INLINEFORM7 , an additional triplet INLINEFORM8 is added to the mini-batch to allow all segments to serve as anchors. This is a slight departure from earlier work BIBREF13 , which we found to improve stability in training and performance on the development set. In preliminary experiments, we compared two methods for choosing the negative examples INLINEFORM0 during training, a uniform sampling approach and a non-uniform one. In the case of uniform sampling, we sample INLINEFORM1 uniformly at random from the full set of training examples with labels different from INLINEFORM2 . This sampling method requires only word-pair supervision. In the case of non-uniform sampling, INLINEFORM3 is sampled in two steps. First, we construct a distribution INLINEFORM4 over word labels INLINEFORM5 and sample a different label from it. Second, we sample an example uniformly from within the subset with the chosen label. The goal of this method is to speed up training by targeting pairs that violate the margin constraint. To construct the multinomial PMF INLINEFORM6 , we maintain an INLINEFORM7 matrix INLINEFORM8 , where INLINEFORM9 is the number of unique word labels in training. Each word label corresponds to an integer INLINEFORM10 INLINEFORM11 [1, INLINEFORM12 ] and therefore a row in INLINEFORM13 . The values in a row of INLINEFORM14 are considered similarity scores, and we can retrieve the desired PMF for each row by normalizing by its sum. At the start of each epoch, we initialize INLINEFORM0 with 0's along the diagonal and 1's elsewhere (which reduces to uniform sampling). For each training pair INLINEFORM1 , we update INLINEFORM2 for both INLINEFORM3 and INLINEFORM4 :  INLINEFORM0  The PMFs INLINEFORM0 are updated after the forward pass of an entire mini-batch. The constant INLINEFORM1 enforces a potentially stronger constraint than is used in the INLINEFORM2 loss, in order to promote diverse sampling. In all experiments, we set INLINEFORM3 . This is a heuristic approach, and it would be interesting to consider various alternatives. Preliminary experiments showed that the non-uniform sampling method outperformed uniform sampling, and in the following we report results with non-uniform sampling. We optimize the Siamese network model using SGD with Nesterov momentum for 15 epochs. The learning rate is initialized to 0.001 and dropped every 3 epochs until no improvement is seen on the dev set. The final model is taken from the epoch with the highest dev set AP. All models were implemented in Torch BIBREF37 and used the rnn library of BIBREF38 . ",Results," Based on development set results, our final embedding models are LSTM networks with 3 stacked layers and 3 fully connected layers, with output dimensionality of 1024 in the case of Siamese networks. Final test set results are given in Table TABREF7 . We include a comparison with the best prior results on this task from BIBREF13 , as well as the result of using standard DTW on the input MFCCs (reproduced from BIBREF13 ) and the best prior result using DTW, obtained with frame features learned with correlated autoencoders BIBREF21 . Both classifier and Siamese LSTM embedding models outperform all prior results on this task of which we are aware. We next analyze the effects of model design choices, as well as the learned embeddings themselves. ",Effect of model structure,"Table TABREF10 shows the effect on development set performance of the number of stacked layers INLINEFORM0 , the number of fully connected layers INLINEFORM1 , and LSTM vs. GRU cells, for classifier-based embeddings. The best performance in this experiment is achieved by the LSTM network with INLINEFORM2 . However, performance still seems to be improving with additional layers, suggesting that we may be able to further improve performance by adding even more layers of either type. However, we fixed the model to INLINEFORM3 in order to allow for more experimentation and analysis within a reasonable time. Table TABREF10 reveals an interesting trend. When only one fully connected layer is used, the GRU networks outperform the LSTMs given a sufficient number of stacked layers. On the other hand, once we add more fully connected layers, the LSTMs outperform the GRUs. In the first few lines of Table TABREF10 , we use 2, 3, and 4 layer stacks of LSTMs and GRUs while holding fixed the number of fully-connected layers at INLINEFORM0 . There is clear utility in stacking additional layers; however, even with 4 stacked layers the RNNs still underperform the CNN-based embeddings of BIBREF13 until we begin adding fully connected layers. After exploring a variety of stacked RNNs, we fixed the stack to 3 layers and varied the number of fully connected layers. The value of each additional fully connected layer is clearly greater than that of adding stacked layers. All networks trained with 2 or 3 fully connected layers obtain more than 0.4 AP on the development set, while stacked RNNs with 1 fully connected layer are at around 0.3 AP or less. This may raise the question of whether some simple fully connected model may be all that is needed; however, previous work has shown that this approach is not competitive BIBREF13 , and convolutional or recurrent layers are needed to summarize arbitrary-length segments into a fixed-dimensional representation. ",Effect of embedding dimensionality,"For the Siamese networks, we varied the output embedding dimensionality, as shown in Fig. FIGREF11 . This analysis shows that the embeddings learned by the Siamese RNN network are quite robust to reduced dimensionality, outperforming the classifier model for all dimensionalities 32 or higher and outperforming previously reported dev set performance with CNN-based embeddings BIBREF13 for all dimensionalities INLINEFORM0 . ",Effect of training vocabulary,"We might expect the learned embeddings to be more accurate for words that are seen in training than for ones that are not. Fig. FIGREF11 measures this effect by showing performance as a function of the number of occurrences of the dev words in the training set. Indeed, both model types are much more successful for in-vocabulary words, and their performance improves the higher the training frequency of the words. However, performance increases more quickly for the Siamese network than for the classifier as training frequency increases. This may be due to the fact that, if a word type occurs at least INLINEFORM0 times in the classifier training set, then it occurs at least INLINEFORM1 times in the Siamese paired training data. ",Visualization of embeddings,"In order to gain a better qualitative understanding of the differences between clasiffier and Siamese-based embeddings, and of the learned embedding space more generally, we plot a two-dimensional visualization of some of our learned embeddings via t-SNE BIBREF40 in Fig. FIGREF12 . For both classifier and Siamese embeddings, there is a marked difference in the quality of clusters formed by embeddings of words that were previously seen vs. previously unseen in training. However, the Siamese network embeddings appear to have better relative distances between word clusters with similar and dissimilar pronunciations. For example, the word programs appears equidistant from problems and problem in the classifier-based embedding space, but in the Siamese embedding space problems falls between problem and programs. Similarly, the cluster for democracy shifts with respect to actually and especially to better respect differences in pronunciation. More study of learned embeddings, using more data and word types, is needed to confirm such patterns in general. Improvements in unseen word embeddings from the classifier embedding space to the Siamese embedding space (such as for democracy, morning, and basketball) are a likely result of optimizing the model for relative distances between words. ",Conclusion," Our main finding is that RNN-based acoustic word embeddings outperform prior approaches, as measured via a word discrimination task related to query-by-example search. Our best results are obtained with deep LSTM RNNs with a combination of several stacked layers and several fully connected layers, optimized with a contrastive Siamese loss. Siamese networks have the benefit that, for any given training data set, they are effectively trained on a much larger set, in the sense that they measure a loss and gradient for every possible pair of data points. Our experiments suggest that the models could still be improved with additional layers. In addition, we have found that, for the purposes of acoustic word embeddings, fully connected layers are very important and have a more significant effect per layer than stacked layers, particularly when trained with the cross entropy loss function. These experiments represent an initial exploration of sequential neural models for acoustic word embeddings. There are a number of directions for further work. For example, while our analyses suggest that Siamese networks are better than classifier-based models at embedding previously unseen words, our best embeddings are still much poorer for unseen words. Improvements in this direction may come from larger training sets, or may require new models that better model the shared structure between words. Other directions for future work include additional forms of supervision and training, as well as application to downstream tasks.",,,,,,,,,,,,,,,,,How do they represent input features of their model to train embeddings?,d40662236eed26f17dd2a3a9052a4cee1482d7d6,infinity,unfamiliar,no,,2cfd959e433f290bb50b55722370f0d22fe090b7,False,,,"An acoustic word embedding is a function that takes as input a speech segment corresponding to a word, INLINEFORM0 , where each INLINEFORM1 is a vector of frame-level acoustic features, and outputs a fixed-dimensional vector representing the segment, INLINEFORM2 . The basic embedding model structure we use is shown in Fig. FIGREF1 . The model consists of a deep RNN with some number INLINEFORM3 of stacked layers, whose final hidden state vector is passed as input to a set of INLINEFORM4 of fully connected layers; the output of the final fully connected layer is the embedding INLINEFORM5 .","An acoustic word embedding is a function that takes as input a speech segment corresponding to a word, INLINEFORM0 , where each INLINEFORM1 is a vector of frame-level acoustic features, and outputs a fixed-dimensional vector representing the segment, INLINEFORM2 .",1fd4f3fbe7b6046c29581d726d5cfe3e080fd7c8,71f73551e7aabf873649e8fe97aefc54e6dd14f8,,,,,,,,,Which dimensionality do they use for their embeddings?,1d791713d1aa77358f11501f05c108045f53c8aa,infinity,unfamiliar,no,,2cfd959e433f290bb50b55722370f0d22fe090b7,False,,,"Our classifier-based embeddings use LSTM or GRU networks with 2–4 stacked layers and 1–3 fully connected layers. The final embedding dimensionality is equal to the number of unique word labels in the training set, which is 1061. The recurrent hidden state dimensionality is fixed at 512 and dropout BIBREF32 between stacked recurrent layers is used with probability INLINEFORM0 . The fully connected hidden layer dimensionality is fixed at 1024. Rectified linear unit (ReLU) non-linearities and dropout with INLINEFORM1 are used between fully-connected layers. However, between the final recurrent hidden state output and the first fully-connected layer no non-linearity or dropout is applied. These settings were determined through experiments on the development set.","The final embedding dimensionality is equal to the number of unique word labels in the training set, which is 1061.",1296db0535d800668b7dfc49d903edf11643d543,71f73551e7aabf873649e8fe97aefc54e6dd14f8,Which dataset do they use?,6b6360fab2edc836901195c0aba973eae4891975,infinity,unfamiliar,no,,2cfd959e433f290bb50b55722370f0d22fe090b7,False,,,"The data used for this task is drawn from the Switchboard conversational English corpus BIBREF31 . The word segments range from 50 to 200 frames in length. The acoustic features in each frame (the input to the word embedding models INLINEFORM0 ) are 39-dimensional MFCCs+ INLINEFORM1 + INLINEFORM2 . We use the same train, development, and test partitions as in prior work BIBREF13 , BIBREF11 , and the same acoustic features as in BIBREF13 , for as direct a comparison as possible. The train set contains approximately 10k example segments, while dev and test each contain approximately 11k segments (corresponding to about 60M pairs for computing the dev/test AP). As in BIBREF13 , when training the classification-based embeddings, we use a subset of the training set containing all word types with a minimum of 3 occurrences, reducing the training set size to approximately 9k segments.",The data used for this task is drawn from the Switchboard conversational English corpus BIBREF31 .,2aa70ad856356c985fd3ab88b850c08da935d830,71f73551e7aabf873649e8fe97aefc54e6dd14f8,,,,,,,,By how much do they outpeform previous results on the word discrimination task?,b6b5f92a1d9fa623b25c70c1ac67d59d84d9eec8,infinity,unfamiliar,no,,2cfd959e433f290bb50b55722370f0d22fe090b7,False,,Their best average precision tops previous best result by 0.202,"FLOAT SELECTED: Table 1: Final test set results in terms of average precision (AP). Dimensionalities marked with * refer to dimensionality per frame for DTW-based approaches. For CNN and LSTM models, results are given as means over several training runs (5 and 10, respectively) along with their standard deviations.","FLOAT SELECTED: Table 1: Final test set results in terms of average precision (AP). Dimensionalities marked with * refer to dimensionality per frame for DTW-based approaches. For CNN and LSTM models, results are given as means over several training runs (5 and 10, respectively) along with their standard deviations.",e29d3437584259c203f003372b6df706a73753c3,71f73551e7aabf873649e8fe97aefc54e6dd14f8,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2-Figure1-1.png,"Fig. 1: LSTM-based acoustic word embedding model. For GRUbased models, the structure is the same, but the LSTM cells are replaced with GRU cells, and there is no cell activation vector; the recurrent connections only carry the hidden state vector hlt.",5-Figure2-1.png,Fig. 2: Effect of embedding dimensionality (left) and occurrences in training set (right).,5-Table1-1.png,"Table 1: Final test set results in terms of average precision (AP). Dimensionalities marked with * refer to dimensionality per frame for DTW-based approaches. For CNN and LSTM models, results are given as means over several training runs (5 and 10, respectively) along with their standard deviations.",5-Table2-1.png,"Table 2: Average precision on the dev set, using classifier-based embeddings. S = # stacked layers, F = # fully connected layers.",6-Figure3-1.png,Fig. 3: t-SNE visualization of word embeddings from the dev set produced by the classifier (top) vs. Siamese (bottom) models. Word labels seen at training time are denoted by triangles and word labels unseen at training time are denoted by circles.,,,,,,,,,,,1061,Switchboard conversational English corpus,,,,,,,,,,,,a vector of frame-level acoustic features,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Multilingual sequence-to-sequence speech recognition: architecture, transfer learning, and language modeling","Sequence-to-sequence (seq2seq) approach for low-resource ASR is a relatively new direction in speech research. The approach benefits by performing model training without using lexicon and alignments. However, this poses a new problem of requiring more data compared to conventional DNN-HMM systems. In this work, we attempt to use data from 10 BABEL languages to build a multi-lingual seq2seq model as a prior model, and then port them towards 4 other BABEL languages using transfer learning approach. We also explore different architectures for improving the prior multilingual seq2seq model. The paper also discusses the effect of integrating a recurrent neural network language model (RNNLM) with a seq2seq model during decoding. Experimental results show that the transfer learning approach from the multilingual model shows substantial gains over monolingual models across all 4 BABEL languages. Incorporating an RNNLM also brings significant improvements in terms of %WER, and achieves recognition performance comparable to the models trained with twice more training data.",Introduction,"The sequence-to-sequence (seq2seq) model proposed in BIBREF0 , BIBREF1 , BIBREF2 is a neural architecture for performing sequence classification and later adopted to perform speech recognition in BIBREF3 , BIBREF4 , BIBREF5 . The model allows to integrate the main blocks of ASR such as acoustic model, alignment model and language model into a single framework. The recent ASR advancements in connectionist temporal classification (CTC) BIBREF5 , BIBREF4 and attention BIBREF3 , BIBREF6 based approaches has created larger interest in speech community to use seq2seq models. To leverage performance gains from this model as similar or better to conventional hybrid RNN/DNN-HMM models requires a huge amount of data BIBREF7 . Intuitively, this is due to the wide-range role of the model in performing alignment and language modeling along with acoustic to character label mapping at each iteration. In this paper, we explore the multilingual training approaches BIBREF8 , BIBREF9 , BIBREF10 used in hybrid DNN/RNN-HMMs to incorporate them into the seq2seq models. In a context of applications of multilingual approaches towards seq2seq model, CTC is mainly used instead of the attention models. A multilingual CTC is proposed in BIBREF11 , which uses a universal phoneset, FST decoder and language model. The authors also use linear hidden unit contribution (LHUC) BIBREF12 technique to rescale the hidden unit outputs for each language as a way to adapt to a particular language. Another work BIBREF13 on multilingual CTC shows the importance of language adaptive vectors as auxiliary input to the encoder in multilingual CTC model. The decoder used here is a simple INLINEFORM0 decoder. An extensive analysis on multilingual CTC mainly focusing on improving under limited data condition is performed in BIBREF14 . Here, the authors use a word level FST decoder integrated with CTC during decoding. On a similar front, attention models are explored within a multilingual setup in BIBREF15 , BIBREF16 based on attention-based seq2seq to build a model from multiple languages. The data is just combined together assuming the target languages are seen during the training. And, hence no special transfer learning techniques were used here to address the unseen languages during training. The main motivation and contribution behind this work is as follows:",Sequence-to-Sequence Model,"In this work, we use the attention based approach BIBREF1 as it provides an effective methodology to perform sequence-to-sequence (seq2seq) training. Considering the limitations of attention in performing monotonic alignment BIBREF18 , BIBREF19 , we choose to use CTC loss function to aid the attention mechanism in both training and decoding. The basic network architecture is shown in Fig. FIGREF7 . Let INLINEFORM0 be a INLINEFORM1 -length speech feature sequence and INLINEFORM2 be a INLINEFORM3 -length grapheme sequence. A multi-objective learning framework INLINEFORM4 proposed in BIBREF17 is used in this work to unify attention loss INLINEFORM5 and CTC loss INLINEFORM6 with a linear interpolation weight INLINEFORM7 , as follows: DISPLAYFORM0  The unified model allows to obtain both monotonicity and effective sequence level training.  INLINEFORM0 represents the posterior probability of character label sequence INLINEFORM1 w.r.t input sequence INLINEFORM2 based on the attention approach, which is decomposed with the probabilistic chain rule, as follows: DISPLAYFORM0  where INLINEFORM0 denotes the ground truth history. Detailed explanations about the attention mechanism is described later. Similarly, INLINEFORM0 represents the posterior probability based on the CTC approach. DISPLAYFORM0  where INLINEFORM0 is a CTC state sequence composed of the original grapheme set and the additional blank symbol. INLINEFORM1 is a set of all possible sequences given the character sequence INLINEFORM2 . The following paragraphs explain the encoder, attention decoder, CTC, and joint decoding used in our approach.",Data details and experimental setup,"In this work, the experiments are conducted using the BABEL speech corpus collected from the IARPA babel program. The corpus is mainly composed of conversational telephone speech (CTS) but some scripted recordings and far field recordings are presented as well. Table TABREF14 presents the details of the languages used in this work for training and evaluation. 80 dimensional Mel-filterbank (fbank) features are then extracted from the speech samples using a sliding window of size 25 ms with 10ms stride. KALDI toolkit BIBREF24 is used to perform the feature processing. The fbank features are then fed to a seq2seq model with the following configuration: The Bi-RNN BIBREF25 models mentioned above uses a LSTM BIBREF26 cell followed by a projection layer (BLSTMP). In our experiments below, we use only a character-level seq2seq model trained by CTC and attention decoder. Thus in the following experiments we intend to use character error rate (% CER) as a suitable measure to analyze the model performance. However, in section SECREF26 we integrate a character-level RNNLM BIBREF27 with seq2seq model externally and showcase the performance in terms of word error rate (% WER). In this case the words are obtained by concatenating the characters and the space together for scoring with reference words. All experiments are implemented in ESPnet, end-to-end speech processing toolkit BIBREF28 .",Multilingual experiments,"Multilingual approaches used in hybrid RNN/DNN-HMM systems BIBREF10 have been used for for tackling the problem of low-resource data condition. Some of these approaches include language adaptive training and shared layer retraining BIBREF29 . Among them, the most benefited method is the parameter sharing technique BIBREF10 . To incorporate the former approach into encoder, CTC and attention decoder model, we performed the following experiments:",Stage 0 - Naive approach,"In this approach, the model is first trained with 10 multiple languages as denoted in table TABREF14 approximating to 600 hours of training data. data from all languages available during training is used to build a single seq2seq model. The model is trained with a character label set composed of characters from all languages including both train and target set as mentioned in table TABREF14 . The model provides better generalization across languages. Languages with limited data when trained with other languages allows them to be robust and helps in improving the recognition performance. In spite of being simple, the model has limitations in keeping the target language data unseen during training. Table TABREF16 shows the recognition performance of naive multilingual approach using BLSTMP and VGG model against a monolingual model trained with BLSTMP. The results clearly indicate that having a better architecture such as VGG-BLSTM helps in improving multilingual performance. Except Pashto, Georgian and Tokpisin, the multilingual VGG-BLSTM model gave 8.8 % absolute gain in average over monolingual model. In case of multilingual BLSTMP, except Pashto and Georgian an absolute gain of 5.0 % in average is observed over monolingual model. Even though the VGG-BLSTM gave improvements, we were not able to perform stage-1 and stage-2 retraining with it due to time constraints. Thus, we proceed further with multilingual BLSTMP model for retraining experiments tabulated below.",Stage 1 - Retraining decoder only,"To alleviate the limitation in the previous approach, the final layer of the seq2seq model which is mainly responsible for classification is retrained to the target language. In previous works BIBREF10 , BIBREF29 related to hybrid DNN/RNN models and CTC based models BIBREF11 , BIBREF14 the softmax layer is only adapted. However in our case, the attention decoder and CTC decoder both have to be retrained to the target language. This means the CTC and attention layers are only updated for gradients during this stage. We found using SGD optimizer with initial learning rate of INLINEFORM0 works better for retraining compared to AdaDelta. The learning rate is decayed in this training at a factor of INLINEFORM0 if there is a drop in validation accuracy. Table TABREF20 shows the performance of simply retraining the last layer using a single target language Assamese.",Stage 2 - Finetuning both encoder and decoder,"Based on the observations from stage-1 model in section SECREF22 , we found that simply retraining the decoder towards a target language resulted in degrading %CER the performance from 45.6 to 61.3. This is mainly due to the difference in distribution across encoder and decoder. So, to alleviate this difference the encoder and decoder is once again retrained or fine-tuned using the model from stage-1. The optimizer used here is SGD as in stage-1, but the initial learning rate is kept to INLINEFORM0 and decayed based on validation performance. The resulting model gave an absolute gain of 1.6% when finetuned a multilingual model after 4th epoch. Also, finetuning a model after 15th epoch gave an absolute gain of 4.3%. To further investigate the performance of this approach across different target data sizes, we split the train set into INLINEFORM0 5 hours, INLINEFORM1 10 hours, INLINEFORM2 20 hours and INLINEFORM3 full set. Since, in this approach the model is only finetuned by initializing from stage-1 model, the model architecture is fixed for all data sizes. Figure FIGREF23 shows the effectiveness of finetuning both encoder and decoder. The gains from 5 to 10 hours was more compared to 20 hours to full set. Table TABREF25 tabulates the % CER obtained by retraining the stage-1 model with INLINEFORM0 full set of target language data. An absolute gain is observed using stage-2 retraining across all languages compared to monolingual model.",Multilingual RNNLM,"In an ASR system, a language model (LM) takes an important role by incorporating external knowledge into the system. Conventional ASR systems combine an LM with an acoustic model by FST giving a huge performance gain. This trend is also shown in general including hybrid ASR systems and neural network-based sequence-to-sequence ASR systems. The following experiments show a benefit of using a language model in decoding with the previous stage-2 transferred models. Although the performance gains in %CER are also generally observed over all target languages, the improvement in %WER was more distinctive. The results shown in the following Fig. FIGREF27 are in %WER. “whole” in each figure means we used all the available data for the target language as full set explained before.   We used a character-level RNNLM, which was trained with 2-layer LSTM on character sequences. We use all available paired text in the corresponding target language to train the LM for the language. No external text data were used. All language models are trained separately from the seq2seq models. When building dictionary, we combined all the characters over all 15 languages mentioned in table TABREF14 to make them work with transferred models. Regardless of the amount of data used for transfer learning, the RNNLM provides consistent gains across all languages over different data sizes. As explained already, language models were trained separately and used to decode jointly with seq2seq models. The intuition behind it is to use the separately trained language model as a complementary component that works with a implicit language model within a seq2seq decoder. The way of RNNLM assisting decoding follows the equation below: DISPLAYFORM0   INLINEFORM0 is a scaling factor that combines the scores from a joint decoding eq.( EQREF13 ) with RNN-LM, denoted as INLINEFORM1 . This approach is called shallow fusion. Our experiments for target languages show that the gains from adding RNNLM are consistent regardless of the amount of data used for transfer learning. In other words, in Figure FIGREF27 , the gap between two lines are almost consistent over all languages. Also, we observe the gain we get by adding RNN-LM in decoding is large. For example, in the case of assamese, the gain by RNN-LM in decoding with a model retrained on 5 hours of the target language data is almost comparable with the model stage-2 retrained with 20 hours of target language data. On average, absolute gain INLINEFORM0 6% is obtained across all target languages as noted in table TABREF28 .",Conclusion,"In this work, we have shown the importance of transfer learning approach such as stage-2 multilingual retraining in a seq2seq model setting. Also, careful selection of train and target languages from BABEL provide a wide variety in recognition performance (%CER) and helps in understanding the efficacy of seq2seq model. The experiments using character-based RNNLM showed the importance of language model in boosting recognition performance (%WER) over all different hours of target data available for transfer learning. Table TABREF25 and TABREF28 summarizes, the effect of these techniques in terms of %CER and %WER. These methods also show their flexibility in incorporating it in attention and CTC based seq2seq model without compromising loss in performance.",Future work,"We could use better architectures such as VGG-BLSTM as a multilingual prior model before transferring them to a new target language by performing stage-2 retraining. The naive multilingual approach can be improved by including language vectors as input or target during training to reduce the confusions. Also, investigation of multilingual bottleneck features BIBREF30 for seq2seq model can provide better performance. Apart from using the character level language model as in this work, a word level RNNLM can be connected during decoding to further improve %WER. The attention based decoder can be aided with the help of RNNLM using cold fusion approach during training to attain a better-trained model. In near future, we will incorporate all the above techniques to get comparable performance with the state-of-the-art hybrid DNN/RNN-HMM systems.",,,,,,,,,,,,,,,,,,,,,,,What data do they train the language models on?,fb56743e942883d7e74a73c70bd11016acddc348,five,familiar,no,,5053f146237e8fc8859ed3984b5d3f02f39266b7,False,,,"In this work, the experiments are conducted using the BABEL speech corpus collected from the IARPA babel program. The corpus is mainly composed of conversational telephone speech (CTS) but some scripted recordings and far field recordings are presented as well. Table TABREF14 presents the details of the languages used in this work for training and evaluation.","In this work, the experiments are conducted using the BABEL speech corpus collected from the IARPA babel program. ",84557b762ca8e23100b16065c4a0968337b65221,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,Do they report BLEU scores?,093dd1e403eac146bcd19b51a2ace316b36c6264,five,familiar,no,,5053f146237e8fc8859ed3984b5d3f02f39266b7,False,False,,,,bcc58d28e8b4a97d534cd089ecb5fdc3af7254b1,258ee4069f740c400c0049a2580945a1cc7f044c,What languages do they use?,1adbdb5f08d67d8b05328ccc86d297ac01bf076c,five,familiar,no,,5053f146237e8fc8859ed3984b5d3f02f39266b7,False,,"Train languages are: Cantonese, Bengali, Pashto, Turkish, Vietnamese, Haitian, Tamil, Kurdish, Tokpisin and Georgian, while Assamese, Tagalog, Swahili, Lao are used as target languages.","In this work, the experiments are conducted using the BABEL speech corpus collected from the IARPA babel program. The corpus is mainly composed of conversational telephone speech (CTS) but some scripted recordings and far field recordings are presented as well. Table TABREF14 presents the details of the languages used in this work for training and evaluation. FLOAT SELECTED: Table 1: Details of the BABEL data used for performing the multilingual experiments",Table TABREF14 presents the details of the languages used in this work for training and evaluation. FLOAT SELECTED: Table 1: Details of the BABEL data used for performing the multilingual experiments,a150d22edd655723d0c8bb780467c01ba522b3ef,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,What architectures are explored to improve the seq2seq model?,da82b6dad2edd4911db1dc59e4ccd7f66c5fd79c,five,familiar,no,,5053f146237e8fc8859ed3984b5d3f02f39266b7,False,,,"Table TABREF16 shows the recognition performance of naive multilingual approach using BLSTMP and VGG model against a monolingual model trained with BLSTMP. The results clearly indicate that having a better architecture such as VGG-BLSTM helps in improving multilingual performance. Except Pashto, Georgian and Tokpisin, the multilingual VGG-BLSTM model gave 8.8 % absolute gain in average over monolingual model. In case of multilingual BLSTMP, except Pashto and Georgian an absolute gain of 5.0 % in average is observed over monolingual model. Even though the VGG-BLSTM gave improvements, we were not able to perform stage-1 and stage-2 retraining with it due to time constraints. Thus, we proceed further with multilingual BLSTMP model for retraining experiments tabulated below. We used a character-level RNNLM, which was trained with 2-layer LSTM on character sequences. We use all available paired text in the corresponding target language to train the LM for the language. No external text data were used. All language models are trained separately from the seq2seq models. When building dictionary, we combined all the characters over all 15 languages mentioned in table TABREF14 to make them work with transferred models. Regardless of the amount of data used for transfer learning, the RNNLM provides consistent gains across all languages over different data sizes.","Table TABREF16 shows the recognition performance of naive multilingual approach using BLSTMP and VGG model against a monolingual model trained with BLSTMP. The results clearly indicate that having a better architecture such as VGG-BLSTM helps in improving multilingual performance. We used a character-level RNNLM, which was trained with 2-layer LSTM on character sequences.",536bde96e2614e35b00f7ac9895813df59846366,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4-Figure1-1.png,"Fig. 1: Hybrid attention/CTC network with LM extension: the shared encoder is trained by both CTC and attention model objectives simultaneously. The joint decoder predicts an output label sequence by the CTC, attention decoder and RNN-LM.",5-Table1-1.png,Table 1: Details of the BABEL data used for performing the multilingual experiments,5-Table2-1.png,Table 2: Experiment details,5-Table4-1.png,Table 4: Comparison of naive approach and training only the last layer performed using the Assamese language,6-Table3-1.png,Table 3: Recognition performance of naive multilingual approach for eval set of 10 BABEL training languages trained with the train set of same languages,6-Table5-1.png,Table 5: Stage-2 retraining across all languages with full set of target language data,,,,,,,,,,,6-Figure2-1.png,"Fig. 2: Difference in performance for 5 hours, 10 hours, 20 hours and full set of target language data used to retrain a multilingual model from stage-1",7-Table6-1.png,Table 6: Recognition performance in %WER using stage-2 retraining and multilingual RNNLM,7-Figure3-1.png,Fig. 3: Recognition performance after integrating RNNLM during decoding in %WER for different amounts of target data,,,,,, BABEL speech corpus ,,,,,,,,,VGG-BLSTM character-level RNNLM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Back Attention Knowledge Transfer for Low-resource Named Entity Recognition,"In recent years, great success has been achieved in the field of natural language processing (NLP), thanks in part to the considerable amount of annotated resources. For named entity recognition (NER), most languages do not have such an abundance of labeled data, so the performances of those languages are comparatively lower. To improve the performance, we propose a general approach called Back Attention Network (BAN). BAN uses translation system to translate other language sentences into English and utilizes the pre-trained English NER model to get task-specific information. After that, BAN applies a new mechanism named back attention knowledge transfer to improve the semantic representation, which aids in generation of the result. Experiments on three different language datasets indicate that our approach outperforms other state-of-the-art methods.",Introduction,"Named entity recognition (NER) is a sequence tagging task that extracts the continuous tokens into specified classes, such as person names, organizations and locations. Current state-of-the-art approaches for NER usually base themselves on long short-term memory recurrent neural networks (LSTM RNNs) and a subsequent conditional random field (CRF) to predict the sequence labels BIBREF0 . Performances of neural NER methods are compromised if the training data are not enough BIBREF1 . This problem is severe for many languages due to a lack of labeled datasets, e.g., German and Spanish. In comparison, NER on English is well developed and there exist abundant labeled data for training purpose. Therefore, in this work, we regard English as a high-resource language, while other languages, even Chinese, as low-resource languages. There is an intractable problem when leveraging English NER system for other languages. The sentences with the same meaning in different languages may have different lengths and the positions of words in these sentences usually do not correspond. Previous work such as BIBREF2 used each single word translation information to enrich the monolingual word embedding. To our knowledge, there is no approach that employs the whole translation information to improve the performance of the monolingual NER system. To address above problem, we introduce an extension to the BiLSTM-CRF model, which could obtain transferred knowledge from a pre-trained English NER system. First, we translate other languages into English. Since the proposed models of BIBREF3 and BIBREF4 , the performance of attention-based machine translation systems is close to the human level. The attention mechanism can make the translation results more accurate. Furthermore, this mechanism has another useful property: the attention weights can represent the alignment information. After translating the low-resource language into English, we utilize the pre-trained English NER model to predict the sentences and record the output states of BiLSTM in this model. The states contain the semantic and task-specific information of the sentences. By using soft alignment attention weights as a transformation matrix, we manage to transfer the knowledge of high resource language — English to other languages. Finally, using both word vectors and the transfer knowledge, we obtain new state-of-the-art results on four datasets.",Model,"In this section, we will introduce the BAN in three parts. Our model is based on the mainstream NER model BIBREF5 , using BiLSTM-CRF as the basic network structure. Given a sentence INLINEFORM0 and corresponding labels INLINEFORM1 , where INLINEFORM2 denotes the INLINEFORM3 th token and INLINEFORM4 denotes the INLINEFORM5 th label. The NER task is to estimate the probability INLINEFORM6 . Figure FIGREF1 shows the main architecture of our model.",Pre-trained Translation and NER Model,"Attention-base translation model We use the system of BIBREF6 , a convolutional sequence to sequence model. It divides translation process into two steps. First, in the encoder step, given an input sentence INLINEFORM0 of length INLINEFORM1 , INLINEFORM2 represents each word as word embedding INLINEFORM3 . After that, we obtain the absolute position of input elements INLINEFORM4 . Both vectors are concatenated to get input sentence representations INLINEFORM5 . Similarly, output elements INLINEFORM6 generated from decoder network have the same structure. A convolutional neural network (CNN) is used to get the hidden state of the sentence representation from left to right. Second, in the decoder step, attention mechanism is used in each CNN layer. In order to acquire the attention value, we combine the current decoder state INLINEFORM7 with the embedding of previous decoder output value INLINEFORM8 : DISPLAYFORM0  For INLINEFORM0 th layer, the attention INLINEFORM1 of the INLINEFORM2 th source element and INLINEFORM3 th state is computed as a dot-product between the decoder state summary INLINEFORM4 and each output INLINEFORM5 of the last encoder layer: DISPLAYFORM0  Then we follow the normal decoder implementation and get target sentence INLINEFORM0 by beam search algorithm. Pre-trained English NER model We construct the English NER system following BIBREF7 . This system uses a bidirectional LSTM as a character-level language model to take context information for word embedding generation. The hidden states of the character language model (CharLM) are used to create contextualized word embeddings. The final embedding INLINEFORM0 is concatenated by the CharLM embedding INLINEFORM1 and GLOVE embedding INLINEFORM2 BIBREF8 . A standard BiLSTM-CRF named entity recognition model BIBREF0 takes INLINEFORM3 to address the NER task.",Back Attention Knowledge Transfer,"The sentences in low-resource languages are used as input to the model. Given a input sentence INLINEFORM0 in low-resource language, we use pre-trained translation model to translate INLINEFORM1 into English and the output is INLINEFORM2 . Simultaneously, we record the average of values for all INLINEFORM3 attention layers: DISPLAYFORM0  After that, we use the pre-trained English NER model to predict the translated sentence INLINEFORM0 . Then, we have the BiLSTM output states: DISPLAYFORM0  where INLINEFORM0 and INLINEFORM1 denote the INLINEFORM2 th forward and backward outputs, respectively. INLINEFORM3 contains the semantic and task-specific information of the translated sentence. And the INLINEFORM4 th row of attention weights matrix INLINEFORM5 represents the correlation between source word INLINEFORM6 with all words in target sentence INLINEFORM7 . Thereafter, to obtain the transfer information INLINEFORM8 of source word, we reversely use the attention weights: DISPLAYFORM0  where INLINEFORM0 represent the whole outputs of BiLSTM, and INLINEFORM1 , INLINEFORM2 . INLINEFORM3 denotes the transfer information of INLINEFORM4 th word in low-resource language and has the same dimensions with INLINEFORM5 .",Named Entity Recognition Architecture,"The low-resource language named entity recognition architecture is based on BIBREF5 . The word embeddings of low-resource language are passed into a BiLSTM-CRF sequence labeling network. The embeddings INLINEFORM0 are used as inputs to the BiLSTM. Then we have: DISPLAYFORM0  Before passing the forward and backward output states INLINEFORM0 into CRF, we concatenate INLINEFORM1 and INLINEFORM2 as a new representation: DISPLAYFORM0  CRF model uses INLINEFORM0 to give the final sequence probability on the possible sequence label INLINEFORM1 : DISPLAYFORM0  At last, the named entity labels are predicted by: DISPLAYFORM0 ",Experiments,"We use experiments to evaluate the effectiveness of our proposed method on NER task. On three different low-resource languages, we conducted an experimental evaluation to prove the effectiveness of our back attention mechanism on the NER task. Four datasets are used in our work, including CoNLL 2003 German BIBREF9 , CoNLL 2002 Spanish BIBREF10 , OntoNotes 4 BIBREF11 and Weibo NER BIBREF12 . All the annotations are mapped to the BIOES format. Table TABREF14 shows the detailed statistics of the datasets.",Experimental Setup,"We implement the basic BiLSTM-CRF model using PyTorch framework. FASTTEXT embeddings are used for generating word embeddings. Translation models are trained on United Nation Parallel Corpus. For pre-trained English NER system, we use the default NER model of Flair.",Settings,"We train our NER model using vanilla SGD with no momentum for 150 epochs, with an initial learning rate of 0.1 and a learning rate annealing method in which the train loss does not fall in 3 consecutive epochs. The hidden size of BiLSTM model is set to 256 and mini-batch size is set to 16. Dropout is applied to word embeddings with a rate of 0.1 and to BiLSTM with a rate of 0.5. We repeat each experiment 5 times under different random seeds and report the average of test set as final performance.",German and Spanish NER,"Experimental results of German and Spanish are shown in table TABREF20 . Evaluation metric is F1-score. We can find that our method CharLM+BiLSTM-CRF+BAN yields the best performance on two languages. And after adding our network to each of the basic models, the performance of each model has been improved. This suggests that the transfer information, obtained from BAN, is helpful for low-resource NER.",Chinese NER,"Chinese is distinct from Latin-based languages. Thence, there are some tricks when processing Chinese corpus. But we only suppose to verify the validity of our method, so we just use the character-level embeddings. Table TABREF22 shows the results on Chinese OntoNotes 4.0. Adding BAN to baseline model leads to an increase from 63.25% to 72.15% F1-score. In order to further improve the performance, we use the BERT model BIBREF20 to produce word embeddings. With no segmentation, we surpass the previous state-of-the-art approach by 6.33% F1-score. For Weibo dataset, the experiment results are shown in Table TABREF23 , where NE, NM and Overall denote named entities, nominal entities and both. The baseline model gives a 33.18% F1-score. Using the transfer knowledge by BAN, the baseline model achieves an immense improvement in F1-score, rising by 10.39%. We find that BAN still gets consistent improvement on a strong model. With BAN, the F1-score of BERT+BiLSTM+CRF increases to 70.76%.",Task-Specific Information from Back Attention Network," BIBREF21 indicates that the representations from higher-level layers of NLP models are more task-specific. Although we do the same task among different languages, the target domains of different datasets are slightly different. So, to prove that back attention knowledge generated by BAN could capture valuable task-specific information between different languages, we use the back attention knowledge alone as word embedding to predict Weibo dataset. We compare three different word embeddings on the baseline model. Experimental results are shown in Table TABREF25 and illustrate that back attention knowledge from BAN has inherent semantic information.",Analysis,"Our proposed approach is the first to leverage hidden states of NER model from another language to improve monolingual NER performance. The training time with or without BAN is almost the same due to the translation module and the English NER module are pre-trained. On large datasets, our model makes a small improvement because some of transfer knowledge obtained from our method is duplicated with the information learned by the monolingual models. On small datasets, e.g., Weibo dataset, a great improvement has been achieved after adding transfer knowledge to the baseline model. The reason maybe is that these datasets are too small to be fully trained and the test datasets have many non-existent characters of the training dataset, even some unrecognized characters. Therefore, some tags labeled incorrectly by monolingual models could be labeled correctly with the additional transfer knowledge which contains task-specific information obtained from BAN. So, the transfer information plays an important role in this dataset.",Conclusion,"In this paper, we seek to improve the performance of NER on low-resource languages by leveraging the well-trained English NER system. This is achieved by way of BAN, which is a simple but extensible approach. It can transfer information between different languages. Empirical experiments show that, on small datasets, our approach can lead to significant improvement on the performance. This property is of great practical importance for low-resource languages. In future work, we plan to extend our method on other NLP tasks, e.g., relation extraction, coreference resolution.",,,,,,,,,,,,,,,,,Which translation system do they use to translate to English?,c45feda62f23245f53e855706e2d8ea733b7fd03,infinity,familiar,no,,2cfd959e433f290bb50b55722370f0d22fe090b7,False,,Attention-based translation model with convolution sequence to sequence model,"Attention-base translation model We use the system of BIBREF6 , a convolutional sequence to sequence model. It divides translation process into two steps. First, in the encoder step, given an input sentence INLINEFORM0 of length INLINEFORM1 , INLINEFORM2 represents each word as word embedding INLINEFORM3 . After that, we obtain the absolute position of input elements INLINEFORM4 . Both vectors are concatenated to get input sentence representations INLINEFORM5 . Similarly, output elements INLINEFORM6 generated from decoder network have the same structure. A convolutional neural network (CNN) is used to get the hidden state of the sentence representation from left to right. Second, in the decoder step, attention mechanism is used in each CNN layer. In order to acquire the attention value, we combine the current decoder state INLINEFORM7 with the embedding of previous decoder output value INLINEFORM8 : DISPLAYFORM0","Attention-base translation model We use the system of BIBREF6 , a convolutional sequence to sequence model.",7ee6a452970790d614ad6eed09a3d005ad3aca0f,c1018a31c3272ce74964a3280069f62f314a1a58,,,,,,,,,Which languages do they work with?,9785ecf1107090c84c57112d01a8e83418a913c1,infinity,familiar,no,,2cfd959e433f290bb50b55722370f0d22fe090b7,False,,,"We use experiments to evaluate the effectiveness of our proposed method on NER task. On three different low-resource languages, we conducted an experimental evaluation to prove the effectiveness of our back attention mechanism on the NER task. Four datasets are used in our work, including CoNLL 2003 German BIBREF9 , CoNLL 2002 Spanish BIBREF10 , OntoNotes 4 BIBREF11 and Weibo NER BIBREF12 . All the annotations are mapped to the BIOES format. Table TABREF14 shows the detailed statistics of the datasets. Table TABREF22 shows the results on Chinese OntoNotes 4.0. Adding BAN to baseline model leads to an increase from 63.25% to 72.15% F1-score. In order to further improve the performance, we use the BERT model BIBREF20 to produce word embeddings. With no segmentation, we surpass the previous state-of-the-art approach by 6.33% F1-score. For Weibo dataset, the experiment results are shown in Table TABREF23 , where NE, NM and Overall denote named entities, nominal entities and both. The baseline model gives a 33.18% F1-score. Using the transfer knowledge by BAN, the baseline model achieves an immense improvement in F1-score, rising by 10.39%. We find that BAN still gets consistent improvement on a strong model. With BAN, the F1-score of BERT+BiLSTM+CRF increases to 70.76%.","Four datasets are used in our work, including CoNLL 2003 German BIBREF9 , CoNLL 2002 Spanish BIBREF10 , OntoNotes 4 BIBREF11 and Weibo NER BIBREF12 .  Table TABREF22 shows the results on Chinese OntoNotes 4.0. ",4dc8976808f837f745e8635ee5eea1e3ea340bec,c1018a31c3272ce74964a3280069f62f314a1a58,Which pre-trained English NER model do they use?,e051d68a7932f700e6c3f48da57d3e2519936c6d,infinity,familiar,no,,2cfd959e433f290bb50b55722370f0d22fe090b7,False,,Bidirectional LSTM based NER model of Flair,"Pre-trained English NER model We construct the English NER system following BIBREF7 . This system uses a bidirectional LSTM as a character-level language model to take context information for word embedding generation. The hidden states of the character language model (CharLM) are used to create contextualized word embeddings. The final embedding INLINEFORM0 is concatenated by the CharLM embedding INLINEFORM1 and GLOVE embedding INLINEFORM2 BIBREF8 . A standard BiLSTM-CRF named entity recognition model BIBREF0 takes INLINEFORM3 to address the NER task. We implement the basic BiLSTM-CRF model using PyTorch framework. FASTTEXT embeddings are used for generating word embeddings. Translation models are trained on United Nation Parallel Corpus. For pre-trained English NER system, we use the default NER model of Flair.","Pre-trained English NER model We construct the English NER system following BIBREF7 . This system uses a bidirectional LSTM as a character-level language model to take context information for word embedding generation.  For pre-trained English NER system, we use the default NER model of Flair.",24c6988b3b8cb3ee9c01fece408786c840551858,c1018a31c3272ce74964a3280069f62f314a1a58,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2-Figure1-1.png,"Figure 1: The architecture of BAN. The source sentences are translated into English and recorded the attention weights. Then the sentences are put into English NER model. After acquiring the outputs of BiLSTM in the English model, we use back attention mechanism to obtain transfer knowledge to aid in generation of the result.",3-Table1-1.png,Table 1: statistic of sentences,3-Table2-1.png,Table 2: Evaluation on low-resource NER,4-Table3-1.png,Table 3: Evaluation on OntoNotes 4.0,4-Table4-1.png,Table 4: Evaluation on Weibo NER,4-Table5-1.png,Table 5: Comparison of different embeddings,,,,,,,,,German Spanish Chinese,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning,"In sequence to sequence learning, the self-attention mechanism proves to be highly effective, and achieves significant improvements in many tasks. However, the self-attention mechanism is not without its own flaws. Although self-attention can model extremely long dependencies, the attention in deep layers tends to overconcentrate on a single token, leading to insufficient use of local information and difficultly in representing long sequences. In this work, we explore parallel multi-scale representation learning on sequence data, striving to capture both long-range and short-range language structures. To this end, we propose the Parallel MUlti-Scale attEntion (MUSE) and MUSE-simple. MUSE-simple contains the basic idea of parallel multi-scale sequence representation learning, and it encodes the sequence in parallel, in terms of different scales with the help from self-attention, and pointwise transformation. MUSE builds on MUSE-simple and explores combining convolution and self-attention for learning sequence representations from more different scales. We focus on machine translation and the proposed approach achieves substantial performance improvements over Transformer, especially on long sequences. More importantly, we find that although conceptually simple, its success in practice requires intricate considerations, and the multi-scale attention must build on unified semantic space. Under common setting, the proposed model achieves substantial performance and outperforms all previous models on three main machine translation tasks. In addition, MUSE has potential for accelerating inference due to its parallelism. Code will be available at this https URL",Introduction,"In recent years, Transformer has been remarkably adept at sequence learning tasks like machine translation BIBREF0, BIBREF1, text classification BIBREF2, BIBREF3, language modeling BIBREF4, BIBREF5, etc. It is solely based on an attention mechanism that captures global dependencies between input tokens, dispensing with recurrence and convolutions entirely. The key idea of the self-attention mechanism is updating token representations based on a weighted sum of all input representations. However, recent research BIBREF6 has shown that the Transformer has surprising shortcomings in long sequence learning, exactly because of its use of self-attention. As shown in Figure 1 (a), in the task of machine translation, the performance of Transformer drops with the increase of the source sentence length, especially for long sequences. The reason is that the attention can be over-concentrated and disperse, as shown in Figure 1 (b), and only a small number of tokens are represented by attention. It may work fine for shorter sequences, but for longer sequences, it causes insufficient representation of information and brings difficulty for the model to comprehend the source information intactly. In recent work, local attention that constrains the attention to focus on only part of the sequences BIBREF7, BIBREF8 is used to address this problem. However, it costs self-attention the ability to capture long-range dependencies and also does not demonstrate effectiveness in sequence to sequence learning tasks. To build a module with both inductive bias of local and global context modelling in sequence to sequence learning, we hybrid self-attention with convolution and present Parallel multi-scale attention called MUSE. It encodes inputs into hidden representations and then applies self-attention and depth-separable convolution transformations in parallel. The convolution compensates for the insufficient use of local information while the self-attention focuses on capturing the dependencies. Moreover, this parallel structure is highly extensible, and new transformations can be easily introduced as new parallel branches, and is also favourable to parallel computation. The main contributions are summarized as follows: We find that the attention mechanism alone suffers from dispersed weights and is not suitable for long sequence representation learning. The proposed method tries to address this problem and achieves much better performance on generating long sequence. We propose a parallel multi-scale attention and explore a simple but efficient method to successfully combine convolution with self-attention all in one module. MUSE outperforms all previous models with same training data and the comparable model size, with state-of-the-art BLEU scores on three main machine translation tasks. MUSE-simple introduce parallel representation learning and brings expansibility and parallelism. Experiments show that the inference speed can be increased by 31% on GPUs.",MUSE: Parallel Multi-Scale Attention,"Like other sequence-to-sequence models, MUSE also adopts an encoder-decoder framework. The encoder takes a sequence of word embeddings $(x_1, \cdots , x_n)$ as input where $n$ is the length of input. It transfers word embeddings to a sequence of hidden representation ${z} = (z_1, \cdots , z_n)$. Given ${z}$, the decoder is responsible for generating a sequence of text $(y_1, \cdots , y_m)$ token by token. The encoder is a stack of $N$ MUSE modules. Residual mechanism and layer normalization are used to connect two adjacent layers. The decoder is similar to encoder, except that each MUSE module in the decoder not only captures features from the generated text representations but also performs attention over the output of the encoder stack through additional context attention. Residual mechanism and layer normalization are also used to connect two modules and two adjacent layers. The key part in the proposed model is the MUSE module, which contains three main parts: self-attention for capturing global features, depth-wise separable convolution for capturing local features, and a position-wise feed-forward network for capturing token features. The module takes the output of $(i-1)$ layer as input and generates the output representation in a fusion way: where “Attention” refers to self-attention, “Conv” refers to dynamic convolution, “Pointwise” refers to a position-wise feed-forward network. The followings list the details of each part. We also propose MUSE-simple, a simple version of MUSE, which generates the output representation similar to the MUSE model except for that it dose not the include convolution operation:",MUSE: Parallel Multi-Scale Attention ::: Attention Mechanism for Global Context Representation,"Self-attention is responsible for learning representations of global context. For a given input sequence $X$, it first projects $X$ into three representations, key $K$, query $Q$, and value $V$. Then, it uses a self-attention mechanism to get the output representation: Where $W^O$, $W^Q$, $W^K$, and $W^V$ are projection parameters. The self-attention operation $\sigma $ is the dot-production between key, query, and value pairs: Note that we conduct a projecting operation over the value in our self-attention mechanism $V_1=VW^V$ here.",MUSE: Parallel Multi-Scale Attention ::: Convolution for Local Context Modeling,"We introduce convolution operations into MUSE to capture local context. To learn contextual sequence representations in the same hidden space, we choose depth-wise convolution BIBREF9 (we denote it as DepthConv in the experiments) as the convolution operation because it includes two separate transformations, namely, point-wise projecting transformation and contextual transformation. It is because that original convolution operator is not separable, but DepthConv can share the same point-wise projecting transformation with self-attention mechanism. We choose dynamic convolution BIBREF10, the best variant of DepthConv, as our implementation. Each convolution sub-module contains multiple cells with different kernel sizes. They are used for capturing different-range features. The output of the convolution cell with kernel size $k$ is: where $W^{V}$ and $W^{out}$ are parameters, $W^{V}$ is a point-wise projecting transformation matrix. The $Depth\_conv$ refers to depth convolution in the work of BIBREF10. For an input sequence $X$, the output $O$ is computed as: where $d$ is the hidden size. Note that we conduct the same projecting operation over the input in our convolution mechanism $V_2=XW^V$ here with that in self-attention mechanism. Shared projection To learn contextual sequence representations in the same hidden space, the projection in the self-attention mechanism $V_1=VW_V$ and that in the convolution mechanism $V_2=XW^V$ is shared. Because the shared projection can project the input feature into the same hidden space. If we conduct two independent projection here: $V_1=VW_1^V$ and $V_2=XW^V_2$, where $W_1^V$ and $W_2^V$ are two parameter matrices, we call it as separate projection. We will analyze the necessity of applying shared projection here instead of separate projection. Dynamically Selected Convolution Kernels We introduce a gating mechanism to automatically select the weight of different convolution cells.",MUSE: Parallel Multi-Scale Attention ::: Point-wise Feed-forward Network for Capturing Token Representations,"To learn token level representations, MUSE concatenates an self-attention network with a position-wise feed-forward network at each layer. Since the linear transformations are the same across different positions, the position-wise feed-forward network can be seen as a token feature extractor. where $W_1$, $b_1$, $W_2$, and $b_2$ are projection parameters.",Experiment,"We evaluate MUSE on four machine translation tasks. This section describes the datasets, experimental settings, detailed results, and analysis.",Experiment ::: Datasets,"WMT14 En-Fr and En-De datasets The WMT 2014 English-French translation dataset, consisting of $36M$ sentence pairs, is adopted as a big dataset to test our model. We use the standard split of development set and test set. We use newstest2014 as the test set and use newstest2012 +newstest2013 as the development set. Following BIBREF11, we also adopt a joint source and target BPE factorization with the vocabulary size of $40K$. For medium dataset, we borrow the setup of BIBREF0 and adopt the WMT 2014 English-German translation dataset which consists of $4.5M$ sentence pairs, the BPE vocabulary size is set to $32K$. The test and validation datasets we used are the same as BIBREF0. IWSLT De-En and En-Vi datasets Besides, we perform experiments on two small IWSLT datasets to test the small version of MUSE with other comparable models. The IWSLT 2014 German-English translation dataset consists of $160k$ sentence pairs. We also adopt a joint source and target BPE factorization with the vocabulary size of $32K$. The IWSLT 2015 English-Vietnamese translation dataset consists of $133K$ training sentence pairs. For the En-Vi task, we build a dictionary including all source and target tokens. The vocabulary size for English is $17.2K$, and the vocabulary size for the Vietnamese is $6.8K$.",Experiment ::: Experimental Settings ::: Model,"For fair comparisons, we only compare models reported with the comparable model size and the same training data. We do not compare BIBREF12 because it is an ensemble method. We build MUSE-base and MUSE-large with the parameter size comparable to Transformer-base and Transformer-large. We adopt multi-head attention BIBREF0 as implementation of self-attention in MUSE module. The number of attention head is set to 4 for MUSE-base and 16 for MUSE-large. We also add the network architecture built by MUSE-simple in the similar way into the comparison. MUSE consists of 12 residual blocks for encoder and 12 residual blocks for decoder, the dimension is set to 384 for MUSE-base and 768 for MUSE-large. The hidden dimension of non linear transformation is set to 768 for MUSE-base and 3072 for MUSE-large. The MUSE-large is trained on 4 Titan RTX GPUs while the MUSE-base is trained on a single NVIDIA RTX 2080Ti GPU. The batch size is calculated at the token level, which is called dynamic batching BIBREF0. We adopt dynamic convolution as the variant of depth-wise separable convolution. We tune the kernel size on the validation set. For convolution with a single kernel, we use the kernel size of 7 for all layers. In case of dynamic selected kernels, the kernel size is 3 for small kernels and 15 for large kernels for all layers.",Experiment ::: Experimental Settings ::: Training,"The training hyper-parameters are tuned on the validation set. MUSE-large For training MUSE-large, following BIBREF13, parameters are updated every 32 steps. We train the model for $80K$ updates with a batch size of 5120 for En-Fr, and train the model for ${30K}$ updates with a batch size of 3584 for En-De. The dropout rate is set to $0.1$ for En-Fr and ${0.3}$ for En-De. We borrow the setup of optimizer from BIBREF10 and use the cosine learning rate schedule with 10000 warmup steps. The max learning rate is set to $0.001$ on En-De translation and ${0.0007}$ on En-Fr translation. For checkpoint averaging, following BIBREF10, we tune the average checkpoints for En-De translation tasks. For En-Fr translation, we do not average checkpoint but use the final single checkpoint. MUSE-base We train and test MUSE-base on two small datasets, IWSLT 2014 De-En translation and IWSLT2015 En-Vi translation. Following BIBREF0, we use Adam optimizer with a learning rate of $0.001$. We use the warmup mechanism and invert the learning rate decay with warmup updates of $4K$. For the De-En dataset, we train the model for $20K$ steps with a batch size of $4K$. The parameters are updated every 4 steps. The dropout rate is set to $0.4$. For the En-Vi dataset, we train the model for $10K$ steps with a batch size of $4K$. The parameters are also updated every 4 steps. The dropout rate is set to $0.3$. We save checkpoints every epoch and average the last 10 checkpoints for inference.",Experiment ::: Experimental Settings ::: Evaluation,"During inference, we adopt beam search with a beam size of 5 for De-En, En-Fr and En-Vi translation tasks. The length penalty is set to 0.8 for En-Fr according to the validation results, 1 for the two small datasets following the default setting of BIBREF14. We do not tune beam width and length penalty but use the setting reported in BIBREF0. The BLEU metric is adopted to evaluate the model performance during evaluation.",Experiment ::: Results,"As shown in Table TABREF24, MUSE outperforms all previously models on En-De and En-Fr translation, including both state-of-the-art models of stand alone self-attention BIBREF0, BIBREF13, and convolutional models BIBREF11, BIBREF15, BIBREF10. This result shows that either self-attention or convolution alone is not enough for sequence to sequence learning. The proposed parallel multi-scale attention improves over them both on En-De and En-Fr. Compared to Evolved Transformer BIBREF19 which is constructed by NAS and also mixes convolutions of different kernel size, MUSE achieves 2.2 BLEU gains in En-Fr translation. Relative position or local attention constraints bring improvements over origin self-attention model, but parallel multi-scale outperforms them. MUSE can also scale to small model and small datasets, as depicted in Table TABREF25, MUSE-base pushes the state-of-the-art from 35.7 to 36.3 on IWSLT De-En translation dataset. It is shown in Table TABREF24 and Table TABREF25 that MUSE-simple which contains the basic idea of parallel multi-scale attention achieves state-of-the-art performance on three major machine translation datasets.",Experiment ::: How do we propose effective parallel multi-scale attention?,"In this subsection we compare MUSE and its variants on IWSLT 2015 De-En translation to answer the question. Does concatenating self-attention with convolution certainly improve the model? To bridge the gap between point-wise transformation which learns token level representations and self-attention which learns representations of global context, we introduce convolution to enhance our multi-scale attention. As we can see from the first experiment group of Table TABREF27, convolution is important in the parallel multi-scale attention. However, it is not easy to combine convolution and self-attention in one module to build better representations on sequence to sequence tasks. As shown in the first line of both second and third group of Table TABREF27, simply learning local representations by using convolution or depth-wise separable convolution in parallel with self-attention harms the performance. Furthermore, combining depth-wise separable convolution (in this work we choose its best variant dynamic convolution as implementation) is even worse than combining convolution. Why do we choose DepthConv and what is the importance of sharing Projection of DepthConv and self-attention? We conjecture that convolution and self-attention both learn contextual sequence representations and they should share the point transformation and perform the contextual transformation in the same hidden space. We first project the input to a hidden representation and perform a variant of depth-wise convolution and self-attention transformations in parallel. The fist two experiments in third group of Table TABREF27 show that validating the utility of sharing Projection in parallel multi-scale attention, shared projection gain 1.4 BLEU scores over separate projection, and bring improvement of 0.5 BLEU scores over MUSE-simple (without DepthConv). How much is the kernel size? Comparative experiments show that the too large kernel harms performance both for DepthConv and convolution. Since there exists self-attention and point-wise transformations, simply applying the growing kernel size schedule proposed in SliceNet BIBREF15 doesn't work. Thus, we propose to use dynamically selected kernel size to let the learned network decide the kernel size for each layer.",Experiment ::: Further Analysis ::: Parallel multi-scale attention brings time efficiency on GPUs,"The underlying parallel structure (compared to the sequential structure in each block of Transformer) allows MUSE to be efficiently computed on GPUs. For example, we can combine small matrices into large matrices, and while it does not reduce the number of actual operations, it can be better paralleled by GPUs to speed up computation. Concretely, for each MUSE module, we first concentrate $W^Q,W^K,W^V$ of self-attention and $W_1$ of point feed-forward transformation into a single encoder matrix $W^{Enc}$, and then perform transformation such as self-attention, depth-separable convolution, and nonlinear transformation, in parallel, to learn multi-scale representations in the hidden layer. $W^O,W_2,W^{out}$ can also be combined a single decoder matrix $W^{Dec}$. The decoder of sequence to sequence architecture can be implemented similarly. In Table TABREF31, we conduct comparisons to show the speed gains with the aforementioned implementation, and the batch size is set to one sample per batch to simulate online inference environment. Under the settings, where the numbers of parameters are similar for MUSE and Transformer, about 31% increase in inference speed can be obtained. The experiments use MUSE with 6 MUSE-simple modules and Transformer with 6 base blocks. The hidden size is set to 512. Parallel multi-scale attention generates much better long sequence As demonstrated in Figure FIGREF32, MUSE generates better sequences of various length than self-attention, but it is remarkably adept at generate long sequence, e.g. for sequence longer than 100, MUSE is two times better. Lower layers prefer local context and higher layers prefer more contextual representations MUSE contains multiple dynamic convolution cells, whose streams are fused by a gated mechanism. The weight for each dynamic cell is a scalar. Here we analyze the weight of different dynamic convolution cells in different layers. Figure FIGREF32 shows that as the layer depth increases, the weight of dynamic convolution cells with small kernel sizes gradually decreases. It demonstrates that lower layers prefer local features while higher layers prefer global features. It is corresponding to the finding in BIBREF26. MUSE not only gains BLEU scores, but also generates more reasonable sentences and increases the translation quality. We conduct the case study on the De-En dataset and the cases are shown in Table TABREF34 in Appendix. In case 1, although the baseline transformer translates many correct words according to the source sentence, the translated sentence is not fluent at all. It indicates that Transformer does not capture the relationship between some words and their neighbors, such as “right” and “clap”. By contrast, MUSE captures them well by combining local convolution with global self-attention. In case 2, the cause adverbial clause is correctly translated by MUSE while transformer misses the word “why” and fails to translate it.",Related Work,"Sequence to sequence learning is an important task in machine learning. It evolves understanding and generating sequence. Machine translation is the touchstone of sequence to sequence learning. Traditional approaches usually adopt long-short term memory networks BIBREF27, BIBREF28 to learn the representation of sequences. However, these models either are built upon auto-regressive structures requiring longer encoding time or perform worse on real-world natural language processing tasks. Recent studies explore convolutional neural networks (CNN) BIBREF11 or self-attention BIBREF0 to support high-parallel sequence modeling and does not require auto-regressive structure during encoding, thus bringing large efficiency improvements. They are strong at capturing local or global dependencies. There are several studies on combining self-attention and convolution. However, they do not surpass both convectional and self-attention mechanisms. BIBREF4 propose to augment convolution with self attention by directly concentrating them in computer vision tasks. However, as demonstrated in Table TABREF27 there method does not work for sequence to sequence learning task. Since state-of-the-art models on question answering tasks still consist on self-attention and do no adopt ideas in QAnet BIBREF29. Both self-attention BIBREF13 and convolution BIBREF10 outperforms Evolved transformer by near 2 BLEU scores on En-Fr translation. It seems that learning global and local context through stacking self-attention and convolution layers does not beat either self-attention or convolution models. In contrast, the proposed parallel multi-scale attention outperforms previous convolution or self-attention based models on main translation tasks, showing its effectiveness for sequence to sequence learning.",Conclusion and Future work,"Although the self-attention mechanism has been prevalent in sequence modeling, we find that attention suffers from dispersed weights especially for long sequences, resulting from the insufficient local information. To address this problem, we present Parallel Multi-scale Attention (MUSE) and MUSE-simple. MUSE-simple introduces the idea of parallel multi-scale attention into sequence to sequence learning. And MUSE fuses self-attention, convolution, and point-wise transformation together to explicitly learn global, local and token level sequence representations. Especially, we find from empirical results that the shared projection plays important part in its success, and is essential for our multi-scale learning. Beyond the inspiring new state-of-the-art results on three major machine translation datasets, detailed analysis and model variants also verify the effectiveness of MUSE. For future work, the parallel structure is highly extensible and provide many opportunities to improve these models. In addition, given the success of shared projection, we would like to explore its detailed effects on contextual representation learning. Finally, we are exited about future of parallel multi-scale attention and plan to apply this simple but effective idea to other tasks including image and speech.",Conclusion and Future work ::: Acknowledgments,This work was supported in part by National Natural Science Foundation of China (No. 61673028).,,,,,,,,,,,What evaluation metric is used?,6e4505609a280acc45b0a821755afb1b3b518ffd,two,familiar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,,,"During inference, we adopt beam search with a beam size of 5 for De-En, En-Fr and En-Vi translation tasks. The length penalty is set to 0.8 for En-Fr according to the validation results, 1 for the two small datasets following the default setting of BIBREF14. We do not tune beam width and length penalty but use the setting reported in BIBREF0. The BLEU metric is adopted to evaluate the model performance during evaluation.",The BLEU metric is adopted to evaluate the model performance during evaluation.,80f7986f576e8d31ae1d31e2b5367edac7b0368d,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,,,,,,,,,What datasets are used?,9bd938859a8b063903314a79f09409af8801c973,two,familiar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,,,"WMT14 En-Fr and En-De datasets The WMT 2014 English-French translation dataset, consisting of $36M$ sentence pairs, is adopted as a big dataset to test our model. We use the standard split of development set and test set. We use newstest2014 as the test set and use newstest2012 +newstest2013 as the development set. Following BIBREF11, we also adopt a joint source and target BPE factorization with the vocabulary size of $40K$. For medium dataset, we borrow the setup of BIBREF0 and adopt the WMT 2014 English-German translation dataset which consists of $4.5M$ sentence pairs, the BPE vocabulary size is set to $32K$. The test and validation datasets we used are the same as BIBREF0. IWSLT De-En and En-Vi datasets Besides, we perform experiments on two small IWSLT datasets to test the small version of MUSE with other comparable models. The IWSLT 2014 German-English translation dataset consists of $160k$ sentence pairs. We also adopt a joint source and target BPE factorization with the vocabulary size of $32K$. The IWSLT 2015 English-Vietnamese translation dataset consists of $133K$ training sentence pairs. For the En-Vi task, we build a dictionary including all source and target tokens. The vocabulary size for English is $17.2K$, and the vocabulary size for the Vietnamese is $6.8K$.","WMT14 En-Fr and En-De datasets The WMT 2014 English-French translation dataset, consisting of $36M$ sentence pairs, is adopted as a big dataset to test our model. For medium dataset, we borrow the setup of BIBREF0 and adopt the WMT 2014 English-German translation dataset which consists of $4.5M$ sentence pairs, the BPE vocabulary size is set to $32K$. IWSLT De-En and En-Vi datasets Besides, we perform experiments on two small IWSLT datasets to test the small version of MUSE with other comparable models. The IWSLT 2014 German-English translation dataset consists of $160k$ sentence pairs. We also adopt a joint source and target BPE factorization with the vocabulary size of $32K$. The IWSLT 2015 English-Vietnamese translation dataset consists of $133K$ training sentence pairs.",c12e2988aa912fff95d557673c627c1b1536bc38,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,What are three main machine translation tasks?,68ba5bf18f351e8c83fae7b444cc50bef7437f13,two,familiar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,,,"During inference, we adopt beam search with a beam size of 5 for De-En, En-Fr and En-Vi translation tasks. The length penalty is set to 0.8 for En-Fr according to the validation results, 1 for the two small datasets following the default setting of BIBREF14. We do not tune beam width and length penalty but use the setting reported in BIBREF0. The BLEU metric is adopted to evaluate the model performance during evaluation.","During inference, we adopt beam search with a beam size of 5 for De-En, En-Fr and En-Vi translation tasks.",a6ee673b81a1c88bb9be0d6e87732b611cda1e2f,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,,,,,,,,How big is improvement in performance over Transformers?,f6a1125c5621a2f32c9bcdd188dff14efa096083,two,familiar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,,,"As shown in Table TABREF24, MUSE outperforms all previously models on En-De and En-Fr translation, including both state-of-the-art models of stand alone self-attention BIBREF0, BIBREF13, and convolutional models BIBREF11, BIBREF15, BIBREF10. This result shows that either self-attention or convolution alone is not enough for sequence to sequence learning. The proposed parallel multi-scale attention improves over them both on En-De and En-Fr. Compared to Evolved Transformer BIBREF19 which is constructed by NAS and also mixes convolutions of different kernel size, MUSE achieves 2.2 BLEU gains in En-Fr translation.","As shown in Table TABREF24, MUSE outperforms all previously models on En-De and En-Fr translation, including both state-of-the-art models of stand alone self-attention BIBREF0, BIBREF13, and convolutional models BIBREF11, BIBREF15, BIBREF10. This result shows that either self-attention or convolution alone is not enough for sequence to sequence learning. The proposed parallel multi-scale attention improves over them both on En-De and En-Fr.

Compared to Evolved Transformer BIBREF19 which is constructed by NAS and also mixes convolutions of different kernel size, MUSE achieves 2.2 BLEU gains in En-Fr translation.",0ba22733c6dbfbff8382e85c59fc336137d5cbf9,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2-Figure1-1.png,"Figure 1: The left figure shows that the performance drops largely with the increase of sentence length on the De-En dataset. The right figure shows the attention map from the 3-th encoder layer. As we can see, the attention map is too dispersed to capture sufficient information. For example, “[EOS]”, contributing little to word alignment, is surprisingly over attended.",3-Figure2-1.png,"Figure 2: Multi-scale attention hybrids point-wise transformation, convolution, and self-attention to learn multi-scale sequence representations in parallel. We project convolution and self-attention into the same space to learn contextual representations.",6-Table1-1.png,Table 1: MUSE-large outperforms all previous models under the standard training and evaluation setting on WMT14 En-De and WMT14 En-Fr datasets.,6-Table2-1.png,Table 2: MUSE-base outperforms previous state-of-the-art models on IWSLT De-En translation datasets and outperforms previous models without BPE processing on IWSLT En-Vi.,7-Table3-1.png,Table 3: Comparisons between MUSE and its variants on the IWSLT 2015 De-En translation task.,8-Table4-1.png,Table 4: The comparison between the inference speed of MUSE and Transformer.,,,,,,,,,WMT14 En-Fr and En-De datasets IWSLT De-En and En-Vi datasets,"De-En, En-Fr and En-Vi translation tasks",8-Figure3-1.png,"Figure 3: BLEU scores of models on different groups with different source sentence lengths. The experiments are conducted on the De-En dataset. MUSE performs better than Transformer, especially on long sentences.",8-Figure4-1.png,Figure 4: Dynamically selected kernels at each layer: The blue bars represent the ratio between the percentage of the convolution with smaller kernel sizes and the percentage of the convolution with large kernel sizes.,12-Table5-1.png,"Table 5: Case study on the De-En dataset. The red bolded words denote the wrong translation and blue bolded words denote the correct translation. In case 1, transformer fails to capture the relationship between some words and their neighbors, such as “right” and “clap”. In case 2, the cause adverbial clause is correctly translated by MUSE while transformer misses the word “why” and fails to translate it.",,,,,,The BLEU metric ,,,,,,,,,2.2 BLEU gains,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Modeling Global Syntactic Variation in English Using Dialect Classification,This paper evaluates global-scale dialect identification for 14 national varieties of English as a means for studying syntactic variation. The paper makes three main contributions: (i) introducing data-driven language mapping as a method for selecting the inventory of national varieties to include in the task; (ii) producing a large and dynamic set of syntactic features using grammar induction rather than focusing on a few hand-selected features such as function words; and (iii) comparing models across both web corpora and social media corpora in order to measure the robustness of syntactic variation across registers.,Syntactic Variation Around the World,"This paper combines grammar induction (Dunn, 2018a, 2018b, 2019) and text classification (Joachims, 1998) to model syntactic variation across national varieties of English. This classification-based approach is situated within the task of dialect identification (Section 2) and evaluated against other baselines for the task (Sections 7 and 8). But the focus is modelling syntactic variation on a global-scale using corpus data. On the one hand, the problem is to use a model of syntactic preferences to predict an author's dialect membership (Dunn, 2018c). On the other hand, the problem is to take a spatially-generic grammar of English that is itself learned from raw text (c.f., Zeman, et al., 2017; Zeman, et al., 2018) and adapt that grammar using dialect identification as an optimization task: which constructions are more likely to occur in a specific regional variety? Because we want a complete global-scale model, we first have to ask: how many national varieties of English are there? This question, considered in Sections 3 and 4, is essential for determining the inventory of regional varieties that need to be included in the dialect identification task. This paper uses data-driven language mapping to find out where English is consistently used, given web data and Twitter data, in order to avoid the arbitrary selection of dialect areas. This is important for ensuring that each construction in the grammar receives the best regional weighting. What syntactic features are needed to represent variation in English? As discussed in Section 6, this paper uses grammar induction on a large background corpus to provide a replicable and dynamic feature space in order to avoid arbitrary limitations (e.g., lists of function words). The other side of this problem is to optimize grammar induction for regional dialects by using an identification task to learn regional weights for each part of the grammar: how much does a single generic grammar of English vary across dialects? To what degree does it represent a single dominant dialect? Finally, a corpus-based approach to variation is restricted to the specific domains or registers that are present in the corpus. To what degree is such a model of variation limited to a specific register? This paper uses both web-crawled corpora and social media corpora to explore the robustness of dialect models across domains (Section 8). Along these same lines, how robust is a model of syntactic variation to the presence of a few highly predictive features? This paper uses unmasking, a method from authorship verification (Koppel, et al., 2007), to evaluate the stability of dialect models over rounds of feature pruning (Section 9).",Previous Work,"Because of its long history as a colonial language (Kachru, 1990), English is now used around the world by diverse national communities. In spite of the global character of English, dialectology and sociolinguistics continue to focus largely on sub-national dialects of English within so-called inner-circle varieties (for example, Labov, et al., 2016; Strelluf, 2016; Schreier, 2016; Clark & Watson, 2016). This paper joins recent work in taking a global approach by using geo-referenced texts to represent national varieties (e.g., Dunn, 2018c; Tamaredo, 2018; Calle-Martin & Romero-Barranco, 2017; Szmrecsanyi, et al., 2016; Sanders, 2010, 2007; c.f., Davies & Fuchs, 2015). For example, this study of dialect classification contains inner-circle (Australia, Canada, United Kingdom, Ireland, New Zealand, United States), outer-circle (India, Malaysia, Nigeria, Philippines, Pakistan, South Africa), and expanding-circle (Switzerland, Portugual) varieties together in a single model. The problem is that these more recent approaches, while they consider more varieties of English, have arbitrarily limited the scope of variation by focusing on a relatively small number of features (Grafmiller & Szmrecsanyi, 2018; Kruger & van Rooy, 2018; Schilk & Schaub, 2016; Collins, 2012). In practical terms, such work uses a smaller range of syntactic representations than comparable work in authorship analysis (c.f., Grieve, 2007; Hirst & Feiguina, 2007; Argamon & Koppel, 2013). From a different perspective, we could view the modelling of dialectal variation as a classification task with the goal of predicting which dialect a sample belongs to. Previous work has draw on many representations that either directly or indirectly capture syntactic patterns (Gamallo, et al., 2016; Barbaresi, 2018; Kreutz & Daelemans, 2018; Kroon, et al., 2018). Given a search for the highest-performing approach, other work has shown that methods and features without a direct linguistic explanation can still achieve impressive accuracies (McNamee, 2016; Ionescu & Popescu, 2016; Belinkov & Glass, 2016; Ali, 2018). On the other hand, there is a conceptual clash between potentially topic-based methods for dialect identification and other tasks that explicitly model place-specific language use. For example, text-based geo-location can use place-based topics to identify where a document is from (c.f., Wing & Baldridge, 2014; Hulden, et al., 2015; Lourentzou, et al., 2017). And, at the same time, place-based topics can be used for both characterizing the functions of a location (c.f., Adams & McKenzie, 2018; Adams, 2015) and disambiguating gazeteers (c.f., Ju, et al., 2016). This raises an important conceptual problem: when does predictive accuracy reflect dialects as opposed to either place-references or place-based content? While geo-referenced corpora capture both types of information, syntactic representations focus specifically on linguistic variation while place-references and place-based topics are part of document content rather than linguistic structure.",Where Is English Used?,"The goal of this paper is to model syntactic variation across all major or robust varieties of English. But how do we know which varieties should be included? Rather than select some set of varieties based on convenience, we take a data-driven approach by collecting global web-crawled data and social media data to determine where English is used. This approach is biased towards developed countries with access to digital technologies. As shown in Table 1, however, enough global language data is available from both sources to determine where national varieties of English exist. Data comes from two sources of digital texts: web pages from the Common Crawl and social media from Twitter. Both types of data have been used previously to study dialectal and spatial variation in language. More commonly, geo-referenced Twitter data has been taken to represent language-use in specific places (e.g., Eisenstein, et al., 2010; Roller, et al., 2012; Kondor, et al., 2013; Mocanu, et al., 2013; Eisenstein, et al., 2014; Graham, et al., 2014; Donoso & Sanchez, 2017); regional variation in Twitter usage was also the subject of a shared task at PAN-17 (Rangel, et al., 2017). Web-crawled data has also been curated and prepared for the purpose of studying spatial variation (Goldhahn, et al., 2012; Davies & Fuchs, 2015), including the use of country-level domains for geo-referencing (Cook & Brinton, 2017). This paper builds on such previous work by systematically collecting geo-referenced data from both sources on a global scale. The full web corpus is available for download. For the Common Crawl data (abbreviated as CC), language samples are geo-located using country-specific top-level domains. The assumption is that a language sample from a web-site under the .ca domain originated from Canada (c.f., Cook & Brinton, 2017). This approach to regionalization does not assume that whoever produced that language sample was born in Canada or represents a traditional Canadian dialect group; rather, the assumption is only that the sample represents someone in Canada who is producing language data. Some countries are not available because their top-level domains are used for other purposes (i.e., .ai, .fm, .io, .ly, .ag, .tv). Domains that do not contain geographic information are also removed from consideration (e.g., .com sites). The Common Crawl dataset covers 2014 through the end of 2017, totalling 81.5 billion web pages. As shown in Table 1, after processing this produces a corpus of 16.65 billion words. The basic procedure for processing the Common Crawl data is to look at text within paragraph tags: any document with at least 40 words within paragraph tags from a country-level domain is processed. Noise like navigational items, boilerplate text, and error messages is removed using heuristic searches and also using deduplication: any text that occurs multiple times on the same site or multiple times within the same month is removed. A second round of deduplication is used over the entire dataset to remove texts in the same language that occur in the same country. Its limited scope makes this final deduplication stage possible. For reproducibility, the code used for collecting and processing the Common Crawl data is also made available. The use of country-level domains for geo-referencing raises two questions: First, are there many domains that are not available because they are not used or are used for non-geographic purposes? After removing irrelevant domains like .tv, the CC dataset covers 166 countries (30 of which are not included in the Twitter corpus) while the Twitter corpus covers 169 countries (33 of which are not included in the CC corpus). Thus, while the use of domains does remove some countries from consideration, the effect is limited. Second, does the amount of data for each country domain reflect the actual number of web pages from that country? In other words, some countries like the United States are less likely to use their top-level codes. However, the United States is still well-represented in the model. The bigger worry is that regional varieties from Africa or East Asia, both of which are under-represented in these datasets, might be missing from the model. For the Twitter corpus, a spatial search is used to collect Tweets from within a 50km radius of 10k cities. Such a search avoids biasing the selection by using language-specific keywords or hashtags. The Twitter data covers the period from May of 2017 until early 2019. This creates a corpus containing 1,066,038,000 Tweets. The language identification component, however, only provides reliable predictions for samples containing at least 50 characters. Thus, the corpus is pruned to include only those Tweets above that length threshold. As shown in Table 1, this produces a corpus containing 4.14 billion words with a global distribution. Language identification (LID) is important here because a failure to identify some regional varieties of English will ultimately bias the model. The LID system used is available for testing. But given that the focus is a major language, English, the performance of LID is not a significant factor in the overall model of syntactic variation. The datasets summarized in Table 1 include many languages other than English. The purpose is to provide background information about where robust varieties of English are found: where is English discovered when the search is not biased by looking only for English? On the one hand, some regions may be under-represented in these datasets; if national varieties are missing from a region, it could be (i) that there is no national variety of English or (ii) that there is not enough data available from that region. On the other hand, Table 1 shows that each region is relatively well-represented, providing confidence that we are not missing other important varieties.",How Many Varieties of English?,"We take a simple threshold-based approach to the question of which regional varieties to include: any national variety that has at least 15 million words in both the Common Crawl and Twitter datasets is included in the attempt to model all global varieties of English. This threshold is chosen in order to ensure that sufficient training/testing/development samples are available for each variety. The inventory of national varieties in Table 2 is entirely data-driven and does not depend on distinctions like dialects vs. varieties, inner-circle vs. outer-circle, or native vs. non-native. Instead, the selection is empirical: any area with a large amount of observed English usage is assumed to represent a regional variety. Since the regions here are based on national boundaries, we call these national varieties. We could just as easily call them national dialects. Nevertheless, the inventory (sorted by region) contains within it some important combinations. There are two African varieties, two south Asian varieties, two southeast Asian varieties, two native-speaker European varieties and two non-native-speaker European varieties. Taken together, these pairings provide a rich ground for experimentation. Are geographically closer varieties more linguistically similar? Is there an empirical reality to the distinction between inner-circle and outer-circle varieties (e.g., American English vs. Malaysian English)? The importance of this language-mapping approach is that it does not assume the inventory of regions.",Data Preparation and Division,"The goal of this paper is to model syntactic variation using geo-referenced documents taken from web-crawled and social media corpora. Such geo-referenced documents represent language use in a particular place but, unlike traditional dialect surveys, there is no assurance that individual authors are native speakers from that place. We have to assume that most language samples from a given country represent the native English variety of that country. For example, many non-local residents live in Australia; we only have to assume that most speakers observed in Australia are locals. In order to average out the influence of out-of-place samples, we use random aggregation to create samples of exactly 1,000 words in both corpora. For example, in the Twitter corpus this means that an average of 59 individual Tweets from a place are combined into a single sample. First, this has the effect of providing more constructions per sample, making the modeling task more approachable. Second and more importantly, individual out-of-place Tweets are reduced in importance because they are aggregated with other Tweets presumably produced by local speakers. The datasets are formed into training, testing, and development sets as follows: First, 2k samples are used for development purposes regardless of the amount of data from a given regional variety. Depending on the size of each variety, at least 12k training and 2.5k testing samples are available. Because some varieties are represented by much larger corpora (i.e., Tweets from American English), a maximum of 25k training samples and 5k testing samples are allowed per variety per register. This creates a corpus with 327,500 training and 66,500 testing samples (CC) and a corpus with 308,000 training and 64,000 testing samples (TW). As summarized in Table 3, these datasets contain significantly more observations than have been used in previous work (c.f., Dunn, 2018c).",Learning the Syntactic Feature Space,"Past approaches to syntactic representation for this kind of task used part-of-speech n-grams (c.f., Hirst & Feiguina, 2007) or lists of function words (c.f., Argamon & Koppel, 2013) to indirectly represent grammatical patterns. Recent work (Dunn, 2018c), however, has introduced the use of a full-scale syntactic representations based on grammar induction (Dunn, 2017, 2018a, 2019) within the Construction Grammar paradigm (CxG: Langacker, 2008; Goldberg, 2006). The idea is that this provides a replicable syntactic representation. A CxG, in particular, is useful for text classification tasks because it is organized around complex constructions that can be quantified using frequency. For example, the ditransitive construction in (1) is represented using a sequence of slot-constraints. Some of these slots have syntactic fillers (i.e., noun) and some have joint syntactic-semantic fillers (i.e., V:transfer). Any utterance, as in (2) or (3), that satisfies these slot-constraints counts as an example or instance of the construction. This provides a straight-forward quantification of a grammar as a one-hot encoding of construction frequencies. (1) [noun – V:transfer – N:animate – noun]  (2) “He mailed Mary a letter.""  (3) “She gave me a hand."" This paper compares two learned CxGs: first, the same grammar used in previous work (Dunn, 2018c); second, a new grammar learned with an added association-based transition extraction algorithm (Dunn, 2019). These are referred to as CxG-1 (the frequency-based grammar in Dunn, 2019) and CxG-2 (the association-based grammar), respectively. Both are learned from web-crawled corpora separate from the corpora used for modeling regional varieties (from Baroni, et al., 2009; Majli̧s & Žabokrtský, 2012; Benko, 2014; and the data provided for the CoNLL 2017 Shared Task: Ginter, et al., 2017). The exact datasets used are available. In both cases a large background corpus is used to represent syntactic constructions that are then quantified in samples from regional varieties. The grammar induction algorithm itself operates in folds, optimizing grammars against individual test sets and then aggregating these fold-specific grammars at the end. This creates, in effect, one large umbrella-grammar that potentially over-represents a regional dialect. From the perspective of the grammar, we can think of false positives (the umbrella-grammar contains constructions that a regional dialect does not use) and false negatives (the umbrella-grammar is missing constructions that are important to a regional dialect). For dialect identification as a task, only missing constructions will reduce prediction performance. How well do CxG-1 and CxG-2 represent the corpora from each regional variety? While prediction accuracies are the ultimate evaluation, we can also look at the average frequency across all constructions for each national dialect. Because the samples are fixed in length, we would expect the same frequencies across all dialects. On the other hand, false positive constructions (which are contained in the umbrella-grammar but do not occur frequently in a national dialect) will reduce the overall feature density for that dialect. Because the classification results do not directly evaluate false positive constructions, we investigate this in Table 4 using the average feature density: the total average frequency per sample, representing how many syntactic constructions from the umbrella-grammar are present in each regional dialect. This is adjusted to show differences from the average for each grammar (i.e., CxG-1 and CxG-2 are each calculated independently). First, CxG-1 has a smaller range of feature densities, with the lowest variety (Portugal English) being only 10.41% different from the highest variety (UK English). This range is much higher for CxG-2, with a 36.01% difference between the lowest variety (Philippines English) and the highest variety (Irish English). One potential explanation for the difference is that CxG-2 is a better fit for the inner-circle dominated training data. This is a question for future work. For now, both grammars pattern together in a general sense: the highest feature density is found in UK English and varieties more similar to UK English (Ireland, Australia). The lowest density is found in under-represented varieties such as Portugal English or Philippines English. Any grammar-adaptation based on dialect identification will struggle to add unknown constructions from these varieties.",Modeling National Varieties,"The main set of experiments uses a Linear Support Vector Machine (Joachims, 1998) to classify dialects using CxG features. Parameters are tuned using the development data. Given the general robust performance of SVMs in the literature relative to other similar classifiers on variation tasks (c.f., Dunn, et al., 2016), we forego a systematic evaluation of classifiers. We start, in Table 5, with an evaluation of baselines by feature type and dataset. We have two general types of features: purely syntactic representations (CxG-1, CxG-2, Function words) and potentially topic-based features (unigrams, bigrams, trigrams). The highest performing feature on both datasets is simple lexical unigrams, at 30k dimensions. We use a hashing vectorizer to avoid a region-specific bias: the vectorizer does not need to be trained or initialized against a specific dataset so there is no chance that one of the varieties will be over-represented in determining which n-grams are included. But this has the side-effect of preventing the inspection of individual features. Vectors for all experiments are available, along with the trained models that depend on these vectors. As n increases, n-grams tend to represent structural rather than topical information. In this case, performance decreases as n increases. We suggest that this decrease provides an indication that the performance of unigrams is based on location-specific content (e.g., “Chicago"" vs. “Singapore"") rather than on purely linguistic lexical variation (e.g., “jeans"" vs. “denim""). How do we differentiate between predictions based on place-names, those based on place-specific content, and those based on dialectal variation? That is a question for future work. For example, is it possible to identify and remove location-specific content terms? Here we focus instead on using syntactic representations that are not subject to such interference. Within syntactic features, function words perform the worst on both datasets with F1s of 0.65 and 0.55. This is not surprising because function words in English do not represent syntactic structures directly; they are instead markers of the types of structures being used. CxG-1 comes next with F1s of 0.80 and 0.76, a significant improvement over the function-word baseline but not approaching unigrams. Note that the experiments using this same grammar in previous work (Dunn, 2018c) were applied to samples of 2k words each. Finally, CxG-2 performs the best, with F1s of 0.96 and 0.92, falling behind unigrams but rivaling bigrams and surpassing trigrams. Because of this, the more detailed experiments below focus only on the CxG-2 grammar. A closer look at both datasets by region for CxG-2 is given in Table 6. The two datasets (web-crawled and social media) present some interesting divergences. For example, Australian English is among the better performing varieties on the CC dataset (F1 = 0.97) but among the worst performing varieties on Twitter (F1 = 0.83). This is the case even though the variety we would assume would be most-often confused with Australian English (New Zealand English) has a stable F1 across domains (both are 0.91). An examination of the confusion matrix (not shown), reveals that errors between New Zealand and Australia are similar between datasets but that the performance of Australian English on Twitter data is reduced by confusion between Australian and Canadian English. In Table 4 we saw that the umbrella-grammar (here, CxG-2) better represents inner-circle varieties, specifically UK English and more closely related varieties. This is probably an indication of the relative representation of the different varieties used to train the umbrella-grammar: grammar induction will implicitly model the variety it is exposed to. It is interesting, then, that less typical varieties like Pakistan English and Philippines English (which had lower feature densities) have higher F1s in the dialect identification task. On the one hand, the syntactic differences between these varieties and inner-circle varieties means that the umbrella-grammar misses some of their unique constructions. On the other hand, their greater syntactic difference makes these varieties easier to identify: they are more distinct in syntactic terms even though they are less well represented. Which varieties are the most similar syntactically given this model? One way to quantify similarity is using errors: which varieties are the most frequently confused? American and Canadian English have 221 misclassified samples (CC), while Canadian and UK English are only confused 36 times. This reflects an intuition that Canadian English is much more similar to American English than it is to UK English. New Zealand and Australian English have 101 misclassifications (again, on CC); but New Zealand and South African English have 266. This indicates that New Zealand English is more syntactically similar to South African English than to Australian English. However, more work on dialect similarity is needed to confirm these findings across different datasets.",Varieties on the Web and Social Media,"How robust are models of syntactic variation across domains: in other words, does web-crawled data provide the same patterns as social media data? We conduct two types of experiments to evaluate this: First, we take dialect as a cross-domain phenomenon and train/test models on both datasets together, ignoring the difference between registers. Second, we evaluate models trained entirely on web-crawled data against testing data from social media (and vice-versa), evaluating a single model across registers. The point is to evaluate the impact of registers on syntactic variation: does Australian English have the same profile on both the web and on Twitter? Starting with the register-agnostic experiments, Table 8 shows the classification performance if we lump all the samples into a single dataset (however, the same training and testing data division is still maintained). The overall F1 is the same as the Twitter-only results in Table 6. On the other hand, varieties like Australian English that performed poorly in Twitter perform somewhat better under these conditions. Furthermore, the observation made above that outer-circle varieties are more distinct remains true: the highest performing varieties are the least proto-typical (i.e., Indian English and Philippines English). But a single model does not perform well across the two datasets, as shown in Table 7. The model trained on Twitter data does perform somewhat better than its counterpart, but in both cases there is a significant drop in performance. On the one hand, this is not surprising given differences in the two registers: we expect some reduction in classification performance across domains like this. For example, the unigram baseline suffers a similar reduction to F1s of 0.49 (trained on CC) and 0.55 (trained on Twitter). On the other hand, we would have more confidence in this model of syntactic variation if there was a smaller drop in accuracy. How can we better estimate grammars and variations in grammars across these different registers? Is it a problem of sampling different populations or is there a single population that is showing different linguistic behaviours? These are questions for future work.",Unmasking Dialects,"How robust are classification-based dialect models to a small number of highly predictive features? A high predictive accuracy may disguise a reliance on just a few syntactic variants. Within authorship verification, unmasking has been used as a meta-classification technique to measure the depth of the difference between two text types (Koppel, et al., 2007). The technique uses a linear classifier to distinguish between two texts using chunks of the texts as samples. Here we distinguish between dialects with individual samples as chunks. After each round of classification, the most predictive features are removed. In this case, the highest positive and negative features for each regional dialect are removed for the next classification round. Figure 1 shows the unmasking curve over 100 rounds using the F1 score. Given that there are 14 regional dialects in the model, Figure 1 represents the removal of approximately 2,800 features. For both datasets, the unigram baseline degrades less quickly than the syntactic model. On the one hand, it has significantly more features in total, so that there are more features to support the classification. On the other hand, given that the most predictive features are being removed, this shows that the lexical model has a deeper range of differences available to support classification than the syntactic model. Within the syntactic models, the classifier trained on web-crawled data degrades less quickly than the Twitter model and maintains a higher performance throughout. This unmasking curve is simply a method for visualizing the robustness of a classification model. The syntactic model is less robust to unmasking than the lexical model. At the same time, we know that the syntactic model does not rely on place-names and place-based content and thus represents a more traditional linguistic approach to variation.",Discussion,"This paper has used data-driven language mapping to select national dialects of English to be included in a global dialect identification model. The main experiments have focused on a dynamic syntactic feature set, showing that it is possible to predict dialect membership within-domain with only a small loss of performance against lexical models. This work raises two remaining problems: First, we know that location-specific content (i.e., place names, place references, national events) can be used for geo-location and text-based models of place. To what degree does a lexical approach capture linguistic variation (i.e., “pop"" vs. “soda"") and to what degree is it capturing non-linguistic information (i.e., “Melbourne"" vs. “London"")? This is an essential problem for dialect identification models. A purely syntactic model does not perform as well as a lexical model, but it does come with more guarantees. Second, we have seen that inner-circle varieties have higher feature densities given the grammars used here. This implies that there are syntactic constructions in varieties like Philippines English that have not been modeled by the grammar induction component. While dialect identification can be used to optimize regional weights for known constructions, how can such missing constructions be adapted? This remains a challenge. While the less proto-typical dialects have higher F1s (i.e., Pakistan English), they also have lower feature densities. This indicates that some of their constructions are missing from the grammar. Nevertheless, this paper has shown that a broader syntactic feature space can be used to model the difference between many national varieties of English.",,,,,,,,,,,,,,,,,,,,,,,What do the models that they compare predict?,43d8057ff0d3f0c745a7164aed7ed146674630e0,infinity,unfamiliar,no,,50d8b4a941c26b89482c94ab324b5a274f9ced66,False,,,"The main set of experiments uses a Linear Support Vector Machine (Joachims, 1998) to classify dialects using CxG features. Parameters are tuned using the development data. Given the general robust performance of SVMs in the literature relative to other similar classifiers on variation tasks (c.f., Dunn, et al., 2016), we forego a systematic evaluation of classifiers. This paper has used data-driven language mapping to select national dialects of English to be included in a global dialect identification model. The main experiments have focused on a dynamic syntactic feature set, showing that it is possible to predict dialect membership within-domain with only a small loss of performance against lexical models. This work raises two remaining problems:","The main set of experiments uses a Linear Support Vector Machine (Joachims, 1998) to classify dialects using CxG features. This paper has used data-driven language mapping to select national dialects of English to be included in a global dialect identification model. ",a85e7b72967bf239b184ce129eec43a5e8df773f,7dd5db428d7a43d2945b97c0c07fa56af4eb02ae,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2-Table1-1.png,Table 1: Background Corpus Size in Words by Region,4-Table3-1.png,Table 3: Samples by Function and Dataset,4-Table2-1.png,Table 2: English Varieties by Dataset in N. Words,5-Table4-1.png,Table 4: Relative Average Feature Density,6-Table5-1.png,Table 5: Classification Performance By Feature Set,7-Table6-1.png,Table 6: Within-Domain Classification Performance (CxG-2),,,,,,,,,,,8-Table7-1.png,"Table 7: Cross-Domain Models, Trained on CC (Left) and Trained on TW (Right), CxG-2",8-Table8-1.png,Table 8: Single-Set Classification Performance,9-Figure1-1.png,Figure 1: Performance Over 100 Rounds of Unmasking (F1),,,,,,national dialects of English,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Machine Translation from Natural Language to Code using Long-Short Term Memory,"Making computer programming language more understandable and easy for the human is a longstanding problem. From assembly language to present day’s object-oriented programming, concepts came to make programming easier so that a programmer can focus on the logic and the architecture rather than the code and language itself. To go a step further in this journey of removing human-computer language barrier, this paper proposes machine learning approach using Recurrent Neural Network (RNN) and Long-Short Term Memory (LSTM) to convert human language into programming language code. The programmer will write expressions for codes in layman’s language, and the machine learning model will translate it to the targeted programming language. The proposed approach yields result with 74.40% accuracy. This can be further improved by incorporating additional techniques, which are also discussed in this paper.",Introduction,"Removing computer-human language barrier is an inevitable advancement researchers are thriving to achieve for decades. One of the stages of this advancement will be coding through natural human language instead of traditional programming language. On naturalness of computer programming D. Knuth said, “Let us change our traditional attitude to the construction of programs: Instead of imagining that our main task is to instruct a computer what to do, let us concentrate rather on explaining to human beings what we want a computer to do.”BIBREF0. Unfortunately, learning programming language is still necessary to instruct it. Researchers and developers are working to overcome this human-machine language barrier. Multiple branches exists to solve this challenge (i.e. inter-conversion of different programming language to have universally connected programming languages). Automatic code generation through natural language is not a new concept in computer science studies. However, it is difficult to create such tool due to these following three reasons– Programming languages are diverse An individual person expresses logical statements differently than other Natural Language Processing (NLP) of programming statements is challenging since both human and programming language evolve over time In this paper, a neural approach to translate pseudo-code or algorithm like human language expression into programming language code is proposed.",Problem Description,"Code repositories (i.e. Git, SVN) flourished in the last decade producing big data of code allowing data scientists to perform machine learning on these data. In 2017, Allamanis M et al. published a survey in which they presented the state-of-the-art of the research areas where machine learning is changing the way programmers code during software engineering and development process BIBREF1. This paper discusses what are the restricting factors of developing such text-to-code conversion method and what problems need to be solved–",Problem Description ::: Programming Language Diversity,"According to the sources, there are more than a thousand actively maintained programming languages, which signifies the diversity of these language . These languages were created to achieve different purpose and use different syntaxes. Low-level languages such as assembly languages are easier to express in human language because of the low or no abstraction at all whereas high-level, or Object-Oriented Programing (OOP) languages are more diversified in syntax and expression, which is challenging to bring into a unified human language structure. Nonetheless, portability and transparency between different programming languages also remains a challenge and an open research area. George D. et al. tried to overcome this problem through XML mapping BIBREF2. They tried to convert codes from C++ to Java using XML mapping as an intermediate language. However, the authors encountered challenges to support different features of both languages.",Problem Description ::: Human Language Factor,"One of the motivations behind this paper is - as long as it is about programming, there is a finite and small set of expression which is used in human vocabulary. For instance, programmers express a for-loop in a very few specific ways BIBREF3. Variable declaration and value assignment expressions are also limited in nature. Although all codes are executable, human representation through text may not due to the semantic brittleness of code. Since high-level languages have a wide range of syntax, programmers use different linguistic expressions to explain those. For instance, small changes like swapping function arguments can significantly change the meaning of the code. Hence the challenge remains in processing human language to understand it properly which brings us to the next problem-",Problem Description ::: NLP of statements,"Although there is a finite set of expressions for each programming statements, it is a challenge to extract information from the statements of the code accurately. Semantic analysis of linguistic expression plays an important role in this information extraction. For instance, in case of a loop, what is the initial value? What is the step value? When will the loop terminate? Mihalcea R. et al. has achieved a variable success rate of 70-80% in producing code just from the problem statement expressed in human natural language BIBREF3. They focused solely on the detection of step and loops in their research. Another research group from MIT, Lei et al. use a semantic learning model for text to detect the inputs. The model produces a parser in C++ which can successfully parse more than 70% of the textual description of input BIBREF4. The test dataset and model was initially tested and targeted against ACM-ICPC participantsínputs which contains diverse and sometimes complex input instructions. A recent survey from Allamanis M. et al. presented the state-of-the-art on the area of naturalness of programming BIBREF1. A number of research works have been conducted on text-to-code or code-to-text area in recent years. In 2015, Oda et al. proposed a way to translate each line of Python code into natural language pseudocode using Statistical Machine Learning Technique (SMT) framework BIBREF5 was used. This translation framework was able to - it can successfully translate the code to natural language pseudo coded text in both English and Japanese. In the same year, Chris Q. et al. mapped natural language with simple if-this-then-that logical rules BIBREF6. Tihomir G. and Viktor K. developed an Integrated Development Environment (IDE) integrated code assistant tool anyCode for Java which can search, import and call function just by typing desired functionality through text BIBREF7. They have used model and mapping framework between function signatures and utilized resources like WordNet, Java Corpus, relational mapping to process text online and offline. Recently in 2017, P. Yin and G. Neubig proposed a semantic parser which generates code through its neural model BIBREF8. They formulated a grammatical model which works as a skeleton for neural network training. The grammatical rules are defined based on the various generalized structure of the statements in the programming language.",Proposed Methodology,"The use of machine learning techniques such as SMT proved to be at most 75% successful in converting human text to executable code. BIBREF9. A programming language is just like a language with less vocabulary compared to a typical human language. For instance, the code vocabulary of the training dataset was 8814 (including variable, function, class names), whereas the English vocabulary to express the same code was 13659 in total. Here, programming language is considered just like another human language and widely used SMT techniques have been applied.",Proposed Methodology ::: Statistical Machine Translation,"SMT techniques are widely used in Natural Language Processing (NLP). SMT plays a significant role in translation from one language to another, especially in lexical and grammatical rule extraction. In SMT, bilingual grammatical structures are automatically formed by statistical approaches instead of explicitly providing a grammatical model. This reduces months and years of work which requires significant collaboration between bi-lingual linguistics. Here, a neural network based machine translation model is used to translate regular text into programming code.",Proposed Methodology ::: Statistical Machine Translation ::: Data Preparation,"SMT techniques require a parallel corpus in thr source and thr target language. A text-code parallel corpus similar to Fig. FIGREF12 is used in training. This parallel corpus has 18805 aligned data in it . In source data, the expression of each line code is written in the English language. In target data, the code is written in Python programming language.",Proposed Methodology ::: Statistical Machine Translation ::: Vocabulary Generation,"To train the neural model, the texts should be converted to a computational entity. To do that, two separate vocabulary files are created - one for the source texts and another for the code. Vocabulary generation is done by tokenization of words. Afterwards, the words are put into their contextual vector space using the popular word2vec BIBREF10 method to make the words computational.",Proposed Methodology ::: Statistical Machine Translation ::: Neural Model Training,"In order to train the translation model between text-to-code an open source Neural Machine Translation (NMT) - OpenNMT implementation is utilized BIBREF11. PyTorch is used as Neural Network coding framework. For training, three types of Recurrent Neural Network (RNN) layers are used – an encoder layer, a decoder layer and an output layer. These layers together form a LSTM model. LSTM is typically used in seq2seq translation. In Fig. FIGREF13, the neural model architecture is demonstrated. The diagram shows how it takes the source and target text as input and uses it for training. Vector representation of tokenized source and target text are fed into the model. Each token of the source text is passed into an encoder cell. Target text tokens are passed into a decoder cell. Encoder cells are part of the encoder RNN layer and decoder cells are part of the decoder RNN layer. End of the input sequence is marked by a $<$eos$>$ token. Upon getting the $<$eos$>$ token, the final cell state of encoder layer initiate the output layer sequence. At each target cell state, attention is applied with the encoder RNN state and combined with the current hidden state to produce the prediction of next target token. This predictions are then fed back to the target RNN. Attention mechanism helps us to overcome the fixed length restriction of encoder-decoder sequence and allows us to process variable length between input and output sequence. Attention uses encoder state and pass it to the decoder cell to give particular attention to the start of an output layer sequence. The encoder uses an initial state to tell the decoder what it is supposed to generate. Effectively, the decoder learns to generate target tokens, conditioned on the input sequence. Sigmoidal optimization is used to optimize the prediction.",Result Analysis,"Training parallel corpus had 18805 lines of annotated code in it. The training model is executed several times with different training parameters. During the final training process, 500 validation data is used to generate the recurrent neural model, which is 3% of the training data. We run the training with epoch value of 10 with a batch size of 64. After finishing the training, the accuracy of the generated model using validation data from the source corpus was 74.40% (Fig. FIGREF17). Although the generated code is incoherent and often predict wrong code token, this is expected because of the limited amount of training data. LSTM generally requires a more extensive set of data (100k+ in such scenario) to build a more accurate model. The incoherence can be resolved by incorporating coding syntax tree model in future. For instance– ""define the method tzname with 2 arguments: self and dt."" is translated into– def __init__ ( self , regex ) :. The translator is successfully generating the whole codeline automatically but missing the noun part (parameter and function name) part of the syntax.",Conclusion & Future Works,"The main advantage of translating to a programming language is - it has a concrete and strict lexical and grammatical structure which human languages lack. The aim of this paper was to make the text-to-code framework work for general purpose programming language, primarily Python. In later phase, phrase-based word embedding can be incorporated for improved vocabulary mapping. To get more accurate target code for each line, Abstract Syntax Tree(AST) can be beneficial. The contribution of this research is a machine learning model which can turn the human expression into coding expressions. This paper also discusses available methods which convert natural language to programming language successfully in fixed or tightly bounded linguistic paradigm. Approaching this problem using machine learning will give us the opportunity to explore the possibility of unified programming interface as well in the future.",Acknowledgment,"We would like to thank Dr. Khandaker Tabin Hasan, Head of the Depertment of Computer Science, American International University-Bangladesh for his inspiration and encouragement in all of our research works. Also, thanks to Future Technology Conference - 2019 committee for partially supporting us to join the conference and one of our colleague - Faheem Abrar, Software Developer for his thorough review and comments on this research work and supporting us by providing fund.",,,,,,,,,,,,,,,,,What additional techniques are incorporated?,db9021ddd4593f6fadf172710468e2fdcea99674,two,unfamiliar,no,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,True,,,,,712162ee41fcd33e17f5974b52db5ef08caa28ef,258ee4069f740c400c0049a2580945a1cc7f044c,False,incorporating coding syntax tree model,,,"Although the generated code is incoherent and often predict wrong code token, this is expected because of the limited amount of training data. LSTM generally requires a more extensive set of data (100k+ in such scenario) to build a more accurate model. The incoherence can be resolved by incorporating coding syntax tree model in future. For instance– ""define the method tzname with 2 arguments: self and dt."" is translated into– def __init__ ( self , regex ) :. The translator is successfully generating the whole codeline automatically but missing the noun part (parameter and function name) part of the syntax.","Although the generated code is incoherent and often predict wrong code token, this is expected because of the limited amount of training data. LSTM generally requires a more extensive set of data (100k+ in such scenario) to build a more accurate model. The incoherence can be resolved by incorporating coding syntax tree model in future. For instance–

""define the method tzname with 2 arguments: self and dt.""

is translated into–

def __init__ ( self , regex ) :.

The translator is successfully generating the whole codeline automatically but missing the noun part (parameter and function name) part of the syntax.",ca3b72709cbea8e97d402eef60ef949c8818ae6f,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,What dataset do they use?,8ea4bd4c1d8a466da386d16e4844ea932c44a412,two,unfamiliar,no,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,A parallel corpus where the source is an English expression of code and the target is Python code.,"SMT techniques require a parallel corpus in thr source and thr target language. A text-code parallel corpus similar to Fig. FIGREF12 is used in training. This parallel corpus has 18805 aligned data in it . In source data, the expression of each line code is written in the English language. In target data, the code is written in Python programming language.","SMT techniques require a parallel corpus in thr source and thr target language. A text-code parallel corpus similar to Fig. FIGREF12 is used in training. This parallel corpus has 18805 aligned data in it . In source data, the expression of each line code is written in the English language. In target data, the code is written in Python programming language.",4fe5615cf767f286711731cd0059c208e82a0974,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,Do they compare to other models?,92240eeab107a4f636705b88f00cefc4f0782846,two,unfamiliar,no,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,False,,,,f669e556321ae49a72f0b9be6c4b7831e37edf1d,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,What is the architecture of the system?,4196d329061f5a9d147e1e77aeed6a6bd9b35d18,two,unfamiliar,no,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,,"In order to train the translation model between text-to-code an open source Neural Machine Translation (NMT) - OpenNMT implementation is utilized BIBREF11. PyTorch is used as Neural Network coding framework. For training, three types of Recurrent Neural Network (RNN) layers are used – an encoder layer, a decoder layer and an output layer. These layers together form a LSTM model. LSTM is typically used in seq2seq translation.","For training, three types of Recurrent Neural Network (RNN) layers are used – an encoder layer, a decoder layer and an output layer. These layers together form a LSTM model. LSTM is typically used in seq2seq translation.",c499a5ca56894e542c2c4eabe925b81a2ea4618e,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,How long are expressions in layman's language?,a37e4a21ba98b0259c36deca0d298194fa611d2f,zero,unfamiliar,no,computer vision,258ee4069f740c400c0049a2580945a1cc7f044c,True,,,,,,469be6a5ce7968933dd77a4449dd88ee01d3d579,258ee4069f740c400c0049a2580945a1cc7f044c,What additional techniques could be incorporated to further improve accuracy?,321429282557e79061fe2fe02a9467f3d0118cdd,zero,unfamiliar,no,computer vision,258ee4069f740c400c0049a2580945a1cc7f044c,False,,,"The main advantage of translating to a programming language is - it has a concrete and strict lexical and grammatical structure which human languages lack. The aim of this paper was to make the text-to-code framework work for general purpose programming language, primarily Python. In later phase, phrase-based word embedding can be incorporated for improved vocabulary mapping. To get more accurate target code for each line, Abstract Syntax Tree(AST) can be beneficial.","In later phase, phrase-based word embedding can be incorporated for improved vocabulary mapping. To get more accurate target code for each line, Abstract Syntax Tree(AST) can be beneficial.",2ef1c3976eec3f9d17efac630b098f10d86931e4,258ee4069f740c400c0049a2580945a1cc7f044c,What programming language is target language?,891cab2e41d6ba962778bda297592c916b432226,zero,unfamiliar,no,computer vision,258ee4069f740c400c0049a2580945a1cc7f044c,False,,,"SMT techniques require a parallel corpus in thr source and thr target language. A text-code parallel corpus similar to Fig. FIGREF12 is used in training. This parallel corpus has 18805 aligned data in it . In source data, the expression of each line code is written in the English language. In target data, the code is written in Python programming language.","In target data, the code is written in Python programming language.",3aa253475c66a97de49bc647af6be28b75a92be4,258ee4069f740c400c0049a2580945a1cc7f044c,What dataset is used to measure accuracy?,1eeabfde99594b8d9c6a007f50b97f7f527b0a17,zero,unfamiliar,no,computer vision,258ee4069f740c400c0049a2580945a1cc7f044c,False,,,"Training parallel corpus had 18805 lines of annotated code in it. The training model is executed several times with different training parameters. During the final training process, 500 validation data is used to generate the recurrent neural model, which is 3% of the training data. We run the training with epoch value of 10 with a batch size of 64. After finishing the training, the accuracy of the generated model using validation data from the source corpus was 74.40% (Fig. FIGREF17).","During the final training process, 500 validation data is used to generate the recurrent neural model, which is 3% of the training data. After finishing the training, the accuracy of the generated model using validation data from the source corpus was 74.40%",d07da696fb0d6e94d658c0950e239bb87edb1633,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,,,,,,,,4-Figure1-1.png,Fig. 1. Text-Code bi-lingual corpus,5-Figure2-1.png,Fig. 2. Neural training model architecture of Text-To-Code,6-Figure3-1.png,Fig. 3. Accuracy gain in progress of training the RNN,,,,,,,,,,,,,,,,,,,,,,,,,,,phrase-based word embedding Abstract Syntax Tree(AST),,False, text-code parallel corpus,,,"SMT techniques require a parallel corpus in thr source and thr target language. A text-code parallel corpus similar to Fig. FIGREF12 is used in training. This parallel corpus has 18805 aligned data in it . In source data, the expression of each line code is written in the English language. In target data, the code is written in Python programming language.",A text-code parallel corpus similar to Fig. FIGREF12 is used in training. This parallel corpus has 18805 aligned data in it .,e21d60356450f2765a322002352ee1b8ceb50253,258ee4069f740c400c0049a2580945a1cc7f044c,seq2seq translation,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,validation data,Python,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Casting a Wide Net: Robust Extraction of Potentially Idiomatic Expressions,"Idiomatic expressions like `out of the woods' and `up the ante' present a range of difficulties for natural language processing applications. We present work on the annotation and extraction of what we term potentially idiomatic expressions (PIEs), a subclass of multiword expressions covering both literal and non-literal uses of idiomatic expressions. Existing corpora of PIEs are small and have limited coverage of different PIE types, which hampers research. To further progress on the extraction and disambiguation of potentially idiomatic expressions, larger corpora of PIEs are required. In addition, larger corpora are a potential source for valuable linguistic insights into idiomatic expressions and their variability. We propose automatic tools to facilitate the building of larger PIE corpora, by investigating the feasibility of using dictionary-based extraction of PIEs as a pre-extraction tool for English. We do this by assessing the reliability and coverage of idiom dictionaries, the annotation of a PIE corpus, and the automatic extraction of PIEs from a large corpus. Results show that combinations of dictionaries are a reliable source of idiomatic expressions, that PIEs can be annotated with a high reliability (0.74-0.91 Fleiss' Kappa), and that parse-based PIE extraction yields highly accurate performance (88% F1-score). Combining complementary PIE extraction methods increases reliability further, to over 92% F1-score. Moreover, the extraction method presented here could be extended to other types of multiword expressions and to other languages, given that sufficient NLP tools are available.",Introduction,"Idiomatic expressions pose a major challenge for a wide range of applications in natural language processing BIBREF0. These include machine translation BIBREF1, BIBREF2, semantic parsing BIBREF3, sentiment analysis BIBREF4, and word sense disambiguation BIBREF5. Idioms show significant syntactic and morphological variability (e.g. beans being spilled for spill the beans), which makes them hard to find automatically. Moreover, their non-compositional nature makes idioms really hard to interpret, because their meaning is often very different from the meanings of the words that make them up. Hence, successful systems need not only be able to recognise idiomatic expressions in text or dialogue, but they also need to give a proper interpretation to them. As a matter of fact, current language technology performs badly on idiom understanding, a phenomenon that perhaps has not received enough attention. Nearly all current language technology used in NLP applications is based on supervised machine learning. This requires large amounts of labelled data. In the case of idiom interpretation, however, only small datasets are available. These contain just a couple of thousand idiom instances, covering only about fifty different types of idiomatic expressions. In fact, existing annotated corpora tend to cover only a small set of idiom types, comprising just a few syntactic patterns (e.g., verb-object combinations), of which a limited number of instances are extracted from a large corpus. This is not surprising as preparing and compiling such corpora involves a large amount of manual extraction work, especially if one wants to allow for form variation in the idiomatic expressions (for example, extracting cooking all the books for cook the books). This work involves both the crafting of syntactic patterns to match potential idiomatic expressions and the filtering of false extractions (non-instances of the target expression e.g. due to wrong parses), and increases with the amount of idiom types included in the corpus (which, in the worst case, means an exponential increase in false extractions). Thus, building a large corpus of idioms, especially one that covers many types in many syntactic constructions, is costly. If a high-precision, high-recall system can be developed for the task of extracting the annotation candidates, this cost will be greatly reduced, making the construction of a large corpus much more feasible. The variability of idioms has been a significant topic of interest among researchers of idioms. For example, BIBREF6 investigates the internal and external modification of a set of idioms in a large English corpus, whereas BIBREF7, quantifies and classifies the variation of a set of idioms in a large corpus of Dutch, setting up a useful taxonomy of variation types. Both find that, although idiomatic expressions mainly occur in their dictionary form, there is a significant minority of idiom instances that occur in non-dictionary variants. Additionally, BIBREF8 show that idiom variants retain their idiomatic meaning more often and are processed more easily than previously assumed. This emphasises the need for corpora covering idiomatic expressions to include these variants, and for tools to be robust in dealing with them. As such, the aim of this article is to describe methods and provide tools for constructing larger corpora annotated with a wider range of idiom types than currently in existence due to the reduced amount of manual labour required. In this way we hope to stimulate further research in this area. In contrast to previous approaches, we want to catch as many idiomatic expressions as possible, and we achieve this by casting a wide net, that is, we consider the widest range of possible idiom variants first and then filter out any bycatch in a way that requires the least manual effort. We expect research will benefit from having larger corpora by improving evaluation quality, by allowing for the training of better supervised systems, and by providing additional linguistic insight into idiomatic expressions. A reliable method for extracting idiomatic expressions is not only needed for building an annotated corpus, but can also be used as part of an automatic idiom processing pipeline. In such a pipeline, extracting potentially idiomatic expressions can be seen as a first step before idiom disambiguation, and the combination of the two modules then functions as an complete idiom extraction system. The main research question that we aim to answer in this article is whether dictionary-based extraction of potentially idiomatic expressions is robust and reliable enough to facilitate the creation of wide-coverage sense-annotated idiom corpora. By answering this question we make several contributions to research on multiword expressions, in particular that of idiom extraction. Firstly, we provide an overview of existing research on annotating idiomatic expressions in corpora, showing that current corpora cover only small sets of idiomatic types (Section SECREF3). Secondly, we quantify the coverage and reliability of a set of idiom dictionaries, demonstrating that there is little overlap between resources (Section SECREF4). Thirdly, we develop and release an evaluation corpus for extracting potentially idiomatic expressions from text (Section SECREF5). Finally, various extraction systems and combinations thereof are implemented, made available to the research community, and evaluated empirically (Section SECREF6).",New Terminology: Potentially Idiomatic Expression (PIE),"The ambiguity of phrases like wake up and smell the coffee poses a terminological problem. Usually, these phrases are called idiomatic expressions, which is suitable when they are used in an idiomatic sense, but not so much when they are used in a literal sense. Therefore, we propose a new term: potentially idiomatic expressions, or PIEs for short. The term potentially idiomatic expression refers to those expressions which can have an idiomatic meaning, regardless of whether they actually have that meaning in a given context. So, see the light is a PIE in both `After another explanation, I finally saw the light' and `I saw the light of the sun through the trees', while it is an idiomatic expression in the first context, and a literal phrase in the latter context. The processing of PIEs involves three main challenges: the discovery of (new) PIE types, the extraction of instances of known PIE types in text, and the disambiguation of PIE instances in context. Here, we propose calling the discovery task simply PIE discovery, the extraction task simply PIE extraction, and the disambiguation task PIE disambiguation. Note that these terms contrast with the terms used in existing research. There, the discovery task is called type-based idiom detection and the disambiguation task is called token-based idiom detection (cf. BIBREF10, BIBREF11), although this usage is not always consistent. Because these terms are very similar, they are potentially confusing, and that is why we propose novel terminology. Other terminology comes from literature on multiword expressions (MWEs) more generally, i.e. not specific to idioms. Here, the task of finding new MWE types is called MWE discovery and finding instances of known MWE types is called MWE identification BIBREF12. Note, however, that MWE identification generally consists of finding only the idiomatic usages of these types (e.g. BIBREF13). This means that MWE identification consists of both the extraction and disambiguation tasks, performed jointly. In this work, we propose to split this into two separate tasks, and we are concerned only with the PIE extraction part, leaving PIE disambiguation as a separate problem.",Related Work,"This section is structured so as to reflect the dual contribution of the present work. First, we discuss existing resources annotated for idiomatic expressions. Second, we discuss existing approaches to the automatic extraction of idioms.",Related Work ::: Annotated Corpora and Annotation Schemes for Idioms,"There are four sizeable sense-annotated PIE corpora for English: the VNC-Tokens Dataset BIBREF9, the Gigaword dataset BIBREF14, the IDIX Corpus BIBREF10, and the SemEval-2013 Task 5 dataset BIBREF15. An overview of these corpora is presented in Table TABREF7.",Related Work ::: Annotated Corpora and Annotation Schemes for Idioms ::: VNC-Tokens,"The VNC-Tokens dataset contains 53 different PIE types. BIBREF9 extract up to 100 instances from the British National Corpus for each type, for a total of 2,984 instances. These types are based on a pre-existing list of verb-noun combinations and were filtered for frequency and whether two idiom dictionaries both listed them. Instances were extracted automatically, by parsing the corpus and selecting all sentences with the right verb and noun in a direct-object relation. It is unclear whether the extracted sentences were manually checked, but no false extractions are mentioned in the paper or present in the dataset. All extracted PIE instances were annotated for sense as either idiomatic, literal or unclear. This is a self-explanatory annotation scheme, but BIBREF9 note that senses are not binary, but can form a continuum. For example, the idiomaticity of have a word in `You have my word' is different from both the literal sense in `The French have a word for this' and the figurative sense in `My manager asked to have a word'. They instructed annotators to choose idiomatic or literal even in ambiguous middle-of-the-continuum cases, and restrict the unclear label only to cases where there is not enough context to disambiguate the meaning of the PIE.",Related Work ::: Annotated Corpora and Annotation Schemes for Idioms ::: Gigaword,"BIBREF14 present a corpus of 17 PIE types, for which they extracted all instances from the Gigaword corpus BIBREF18, yielding a total of 3,964 instances. BIBREF14 extracted these instances semi-automatically by manually defining all inflectional variants of the verb in the PIE and matching these in the corpus. They did not allow for inflectional variations in non-verb words, nor did they allow intervening words. They annotated these potential idioms as either literal or figurative, excluding ambiguous and unclear instances from the dataset.",Related Work ::: Annotated Corpora and Annotation Schemes for Idioms ::: IDIX,"BIBREF10 build on the methodology of BIBREF14, but annotate a larger set of idioms (52 types) and extract all occurrences from the BNC rather than the Gigaword corpus, for a total of 4,022 instances including false extractions. BIBREF10 use a more complex semi-automatic extraction method, which involves parsing the corpus, manually defining the dependency patterns that match the PIE, and extracting all sentences containing those patterns from the corpus. This allows for larger form variations, including intervening words and inflectional variation of all words. In some cases, this yields many non-PIE extractions, as for recharge one's batteries in Example SECREF10. These were not filtered out before annotation, but rather filtered out as part of the annotation process, by having false extraction as an additional annotation label. For sense annotation, they use an extensive tagset, distinguishing literal, non-literal, both, meta-linguistic, embedded, and undecided labels. Here, the both label (Example SECREF10) is used for cases where both senses are present, often as a form of deliberate word play. The meta-linguistic label (Example SECREF10) applies to cases where the PIE instance is used as a linguistic item to discuss, not as part of a sentence. The embedded label (Example SECREF10) applies to cases where the PIE is embedded in a larger figurative context, which makes it impossible to say whether a literal or figurative sense is more applicable. The undecided label is used for unclear and undecidable cases. They take into account the fact that a PIE can have multiple figurative senses, and enumerate these separately as part of the annotation. . These high-performance, rugged tools are claimed to offer the best value for money on the market for the enthusiastic d-i-yer and tradesman, and for the first time offer the possibility of a battery recharging time of just a quarter of an hour. (from IDIX corpus, ID #314) . Left holding the baby, single mothers find it hard to fend for themselves. (from BIBREF10, p.642) . It has long been recognised that expressions such as to pull someone's leg, to have a bee in one's bonnet, to kick the bucket, to cook someone's goose, to be off one's rocker, round the bend, up the creek, etc. are semantically peculiar. (from BIBREF10, p.642) . You're like a restless bird in a cage. When you get out of the cage, you'll fly very high. (from BIBREF10, p.642) The both, meta-linguistic, and embedded labels are useful and linguistically interesting distinctions, although they occur very rarely (0.69%, 0.15%, and an unknown %, respectively). As such, we include these cases in our tagset (see Section SECREF5), but group them under a single label, other, to reduce annotation complexity. We also follow BIBREF10 in that we combine both the PIE/non-PIE annotation and the sense annotation in a single task.",Related Work ::: Annotated Corpora and Annotation Schemes for Idioms ::: SemEval-2013 Task 5b,"BIBREF15 created a dataset for SemEval-2013 Task 5b, a task on detecting semantic compositionality in context. They selected 65 PIE types from Wiktionary, and extracted instances from the ukWaC corpus BIBREF17, for a total of 4,350 instances. It is unclear how they extracted the instances, and how much variation was allowed for, although there is some inflectional variation in the dataset. An unspecified amount of manual filtering was done on the extracted instances. The extracted PIE instances were labelled as literal, idiomatic, both, or undecidable. Interestingly, they crowdsourced the sense annotations using CrowdFlower, with high agreement (90%–94% pairwise). Undecidable cases and instances on which annotators disagreed were removed from the dataset.",Related Work ::: Annotated Corpora and Annotation Schemes for Idioms ::: General Multiword Expression Corpora,"In addition to the aforementioned idiom corpora, there are also corpora focused on multiword expressions (MWEs) in a more general sense. As idioms are a subcategory of MWEs, these corpora also include some idioms. The most important of these are the PARSEME corpus BIBREF19 and the DiMSUM corpus BIBREF20. DiMSUM provides annotations of over 5,000 MWEs in approximately 90K tokens of English text, consisting of reviews, tweets and TED talks. However, they do not categorise the MWEs into specific types, meaning we cannot easily quantify the number of idioms in the corpus. In contrast to the corpus-specific sense labels seen in other corpora, DiMSUM annotates MWEs with WordNet supersenses, which provide a broad category of meaning for each MWE. Similarly, the PARSEME corpus consists of over 62K MWEs in almost 275K tokens of text across 18 different languages (with the notable exception of English). The main differences with DiMSUM, except for scale and multilingualism, are that it only includes verbal MWEs, and that subcategorisation is performed, including a specific category for idioms. Idioms make up almost a quarter of all verbal MWEs in the corpus, although the proportion varies wildly between languages. In both corpora, MWE annotation was done in an unrestricted manner, i.e. there was not a predefined set of expressions to which annotation was restricted.",Related Work ::: Annotated Corpora and Annotation Schemes for Idioms ::: Overview,"In sum, there is large variation in corpus creation methods, regarding PIE definition, extraction method, annotation schemes, base corpus, and PIE type inventory. Depending on the goal of the corpus, the amount of deviation that is allowed from the PIE's dictionary form to the instances can be very little BIBREF14, to quite a lot BIBREF10. The number of PIE types covered by each corpus is limited, ranging from 17 to 65 types, often limited to one or more syntactic patterns. The extraction of PIE instances is usually done in a semi-automatic manner, by manually defining patterns in a text or parse tree, and doing some manual filtering afterwards. This works well, but an extension to a large number of PIE types (e.g. several hundreds) would also require a large increase in the amount of manual effort involved. Considering the sense annotations done on the PIE corpora, there is significant variation, with BIBREF9 using only three tags, whereas BIBREF10 use six. Outside of PIE-specific corpora there are MWE corpora, which provide a different perspective. A major difference there is that annotation is not restricted to a pre-specified set of expressions, which has not been done for PIEs specifically.",Related Work ::: Extracting Idioms from Corpora,"There are two main approaches to idiom extraction. The first approach aims to distinguish idioms from other multiword phrases, where the main purpose is to expand idiom inventories with rare or novel expressions BIBREF21, BIBREF22, BIBREF23, BIBREF24. The second approach aims to extract all occurrences of a known idiomatic expression in a text. In this paper, we focus on the latter approach. We rely on idiom dictionaries to provide a list of PIE types, and build a system that extracts all instances of those PIE types from a corpus. High-quality idiom dictionaries exist for most well-resourced languages, but their reliability and coverage is not known. As such, we quantify the coverage of dictionaries in Section SECREF4. There is, to the best of our knowledge, no existing work that focuses on dictionary-based PIE extraction. However, there is closely-related work by BIBREF25, who present a system for the dictionary-based extraction of verb-noun combinations (VNCs) in English and Spanish. In their case, the VNCs can be any kind of multiword expression, which they subdivide into literal expressions, collocations, light verb constructions, metaphoric expressions, and idioms. They extract 173 English VNCs and 150 Spanish VNCs and annotate these with both their lexico-semantic MWE type and the amount of morphosyntactic variation they exhibit. BIBREF25 then compare a word sequence-based method, a chunking-based method, and a parse-based method for VNC extraction. Each method relies on the morpho-syntactic information in order to limit false extractions. Precision is evaluated manually on a sample of the extracted VNCs, and recall is estimated by calculating the overlap between the output of the three methods. Evaluation shows that the methods are highly complementary both in recall, since they extract different VNCs, and in precision, since combining the extractors yields fewer false extractions. Whereas BIBREF25 focus on both idiomatic and literal uses of the set of expressions, like in this paper, BIBREF26 tackle only half of that task, namely extracting only literal uses of a given set of VMWEs in Polish. This complicates the task, since it combines extracting all occurrences of the VMWEs and then distinguishing literal from idiomatic uses. Interestingly, they also experiment with models of varying complexity, i.e. just words, part-of-speech tags, and syntactic structures. Their results are hard to put into perspective however, since the frequency of literal VMWEs in their corpus is very rare, whereas corpora containing PIEs tend to show a more balanced distribution. Other similar work to ours also focuses on MWEs more generally, or on different subtypes of MWEs. In addition, these tend to combine both extraction and disambiguation in that they aim to extract only idiomatically used instances of the MWE, without extracting literally used instances or non-instances. Within this line of work, BIBREF27 focuses on verb-particle constructions, BIBREF28 on verbal MWEs (including idioms), and BIBREF29 on verbal MWEs (especially non-canonical variants). Both BIBREF28 and BIBREF29 rely on a pre-defined set of expressions, whereas BIBREF27 also extracts unseen expressions, although based on a pre-defined set of particles and within the vary narrow syntactic frame of verb-particle constructions. The work of BIBREF27 is most similar to ours in that it builds an unsupervised system using existing NLP tools (PoS taggers, chunkers, parsers) and finds that a combination of systems using those tools performs best, as we find in Section SECREF69. BIBREF28 and BIBREF29, by contrast, use supervised classifiers which require training data, not just for the task in general, but specific to the set of expressions used in the task. Although our approach is similar to that of BIBREF25, both in the range of methods used and in the goal of extracting certain multiword expressions regardless of morphosyntactic variation, there are two main differences. First, we use dictionaries, but extract entries automatically and do not manually annotate their type and variability. As a result, our methods rely only on the surface form of the expression taken from the dictionary. Second, we evaluate precision and recall in a more rigorous way, by using an evaluation corpus exhaustively annotated for PIEs. In addition, we do not put any restriction on the syntactic type of the expressions to be extracted, which BIBREF27, BIBREF28, BIBREF25, and BIBREF29 all do.",Coverage of Idiom Inventories ::: Background,"Since our goal is developing a dictionary-based system for extracting potentially idiomatic expressions, we need to devise a proper method for evaluating such a system. This is not straightforward, even though the final goal of such a system is simple: it should extract all potentially idiomatic expressions from a corpus and nothing else, regardless of their sense and the form they are used in. The type of system proposed here hence has two aspects that can be evaluated: the dictionary that it is using as a resource for idiomatic expression, and the extractor component that finds idioms in a corpus. The difficulty here is that there is no undisputed and unambiguous definition of what counts as an idiom BIBREF30, as is the case with multiword expressions in general BIBREF12. Of course, a complete set of idiomatic expressions for English (or any other language) is impossible to get due to the broad and ever-changing nature of language. This incompleteness is exacerbated by the ambiguity problem: if we had a clear definition of idiom we could make an attempt of evaluating idiom dictionaries on their accuracy, but it is practically impossible to come up with a definition of idiom that leaves no room for ambiguity. This ambiguity, among others, creates a large grey area between clearly non-idiomatic phrases on the one hand (e.g. buy a house), and clear potentially idiomatic phrases on the other hand (e.g. buy the farm). As a consequence, we cannot empirically evaluate the coverage of the dictionaries. Instead, in this work, we will quantify the divergence between various idiom dictionaries and corpora, with regard to their idiom inventories. If they show large discrepancies, we take that to mean that either there is little agreement on definitions of idiom or the category is so broad that a single resource can only cover a small proportion. Conversely, if there is large agreement, we assume that idiom resources are largely reliable, and that there is consensus around what is, and what is not, an idiomatic expression. We use different idiom resources and assume that the combined set of resources yields an approximation of the true set of idioms in English. A large divergence between the idiom inventories of these resources would then suggest a low recall for a single resource, since many other idioms are present in the other resources. Conversely, if the idiom inventories largely overlap, that indicates that a single resource can already yield decent coverage of idioms in the English language. The results of the dictionary comparisons are in Section SECREF36.",Coverage of Idiom Inventories ::: Selected Idiom Resources (Data and Method),"We evaluate the quality of three idiom dictionaries by comparing them to each other and to three idiom corpora. Before we report on the comparison we first describe why we select and how we prepare these resources. We investigate the following six idiom resources: Wiktionary; the Oxford Dictionary of English Idioms (ODEI, BIBREF31); UsingEnglish.com (UE); the Sporleder corpus BIBREF10; the VNC dataset BIBREF9; and the SemEval-2013 Task 5 dataset BIBREF15. These dictionaries were selected because they are available in digital format. Wiktionary and UsingEnglish have the added benefit of being freely available. However, they are both crowdsourced, which means they lack professional editing. In contrast, ODEI is a traditional dictionary, created and edited by lexicographers, but it has the downside of not being freely available. For Wiktionary, we extracted all idioms from the category `English Idioms' from the English version of Wiktionary. We took the titles of all pages containing a dictionary entry and considered these idioms. Since we focus on multiword idiomatic expressions, we filtered out all single-word entries in this category. More specifically, since Wiktionary is a constantly changing resource, we used the 8,482 idioms retrieved on 10-03-2017, 15:30. We used a similar extraction method for UE, a web page containing freely available resources for ESL learners, including a list of idioms. We extracted all idioms which have publicly available definitions, which numbered 3,727 on 10-03-2017, 15:30. Again, single-word entries and duplicates were filtered out. Concerning ODEI, all idioms from the e-book version were extracted, amounting to 5,911 idioms scraped on 13-03-2017, 10:34. Here we performed an extra processing step to expand idioms containing content in parentheses, such as a tough (or hard) nut (to crack). Using a set of simple expansion rules and some hand-crafted exceptions, we automatically generated all variants for this idiom, with good, but not perfect accuracy. For the example above, the generated variants are: {a tough nut, a tough nut to crack, a hard nut, a hard nut to crack}. The idioms in the VNC dataset are in the form verb_noun, e.g. blow_top, so they were manually expanded to a regular dictionary form, e.g. blow one's top before comparison.",Coverage of Idiom Inventories ::: Method,"In many cases, using simple string-match to check overlap in idioms does not work, as exact comparison of idioms misses equivalent idioms that differ only slightly in dictionary form. Differences between resources are caused by, for example: inflectional variation (crossing the Rubicon — cross the Rubicon); variation in scope (as easy as ABC — easy as ABC); determiner variation (put the damper on — put a damper on); spelling variation (mind your p's and q's — mind your ps and qs); order variation (call off the dogs — call the dogs off); and different conventions for placeholder words (recharge your batteries — recharge one's batteries), where both your and one's can generalise to any possessive personal pronoun. These minor variations do not fundamentally change the nature of the idiom, and we should count these types of variation as belonging to the same idiom (see also BIBREF32, who devise a measure to quantify different types of variation allowed by specific MWEs). So, to get a good estimate of the true overlap between idiom resources, these variations need to be accounted for, which we do in our flexible matching approach. There is one other case of variation not listed above, namely lexical variation (e.g. rub someone up the wrong way - stroke someone the wrong way). We do not abstract over this, since we consider lexical variation to be a more fundamental change to the nature of the idiom. That is, a lexical variant is an indicator of the coverage of the dictionary, where the other variations are due to different stylistic conventions and do not indicate actual coverage. In addition, it is easy to abstract over the other types of variation in an NLP application, but this is not the case for lexical variation. The overlap counts are estimated by abstracting over all variations except lexical variation in a semi-automatic manner, using heuristics and manual checking. Potentially overlapping idioms are selected using the following set of heuristics: whether an idiom from one resource is a substring (including gaps) of an idiom in the other resource, whether the words of an idiom form a subset of the words of an idiom in the other resource, and whether there is an idiom in the other resource which has a Levenshtein ratio of over 0.8. The Levenshtein ratio is an indicator of the Levenshtein distance between the two idioms relative to their length. These potential matches are then judged manually on whether they are really forms of the same idiom or not.",Coverage of Idiom Inventories ::: Results,"The results of using exact string matching to quantify the overlap between the dictionaries is illustrated in Figure FIGREF37. Overlap between the three dictionaries is low. A possible explanation for this lies with the different nature of the dictionaries. Oxford is a traditional dictionary, created and edited by professional lexicographers, whereas Wiktionary is a crowdsourced dictionary open to everyone, and UsingEnglish is similar, but focused on ESL-learners. It is likely that these different origins result in different idiom inventories. Similarly, we would expect that the overlap between a pair of traditional dictionaries, such as the ODEI and the Penguin Dictionary of English Idioms BIBREF33 would be significantly higher. It should also be noted, however, that comparisons between more similar dictionaries also found relatively little overlap (BIBREF34; BIBREF35). A counterpoint is provided by BIBREF36, who quantifies coverage of verb-particle constructions in three different dictionaries and finds large overlap – perhaps because verb-particle are a more restricted class. As noted previously, using exact string matching is a very limited approach to calculating overlap. Therefore, we used heuristics and manual checking to get more precise numbers, as shown in Table TABREF39, which also includes the three corpora in addition to the three dictionaries. As the manual checking only involved judging similar idioms found in pairs of resources, we cannot calculate three-way overlap as in Figure FIGREF37. The counts of the pair-wise overlap between dictionaries differ significantly between the two methods, which serves to illustrate the limitations of using only exact string matching and the necessity of using more advanced methods and manual effort. Several insights can be gained from the data in Table TABREF39. The relation between Wiktionary and the SemEval corpus is obvious (cf. Section SECREF12), given the 96.92% coverage. For the other dictionary-corpus pairs, the coverage increases proportionally with the size of the dictionary, except in the case of UsingEnglish and the Sporleder corpus. The proportional increase indicates no clear qualitative differences between the dictionaries, i.e. one does not have a significantly higher percentage of non-idioms than the other, when compared to the corpora. Generally, overlap between dictionaries and corpora is low: the two biggest, ODEI and Wiktionary have only around 30% overlap, while the dictionaries also cover no more than approximately 70% of the idioms used in the various corpora. Overlap between the three corpora is also extremely low, at below 5%. This is unsurprising, since a new dataset is more interesting and useful when it covers a different set of idioms than used in an existing dataset, and thus is likely constructed with this goal in mind.",Corpus Annotation,"In order to evaluate the PIE extraction methods developed in this work (Section SECREF6), we exhaustively annotate an evaluation corpus with all instances of a pre-defined set of PIEs. As part of this, we come up with a workable definition of PIEs, and measure the reliability of PIE annotation by inter-annotator agreement. Assuming that we have a set of idioms, the main problem of defining what is and what is not a potentially idiomatic expression is caused by variation. In principle, potentially idiomatic expression is an instance of a phrase that, when seen without context, could have either an idiomatic or a literal meaning. This is clearest for the dictionary form of the idiom, as in Example SECREF5. Literal uses generally allow all kinds of variation, but not all of these variations allow a figurative interpretation, e.g. Example SECREF5. However, how much variation an idiom can undergo while retaining its figurative interpretation is different for each expression, and judgements of this might vary from one speaker to the other. An example of this is spill the bean, a variant of spill the beans, in Example SECREF5 judged by BIBREF21 as being highly questionable. However, even here a corpus example can be found containing the same variant used in a figurative sense (Example SECREF5). As such, we assume that we cannot know a priori which variants of an expression allow a figurative reading, and are thus a potentially idiomatic expression. Therefore we consider every possible morpho-syntactic variation of an idiom a PIE, regardless of whether it actually allows a figurative reading. We believe the boundaries of this variation can only be determined based on corpus evidence, and even then they are likely variable. Note that a similar question is tackled by BIBREF26, when they establish the boundary between a `literal reading of a VMWE' and a `coincidental co-occurrence'. BIBREF26's answer is similar to ours, in that they count something as a literal reading of a VMWE if it `the same or equivalent dependencies hold between [the expression]'s components as in its canonical form'. . John kicked the bucket last night. . * The bucket, John kicked last night. . ?? Azin spilled the bean. (from BIBREF21) . Alba reveals Fantastic Four 2 details The Invisible Woman actress spills the bean on super sequel (from ukWaC)",Corpus Annotation ::: Evaluating the Extraction Methods,"Evaluating the extraction methods is easier than evaluating dictionary coverage, since the goal of the extraction component is more clearly delimited: given a set of PIEs from one or more dictionaries, extract all occurrences of those PIEs from a corpus. Thus, rather than dealing with the undefined set of all PIEs, we can work with a clearly defined and finite set of PIEs from a dictionary. Because we have a clearly defined set of PIEs, we can exhaustively annotate a corpus for PIEs, and use that annotated corpus for automatic evaluation of extraction methods using recall and precision. This allows us to facilitate and speed up annotation by pre-extracting sentences possibly containing a PIE. After the corpus is annotated, the precision and recall can be easily estimated by comparing the extracted PIE instances to those marked in the corpus. The details of the corpus selection, dictionary selection, extraction heuristic and annotation procedure are presented in Section SECREF46, and the details and results of the various extraction methods are presented in Section SECREF6.",Corpus Annotation ::: Base Corpus and Idiom Selection,"As a base corpus, we use the XML version of the British National Corpus BIBREF37, because of its size, variety, and wide availability. The BNC is pre-segmented into s-units, which we take to be sentences, w-units, which we take to be words, and c-units, punctuation. We then extract the text of all w-units and c-units. We keep the sentence segmentation, resulting in a set of plain text sentences. All sentences are included, except for sentences containing <gap> elements, which are filtered out. These <gap> elements indicate places where material from the original has been left out, e.g. for anonymisation purposes. Since this can result in incomplete sentences that cannot be parsed correctly, we filter out sentences containing these gaps. We use only the written part of the BNC. From this, we extract a set of documents with the aim of having as much genre variation as possible. To achieve this, we select the first document in each genre, as defined by the classCode attribute (e.g. nonAc, commerce, letters). The resulting set of 46 documents makes up our base corpus. Note that these documents vary greatly in size, which means the resulting corpus is varied, but not balanced in terms of size (Table TABREF43). The documents are split across a development and test set, as specified at the end of Section SECREF46. We exclude documents with IDs starting with A0 from all annotation and evaluation procedures, as these were used during development of the extraction tool and annotation guidelines. As for the set of potentially idiomatic expressions, we use the intersection of the three dictionaries, Wiktionary, Oxford, and UsingEnglish. Based on the assumption that, if all three resources include a certain idiom, it must unquestionably be an idiom, we choose the intersection (also see Figure FIGREF37). This serves to exclude questionable entries, like at all, which is in Wiktionary. The final set of idioms used for these experiments consists of 591 different multiword expressions. Although we aim for wide coverage, this is a necessary trade-off to ensure quality. At the same time, it leaves us with a set of idiom types that is approximately ten times larger than present in existing corpora. The set of 591 idioms includes idioms with a large variety of syntactic patterns, of which the most frequent ones are shown in Table TABREF44. The statistics show that the types most prevalent in existing corpora, verb-noun and preposition-noun combinations, are indeed the most frequent ones, but that there is a sizeable minority of types that do not fall into those categories, including coordinated adjectives, coordinated nouns, and nouns with prepositional phrases. This serves to emphasise the necessity of not restricting corpora to a small set of syntactic patterns.",Corpus Annotation ::: Extraction of PIE Candidates,"To annotate the corpus completely manually would require annotators to read the whole corpus, and cross-reference each sentence to a list of almost 600 PIEs, to check whether one of those PIEs occurs in a sentence. We do not consider this a feasible annotation settings, due to both the difficulty of recognising literal usages of idioms and the time cost needed to find enough PIEs, given their low overall frequency. As such, we use a pre-extraction step to present candidates for annotation to the human annotators. Given the corpus and the set of PIEs, we heuristically extract the PIE candidates as follows: given an idiomatic expression, extract every sentence which contains all the defining words of the idiom, in any form. This ensures that all possibly matching sentences get extracted, while greatly pruning the amount of sentences for annotators to look at. In addition, it allows us to present the heuristically matched PIE type and corresponding words to the annotators, which makes it much easier to judge whether something is a PIE or not. This also means that annotators never have to go through the full list of PIEs during the annotation process. Initially, the heuristic simply extracted any sentence containing all the required words, where a word is any of the inflectional variants of the words in the PIE, except for determiners and punctuation. This method produced large amounts of noise, that is, a set of PIE candidates with only a very low percentage of actual PIEs. This was caused by the presence of some highly frequent PIEs with very little defining lexical content, such as on the make, and in the running. For example, with the original method, every sentence containing the preposition on, and any inflectional form of the verb make was extracted, resulting in a huge number of non-PIE candidates. To limit the amount of noise, two restrictions were imposed. The first restrictions disallows word order variation for PIEs which do not contain a verb. The rationale behind this is that word order variation is only possible with PIEs like spill the beans (e.g. the beans were spilled), and not with PIEs like in the running (*the running in??). The second restriction is that we limit the number of words that can be inserted between the words of a PIE, but only for PIEs like on the make, and in the running, i.e. PIEs which only contain prepositions, determiners and a single noun. The number of intervening words was limited to three tokens, allowing for some variation, as in Example SECREF45, but preventing sentences like Example SECREF45 from being extracted. This restriction could result in the loss of some PIE candidates with a large number of intervening words. However, the savings in annotation time clearly outweigh the small loss in recall in this situation. . Either at New Year or before July you can anticipate a change in the everyday running of your life. (in the running - BNC - document CBC - sentence 458) . [..] if [he] hung around near the goal or in the box for that matter instead of running all over the show [..] (in the running - BNC - document J1C - sentence 1341)",Corpus Annotation ::: Annotation Procedure,"The manual annotation procedure consists of three different phases (pilot, double annotation, single annotation), followed by an adjudication step to resolve conflicting annotations. Two things are annotated: whether something is a PIE or not, and if it is a PIE, which sense the PIE is used in. In the first phase (0-100-*), we randomly select hundred of the 2,239 PIE candidates which are then annotated by three annotators. All annotators have a good command of English, are computational linguists, and familiar with the subject. The annotators include the first and last author of this paper. The annotators were provided with a short set of guidelines, of which the main rule-of-thumb for labelling a phrase as a PIE is as follows: any phrase is a PIE when it contains all the words, with the same part-of-speech, and in the same grammatical relations as in the dictionary form of the PIE, ignoring determiners. For sense annotation, annotators were to mark a PIE as idiomatic if it had a sense listed in one of the idiom dictionaries, and as literal if it had a meaning that is a regular composition of its component words. For cases which were undecidable due to lack of context, the ?-label was used. The other-label was used as a container label for all cases in which neither the literal or idiomatic sense was correct (e.g. meta-linguistic uses and embeddings in metaphorical frames, see also Section SECREF10). The first phase of annotation serves to bring to light any inconsistencies between annotators and fill in any gaps in the annotation guidelines. The resulting annotations already show a reasonably high agreement of 0.74 Fleiss' Kappa. Table TABREF48 shows annotation details and agreement statistics for all three phases. The annotation tasks suffixed by -PIE indicate agreement on PIE/non-PIE annotation and the tasks suffixed by -sense indicate agreement on sense annotation for PIEs. In the second phase of annotation (100-600-* & 600-1100-*), another 1000 of the 2239 PIE candidates are selected to be annotated by two pairs of annotators. This shows very high agreement, as shown in Table TABREF48. This is probably due to the improvement in guidelines and the discussion following the pilot round of annotation. The exception to this are the somewhat lower scores for the 600-1100-sense annotation task. Adjudication revealed that this is due almost exclusively because of a different interpretation of the literal and idiomatic senses of a single PIE type: on the ground. Excluding this PIE type, Fleiss' Kappa increases from 0.63 to 0.77. Because of the high agreement on PIE annotation, we deem it sufficient for the remainder (1108 candidates) to be annotated by only the primary annotator in the third phase of annotation (1100-2239-*). The reliability of the single annotation can be checked by comparing the distribution of labels to the multi-annotated parts. This shows that it falls clearly within the ranges of the other parts, both in the proportion of PIEs and idiomatic senses (see Table TABREF49). The single-annotated part has 49.0% PIEs, which is only 4 percentage points above the 44.7% PIEs in the multi-annotated parts. The proportion of idioms is just 2 percentage points higher, with 55.9% versus 53.9.%. Although inter-annotator agreement was high, there was still a significant number of cases in the triple and double annotated PIE candidate sets where not all annotators agreed. These cases were adjudicated through discussion by all annotators, until they were in agreement. In addition, all PIE candidates which initially received the ?-label (unclear or undecidable) for sense or PIE were resolved in the same manner. In the adjudication procedure, annotators were provided with additional context on each side of the idiom, in contrast to the single sentence provided during the initial annotation. The main reason to do adjudication, rather than simply discarding all candidates for which there was disagreement, was that we expected exactly those cases for which there are conflicting annotations to be the most interesting ones, since having non-standard properties would cause the annotations to diverge. Examples of such interesting non-standard cases are at sea as part of a larger satirical frame in Example SECREF46 and cut the mustard in Example SECREF46 where it is used in a headline as wordplay on a Cluedo character. . The bovine heroine has connections with Cowpeace International, and deals with a huge treacle slick at sea. (at sea - BNC - document CBC - sentence 13550) . Why not cut the Mustard? [..] WADDINGTON Games's proposal to axe Reverend Green from the board game Cluedo is a bad one. (cut the mustard - BNC - document CBC - sentence 14548) We split the corpus at the document level. The corpus consists of 45 documents from the BNC, and we split it in such a way that the development set has 1,112 candidates across 22 documents and the test set has 1,127 candidates from 23 documents. Note that this means that the development and test set contain different genres. This ensures that we do not optimise our systems on genre-specific aspects of the data.",Dictionary-based PIE Extraction,"We propose and implement four different extraction methods, of differing complexities: exact string match, fuzzy string match, inflectional string match, and parser-based extraction. Because of the absence of existing work on this task, we compare these methods to each other, where the more basic methods function as baselines. More complex methods serve to shine light on the difficulty of the PIE extraction task; if simple methods already work sufficiently well, the task is not as hard as expected, and vice versa. Below, each of the extraction methods is presented and discussed in detail.",How big PIE datasets are obtained from dictionaries?,346f10ddb34503dfba72b0e49afcdf6a08ecacfa,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,,,"We use only the written part of the BNC. From this, we extract a set of documents with the aim of having as much genre variation as possible. To achieve this, we select the first document in each genre, as defined by the classCode attribute (e.g. nonAc, commerce, letters). The resulting set of 46 documents makes up our base corpus. Note that these documents vary greatly in size, which means the resulting corpus is varied, but not balanced in terms of size (Table TABREF43). The documents are split across a development and test set, as specified at the end of Section SECREF46. We exclude documents with IDs starting with A0 from all annotation and evaluation procedures, as these were used during development of the extraction tool and annotation guidelines.","The resulting set of 46 documents makes up our base corpus. Note that these documents vary greatly in size, which means the resulting corpus is varied, but not balanced in terms of size (Table TABREF43).",ed59c86853fd8a4bb3c7d85027e65f2d29ef6e66,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,What compleentary PIE extraction methods are used to increase reliability further?,2480dfe2d996afef840a81bd920aeb9c26e5b31d,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,,,"We experiment with two such combinations, by simply taking the union of the sets of extracted idioms of both systems, and filtering out duplicates. Results are shown in Table TABREF77. Both combinations show the expected effect: a clear gain in recall at a minimal loss in precision. Compared to the in-context-parsing-based system, the combination with exact string matching yields a gain in recall of over 6%, and the combination with inflectional string matching yields an even bigger gain of almost 8%, at precision losses of 0.6% and 0.8%, respectively. This indicates that the systems are very much complementary in the PIEs they extract. It also means that, when used in practice, combining inflectional string matching and parse-based extraction is the most reliable configuration.","Compared to the in-context-parsing-based system, the combination with exact string matching yields a gain in recall of over 6%, and the combination with inflectional string matching yields an even bigger gain of almost 8%, at precision losses of 0.6% and 0.8%, respectively.",d93f5ad1290adaf515deb047f0b1db733aec3042,258ee4069f740c400c0049a2580945a1cc7f044c,Are PIEs extracted automatically subjected to human evaluation?,0fec9da2bc80a12a7a6d6600b9ecf3e122732b60,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,True,,"For parser-based extraction, systems with and without in-context parsing, ignoring labels, and ignoring directionality are tested. For the three string-based extraction methods, varying numbers of intervening words and case sensitivity are evaluated. Evaluation is done using the development set, consisting of 22 documents and 1112 PIE candidates, and the test set, which consists of 23 documents and 1127 PIE candidates. For each method the best set of parameters and/or options is determined using the development set, after which the best variant by F1-score of each method is evaluated on the test set. Since these documents in the corpus are exhaustively annotated for PIEs (see Section SECREF40), we can calculate true and false positives, and false negatives, and thus precision, recall and F1-score. The exact spans are ignored, because the spans annotated in the evaluation corpus are not completely reliable. These were automatically generated during candidate extraction, as described in Section SECREF45. Rather, we count an extraction as a true positive if it finds the correct PIE type in the correct sentence.","Evaluation is done using the development set, consisting of 22 documents and 1112 PIE candidates, and the test set, which consists of 23 documents and 1127 PIE candidates. For each method the best set of parameters and/or options is determined using the development set, after which the best variant by F1-score of each method is evaluated on the test set.

Since these documents in the corpus are exhaustively annotated for PIEs (see Section SECREF40), we can calculate true and false positives, and false negatives, and thus precision, recall and F1-score.",e1a64011cfafc253b921ae8802fbd0cec376c45d,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,What dictionaries are used for automatic extraction of PIEs?,5499527beadb7f5dd908bd659cad83d6a81119bd,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,,,"We evaluate the quality of three idiom dictionaries by comparing them to each other and to three idiom corpora. Before we report on the comparison we first describe why we select and how we prepare these resources. We investigate the following six idiom resources: Wiktionary; the Oxford Dictionary of English Idioms (ODEI, BIBREF31); UsingEnglish.com (UE); the Sporleder corpus BIBREF10; the VNC dataset BIBREF9; There are four sizeable sense-annotated PIE corpora for English: the VNC-Tokens Dataset BIBREF9, the Gigaword dataset BIBREF14, the IDIX Corpus BIBREF10, and the SemEval-2013 Task 5 dataset BIBREF15. An overview of these corpora is presented in Table TABREF7.","We investigate the following six idiom resources:

Wiktionary;

the Oxford Dictionary of English Idioms (ODEI, BIBREF31);

UsingEnglish.com (UE);

the Sporleder corpus BIBREF10;

the VNC dataset BIBREF9;

and the SemEval-2013 Task 5 dataset BIBREF15.",a9a265600d4eb92c0d9ee062d0076a4ea5f970a9,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4-Table1-1.png,"Table 1 Overview of existing corpora of potentially idiomatic expressions and sense annotations for English. ‘Min’ indicates the count for the least frequent idiom type, ‘Med’ the median, and ‘Max’ the most frequent type. The syntax types column indicates the syntactic patterns of the idiom types included in the dataset. The base corpora are the British National Corpus (BNC, Burnard 2007), ukWaC (Ferraresi et al. 2008), and Gigaword (Graff and Cieri 2003).",11-Figure1-1.png,"Figure 1 Venn diagram of case-insensitive exact string match overlap between the three idiom dictionaries. Note that the numbers in this figure are based on exact string matching, so they differ from the numbers in Table 2, matching of similar, but not identical idioms, as described in Section 4.3.",12-Table2-1.png,"Table 2 Percentage of overlapping idioms between different idiom resources, abstracting over minor variations. Value is the number of idioms in the intersection of two idiom sets, as a percentage of number of idioms in the resource in the left column. For example, 56.60% of idioms in the VNC occur in Wiktionary.",14-Table3-1.png,Table 3 Statistics on the size of the BNC documents used for PIE annotation and the split in development and test set.,15-Table4-1.png,"Table 4 The 10 most frequent syntactic patterns in the set of 591 idiomatic expressions, based on automatic part-of-speech tags produced by Spacy which were manually corrected.",17-Table5-1.png,"Table 5 Details of the annotation phases and inter-annotator agreement statistics. The number of candidates for sense annotation is the number on which all annotators initially agreed that it was a PIE, i.e. pre-adjudication. Note that sense and PIE annotation are split here for clarity of presentation; in practice they were annotated as a joint task.",Dictionary-based PIE Extraction ::: String-based Extraction Methods ::: Exact String Match,"This is, very simply, extracting all instances of the exact dictionary form of the PIE, from the tokenized text of the corpus. Word boundaries are taken into account, so at sea does not match `that seawater'. As a result, all inflectional and other variants of the PIE are ignored.",Dictionary-based PIE Extraction ::: String-based Extraction Methods ::: Fuzzy String Match,"Fuzzy string match is a rough way of dealing with morphological inflection of the words in a PIE. We match all words in the PIE, taking into account word boundaries, and allow for up to 3 additional letters at the end of each word. These 3 additional characters serve to cover inflectional suffixes.",Dictionary-based PIE Extraction ::: String-based Extraction Methods ::: Inflectional String Match,"In inflectional string match, we aim to match all inflected variations of a PIE. This is done by generating all morphological variants of the words in a PIE, generating all combinations of those words, and then using exact string match as described earlier. Generating morphological variations consists of three steps: part-of-speech tagging, morphological analysis, and morphological reinflection. Since inflectional variation only applies to verbs and nouns, we use the Spacy part-of-speech tagger to detect the verbs and nouns. Then, we apply the morphological analyser morpha to get the base, uninflected form of the word, and then use the morphological generation tool morphg to get all possible inflections of the word. Both tools are part of the Morph morphological processing suite BIBREF38. Note that the Morph tools depend on the part-of-speech tag in the input, so that a wrong PoS may lead to an incorrect set of morphological variants. For a PIE like spill the beans, this results in the following set of variants: $\lbrace $spill the bean, spills the bean, spilled the bean, spilling the bean, spill the beans, spills the beans, spilled the beans, spilling the beans$\rbrace $. Since we generate up to 2 variants for each noun, and up to 4 variants for each verb, the number of variants for PIEs containing multiple verbs and nouns can get quite large. On average, 8 additional variants are generated for each potentially idiomatic expression.",Dictionary-based PIE Extraction ::: String-based Extraction Methods ::: Additional Steps,"For all string match-based methods, ways to improve performance are implemented, to make them as competitive as possible. Rather than doing exact string matching, we also allow words to be separated by something other than spaces, e.g. nuts-and-bolts for nuts and bolts. Additionally, there is an option to take into account case distinctions. With the case-sensitive option, case is preserved in the idiom lists, e.g. coals to Newcastle, and the string matching is done in a case-sensitive manner. This increases precision, e.g. by avoiding PIEs as part of proper names, but also comes at a cost of recall, e.g. for sentence-initial PIEs. Thirdly, there is the option to allow for a certain number of intervening words between each pair of words in the PIE. This should improve recall, at the cost of precision. For example, this would yield the true positive make a huge mountain out of a molehill for make a mountain out of a molehill, but also false positives like have a smoke and go for have a go. A third shared property of the string-based methods is the processing of placeholders in PIEs. PIEs containing possessive pronoun placeholders, such as one's and someone's are expanded. That is, we remove the original PIE, and add copies of the PIE where the placeholder is replaced by one of the possessive personal pronouns. For example, a thorn in someone's side is replaced by a thorn in {my, your, his, ...} side. In the case of someone's, we also add a wildcard for any possessively used word, i.e. a thorn in —'s side, to match e.g. a thorn in Google's side. Similarly, we make sure that PIE entries containing —, such as the mother of all —, will match any word for — during extraction. We do the same for someone, for which we substitute objective pronouns. For one, this is not possible, since it is too hard to distinguish from the one used as a number.",exact string matching inflectional string matching,,17-Table6-1.png,"Table 6 Distributional statistics on the annotated PIE corpus, post-adjudication. Adjudication resolved all instances which were disagreed upon and all ?-sense-labels, hence the presence of only 3 sense labels: i(diomatic), l(iteral), and o(ther).",20-Figure2-1.png,Figure 2 Automatic dependency parse of the PIE lose the plot.,20-Figure3-1.png,"Figure 3 Automatic dependency parse of the sentence ‘you might just lose the plot completely’, which contains the PIE lose the plot. From BNC document CH1, sentence 829. Sentence shortened for display convenience.",21-Figure5-1.png,"Figure 5 Automatic dependency parse of the sentence ‘Ephron ups the ante on the sucrose front’, which contains the PIE up the ante. From BNC document CBC, sentence 7022. Sentence shortened for display convenience.",21-Figure4-1.png,Figure 4 Automatic dependency parse of the PIE up the ante.,,46 documents makes up our base corpus,,,,,,,,,Wiktionary Oxford Dictionary of English Idioms UsingEnglish.com (UE) Sporleder corpus VNC dataset SemEval-2013 Task 5 dataset,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,22-Figure6-1.png,"Figure 6 Automatic dependency parse of the sentence ‘the commission will be a laughing stock’, which contains the PIE laughing stock. From BNC document A69, sentence 487. Sentence shortened for display convenience.",22-Figure7-1.png,Figure 7 Automatic dependency parse of the PIE laughing stock.,,,,,,,,23-Figure9-1.png,Figure 9 Automatic dependency parse of the extracted sentence ‘Did they jump ship at Lima?’ containing the PIE jump ship.,24-Table7-1.png,"Table 7 PIE extraction performance in terms of precision, recall, and F1-score of the three string-based systems (exact, fuzzy, and inflectional), with different options, on the development set. The number of words indicates the number of intervening words allowed between the parts of the PIE for matching to occur. CS indicates case-sensitive string matching. The best score for each metric and system is in bold.",25-Table8-1.png,"Table 8 PIE extraction performance in terms of precision, recall, and F1-score of the parser-based system, with different options, on the development set. The best score for each metric is in bold.",25-Table9-1.png,"Table 9 PIE extraction performance in terms of precision, recall, and F1-score of the best variant by F1-score of each of the four systems, on the test set. CS indicates case-sensitive string matching. The best score for each metric is in bold.",,,Dictionary-based PIE Extraction ::: Parser-Based Extraction Methods,"Parser-based extraction is potentially the widest-coverage extraction method, with the capacity to extract both morphological and syntactic variants of the PIE. This should be robust against the most common modifications of the PIE, e.g. through word insertions (spill all the beans), passivisation (the beans were spilled), and abstract over articles (spill beans). In this method, PIEs are extracted using the assumption that any sentence which contains the lemmata of the words in the PIE, in the same dependency relations as in the PIE, contains an instance of the PIE type in question. More concretely, this means that the parse of the sentence should contain the parse tree of the PIE as a subtree. This is illustrated in Figure FIGREF57, which shows the parse tree for the PIE lose the plot, parsed without context. Note that this is a subtree of the parse tree for the sentence `you might just lose the plot completely', which is shown in Figure FIGREF58. Since the sentence parse contains the parse of the PIE, we can conclude that the sentence contains an instance of that PIE and extract the span of the PIE instance. All PIEs are parsed in isolation, based on the assumption that all PIEs can be parsed, since they are almost always well-formed phrases. However, not all PIEs will be parsed correctly, especially since there is no context to resolve ambiguity. Errors tend to occur at the part-of-speech level, where, for example, verb-object combinations like jump ship and touch wood are erroneously tagged as noun-noun compounds. An analysis of the impact of parser error on PIE extraction performance is presented in Section SECREF73. Initially, we use the Spacy parser for parsing both the PIEs and the sentences. Next, the sentence is parsed, and the lemma of the top node of the parsed PIE is matched against the lemmata of the sentence parse. If a match is found, the parse tree of the PIE is matched against the subtree of the matching sentence parse node. If the whole PIE parse tree matches, the span ranging from the first PIE token to the last is extracted. This span can thus include words that are not directly part of the PIE's dictionary form, in order to account for insertions like ships were jumped for jump ship, or have a big heart for have a heart. During the matching, articles (a/an/the) are ignored, and passivisation is accounted for with a special rule. In addition, a number of special cases are dealt with. These are PIEs containing someone('s), something('s), one's, or —. These words are used in PIEs as placeholders for a generic possessor (someone's/something's/one's), generic object (someone/something), or any word of the right PoS (—). For someone's, and something's, we match any possessive pronoun, or (proper) noun + possessive marker. For one's, only possessive pronouns are matched, since this is a placeholder for reflexive possessors. For someone and something, any non-possessive pronoun or (proper) noun is matched. For — wildcards, any word can be matched, as long as it has the right relation to the right head. An additional challenge with these wildcards is that PIEs containing them cannot be parsed, e.g. too — for words is not parseable. This is dealt with by substituting the — by a PoS-ambiguous word, such as fine, or back. Two optional features are added to the parser-based method with the goal of making it more robust to parser errors: generalising over dependency relation labels, and generalising over dependency relation direction. We expect this to increase recall at the cost of precision. In the first no labels setting, we match parts of the parse tree which have the same head lemma and the same dependent lemma, regardless of the relation label. An example of this is Figure FIGREF60, which has the wrong relation label between up and ante. If labels are ignored, however, we can still extract the PIE instance in Figure FIGREF61, which has the correct label. In the no directionality setting, relation labels are also ignored, and in addition the directionality of the relation is ignored, that is, we allow for the reversal of heads and dependents. This benefits performance in a case like Figure FIGREF62, which has stock as the head of laughing in a compound relation, whereas the parse of the PIE (Figure FIGREF63) has laughing as the head of stock in a dobj relation. Note that similar settings were implemented by BIBREF26, who detect literal uses of VMWEs using a parser-based method with either full labelled dependencies, unlabelled dependencies, or directionless unlabelled dependencies (which they call BagOfDeps). They find that recall increases when less restrictions on the dependencies are used, but that this does not hurt precision, as we would expect. However, we cannot draw too many conclusions from these results due to the small size of their evaluation set, which consists of just 72 literal VMWEs in total.",Dictionary-based PIE Extraction ::: Parser-Based Extraction Methods ::: In-Context Parsing,"Since the parser-based method parses PIEs without any context, it often finds an incorrect parse, as for jump ship in Figure FIGREF65. As such, we add an option to the method that aims to increase the number of correct parses by parsing the PIE within context, that is, within a sentence. This can greatly help to disambiguate the parse, as in Figure FIGREF66. If the number of correct parses goes up, the recall of the extraction method should also increase. Naturally, it can also be the case that a PIE is parsed correctly without context, and incorrectly with context. However, we expect the gains to outweigh the losses. The challenge here is thus to collect example sentences containing the PIE. Since the whole point of this work is to extract PIEs from raw text, this provides a catch-22-like situation: we need to extract a sentence containing a PIE in order to extract sentences containing a PIE. The workaround for this problem is to use the exact string matching method with the dictionary form of the PIE and a very large plain text corpus to gather example sentences. By only considering the exact dictionary form we both simplify the finding of example sentences and the extraction of the PIE's parse from the sentence parse. In case multiple example sentences are found, the shortest sentence is selected, since we assume it is easiest to parse. This is also the reason we make use of very large corpora, to increase the likelihood of finding a short, simple sentence. The example sentence extraction method is modified in such a way that sentences where the PIE is used meta-linguistically in quotes, e.g. “the well-known English idiom `to spill the beans' has no equivalents in other languages”, are excluded, since they do not provide a natural context for parsing. When no example sentence can be found in the corpus, we back-off to parsing the PIE without context. After a parse has been found for each PIE (i.e. with or without context), the method proceeds identically to the regular parser-based method. We make use of the combination of two large corpora for the extraction of example sentences: the English Wikipedia, and ukWaC BIBREF17. For the Wikipedia corpus, we use a dump (13-01-2016) of the English-language Wikipedia, and remove all Wikipedia markup. This is done using WikiExtractor. The resulting files still contain some mark-up, which is removed heuristically. The resulting corpus contains mostly clean, raw, untokenized text, numbering approximately 1.78 billion tokens. As for ukWaC, all XML-markup was removed, and the corpus is converted to a one-sentence-per-line format. UkWaC is tokenized, which makes it difficult for a simple string match method to find PIEs containing punctuation, for example day in, day out. Therefore, all spaces before commas, apostrophes, and sentence-final punctuation are removed. The resulting corpus contains approximately 2.05 billion tokens, making for a total of 3.83 billion tokens in the combined ukWaC and Wikipedia corpus.",Dictionary-based PIE Extraction ::: Results,"In order to determine which of the methods described previously produces the highest quality extraction of potentially idiomatic expressions, we evaluate them, in various settings, on the corpus described in Section SECREF5. For parser-based extraction, systems with and without in-context parsing, ignoring labels, and ignoring directionality are tested. For the three string-based extraction methods, varying numbers of intervening words and case sensitivity are evaluated. Evaluation is done using the development set, consisting of 22 documents and 1112 PIE candidates, and the test set, which consists of 23 documents and 1127 PIE candidates. For each method the best set of parameters and/or options is determined using the development set, after which the best variant by F1-score of each method is evaluated on the test set. Since these documents in the corpus are exhaustively annotated for PIEs (see Section SECREF40), we can calculate true and false positives, and false negatives, and thus precision, recall and F1-score. The exact spans are ignored, because the spans annotated in the evaluation corpus are not completely reliable. These were automatically generated during candidate extraction, as described in Section SECREF45. Rather, we count an extraction as a true positive if it finds the correct PIE type in the correct sentence. Note that we judge the system with the highest F1-score to be the best-performing system, since it is a clear and objective criterion. However, when using the system in practice, the best performance depends on the goal. When used as a preprocessing step for PIE disambiguation, the system with the highest F1-score is perhaps the most suitable, but as a corpus building tool, one might want to sacrifice some precision for an increase in recall. This helps to get the most comprehensive annotation of PIEs possible, without overloading the annotators with false extractions (i.e. non-PIEs), by maintaining high precision. The results for each system on the development set are presented in Tables TABREF70 and TABREF71. Generally, results are in line with expectations: (the best) parse-based methods are better than (the best) string-based methods, and within string-based methods, inflectional matching works best. The same goes for the different settings: case-sensitivity increases precision at the cost of recall, allowing intervening words increases recall at the cost of precision, and the same goes for the no labels and no directionality options for parser-based extraction. Overall, in-context parser-based extraction works best, with an F1 of 88.54%, whereas fuzzy matching does very poorly. Within string-based methods, exact matching has the highest precision, but low recall. Fuzzy matching increases recall at a disproportionately large precision cost, whereas inflectional matching combines the best of both worlds and has high recall at a small loss in precision. For the parser-based system, it is notable that parsing idioms within context yields a clear overall improvement by greatly improving recall at a small cost in precision. We evaluate the best variant of each system, as determined by F1-score, on the test set. This gives us an indication of whether the system is robust enough, or was overfitted on the development data. Results on the test set are shown in Table TABREF72. On average, the results are lower than the results on the development set. The string-based methods perform clearly worse, with drops of about 4% F1-score for exact and inflectional match, and a large drop of almost 9% F1-score for fuzzy matching. The parser-based method, on the other hand, is more robust, with a small 0.59% increase in F1-score on the test set.",Dictionary-based PIE Extraction ::: Analysis,"Broadly speaking, the PIE extraction systems presented above perform in line with expectations. It is nevertheless useful to see where the best-performing system misses out, and where improvements like in-context parsing help performance. We analyse the shortcomings of the in-context parser-based system by looking at the false positives and false negatives on the development set. We consider the output of the system with best overall performance, since it will provide the clearest picture. The system extracts 529 PIEs in total, of which 54 are false extractions (false positives), and it misses 69 annotated PIE instances (false negatives). Most false positives stem from the system's failure to capture nuances of PIE annotation. This includes cases where PIEs contain, or are part of, proper nouns (Example SECREF73), PIEs that are part of coordination constructions (Example SECREF73), and incorrect attachments (Example SECREF73). Among these errors, sentences containing proper nouns are an especially frequent problem. . Drama series include [..] airline security thrills in Cleared For Takeoff and Head Over Heels [..] (in the clear - BNC - document CBC - sentence 5177) . They prefer silk, satin or lace underwear in tasteful black or ivory. (in the black - BNC - document CBC - sentence 14673) . [..] `I saw this chap make something out of an ordinary piece of wood — he fashioned it into an exquisite work of art.' (out of the woods - BNC - document ABV - sentence 1300) The main cause of false negatives are errors made by the parser. In order to correctly extract a PIE from a sentence, both the PIE and the sentence have to be parsed correctly, or at least parsed in the same way. This means a missed extraction can be caused by a wrong parse for the PIE or a wrong parse for the sentence. These two error types form the largest class of false negatives. Since some PIE types are rather frequent, a wrong parse for a single PIE type can potentially lead to a large number of missed extractions. It is not surprising that the parser makes many mistakes, since idioms often have unusual syntactic constructions (e.g. come a cropper) and contain words where default part-of-speech tags lead to the wrong interpretation (e.g. round is a preposition in round the bend, not a noun or adjective). This is especially true when idioms are parsed without context, and hence, where in-context parsing provides the largest benefit: the number of PIEs which are parsed incorrectly drops, which leads to F1-scores on those types going from 0% to almost 100% (e.g. in light of and ring a bell). Since parser errors are the main contributor to false negatives, hurting recall, we can observe that parsing idioms in context serves to benefit only recall, by 7 percentage points, at only a small loss in precision. We find that adding context mainly helps for parsing expressions which are structurally relatively simple, but still ambiguous, such as rub shoulders, laughing stock, and round the bend. Compare, for example, the parse trees for laughing stock in isolation and within the extracted context sentence in Figures FIGREF74 and FIGREF75. When parsed in isolation, the relation between the two words is incorrectly labelled as a compound relation, whereas in context it is correctly labelled as a direct object relation. Note however, that for the most difficult PIEs, embedding them in a context does solve the parsing problem: a syntactically odd phrase is hard to phrase (e.g. for the time being), and a syntactically odd phrase in a sentence makes for a syntactically odd sentence that is still hard to parse (e.g. `London for the time being had been abandoned.'). Finding example sentences turned out not to be a problem, since appropriate sentences were found for 559 of 591 PIE types. An alternative method for reducing parser error is to use a different, better parser. The Spacy parser was mainly chosen for implementation convenience and speed, and there are parsers which have better performance, as measured on established parsing benchmarks. To investigate the effectiveness of this method, we used the Stanford Neural Dependency Parser BIBREF39 to extract PIEs in the regular parsing, in-context parsing and the no labels settings. In all cases, using the Stanford parser yielded worse extraction performance than the Spacy parser. A possible explanation for why a supposedly better parser performs worse here is that parsers are optimised and trained to do well on established benchmarks, which consist of complete sentences, often from news texts. This does not necessarily correlate with parsing performance on short (sentences containing) idiomatic phrases. As such, we cannot assume that better overall parsing performance implies PIE extraction performance. It should be noted that, when assessing the quality of PIE extraction performance, the parser-based methods are sensitive to specific PIE types. That is, if a single PIE type is parsed incorrectly, then it is highly probable that all instances of that type are missed. If this type is also highly frequent, this means that a small change in actual performance yields a large change in evaluation scores. Our goal is to have a PIE extraction system that is robust across all PIE types, and thus the current evaluation setting does not align exactly with our aim. Splitting out performance per PIE type reveals whether there is indeed a large variance in performance across types. Table TABREF76 shows the 25 most frequent PIE types in the corpus, and the performance of the in-context-parsing-based system on each. Except two cases (in the black and round the bend), we see that the performance is in the 80–100% range, even showing perfect performance on the majority of types. For none of the types do we see low precision paired with high recall, which indicates that the parser never matches a highly frequent non-PIE phrase. For the system with the no labels and no-directionality options (per-type numbers not shown here), however, this does occur. For example, ignoring the labels for the parse of the PIE have a go leads to the erroneous matching of many sentences containing a form of have to go, which is highly frequent, thus leading to a large drop in precision. Although performance is stable across the most frequent types, among the less frequent types it is more spotty. This hurts overall performance, and there are potential gains in mitigating the poor performance on these types, such as for the time being. At the same time, the string matching methods show much more stable performance across types, and some of them do so with very high precision. As such, a combination of two such methods could boost performance significantly. If we use a high-precision string match-based method, such as the exact string match variant with a precision of 97.35%, recall could be improved for the wrongly parsed PIE types, without a significant loss of precision. We experiment with two such combinations, by simply taking the union of the sets of extracted idioms of both systems, and filtering out duplicates. Results are shown in Table TABREF77. Both combinations show the expected effect: a clear gain in recall at a minimal loss in precision. Compared to the in-context-parsing-based system, the combination with exact string matching yields a gain in recall of over 6%, and the combination with inflectional string matching yields an even bigger gain of almost 8%, at precision losses of 0.6% and 0.8%, respectively. This indicates that the systems are very much complementary in the PIEs they extract. It also means that, when used in practice, combining inflectional string matching and parse-based extraction is the most reliable configuration.",Conclusions and Outlook,"We present an in-depth study on the automatic extraction of potentially idiomatic expressions based on dictionaries. The purpose of automatic dictionary-based extraction is, on the one hand, to function as a pre-extraction step in the building of a large idiom-annotated corpus. On the other hand, it can function as part of an idiom extraction system when combined with a disambiguation component. In both cases, the ultimate goal is to improve the processing of idiomatic expressions within NLP. This work consists of three parts: a comparative evaluation of the coverage of idiom dictionaries, the annotation of a PIE corpus, and the development and evaluation of several dictionary-based PIE extraction methods. In the first part, we present a study of idiom dictionary coverage, which serves to answer the question of whether a single idiom dictionary, or a combination of dictionaries, can provide good coverage of the set of all English idioms. Based on the comparison of dictionaries to each other, we estimate that the overlap between them is limited, varying from 20% to 55%, which indicates a large divergence between the dictionaries. This can be explained by the fact that idioms vary widely by register, genre, language variety, and time period. In our case, it is also likely that the divergence is caused partly by the gap between crowdsourced dictionaries on the one hand, and a dictionary compiled by professional lexicographers on the other. Given these factors, we can conclude that a single dictionary cannot provide even close to complete coverage of English idioms, but that by combining dictionaries from various sources, significant gains can be made. Since `English idioms' are a diffuse and constantly changing set, we have no gold standard to compare to. As such, we conclude that multiple dictionaries should be used when possible, but that we cannot say any anything definitive on the coverage of dictionaries with regard to the complete set of English idioms (which can only be approximated in the first place). A more comprehensive of idiom resources could be made in the future by using more advanced automatic methods for matching, for example by using BIBREF32's (BIBREF32) method for measuring expression variability. This would make it easier to evaluate a larger number of dictionaries, since no manual effort would be required. In the second part, we experiment with the exhaustive annotation of PIEs in a corpus of documents from the BNC. Using a set of 591 PIE types, much larger and more varied than in existing resources, we show that it is very much possible to establish a working definition of PIE that allows for a large amount of variation, while still being useful for reliable annotation. This resulted in high inter-annotator agreement, ranging from 0.74 to 0.91 Fleiss' Kappa. This means that we can build a resource to evaluate a wide-range idiom extraction system with relatively little effort. The final corpus of PIEs with sense annotations is publicly available consists of 2,239 PIE candidates, of which 1,050 actual PIEs instances, and contains 278 different PIE types. Finally, several methods for the automatic extraction of PIE instances were developed and evaluated on the annotated PIE corpus. We tested methods of differing complexity, from simple string match to dependency parse-based extraction. Comparison of these methods revealed that the more computationally complex method, parser-based extraction, works best. Parser-based extraction is especially effective in capturing a larger amount of variation, but is less precise than string-based methods, mostly because of parser error. The best overall setting of this method, which parses idioms within context, yielded an F1-score of 89.13% on the test set. Parser error can be partly compensated by combining the parse-based method and the inflectional string match method, which yields an F1-score of 92.01% (on the development set). This aligns well with the findings by BIBREF27, who found that combining simpler and more complex methods improves over just using a simple method case for extracting verb-particle constructions. This level of performance means that we can use the tool in corpus building. This greatly reduces the amount of manual extraction effort involved, while still maintaining a high level of recall. We make the source code for the different systems publicly available. Note that, although used here in the context of PIE extraction, our methods are equally applicable to other phrase extraction tasks, for example the extraction of light-verb constructions, metaphoric constructions, collocations, or any other type of multiword expression (cf. BIBREF27, BIBREF25, BIBREF26). Similarly, our method can be conceived as a blueprint and extended to languages other than English. For this to be possible, for any given new language one would need a list of target expressions and, in the case of the parser-based method, a reliable syntactic parser. If this is not the case, the inflectional matching method can be used, which requires only a morphological analyser and generator. Obviously, for languages that are morphologically richer than English, one would need to develop strategies aimed at controlling non-exact matches, so as to enhance recall without sacrificing precision. Previous work on Italian, for example, has shown the feasibility of achieving such balance through controlled pattern matching BIBREF40. Languages that are typologically very different from English would obviously require a dedicated approach for the matching of PIEs in corpora, but the overall principles of extraction, using language-specific tools, could stay the same. Currently, no corpora containing annotation of PIEs exist for languages other than English. However, the PARSEME corpus BIBREF19 already contains idioms (only idiomatic readings) for many languages and would only need annotation of literal usages of idioms to make up a set of PIEs. Paired with the Universal Dependencies project BIBREF41, which increasingly provides annotated data as well as processing tools for an ever growing number of languages, this seems an excellent starting point for creating PIE resources in multiple languages.",,,,,,,,,,,,,26-Figure10-1.png,Figure 10 Automatic dependency parse of the PIE rub shoulders.,27-Figure11-1.png,Figure 11 Automatic dependency parse of the extracted sentence ‘Each day they rub shoulders with death.’ containing the PIE rub shoulders.,28-Table10-1.png,Table 10 Extraction performance of the in-context-parsing-based system on each of the 25 most frequent PIE types in the corpus.,29-Table11-1.png,"Table 11 PIE extraction performance of the combined output (union) of a string-based and a parser-based system, on the development set. CS indicates case-sensitive string matching. The best score for each metric is in bold.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multi-Task Bidirectional Transformer Representations for Irony Detection,"Supervised deep learning requires large amounts of training data. In the context of the FIRE2019 Arabic irony detection shared task (IDAT@FIRE2019), we show how we mitigate this need by fine-tuning the pre-trained bidirectional encoders from transformers (BERT) on gold data in a multi-task setting. We further improve our models by by further pre-training BERT on `in-domain' data, thus alleviating an issue of dialect mismatch in the Google-released BERT model. Our best model acquires 82.4 macro F1 score, and has the unique advantage of being feature-engineering free (i.e., based exclusively on deep learning).",Introduction,"The proliferation of social media has provided a locus for use, and thereby collection, of figurative and creative language data, including irony BIBREF0. According to the Merriam-Webster online dictionary, irony refers to “the use of word to express something other than and especially the opposite of the literal meaning."" A complex, controversial, and intriguing linguistic phenomenon, irony has been studied in disciplines such as linguistics, philosophy, and rhetoric. Irony detection also has implications for several NLP tasks such as sentiment analysis, hate speech detection, fake news detection, etc BIBREF0. Hence, automatic irony detection can potentially improve systems designed for each of these tasks. In this paper, we focus on learning irony. More specifically, we report our work submitted to the FIRE 2019 Arabic irony detection task (IDAT@FIRE2019). We focus our energy on an important angle of the problem–the small size of training data. Deep learning is the most successful under supervised conditions with large amounts of training data (tens-to-hundreds of thousands of examples). For most real-world tasks, we hard to obtain labeled data. Hence, it is highly desirable to eliminate, or at least reduce, dependence on supervision. In NLP, pre-training language models on unlabeled data has emerged as a successful approach for improving model performance. In particular, the pre-trained multilingual Bidirectional Encoder Representations from Transformers (BERT) BIBREF1 was introduced to learn language regularities from unlabeled data. Multi-task learning (MTL) is another approach that helps achieve inductive transfer between various tasks. More specifically, MTL leverages information from one or more source tasks to improve a target task BIBREF2, BIBREF3. In this work, we introduce Transformer representations (BERT) in an MTL setting to address the data bottleneck in IDAT@FIRE2019. To show the utility of BERT, we compare to a simpler model with gated recurrent units (GRU) in a single task setting. To identify the utility, or lack thereof, of MTL BERT, we compare to a single task BERT model. For MTL BERT, we train on a number of tasks simultaneously. Tasks we train on are sentiment analysis, gender detection, age detection, dialect identification, and emotion detection. Another problem we face is that the BERT model released by Google is trained only on Arabic Wikipedia, which is almost exclusively Modern Standard Arabic (MSA). This introduces a language variety mismatch due to the irony data involving a number of dialects that come from the Twitter domain. To mitigate this issue, we further pre-train BERT on an in-house dialectal Twitter dataset, showing the utility of this measure. To summarize, we make the following contributions: In the context of the Arabic irony task, we show how a small-sized labeled data setting can be mitigated by training models in a multi-task learning setup. We view different varieties of Arabic as different domains, and hence introduce a simple, yet effective, `in-domain' training measure where we further pre-train BERT on a dataset closer to task domain (in that it involves dialectal tweet data).",Methods,,Methods ::: GRU,"For our baseline, we use gated recurrent units (GRU) BIBREF4, a simplification of long-short term memory (LSTM) BIBREF5, which in turn is a variation of recurrent neural networks (RNNs). A GRU learns based on the following: where the update state $\textbf {\textit {z}}^{(t)}$ decides how much the unit updates its content: where W and U are weight matrices. The candidate activation makes use of a reset gate $\textbf {\textit {r}}^{(t)}$: where $\odot $ is a Hadamard product (element-wise multiplication). When its value is close to zero, the reset gate allows the unit to forget the previously computed state. The reset gate $\textbf {\textit {r}}^{(t)}$ is computed as follows:",Methods ::: BERT,"BERT BIBREF1 is based on the Transformer BIBREF6, a network architecture that depends solely on encoder-decoder attention. The Transformer attention employs a function operating on queries, keys, and values. This attention function maps a query and a set of key-value pairs to an output, where the output is a weighted sum of the values. Encoder of the Transformer in BIBREF6 has 6 attention layers, each of which is composed of two sub-layers: (1) multi-head attention where queries, keys, and values are projected h times into linear, learned projections and ultimately concatenated; and (2) fully-connected feed-forward network (FFN) that is applied to each position separately and identically. Decoder of the Transformer also employs 6 identical layers, yet with an extra sub-layer that performs multi-head attention over the encoder stack. The architecture of BERT BIBREF1 is a multi-layer bidirectional Transformer encoder BIBREF6. It uses masked language models to enable pre-trained deep bidirectional representations, in addition to a binary next sentence prediction task captures context (i.e., sentence relationships). More information about BERT can be found in BIBREF1.",Methods ::: Multi-task Learning,"In multi-task learning (MTL), a learner uses a number of (usually relevant) tasks to improve performance on a target task BIBREF2, BIBREF3. The MTL setup enables the learner to use cues from various tasks to improve the performance on the target task. MTL also usually helps regularize the model since the learner needs to find representations that are not specific to a single task, but rather more general. Supervised learning with deep neural networks requires large amounts of labeled data, which is not always available. By employing data from additional tasks, MTL thus practically augments training data to alleviate need for large labeled data. Many researchers achieve state-of-the-art results by employing MTL in supervised learning settings BIBREF7, BIBREF8. In specific, BERT was successfully used with MTL. Hence, we employ multi-task BERT (following BIBREF8). For our training, we use the same pre-trained BERT-Base Multilingual Cased model as the initial checkpoint. For this MTL pre-training of BERT, we use the same afore-mentioned single-task BERT parameters. We now describe our data.",Data,"The shared task dataset contains 5,030 tweets related to different political issues and events in the Middle East taking place between 2011 and 2018. Tweets are collected using pre-defined keywords (i.e. targeted political figures or events) and the positive class involves ironic hashtags such as #sokhria, #tahakoum, and #maskhara (Arabic variants for “irony""). Duplicates, retweets, and non-intelligible tweets are removed by organizers. Tweets involve both MSA as well as dialects at various degrees of granularity such as Egyptian, Gulf, and Levantine. IDAT@FIRE2019 is set up as a binary classification task where tweets are assigned labels from the set {ironic, non-ironic}. A total of 4,024 tweets were released by organizers as training data. In addition, 1,006 tweets were used by organizers as as test data. Test labels were not release; and teams were expected to submit the predictions produced by their systems on the test split. For our models, we split the 4,024 released training data into 90% TRAIN ($n$=3,621 tweets; `ironic'=1,882 and `non-ironic'=1,739) and 10% DEV ($n$=403 tweets; `ironic'=209 and `non-ironic'=194). We train our models on TRAIN, and evaluate on DEV. Our multi-task BERT models involve six different Arabic classification tasks. We briefly introduce the data for these tasks here: Author profiling and deception detection in Arabic (APDA). BIBREF9 . From APDA, we only use the corpus of author profiling (which includes the three profiling tasks of age, gender, and variety). The organizers of APDA provide 225,000 tweets as training data. Each tweet is labelled with three tags (one for each task). To develop our models, we split the training data into 90% training set ($n$=202,500 tweets) and 10% development set ($n$=22,500 tweets). With regard to age, authors consider tweets of three classes: {Under 25, Between 25 and 34, and Above 35}. For the Arabic varieties, they consider the following fifteen classes: {Algeria, Egypt, Iraq, Kuwait, Lebanon-Syria, Lybia, Morocco, Oman, Palestine-Jordan, Qatar, Saudi Arabia, Sudan, Tunisia, UAE, Yemen}. Gender is labeled as a binary task with {male,female} tags. LAMA+DINA Emotion detection. Alhuzali et al. BIBREF10 introduce LAMA, a dataset for Arabic emotion detection. They use a first-person seed phrase approach and extend work by Abdul-Mageed et al. BIBREF11 for emotion data collection from 6 to 8 emotion categories (i.e. anger, anticipation, disgust, fear, joy, sadness, surprise and trust). We use the combined LAMA+DINA corpus. It is split by the authors as 189,902 tweets training set, 910 as development, and 941 as test. In our experiment, we use only the training set for out MTL experiments. Sentiment analysis in Arabic tweets. This dataset is a shared task on Kaggle by Motaz Saad . The corpus contains 58,751 Arabic tweets (46,940 training, and 11,811 test). The tweets are annotated with positive and negative labels based on an emoji lexicon.",Models ::: GRU,"We train a baseline GRU network with our iorny TRAIN data. This network has only one layer unidirectional GRU, with 500 unites and a linear, output layer. The input word tokens are embedded by the trainable word vectors which are initialized with a standard normal distribution, with $\mu =0$, and $\sigma =1$, i.e., $W \sim N(0,1)$. We use Adam BIBREF12 with a fixed learning rate of $1e-3$ for optimization. For regularization, we use dropout BIBREF13 with a rate of 0.5 on the hidden layer. We set the maximum sequence sequence in our GRU model to 50 words, and use all 22,000 words of training set as vocabulary. We employ batch training with a batch size of 64 for this model. We run the network for 20 epochs and save the model at the end of each epoch, choosing the model that performs highest on DEV as our best model. We report our best result on DEV in Table TABREF22. Our best result is acquired with 12 epochs. As Table TABREF22 shows, the baseline obtains $accuracy=73.70\%$ and $F_1=73.47$.",Models ::: Single-Task BERT,"We use the BERT-Base Multilingual Cased model released by the authors BIBREF1 . The model is trained on 104 languages (including Arabic) with 12 layers, 768 hidden units each, 12 attention heads. The entire model has 110M parameters. The model has 119,547 shared WordPieces vocabulary, and was pre-trained on the entire Wikipedia for each language. For fine-tuning, we use a maximum sequence size of 50 tokens and a batch size of 32. We set the learning rate to $2e-5$ and train for 20 epochs. For single-task learning, we fine-tune BERT on the training set (i.e., TRAIN) of the irony task exclusively. We refer to this model as BERT-ST, ST standing for `single task.' As Table TABREF22 shows, BERT-ST unsurprisingly acquires better performance than the baseline GRU model. On accuracy, BERT-ST is 7.94% better than the baseline. BERT-ST obtains 81.62 $F_1$ which is 7.35 better than the baseline.",Models ::: Multi-Task BERT,"We follow the work of Liu et al. BIBREF8 for training an MTL BERT in that we fine-tune the afore-mentioned BERT-Base Multilingual Cased model with different tasks jointly. First, we fine-tune with the three tasks of author profiling and the irony task simultaneously. We refer to this model trained on the 4 tasks simply as BERT-MT4. BERT-MT5 refers to the model fine-tuned on the 3 author profiling tasks, the emotion task, and the irony task. We also refer to the model fine-tuned on all six tasks (adding the sentiment task mentioned earlier) as BERT-MT6. For MTL BERT, we use the same parameters as the single task BERT listed in the previous sub-section (i.e., Single-Task BERT). In Table TABREF22, we present the performance on the DEV set of only the irony detection task. We note that all the results of multitask learning with BERT are better than those with the single task BERT. The model trained on all six tasks obtains the best result, which is 2.23% accuracy and 2.25% $F_1$ higher than the single task BERT model.",Models ::: In-Domain Pre-Training,"Our irony data involves dialects such as Egyptian, Gulf, and Levantine, as we explained earlier. The BERT-Base Multilingual Cased model we used, however, was trained on Arabic Wikipedia, which is mostly MSA. We believe this dialect mismatch is sub-optimal. As Sun et al. BIBREF14 show, further pre-training with domain specific data can improve performance of a learner. Viewing dialects as constituting different domains, we turn to dialectal data to further pre-train BERT. Namely, we use 1M tweets randomly sampled from an in-house Twitter dataset to resume pre-training BERT before we fine-tune on the irony data. We use BERT-Base Multilingual Cased model as an initial checkpoint and pre-train on this 1M dataset with a learning rate of $2e-5$, for 10 epochs. Then, we fine-tune on MT5 (and then on MT6) with the new further-pre-trained BERT model. We refer to the new models as BERT-1M-MT5 and BERT-1M-MT6, respectively. As Table TABREF22 shows, BERT-1M-MT5 performs best: BERT-1M-MT5 obtains 84.37% accuracy (0.5% less than BERT-MT6) and 83.34 $F_1$ (0.47% less than BERT-MT6).",Models ::: IDAT@FIRE2019 Submission,"For the shared task submission, we use the predictions of BERT-1M-MT5 as our first submitted system. Then, we concatenate our DEV and TRAIN data to compose a new training set (thus using all the training data released by organizers) to re-train BERT-1M-MT5 and BERT-MT6 with the same parameters. We use the predictions of these two models as our second and third submissions. Our second submission obtains 82.4 $F_1$ on the official test set, and ranks $4th$ on this shared task.",Related Work,"Multi-Task Learning. MTL has been effectively used to model several NLP problems. These include, for example, syntactic parsing BIBREF15, sequence labeling BIBREF16, BIBREF17, and text classification BIBREF18. Irony in different languages. Irony detection has been investigated in various languages. For example, Hee et al. BIBREF19 propose two irony detection tasks in English tweets. Task A is a binary classification task (irony vs. non-irony), and Task B is multi-class identification of a specific type of irony from the set {verbal, situational, other-irony, non-ironic}. They use hashtags to automatically collect tweets that they manually annotate using a fine-grained annotation scheme. Participants in this competition construct models based on logistic regression and support vector machine (SVM) BIBREF20, XGBoost BIBREF21, convolutional neural networks (CNNs) BIBREF21, long short-term memory networks (LSTMs) BIBREF22, etc. For the Italian language, Cignarella et al. propose the IronTA shared task BIBREF23, and the best system BIBREF24 is a combination of bi-directional LSTMs, word $n$-grams, and affective lexicons. For Spanish, Ortega-Bueno1 et al. BIBREF25 introduce the IroSvA shared task, a binary classification task for tweets and news comments. The best-performing model on the task, BIBREF26, employs pre-trained Word2Vec, multi-head Transformer encoder and a global average pooling mechanism. Irony in Arabic. Although Arabic is a widely spoken collection of languages ($\sim $ 300 million native speakers) BIBREF27, BIBREF28, there has not been works on irony that we know of on the language. IDAT@FIRE2019 aims at bridging this gap. The closest works in Arabic are those focusing on other text classification tasks such as sentiment analysis BIBREF29, BIBREF30, BIBREF31, BIBREF32, emotion BIBREF10, and dialect identification BIBREF28, BIBREF33, BIBREF34, BIBREF35.",Conclusion,"In this paper, we described our submissions to the Irony Detection in Arabic shared task (IDAT@FIRE2019). We presented how we acquire effective models using pre-trained BERT in a multi-task learning setting. We also showed the utility of viewing different varieties of Arabic as different domains by reporting better performance with models pre-trained with dialectal data rather than exclusively on MSA. Our multi-task model with domain-specific BERT ranks $4th$ in the official IDAT@FIRE2019 evaluation. The model has the advantage of being exclusively based on deep learning. In the future, we will investigate other multi-task learning architectures, and extend our work with semi-supervised methods.",Acknowledgement,"We acknowledge the support of the Natural Sciences and Engineering Research Council of Canada (NSERC), the Social Sciences Research Council of Canada (SSHRC), and Compute Canada (www.computecanada.ca).",,,,,,,,,,,,,,,Why is being feature-engineering free an advantage?,dc57ae854d78aa5d5e8c979826d3e2524d4e9165,five,unfamiliar,no,irony,e8b24c3133e0bec0a6465e1f13acd3a5ed816b37,True,,,,,17dfdb8d991a75967a343b61db898afdf2327080,c1018a31c3272ce74964a3280069f62f314a1a58,,,,,,,,,Where did this model place in the final evaluation of the shared task?,18412237f7faafc6befe975d5bcd348e2b499b55,five,unfamiliar,no,irony,e8b24c3133e0bec0a6465e1f13acd3a5ed816b37,False,,,"For the shared task submission, we use the predictions of BERT-1M-MT5 as our first submitted system. Then, we concatenate our DEV and TRAIN data to compose a new training set (thus using all the training data released by organizers) to re-train BERT-1M-MT5 and BERT-MT6 with the same parameters. We use the predictions of these two models as our second and third submissions. Our second submission obtains 82.4 $F_1$ on the official test set, and ranks $4th$ on this shared task.","Our second submission obtains 82.4 $F_1$ on the official test set, and ranks $4th$ on this shared task.",54c3382fecec47ef37f88604af2bf6bf02e2820b,e70d8110563d53282f1a26e823d27e6f235772db,What in-domain data is used to continue pre-training?,02945c85d6cc4cdd1757b2f2bfa5e92ee4ed14a0,five,unfamiliar,no,irony,e8b24c3133e0bec0a6465e1f13acd3a5ed816b37,False,,,"We view different varieties of Arabic as different domains, and hence introduce a simple, yet effective, `in-domain' training measure where we further pre-train BERT on a dataset closer to task domain (in that it involves dialectal tweet data).","We view different varieties of Arabic as different domains, and hence introduce a simple, yet effective, `in-domain' training measure where we further pre-train BERT on a dataset closer to task domain (in that it involves dialectal tweet data).",d0c72070dcae3cbedd92cf8585d532a3c7a6910f,c1018a31c3272ce74964a3280069f62f314a1a58,,,,,,,,What dialect is used in the Google BERT model and what is used in the task data?,6e51af9088c390829703c6fa966e98c3a53114c1,five,unfamiliar,no,irony,e8b24c3133e0bec0a6465e1f13acd3a5ed816b37,False,,,"Another problem we face is that the BERT model released by Google is trained only on Arabic Wikipedia, which is almost exclusively Modern Standard Arabic (MSA). This introduces a language variety mismatch due to the irony data involving a number of dialects that come from the Twitter domain. To mitigate this issue, we further pre-train BERT on an in-house dialectal Twitter dataset, showing the utility of this measure. To summarize, we make the following contributions: The shared task dataset contains 5,030 tweets related to different political issues and events in the Middle East taking place between 2011 and 2018. Tweets are collected using pre-defined keywords (i.e. targeted political figures or events) and the positive class involves ironic hashtags such as #sokhria, #tahakoum, and #maskhara (Arabic variants for “irony""). Duplicates, retweets, and non-intelligible tweets are removed by organizers. Tweets involve both MSA as well as dialects at various degrees of granularity such as Egyptian, Gulf, and Levantine.","Another problem we face is that the BERT model released by Google is trained only on Arabic Wikipedia, which is almost exclusively Modern Standard Arabic (MSA). The shared task dataset contains 5,030 tweets related to different political issues and events in the Middle East taking place between 2011 and 2018. Tweets involve both MSA as well as dialects at various degrees of granularity such as Egyptian, Gulf, and Levantine.",6b71a7d32bc26c0095340b9926610c7dbe00decc,c1018a31c3272ce74964a3280069f62f314a1a58,,,,,,,,What are the tasks used in the mulit-task learning setup?,07ee4e0277ad1083270131d32a71c3fe062a916d,five,unfamiliar,no,irony,e8b24c3133e0bec0a6465e1f13acd3a5ed816b37,False,Author profiling and deception detection in Arabic LAMA+DINA Emotion detection Sentiment analysis in Arabic tweets,,,"Our multi-task BERT models involve six different Arabic classification tasks. We briefly introduce the data for these tasks here: Author profiling and deception detection in Arabic (APDA). BIBREF9 . From APDA, we only use the corpus of author profiling (which includes the three profiling tasks of age, gender, and variety). The organizers of APDA provide 225,000 tweets as training data. Each tweet is labelled with three tags (one for each task). To develop our models, we split the training data into 90% training set ($n$=202,500 tweets) and 10% development set ($n$=22,500 tweets). With regard to age, authors consider tweets of three classes: {Under 25, Between 25 and 34, and Above 35}. For the Arabic varieties, they consider the following fifteen classes: {Algeria, Egypt, Iraq, Kuwait, Lebanon-Syria, Lybia, Morocco, Oman, Palestine-Jordan, Qatar, Saudi Arabia, Sudan, Tunisia, UAE, Yemen}. Gender is labeled as a binary task with {male,female} tags. LAMA+DINA Emotion detection. Alhuzali et al. BIBREF10 introduce LAMA, a dataset for Arabic emotion detection. They use a first-person seed phrase approach and extend work by Abdul-Mageed et al. BIBREF11 for emotion data collection from 6 to 8 emotion categories (i.e. anger, anticipation, disgust, fear, joy, sadness, surprise and trust). We use the combined LAMA+DINA corpus. It is split by the authors as 189,902 tweets training set, 910 as development, and 941 as test. In our experiment, we use only the training set for out MTL experiments. Sentiment analysis in Arabic tweets. This dataset is a shared task on Kaggle by Motaz Saad . The corpus contains 58,751 Arabic tweets (46,940 training, and 11,811 test). The tweets are annotated with positive and negative labels based on an emoji lexicon.",Our multi-task BERT models involve six different Arabic classification tasks. Author profiling and deception detection in Arabic (APDA). LAMA+DINA Emotion detection. Sentiment analysis in Arabic tweets.,43319bfb4454a9b53022fc8a9e2afd95057d70bb,c1018a31c3272ce74964a3280069f62f314a1a58,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,6-Table1-1.png,Table 1. Model Performance,,,,,,,,,,,,,,,,,,,$4th$,dialectal tweet data,,,,,,,,,,,,,,,,,,,,,"Modern Standard Arabic (MSA) MSA as well as dialects at various degrees of granularity such as Egyptian, Gulf, and Levantine",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Should All Cross-Lingual Embeddings Speak English?,"Most of recent work in cross-lingual word embeddings is severely Anglocentric. The vast majority of lexicon induction evaluation dictionaries are between English and another language, and the English embedding space is selected by default as the hub when learning in a multilingual setting. With this work, however, we challenge these practices. First, we show that the choice of hub language can significantly impact downstream lexicon induction performance. Second, we both expand the current evaluation dictionary collection to include all language pairs using triangulation, and also create new dictionaries for under-represented languages. Evaluating established methods over all these language pairs sheds light into their suitability and presents new challenges for the field. Finally, in our analysis we identify general guidelines for strong cross-lingual embeddings baselines, based on more than just Anglocentric experiments.",Introduction,"Continuous distributional vectors for representing words (embeddings) BIBREF0 have become ubiquitous in modern, neural NLP. Cross-lingual representations BIBREF1 additionally represent words from various languages in a shared continuous space, which in turn can be used for Bilingual Lexicon Induction (BLI). BLI is often the first step towards several downstream tasks such as Part-Of-Speech (POS) tagging BIBREF2, parsing BIBREF3, document classification BIBREF4, and machine translation BIBREF5, BIBREF6, BIBREF7. Often, such shared representations are learned with a two-step process, whether under bilingual or multilingual settings (hereinafter BWE and MWE, respectively). First, monolingual word embeddings are learned over large swaths of text; such pre-trained word embeddings, in fact, are available for several languages and are widely used, like the fastText Wikipedia vectors BIBREF8. Second, a mapping between the languages is learned, in one of three ways: in a supervised manner if dictionaries or parallel data are available to be used for supervision BIBREF9, under minimal supervision e.g. using only identical strings BIBREF10, or even in a completely unsupervised fashion BIBREF11, BIBREF12. Both in bilingual and multilingual settings, it is common that one of the language embedding spaces is the target to which all other languages get aligned to (hereinafter “the hub""). We outline the details in Section SECREF2. Despite all the recent progress in learning cross-lingual embeddings, we identify a major shortcoming to previous work: it is by and large English-centric. Notably, most MWE approaches essentially select English as the hub during training by default, aligning all other language spaces to the English one. We argue and empirically show, however, that English is a poor hub language choice. In BWE settings, on the other hand, it is fairly uncommon to denote which of the two languages is the hub (often this is implied to be the target language). However, we experimentally find that this choice can greatly impact downstream performance, especially when aligning distant languages. This Anglocentricity is even more evident at the evaluation stage. The lexica most commonly used for evaluation are the MUSE lexica BIBREF12 which cover 45 languages, but with translations only from and into English. Even still, alternative evaluation dictionaries are also very English- and European-centric: BIBREF13 report results on English–Italian, BIBREF14 on English–German and English–Finnish, BIBREF11 on Spanish–English and Italian–English, and BIBREF15 between English and Italian, German, Finnish, Spanish, and Turkish. We argue that cross-lingual word embedding mapping methods should look beyond English for their evaluation benchmarks because, compared to all others, English is a language with disproportionately large available data and relatively poor inflectional morphology e.g., it lacks case, gender, and complex verbal inflection systems BIBREF16. These two factors allow for an overly easy evaluation setting which does not necessarily generalize to other language pairs. In light of this, equal focus should instead be devoted to evaluation over more diverse language pairs that also include morphologically rich and low-resource languages. With this work, we attempt to address these shortcomings, providing the following contributions: We show that the choice of the hub when evaluating on diverse language pairs can lead to significantly different performance (e.g., by more than 10 percentage points for BWE over distant languages). We also show that often English is a suboptimal hub for MWE. We identify some general guidelines for choosing a hub language which could lead to stronger baselines; less isometry between the hub and source and target embedding spaces mildly correlates with performance, as does typological distance (a measure of language similarity based on language family membership trees). For distant languages, multilingual systems should in most cases be preferred over bilingual ones. We provide resources for training and evaluation on non-Anglocentric language pairs. We outline a simple triangulation method with which we extend the MUSE dictionaries to an additional 2352 lexicons covering 49 languages, and we present results on a subset of them. We also create new evaluation lexica for under-resourced languages using Azerbaijani, Belarusian, and Galician as our test cases. We additionally provide recipes for creating such dictionaries for any language pair with available parallel data.",Cross-Lingual Word Embeddings and Lexicon Induction,"In the supervised bilingual setting, as formulated by BIBREF1, given two languages $\mathcal {L} = \lbrace l_1,l_2\rbrace $ and their pre-trained row-aligned embeddings $\mathcal {X}_1, \mathcal {X}_2,$ respectively, a transformation matrix $$ is learned such that: The set $\Omega $ can potentially impose a constraint over $$, such as the very popular constraint of restricting it to be orthogonal BIBREF17. Previous work has empirically found that this simple formulation is competitive with other more complicated alternatives BIBREF17, BIBREF12. The orthogonality assumption ensures that there exists a closed-form solution in the form of the Singular Value Decomposition (SVD) of $_1_2^T$. Note that in this case only a single matrix $$ needs to be learned, because $\left\Vert _1 - ^{}_2 \right\Vert =\left\Vert ^{-1}_1 - _2 \right\Vert $, while at the same time a model that minimizes $\left\Vert _1 - _2 \right\Vert $ is as expressive as one minimizing $\left\Vert _1_1 - _2_2 \right\Vert $, and easier to learn. In the minimally supervised or even the unsupervised setting BIBREF11 the popular methods follow an iterative refinement approach BIBREF14. Starting with a seed dictionary (e.g. from identical strings BIBREF18 or numerals) an initial mapping is learned in the same manner as in the supervised setting. The initial mapping, in turn, is used to expand the seed dictionary with high confidence word translation pairs. The new dictionary is then used to learn a better mapping, and so forth the iterations continue until convergence. Similarly, in a multilingual setting, one could start with $N$ languages $\mathcal {L} = \lbrace l_1,l_2,\ldots ,l_N\rbrace $ and their respective pre-trained embeddings $\mathcal {X}_1, \mathcal {X}_2,\ldots ,_N$, and then learn $N-1$ bilingual mappings between a pre-selected target language and all others. Hence, one of the language spaces is treated as a target (the hub) and remains invariant, while all others are mapped into the (now shared) hub language space. Alternatively, those mappings could be jointly learned using the MAT+MPSR methods of BIBREF19 – also taking advantage of the inter-dependencies between any two language pairs. Importantly, though, there is no closed form solution for learning the joint mapping, hence a solution needs to be approximated with gradient-based methods. MAT+MPSR generalizes the adversarial approach of BIBREF11 to multiple languages, and also follows an iterative refinement approach. Similar to BIBREF19, BIBREF20 note that it is important to use all $N^2$ language pairs when optimizing multilingual alignments, rather than just the $N-1$ pairs between the hub and all other languages, and propose a model (UMH) that implements this intuition within a Wasserstein-Procrustes approach. Even though the architecture and modeling approach of MAT+MPSR and of UMH are not the same, the two methods are conceptually similar, as in both cases a language is chosen as the hub, and $N-1$ mappings for the other languages are learned. In either case, English is by default selected to be the hub. The only exception is the study of triplets alignments in BIBREF20, where Spanish is used as the hub for the Spanish–French–Portuguese triplet, although the authors mention that they found little differences due the choice of the hub, in contrast to our general findings. Other than MAT+MPSR and UMH, another unsupervised multilingual approach is that of BIBREF21, who propose to incrementally align multiple languages by adding each new language as a hub. We decided, though, against comparing to this method, because (a) their method requires learning $\mathcal {O}(N^2)$ mappings for relatively small improvements and (b) the order in which the languages are added is an additional hyperparameter that would explode the experimental space.",Cross-Lingual Word Embeddings and Lexicon Induction ::: Lexicon Induction,"One of the most common downstream evaluation tasks for the learned cross-lingual word mappings is Lexicon Induction (LI), the task of retrieving the most appropriate word-level translation for a query word from the mapped embedding spaces. Specialized evaluation (and training) dictionaries have been created for multiple language pairs, with the MUSE dictionaries BIBREF12 most often used, providing word translations between English (En) and 48 other high- to mid-resource languages, as well as on all 30 pairs among 6 very similar Romance and Germanic languages (English, French, German, Spanish, Italian, Portuguese). Given the mapped embedding spaces, the translations are retrieved using a distance metric, with Cross-Lingual Similarity Scaling BIBREF12 as the most common and best performing in the literature. Intuitively, CSLS decreases the scores of pairs that lie in dense areas, increasing the scores of rarer words (which are harder to align). The retrieved pairs are compared to the gold standard and evaluated using precision at $k$ (P@$k$, evaluating how often the correct translation is within the $k$ retrieved nearest neighbours of the query). Throughout this work we report P@1, which is equivalent to accuracy, but we also provide results with P@5 and P@10 in the Appendix. ",Conclusion,"With this work we challenge the standard practices in learning cross-lingual word embeddings. We empirically showed that the choice of the hub language is an important parameter that affects lexicon induction performance in both bilingual (between distant languages) and multilingual settings. More importantly, we hope that by providing new dictionaries and baseline results on several language pairs, we will stir the community towards evaluating all methods in challenging scenarios that include under-represented language pairs. Towards this end, our analysis provides insights and general directions for stronger baselines for non-Anglocentric cross-lingual word embeddings.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,What evaluation metrics did they use?,d3014683dff9976b7c56b72203df99f0e27e9989,,unfamiliar,no,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,,"Given the mapped embedding spaces, the translations are retrieved using a distance metric, with Cross-Lingual Similarity Scaling BIBREF12 as the most common and best performing in the literature. Intuitively, CSLS decreases the scores of pairs that lie in dense areas, increasing the scores of rarer words (which are harder to align). The retrieved pairs are compared to the gold standard and evaluated using precision at $k$ (P@$k$, evaluating how often the correct translation is within the $k$ retrieved nearest neighbours of the query). Throughout this work we report P@1, which is equivalent to accuracy, but we also provide results with P@5 and P@10 in the Appendix.","The retrieved pairs are compared to the gold standard and evaluated using precision at $k$ (P@$k$, evaluating how often the correct translation is within the $k$ retrieved nearest neighbours of the query). Throughout this work we report P@1, which is equivalent to accuracy, but we also provide results with P@5 and P@10 in the Appendix.",c2592428be5720815b85e236aaa8c7a9911d415a,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,What is triangulation?,ed522090941f61e97ec3a39f52d7599b573492dd,,unfamiliar,no,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,"Answer with content missing: (Chapter 3) The concept can be easily explained with an example, visualized in Figure 1. Consider the Portuguese (Pt) word trabalho which, according to the MUSE Pt–En dictionary, has the words job and work as possible En translations. In turn, these two En words can be translated to 4 and 5 Czech (Cs) words respectively. By utilizing the transitive property (which translation should exhibit) we can identify the set of 7 possible Cs translations for the Pt word trabalho.",Conclusion,Conclusion,a339aeb1bb185cd9a0f28bb2b274047c9a933f60,258ee4069f740c400c0049a2580945a1cc7f044c,What languages are explored in this paper?,5d164651a4aed7cf24d53ba9685b4bee8c965933,,unfamiliar,no,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,True,,,,,50a5b49b5204ac0c62b058797fb9d229dd2edeff,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3-Table1-1.png,Table 1: Triangulation and filtering example on Greek–Italian. All words are valid translations of,5-Table2-1.png,"Table 2: Lexicon Induction performance (measured with P@1) over 10 European languages (90 pairs). In each cell, the superscript denotes the hub language that yields the best result for that language pair. µbest: average using the best hub language. µEn: average using the En as the hub. The shaded cells are the only language pairs where a bilingual MUSE system outperforms MAT+MSPR.",6-Table3-1.png,Table 3: Lexicon Induction performance (P@1) over MWEs from 7 typologically distant languages (42 pairs). See Table 2 for notation.,7-Figure2-1.png,Figure 2: Expected gain Gl for the MWE experiments.,8-Table5-1.png,"Table 5: Comparison of bilingual, trilingual, and multilingual systems for distant (left) and related (right) languages. Multilinguality boosts performance significantly on distant languages.",8-Figure3-1.png,Figure 3: The downstream accuracy generally correlates positively with the GH distance of the source and target language vector spaces to the hub language.,,,,,,,,,,,13-Table6-1.png,Table 6: All results from the European-languagesMWE experiment: P@1 (part 1).,14-Table7-1.png,Table 7: All results from the European-languagesMWE experiment: P@1 (part 2).,15-Table8-1.png,Table 8: All results from the European-languagesMWE experiment: P@5 (part 1).,16-Table9-1.png,Table 9: All results from the European-languagesMWE experiment: P@5 (part 2).,17-Table10-1.png,Table 10: All results from the European-languagesMWE experiment: P@10 (part 1).,,"we report P@1, which is equivalent to accuracy we also provide results with P@5 and P@10 in the Appendix",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,18-Table11-1.png,Table 11: All results from the European-languagesMWE experiment: P@10 (part 2).,19-Table12-1.png,Table 12: All results from the distant languages MWE experiment (P@1).,,,,,,,,20-Table13-1.png,Table 13: All results from the distant languages MWE experiment (P@5).,21-Table14-1.png,Table 14: All results from the distant languages MWE experiment (P@10).,22-Table15-1.png,Table 15: BWE results (P@1) with MUSE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adapt or Get Left Behind: Domain Adaptation through BERT Language Model Finetuning for Aspect-Target Sentiment Classification,"Aspect-Target Sentiment Classification (ATSC) is a subtask of Aspect-Based Sentiment Analysis (ABSA), which has many applications e.g. in e-commerce, where data and insights from reviews can be leveraged to create value for businesses and customers. Recently, deep transfer-learning methods have been applied successfully to a myriad of Natural Language Processing (NLP) tasks, including ATSC. Building on top of the prominent BERT language model, we approach ATSC using a two-step procedure: self-supervised domain-specific BERT language model finetuning, followed by supervised task-specific finetuning. Our findings on how to best exploit domain-specific language model finetuning enable us to produce new state-of-the-art performance on the SemEval 2014 Task 4 restaurants dataset. In addition, to explore the real-world robustness of our models, we perform cross-domain evaluation. We show that a cross-domain adapted BERT language model performs significantly better than strong baseline models like vanilla BERT-base and XLNet-base. Finally, we conduct a case study to interpret model prediction errors.",Introduction,"Sentiment Analysis (SA) is an active field of research in Natural Language Processing and deals with opinions in text. A typical application of classical SA in an industrial setting would be to classify a document like a product review into positve, negative or neutral sentiment polarity. In constrast to SA, the more fine-grained task of Aspect Based Sentiment Analysis (ABSA) BIBREF0, BIBREF1 aims at finding both the aspect of an entity like a restaurant and the sentiment associated with this aspect. It is important to note that ABSA comes in two variants. We will use the sentence “I love their dumplings” to explain these variants in detail. Both variants are implemented as a two-step procedure. The first variant is comprised of Aspect-Category Detection (ACD) followed by Aspect-Category Sentiment Classification (ACSC). ACD is a multilabel classification task, where a sentence can be associated with a set of predefined aspect categories like ""food"" and ""service"" in the restaurants domain. In the second step, ACSC, the sentiment polarity associated to the aspect is classified. For our example-sentence the correct result is (“food”, “positive”). The second variant consists of Aspect-Target Extraction (ATE) followed by Aspect-Target Sentiment Classification (ATSC). ATE is a sequence labeling task, where terms like “dumplings” are detected. In the second step, ATSC, the sentiment polarity associated to the aspect-target is determined. In our example the correct result is the tuple (""dumplings"", ""positive""). In this work, we focus on ATSC. In the last years, specialized neural architectures BIBREF2, BIBREF3 have been developed that substantially improved modeling of this target-context relationship. More recently, the Natural Language Processing community experienced a substantial shift towards using pre-trained language models BIBREF4, BIBREF5, BIBREF6, BIBREF7 as a base for many down-stream tasks, including ABSA BIBREF8, BIBREF9, BIBREF10. We still see huge potential that comes with this trend, this is why we approach the ATSC task using the BERT architecture. As shown by BIBREF9, for the ATSC task the performance of models that were pre-trained on general text corpora is improved substantially by finetuning the model on domain-specific corpora — in their case review corpora — that have not been used for pre-training BERT, or other language models. We extend the work by Xu et al. by further investigating the behavior of finetuning the BERT language model in relation to ATSC performance. In particular, our contributions are: The analysis of the influence of the amount of training-steps used for BERT language model finetuning on the Aspect-Target Sentiment Classification performance. The findings on how to exploit BERT language model finetuning enables us to achieve new state-of-the-art performance on the SemEval 2014 restaurants dataset. The analysis of cross-domain adaptation between the laptops and restaurants domain. Adaptation is tested by finetuning the BERT language model self-supervised on the target-domain and then supervised training on the ATSC task in the source-domain. In addition, the performance of training on the combination of both datasets is measured.",Related Works,"We separate our discussion of related work into two areas: First, neural methods applied to ATSC that have improved performance solely by model architecture improvements. Secondly, methods that additionally aim to transfer knowledge from semantically related tasks or domains.",Related Works ::: Architecture Improvements for Aspect-Target Sentiment Classification,"The datasets typically used for Aspect-Target Sentiment Classification are the SemEval 2014 Task 4 datasets BIBREF1 for the restaurants and laptops domain. Unfortunately, both datasets only have a small number of training examples. One common approach to compensate for insufficient training examples is to invent neural architectures that better model ATSC. For example, in the past a big leap in classification performance was achieved with the use of the Memory Network architecture BIBREF3, which uses memory to remember context words and explicitly models attention over both the target word and context. It was found that making full use of context words improves their model compared to previous models BIBREF2 that make use of left- and right-sided context independently. BIBREF8 proposed Attention Encoder Networks (AEN), a modification to the transformer architecture. The authors split the Multi-Head Attention (MHA) layers into Intra-MHA and Inter-MHA layers in order to model target words and context differently, which results in a more lightweight model compared to the transformer architecture. Another recent performance leap was achieved by BIBREF11, who model dependencies between sentiment words explicitly in sentences with more than one aspect-target by using a graph convolutional neural network. They show that their architecture performs particularly well if multiple aspects are present in a sentence.",Related Works ::: Knowledge Transfer for Aspect-Target Sentiment Classification Analysis,"Another approach to compensate for insufficient training examples is to transfer knowledge across domains or across similar tasks. BIBREF12 proposed Multi-Granularity Alignment Networks (MGAN). They use this architecture to transfer knowledge from both an aspect-category classification task and also across different domains. They built a large scale aspect-category dataset specifically for this. BIBREF13 transfer knowledge from a document-level sentiment classification task trained on the amazon review dataset BIBREF14. They successfully apply pre-training by reusing the weights of a Long Short Term Memory (LSTM) network BIBREF15 that has been trained on the document-level sentiment task. In addition, they apply multi-task learning where aspect and document-level tasks are learned simultaneously by minimizing a joint loss function. Similarly, BIBREF9 introduce a multi-task loss function to simultaneously optimize the BERT model's BIBREF7 pre-training objectives as well as a question answering task. In contrast to the methods described above that aim to transfer knowledge from a different source task like question answering or document-level sentiment classification, this paper aims at transferring knowledge across different domains by finetuning the BERT language model.",Methodology,"We approach the Aspect-Target Sentiment Classification task using a two-step procedure. We use the pre-trained BERT architecture as a basis. In the first step we finetune the pre-trained weights of the language model further in a self-supervised way on a domain-specific corpus. In the second step we train the finetuned language model in a supervised way on the ATSC end-task. In the following subsections, we discuss the BERT architecture, how we finetune the language model, and how we transform the ATSC task into a BERT sequence-pair classification task BIBREF10. Finally, we discuss the different end-task training and domain-specific finetuning combinations we employ to evaluate our model's generalization performance not only in-domain but also cross-domain.",Methodology ::: BERT,"The BERT model builds on many previous innovations: contextualized word representations BIBREF4, the transformer architecture BIBREF16, and pre-training on a language modeling task with subsequent end-to-end finetuning on a downstream task BIBREF5, BIBREF6. Due to being deeply bidirectional, the BERT architecture creates very powerful sequence representations that perform extremely well on many downstream tasks BIBREF7. The main innovation of BERT is that instead of using the objective of next-word prediction a different objective is used to train the language model. This objective consists of 2 parts. The first part is the masked language model objective, where the model learns to predict tokens, which have been randomly masked, from the context. The second part is the next-sequence prediction objective, where the model needs to predict if a sequence $B$ would naturally follow the previous sequence $A$. This objective enables the model to capture long-term dependencies better. Both objectives are discussed in more detail in the next section. As a base for our experiments we use the BERTBASE model, which has been pre-trained by the Google research team. It has the following parameters: 12 layers, 768 hidden dimensions per token and 12 attention heads. It has 110 Mio. parameters in total. For finetuning the BERT language model on a specific domain we use the weights of BERTBASE as a starting point.",Methodology ::: BERT Language Model Finetuning,"As the first step of our procedure we perform language model finetuning of the BERT model using domain-specific corpora. Algorithmically, this is equivalent to pre-training. The domain-specific language model finetuning as an intermediate step to ATSC has been shown by BIBREF9. As an extension to their paper we investigate the limits of language model finetuning in terms of how end-task performance is dependent on the amount of training steps. The training input representation for language model finetuning consists of two sequences $s_A$ and $s_B$ in the format of $""\textrm {[CLS]} \ s_{A} \ \textrm {[SEP]} \ s_{B} \ \textrm {[SEP]}""$, where [CLS] is a dummy token used for downstream classification and [SEP] are separator tokens.",Methodology ::: BERT Language Model Finetuning ::: Masked Language Model Objective,"The sequences $A$ and $B$ have tokens randomly masked out in order for the model to learn to predict them. The following example shows why domain-specific finetuning can alleviate the bias from pre-training on a Wikipedia corpus: ""The touchscreen is an [MASK] device"". In the fact-based context of Wikipedia the [MASK] could be ""input"" and in the review domain a typical guess could be the general opinion word ""amazing"".",Methodology ::: BERT Language Model Finetuning ::: Next-Sentence Prediction,"In order to train BERT to capture long-term dependencies better, the model is trained to predict if sequence $B$ follows sequence $A$. If this is the case, sequence A and sequence B are jointly sampled from the same document in the order they are occuring naturally. Otherwise the sequences are sampled randomly from the training corpus.",Methodology ::: Aspect-Target Sentiment Classification,"The ATSC task aims at classifying sentiment polarity into the three classes positive, negative, neutral with respect to an aspect-target. The input to the classifier is a tokenized sentence $s=s_{1:n}$ and a target $t=s_{j:j+m}$ contained in the sentence, where $j < j+m \le n$. Similar to previous work by BIBREF10, we transform the input into a format compatible with BERT sequence-pair classification tasks: $""\textrm {[CLS]} \ s \ \textrm {[SEP]} \ t \ \textrm {[SEP]}""$. In the BERT architecture the position of the token embeddings is structurally maintained after each Multi-Head Attention layer. Therefore, we refer to the last hidden representation of the [CLS] token as $h_{[CLS]} \in \mathbf {R}^{768 \times 1}$. The number of sentiment polarity classes is three. A distribution $p \in [0,1]^3$ over these classes is predicted using a fully-connected layer with 3 output neurons on top of $h_{[CLS]}$, followed by a softmax activation function where $b \in \mathbf {R}^3$ and $W \in \mathbf {R}^{3 \times 768}$. Cross-entropy is used as the training loss. The way we use BERT for classifying the sentiment polaritites is equivalent to how BERT is used for sequence-pair classification tasks in the original paper BIBREF7.",Methodology ::: Domain Adaptation through Language Model Finetuning,"In academia, it is common that the performance of a machine learning model is evaluated in-domain. This means that the model is evaluated on a test set that comes from the same distribution as the training set. In real-world applications this setting is not always valid, as the trained model is used to predict previously unseen data. In order to evaluate the performance of a machine learning model more robustly, its generalization error can be evaluated across different domains, i.e. cross-domain. Additionally, the model itself can be adapted towards a target domain. This is known as Domain Adaptation, which is a special case of Transductive Transfer Learning in the taxonomy of BIBREF17. Here, it is typically assumed that supervised data for a specific task is only available for a source domain $S$, whereas only unsupervised data is available in the target domain $T$. The goal is to optimize performance of the task in the target domain while transferring task-specific knowledge from the source domain. If we map this framework to our challenge, we define Aspect-Target Sentiment Classification as the transfer-task and BERT language model finetuning is used for domain adaptation. In terms of on which domain is finetuned on, the full transfer-procedure can be expressed in the following way: Here, $D_{LM}$ stands for the domain on which the language model is finetuned and can take on the values of Restaurants, Laptops or (Restaurants $\cup $ Laptops). The domain for training $D_{Train}$ can take on the same values, for the joint case case the training datasets for laptops and restaurants are simply combined. The domain for testing $D_{Test}$ can only be take on the values Restaurants or Laptops. Combining finetuning and training steps gives us nine different evaluation scenarios, which we group into the following four categories:",Methodology ::: In-Domain Training,"ATSC is trained on a domain-specific dataset and evaluated on the test set from the same domain. This can be expressed as $D_{LM} \rightarrow T \rightarrow T,$ where $T$ is our target domain and can be either Laptops or Restaurants. It is expected that the performance of the model is best if $D_{LM} = T$.",Methodology ::: Cross-Domain Training,"ATSC is trained on a domain-specific dataset and evaluated on the test set from the other domain. This can be expressed as $D_{LM} \rightarrow S \rightarrow T,$ where $S\ne T$ are source and target domain and can be either Laptops or Restaurants.",Methodology ::: Cross-Domain Adaptation,As a special case of cross-domain Training we expect performance to be optimal if $D_{LM} = T$. This is the variant of Domain Adaptation and is written as $T \rightarrow S \rightarrow T.$,Methodology ::: Joint-Domain Training,"ATSC is trained on both domain-specific datasets jointly and evaluated on both test sets independently. This can be expressed as $D_{LM} \rightarrow (S \cup T) \rightarrow T,$ where $S\ne T$ are source- and target domain and can either be Laptops or Restaurants.",Experiments,"In our experiments we aim to answer the following research questions (RQs): RQ1: How does the number of training iterations in the BERT language model finetuning stage influence the ATSC end-task performance? At what point does performance start to improve, when does it converge? RQ2: When trained in-domain, what ATSC end-task performance can be reached through fully exploitet finetuning of the BERT language model? RQ3: When trained cross-domain in the special case of domain adaptation, what ATSC end-task performance can be reached if BERT language model finetuning is fully exploitet?",Experiments ::: Datasets for Classification and Language Model Finetuning,"We conduct experiments using the two SemEval 2014 Task 4 Subtask 2 datasets BIBREF1 for the laptops and the restaurants domain. The two datasets contain sentences with multiple marked aspect terms that each have a 3-level sentiment polarity (positive, neutral or negative) associated. In the original dataset the conflict label is also present. Here, conflicting labels are dropped for reasons of comparability with BIBREF9. Both datasets are small, detailed statistics are shown in tab:datasets. For BERT language model finetuning we prepare three corpora for the two domains of laptops and restaurants. For the restaurants domain we use Yelp Dataset Challenge reviews and for the laptops domain we use Amazon Laptop reviews BIBREF14. For the laptop domain we filtered out reviews that appear in the SemEval 2014 laptops dataset to avoid training bias for the test data. To be compatible with the next-sentence prediction task used during fine tuning, we removed reviews containing less than two sentences. For the laptop corpus, $1,007,209$ sentences are left after pre-processing. For the restaurants domain more reviews are available, we sampled $10,000,000$ sentences to have a sufficient amount of data for fully exploitet language model finetuning. In order to compensate for the smaller amount of finetuning data in the laptops domain, we finetune for more epochs, 30 epochs in the case of the laptops domain compared to 3 epochs for the restaurants domain, so that the BERT model trains on about 30 million sentences in both cases. This means that 1 sentence can be seen multiple times with a different language model masking. We also create a mixed corpus to jointly finetune both domains. Here, we sample 1 Mio. restaurant reviews and combine them with the laptop reviews. This results in about 2 Mio. reviews that are finetuned for 15 epochs. The exact statistics for the three finetuning corpora are shown in the top of tab:datasets. To be able to reproduce our finetuning corpora, we make the code that is used to generate them available online.",Experiments ::: Hyperparameters,"We use BERTBASE (uncased) as the base for all of our experiments, with the exception of XLNetBASE (cased), which is used as one of the baseline models. For the BERT language model finetuning we use 32 bit floating point computations using the Adam optimizer BIBREF18. The batchsize is set to 32 while the learning rate is set to $3\cdot 10^{-5}$. The maximum input sequence length is set to 256 tokens, which amounts to about 4 sentences per sequence on average. As shown in tab:datasets, we finetune the language models on each domain so that the model trains a total of about 30 Mio. sentences (7.5 Mio. sequences). For training the BERT and XLNet models on the down-stream task of ATSC we use mixed 16 bit and 32 bit floating point computations, the Adam optimizer, and a learning rate of $3\cdot 10^{-5}$ and a batchsize of 32. We train the model for a total of 7 epochs. The validation accuracy converges after about 3 epochs of training on all datasets, but training loss still improves after that. It is important to note that all our results reported are the average of 9 runs with different random initializations. This is needed to measure significance of improvements, as the standard deviation in accuray amounts to roughly $1\%$ for all experiments, see fig:acc-dep-lmiterations.",Experiments ::: Compared Methods,"We compare in-domain results to current state of the art methods, which we will now describe briefly. SDGCN-BERT BIBREF11 explicitly models sentiment dependencies for sentences with multiple aspects with a graph convolutional network. This method is current state-of-the-art on the SemEval 2014 laptops dataset. AEN-BERT BIBREF8 is an attentional encoder network. When used on top of BERT embeddings this method performs especially well on the laptops dataset. BERT-SPC BIBREF8 is BERT used in sentence-pair classification mode. This is exactly the same method as our BERT-base baseline and therefore, we can cross-check the authors results. BERT-PT BIBREF9 uses multi-task fine-tuning prior to downstream classification, where the BERT language model is finetuned jointly with a question answering task. It performs state-of-the-art on the restaurants dataset prior to this paper. To our knowledge, cross- and joint-domain training on the SemEval 2014 Task 4 datasets has not been analyzed so far. Thus, we compare our method to two very strong baselines: BERT and XLNet. BERT-base BIBREF7 is using the pre-trained BERTBASE embeddings directly on the down-stream task without any domain specific language model finetuning. XLNet-base BIBREF19 is a method also based on general language model pre-training similar to BERT. Instead of randomly masking tokens for pre-training like in BERT a more general permutation objective is used, where all possible variants of masking are fully exploitet. Our models are BERT models whose language model has been finetuned on different domain corpora. BERT-ADA Lapt is the BERT language model finetuned on the laptops domain corpus. BERT-ADA Rest is the BERT language model finetuned on the restaurant domain corpus. BERT-ADA Joint is the BERT language model finetuned on the corpus containing an equal amount of laptops and restaurants reviews.",Experiments ::: Results Analysis,"The results of our experiments are shown in fig:acc-dep-lmiterations and tab:results respectively. To answer RQ1, which is concerned with details on domain-specific language model finetuning, we can see in fig:acc-dep-lmiterations that first of all, language model finetuning has a substantial effect on ATSC end-task performance. Secondly, we see that in the laptops domain the performance starts to increase at about 10 Mio. finetuned sentences. This is an interesting insight as one would expect a relation closer to a logarithmic curve. One reason might be that it takes many steps to train knowledge into the BERT language model due to its vast amount of parameters. The model already converges at around 17 Mio. sentences. More finetuning does not improve performance significantly. In addition, we find that different runs have a high variance, the standard deviation amounts to about $1\%$ in accuracy, which justifies averaging over 9 runs to measure differences in model performance reliably. To answer RQ2, which is concerned with in-domain ATSC performance, we see in tab:results that for the in-domain training case, our models BERT-ADA Lapt and BERT-ADA Rest achieve performance close to state-of-the-art on the laptops dataset and new state-of-the-art on the restaurants dataset with accuracies of $79.19\%$ and $87.14\%$, respectively. On the restaurants dataset, this corresponds to an absolute improvement of $2.2\%$ compared to the previous state-of-the-art method BERT-PT. Language model finetuning produces a larger improvement on the restaurants dataset. We think that one reason for that might be that the restaurants domain is underrepresented in the pre-training corpora of BERTBASE. Generally, we find that language model finetuning helps even if the finetuning domain does not match the evaluation domain. We think the reason for this might be that the BERT-base model is pre-trained more on knowledge-based corpora like Wikipedia than on text containing opinions. Another finding is that BERT-ADA Joint performs better on the laptops dataset than BERT-ADA Rest, although the unique amount of laptop reviews are the same in laptops- and joint-corpora. We think that confusion can be created when mixing the domains, but this needs to be investigated further. We also find that the XLNet-base baseline performs generally stronger than BERT-base and even outperforms BERT-ADA Lapt with an accuracy of $79.89\%$ on the laptops dataset. To answer RQ3, which is concerned with domain adaptation, we can see in the grayed out cells in tab:results, which correspond to the cross-domain adaption case where the BERT language model is trained on the target domain, that domain adaptation works well with $2.2\%$ absolute accuracy improvement on the laptops test set and even $3.6\%$ accuracy improvement on the restaurants test set compared to BERT-base. In general, the ATSC task generalizes well cross-domain, with about 2-$3\%$ drop in accuracy compared to in-domain training. We think the reason for this might be that syntactical relationships between the aspect-target and the phrase expressing sentiment polarity as well as knowing the sentiment-polarity itself are sufficient to solve the ATSC task in many cases. For the joint-training case, we find that combining both training datasets improves performance on both test sets. This result is intuitive, as more training data leads to better performance if the domains do not confuse each other. Interesting for the joint-training case is that the BERT-ADA Joint model performs especially strong when measured by the Macro-F1 metric. A reason for this might be that the SemEval 2014 datasets are imbalanced due to dominance of positive label. It seems like through finetuning the language model on both domains the model learns to classify the neutral class much better, especially in the laptops domain.",Conclusion,"We performed experiments on the task of Aspect-Target Sentiment Classification by first finetuning a pre-trained BERT model on a domain specific corpus with subsequent training on the down-stream classification task. We analyzed the behavior of the number of domain-specific BERT language model finetuning steps in relation to the end-task performance. With the findings on how to best exploit BERT language model finetuning we were able to train high performing models, which one of even performs as new state-of-the-art on SemEval 2014 Task 4 restaurants dataset. We further evaluated our models cross-domain to explore the robustness of Aspect-Target Sentiment Classification. We found that in general, this task transfers well between the laptops and the restaurants domain. As a special case we ran a cross-domain adaptation experiments, where the BERT language model is specifically finetuned on the target domain. We achieve significant improvement over unadapted models, a cross-domain adapted model performs even better than a BERT-base model that is trained in-domain. Overall, our findings reveal promising directions for follow-up work. The XLNet-base model performs strongly on the ATSC task. Here, domain-specific finetuning could probably bring significant performance improvements. Another interesting direction for future work would be to investigate cross-domain behavior for an additional domain like hotels, which is more similar to the restaurants domain. Here, it could be interesting to find out if the shared nature of these domain would results in more confusion or if they would behave synergetically.",By how much does their model outperform the baseline in the cross-domain evaluation?,11c77ee117cb4de825016b6ccff59ff021f84a38,,,,sentiment,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,,"To answer RQ3, which is concerned with domain adaptation, we can see in the grayed out cells in tab:results, which correspond to the cross-domain adaption case where the BERT language model is trained on the target domain, that domain adaptation works well with $2.2\%$ absolute accuracy improvement on the laptops test set and even $3.6\%$ accuracy improvement on the restaurants test set compared to BERT-base.","To answer RQ3, which is concerned with domain adaptation, we can see in the grayed out cells in tab:results, which correspond to the cross-domain adaption case where the BERT language model is trained on the target domain, that domain adaptation works well with $2.2\%$ absolute accuracy improvement on the laptops test set and even $3.6\%$ accuracy improvement on the restaurants test set compared to BERT-base.",346f44deaa4c79f35b89d430d7a3605b11d2e569,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,What are the performance results?,0b92fb692feb35d4b4bf4665f7754d283d6ad5f3,,,,sentiment,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,,"To answer RQ2, which is concerned with in-domain ATSC performance, we see in tab:results that for the in-domain training case, our models BERT-ADA Lapt and BERT-ADA Rest achieve performance close to state-of-the-art on the laptops dataset and new state-of-the-art on the restaurants dataset with accuracies of $79.19\%$ and $87.14\%$, respectively. On the restaurants dataset, this corresponds to an absolute improvement of $2.2\%$ compared to the previous state-of-the-art method BERT-PT. Language model finetuning produces a larger improvement on the restaurants dataset. We think that one reason for that might be that the restaurants domain is underrepresented in the pre-training corpora of BERTBASE. Generally, we find that language model finetuning helps even if the finetuning domain does not match the evaluation domain. We think the reason for this might be that the BERT-base model is pre-trained more on knowledge-based corpora like Wikipedia than on text containing opinions. Another finding is that BERT-ADA Joint performs better on the laptops dataset than BERT-ADA Rest, although the unique amount of laptop reviews are the same in laptops- and joint-corpora. We think that confusion can be created when mixing the domains, but this needs to be investigated further. We also find that the XLNet-base baseline performs generally stronger than BERT-base and even outperforms BERT-ADA Lapt with an accuracy of $79.89\%$ on the laptops dataset. In general, the ATSC task generalizes well cross-domain, with about 2-$3\%$ drop in accuracy compared to in-domain training. We think the reason for this might be that syntactical relationships between the aspect-target and the phrase expressing sentiment polarity as well as knowing the sentiment-polarity itself are sufficient to solve the ATSC task in many cases.","To answer RQ2, which is concerned with in-domain ATSC performance, we see in tab:results that for the in-domain training case, our models BERT-ADA Lapt and BERT-ADA Rest achieve performance close to state-of-the-art on the laptops dataset and new state-of-the-art on the restaurants dataset with accuracies of $79.19\%$ and $87.14\%$, respectively. In general, the ATSC task generalizes well cross-domain, with about 2-$3\%$ drop in accuracy compared to in-domain training.",46773036407495e12fc0be2224060db32f6d6676,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5-Table1-1.png,Table 1: Top: Detailed statistics of the corpora for BERT language model finetuning. Bottom: Number of labels for each category of the SemEval 2014 Task 4 Subtask 2 laptop and restaurant datasets for AspectTarget Sentiment Classification.,6-Figure1-1.png,"Figure 1: Accuracy of Aspect-Target Sentiment Classification as a function of the number of sentences the BERT language model has been finetuned on. Marked dots (•) connected through the line are the averages (µ) over 9 runs, a single run is marked as a cross (×). The standard deviation (σ) curves are also drawn (µ ± σ). The model is trained on the SemEval 2014 Task 4 laptops dataset. The language model is finetuned on our laptops domain corpus.",7-Table2-1.png,"Table 2: Summary of results for Aspect-Target Sentiment Classification for in-domain, cross-domain, and jointdomain training on SemEval 2014 Task 4 Subtask 2 datasets. The cells with gray background correspond to the cross-domain adaptation case, where the language model is finetuned on the target domain. As evaluation metrics accuracy (Acc) and Macro-F1 (MF1) are used.",,,,,,,,,,,,,,,"results that for the in-domain training case, our models BERT-ADA Lapt and BERT-ADA Rest achieve performance close to state-of-the-art on the laptops dataset new state-of-the-art on the restaurants dataset with accuracies of $79.19\%$ and $87.14\%$, respectively.",,,,,,,,,,,,,$2.2\%$ absolute accuracy improvement on the laptops test set $3.6\%$ accuracy improvement on the restaurants test set,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Massive vs. Curated Word Embeddings for Low-Resourced Languages. The Case of Yor\`ub\'a and Twi,"The success of several architectures to learn semantic representations from unannotated text and the availability of these kind of texts in online multilingual resources such as Wikipedia has facilitated the massive and automatic creation of resources for multiple languages. The evaluation of such resources is usually done for the high-resourced languages, where one has a smorgasbord of tasks and test sets to evaluate on. For low-resourced languages, the evaluation is more difficult and normally ignored, with the hope that the impressive capability of deep learning architectures to learn (multilingual) representations in the high-resourced setting holds in the low-resourced setting too. In this paper we focus on two African languages, Yor\`ub\'a and Twi, and compare the word embeddings obtained in this way, with word embeddings obtained from curated corpora and a language-dependent processing. We analyse the noise in the publicly available corpora, collect high quality and noisy data for the two languages and quantify the improvements that depend not only on the amount of data but on the quality too. We also use different architectures that learn word representations both from surface forms and characters to further exploit all the available information which showed to be important for these languages. For the evaluation, we manually translate the wordsim-353 word pairs dataset from English into Yor\`ub\'a and Twi. As output of the work, we provide corpora, embeddings and the test suits for both languages.",Introduction,"In recent years, word embeddings BIBREF0, BIBREF1, BIBREF2 have been proven to be very useful for training downstream natural language processing (NLP) tasks. Moreover, contextualized embeddings BIBREF3, BIBREF4 have been shown to further improve the performance of NLP tasks such as named entity recognition, question answering, or text classification when used as word features because they are able to resolve ambiguities of word representations when they appear in different contexts. Different deep learning architectures such as multilingual BERT BIBREF4, LASER BIBREF5 and XLM BIBREF6 have proved successful in the multilingual setting. All these architectures learn the semantic representations from unannotated text, making them cheap given the availability of texts in online multilingual resources such as Wikipedia. However, the evaluation of such resources is usually done for the high-resourced languages, where one has a smorgasbord of tasks and test sets to evaluate on. This is the best-case scenario, languages with tones of data for training that generate high-quality models. For low-resourced languages, the evaluation is more difficult and therefore normally ignored simply because of the lack of resources. In these cases, training data is scarce, and the assumption that the capability of deep learning architectures to learn (multilingual) representations in the high-resourced setting holds in the low-resourced one does not need to be true. In this work, we focus on two African languages, Yorùbá and Twi, and carry out several experiments to verify this claim. Just by a simple inspection of the word embeddings trained on Wikipedia by fastText, we see a high number of non-Yorùbá or non-Twi words in the vocabularies. For Twi, the vocabulary has only 935 words, and for Yorùbá we estimate that 135 k out of the 150 k words belong to other languages such as English, French and Arabic. In order to improve the semantic representations for these languages, we collect online texts and study the influence of the quality and quantity of the data in the final models. We also examine the most appropriate architecture depending on the characteristics of each language. Finally, we translate test sets and annotate corpora to evaluate the performance of both our models together with fastText and BERT pre-trained embeddings which could not be evaluated otherwise for Yorùbá and Twi. The evaluation is carried out in a word similarity and relatedness task using the wordsim-353 test set, and in a named entity recognition (NER) task where embeddings play a crucial role. Of course, the evaluation of the models in only two tasks is not exhaustive but it is an indication of the quality we can obtain for these two low-resourced languages as compared to others such as English where these evaluations are already available. The rest of the paper is organized as follows. Related works are reviewed in Section SECREF2 The two languages under study are described in Section SECREF3. We introduce the corpora and test sets in Section SECREF4. The fifth section explores the different training architectures we consider, and the experiments that are carried out. Finally, discussion and concluding remarks are given in Section SECREF6",Related Work,"The large amount of freely available text in the internet for multiple languages is facilitating the massive and automatic creation of multilingual resources. The resource par excellence is Wikipedia, an online encyclopedia currently available in 307 languages. Other initiatives such as Common Crawl or the Jehovah’s Witnesses site are also repositories for multilingual data, usually assumed to be noisier than Wikipedia. Word and contextual embeddings have been pre-trained on these data, so that the resources are nowadays at hand for more than 100 languages. Some examples include fastText word embeddings BIBREF2, BIBREF7, MUSE embeddings BIBREF8, BERT multilingual embeddings BIBREF4 and LASER sentence embeddings BIBREF5. In all cases, embeddings are trained either simultaneously for multiple languages, joining high- and low-resource data, or following the same methodology. On the other hand, different approaches try to specifically design architectures to learn embeddings in a low-resourced setting. ChaudharyEtAl:2018 follow a transfer learning approach that uses phonemes, lemmas and morphological tags to transfer the knowledge from related high-resource language into the low-resource one. jiangEtal:2018 apply Positive-Unlabeled Learning for word embedding calculations, assuming that unobserved pairs of words in a corpus also convey information, and this is specially important for small corpora. In order to assess the quality of word embeddings, word similarity and relatedness tasks are usually used. wordsim-353 BIBREF9 is a collection of 353 pairs annotated with semantic similarity scores in a scale from 0 to 10. Even the problems detected in this dataset BIBREF10, it is widely used by the community. The test set was originally created for English, but the need for comparison with other languages has motivated several translations/adaptations. In hassanMihalcea:2009 the test was translated manually into Spanish, Romanian and Arabic and the scores were adapted to reflect similarities in the new language. The reported correlation between the English scores and the Spanish ones is 0.86. Later, JoubarneInkpen:2011 show indications that the measures of similarity highly correlate across languages. leviantReichart:2015 translated also wordsim-353 into German, Italian and Russian and used crowdsourcing to score the pairs. Finally, jiangEtal:2018 translated with Google Cloud the test set from English into Czech, Danish and Dutch. In our work, native speakers translate wordsim-353 into Yorùbá and Twi, and similarity scores are kept unless the discrepancy with English is big (see Section SECREF11 for details). A similar approach to our work is done for Gujarati in JoshiEtAl:2019.",Languages under Study ::: Yorùbá,"is a language in the West Africa with over 50 million speakers. It is spoken among other languages in Nigeria, republic of Togo, Benin Republic, Ghana and Sierra Leon. It is also a language of Òrìsà in Cuba, Brazil, and some Caribbean countries. It is one of the three major languages in Nigeria and it is regarded as the third most spoken native African language. There are different dialects of Yorùbá in Nigeria BIBREF11, BIBREF12, BIBREF13. However, in this paper our focus is the standard Yorùbá based upon a report from the 1974 Joint Consultative Committee on Education BIBREF14. Standard Yorùbá has 25 letters without the Latin characters c, q, v, x and z. There are 18 consonants (b, d, f, g, gb, j[dz], k, l, m, n, p[kp], r, s, ṣ, t, w y[j]), 7 oral vowels (a, e, ẹ, i, o, ọ, u), five nasal vowels, (an, $ \underaccent{\dot{}}{e}$n, in, $ \underaccent{\dot{}}{o}$n, un) and syllabic nasals (m̀, ḿ, ǹ, ń). Yorùbá is a tone language which makes heavy use of lexical tones which are indicated by the use of diacritics. There are three tones in Yorùbá namely low, mid and high which are represented as grave ($\setminus $), macron ($-$) and acute ($/$) symbols respectively. These tones are applied on vowels and syllabic nasals. Mid tone is usually left unmarked on vowels and every initial or first vowel in a word cannot have a high tone. It is important to note that tone information is needed for correct pronunciation and to have the meaning of a word BIBREF15, BIBREF12, BIBREF14. For example, owó (money), ọw (broom), òwò (business), w (honour), ọw (hand), and w (group) are different words with different dots and diacritic combinations. According to Asahiah2014, Standard Yorùbá uses 4 diacritics, 3 are for marking tones while the fourth which is the dot below is used to indicate the open phonetic variants of letter ""e"" and ""o"" and the long variant of ""s"". Also, there are 19 single diacritic letters, 3 are marked with dots below (ẹ, ọ, ṣ) while the rest are either having the grave or acute accent. The four double diacritics are divided between the grave and the acute accent as well. As noted in Asahiah2014, most of the Yorùbá texts found in websites or public domain repositories (i) either use the correct Yorùbá orthography or (ii) replace diacritized characters with un-diacritized ones. This happens as a result of many factors, but most especially to the unavailability of appropriate input devices for the accurate application of the diacritical marks BIBREF11. This has led to research on restoration models for diacritics BIBREF16, but the problem is not well solved and we find that most Yorùbá text in the public domain today is not well diacritized. Wikipedia is not an exception.",Languages under Study ::: Twi,"is an Akan language of the Central Tano Branch of the Niger Congo family of languages. It is the most widely spoken of the about 80 indigenous languages in Ghana BIBREF17. It has about 9 million native speakers and about a total of 17–18 million Ghanaians have it as either first or second language. There are two mutually intelligible dialects, Asante and Akuapem, and sub-dialectical variants which are mostly unknown to and unnoticed by non-native speakers. It is also mutually intelligible with Fante and to a large extent Bono, another of the Akan languages. It is one of, if not the, easiest to learn to speak of the indigenous Ghanaian languages. The same is however not true when it comes to reading and especially writing. This is due to a number of easily overlooked complexities in the structure of the language. First of all, similarly to Yorùbá, Twi is a tonal language but written without diacritics or accents. As a result, words which are pronounced differently and unambiguous in speech tend to be ambiguous in writing. Besides, most of such words fit interchangeably in the same context and some of them can have more than two meanings. A simple example is: Me papa aba nti na me ne wo redi no yie no. S wo ara wo nim s me papa ba a, me suban fofor adi. This sentence could be translated as (i) I'm only treating you nicely because I'm in a good mood. You already know I'm a completely different person when I'm in a good mood. (ii) I'm only treating you nicely because my dad is around. You already know I'm a completely different person when my dad comes around. Another characteristic of Twi is the fact that a good number of stop words have the same written form as content words. For instance, “na” or “na” could be the words “and, then”, the phrase “and then” or the word “mother”. This kind of ambiguity has consequences in several natural language applications where stop words are removed from text. Finally, we want to point out that words can also be written with or without prefixes. An example is this same na and na which happen to be the same word with an omissible prefix across its multiple senses. For some words, the prefix characters are mostly used when the word begins a sentence and omitted in the middle. This however depends on the author/speaker. For the word embeddings calculation, this implies that one would have different embeddings for the same word found in different contexts.",Data,"We collect clean and noisy corpora for Yorùbá and Twi in order to quantify the effect of noise on the quality of the embeddings, where noisy has a different meaning depending on the language as it will be explained in the next subsections.",Data ::: Training Corpora,"For Yorùbá, we use several corpora collected by the Niger-Volta Language Technologies Institute with texts from different sources, including the Lagos-NWU conversational speech corpus, fully-diacritized Yorùbá language websites and an online Bible. The largest source with clean data is the JW300 corpus. We also created our own small-sized corpus by web-crawling three Yorùbá language websites (Alàkwé, r Yorùbá and Èdè Yorùbá Rẹw in Table TABREF7), some Yoruba Tweets with full diacritics and also news corpora (BBC Yorùbá and VON Yorùbá) with poor diacritics which we use to introduce noise. By noisy corpus, we refer to texts with incorrect diacritics (e.g in BBC Yorùbá), removal of tonal symbols (e.g in VON Yorùbá) and removal of all diacritics/under-dots (e.g some articles in Yorùbá Wikipedia). Furthermore, we got two manually typed fully-diacritized Yorùbá literature (Ìrìnkèrindò nínú igbó elégbèje and Igbó Olódùmarè) both written by Daniel Orowole Olorunfemi Fagunwa a popular Yorùbá author. The number of tokens available from each source, the link to the original source and the quality of the data is summarised in Table TABREF7. The gathering of clean data in Twi is more difficult. We use as the base text as it has been shown that the Bible is the most available resource for low and endangered languages BIBREF18. This is the cleanest of all the text we could obtain. In addition, we use the available (and small) Wikipedia dumps which are quite noisy, i.e. Wikipedia contains a good number of English words, spelling errors and Twi sentences formulated in a non-natural way (formulated as L2 speakers would speak Twi as compared to native speakers). Lastly, we added text crawled from jw and the JW300 Twi corpus. Notice that the Bible text, is mainly written in the Asante dialect whilst the last, Jehovah's Witnesses, was written mainly in the Akuapem dialect. The Wikipedia text is a mixture of the two dialects. This introduces a lot of noise into the embeddings as the spelling of most words differs especially at the end of the words due to the mixture of dialects. The JW300 Twi corpus also contains mixed dialects but is mainly Akuampem. In this case, the noise comes also from spelling errors and the uncommon addition of diacritics which are not standardised on certain vowels. Figures for Twi corpora are summarised in the bottom block of Table TABREF7.",Data ::: Evaluation Test Sets ::: Yorùbá.,"One of the contribution of this work is the introduction of the wordsim-353 word pairs dataset for Yorùbá. All the 353 word pairs were translated from English to Yorùbá by 3 native speakers. The set is composed of 446 unique English words, 348 of which can be expressed as one-word translation in Yorùbá (e.g. book translates to ìwé). In 61 cases (most countries and locations but also other content words) translations are transliterations (e.g. Doctor is dókítà and cucumber kùkúmbà.). 98 words were translated by short phrases instead of single words. This mostly affects words from science and technology (e.g. keyboard translates to pátákó ìtwé —literally meaning typing board—, laboratory translates to ìyàrá ìṣèwádìí —research room—, and ecology translates to ìm nípa àyíká while psychology translates to ìm nípa dá). Finally, 6 terms have the same form in English and Yorùbá therefore they are retained like that in the dataset (e.g. Jazz, Rock and acronyms such as FBI or OPEC). We also annotate the Global Voices Yorùbá corpus to test the performance of our trained Yorùbá BERT embeddings on the named entity recognition task. The corpus consists of 25 k tokens which we annotate with four named entity types: DATE, location (LOC), organization (ORG) and personal names (PER). Any other token that does not belong to the four named entities is tagged with ""O"". The dataset is further split into training (70%), development (10%) and test (20%) partitions. Table TABREF12 shows the number of named entities per type and partition.",Data ::: Evaluation Test Sets ::: Twi,"Just like Yorùbá, the wordsim-353 word pairs dataset was translated for Twi. Out of the 353 word pairs, 274 were used in this case. The remaining 79 pairs contain words that translate into longer phrases. The number of words that can be translated by a single token is higher than for Yorùbá. Within the 274 pairs, there are 351 unique English words which translated to 310 unique Twi words. 298 of the 310 Twi words are single word translations, 4 transliterations and 16 are used as is. Even if JoubarneInkpen:2011 showed indications that semantic similarity has a high correlation across languages, different nuances between words are captured differently by languages. For instance, both money and currency in English translate into sika in Twi (and other 32 English words which translate to 14 Twi words belong to this category) and drink in English is translated as Nsa or nom depending on the part of speech (noun for the former, verb for the latter). 17 English words fall into this category. In translating these, we picked the translation that best suits the context (other word in the pair). In two cases, the correlation is not fulfilled at all: soap–opera and star–movies are not related in the Twi language and the score has been modified accordingly.",Semantic Representations,"In this section, we describe the architectures used for learning word embeddings for the Twi and Yorùbá languages. Also, we discuss the quality of the embeddings as measured by the correlation with human judgements on the translated wordSim-353 test sets and by the F1 score in a NER task.",Semantic Representations ::: Word Embeddings Architectures,"Modeling sub-word units has recently become a popular way to address out-of-vocabulary word problem in NLP especially in word representation learning BIBREF19, BIBREF2, BIBREF4. A sub-word unit can be a character, character $n$-grams, or heuristically learned Byte Pair Encodings (BPE) which work very well in practice especially for morphologically rich languages. Here, we consider two word embedding models that make use of character-level information together with word information: Character Word Embedding (CWE) BIBREF20 and fastText BIBREF2. Both of them are extensions of the Word2Vec architectures BIBREF0 that model sub-word units, character embeddings in the case of CWE and character $n$-grams for fastText. CWE was introduced in 2015 to model the embeddings of characters jointly with words in order to address the issues of character ambiguities and non-compositional words especially in the Chinese language. A word or character embedding is learned in CWE using either CBOW or skipgram architectures, and then the final word embedding is computed by adding the character embeddings to the word itself: where $w_j$ is the word embedding of $x_j$, $N_j$ is the number of characters in $x_j$, and $c_k$ is the embedding of the $k$-th character $c_k$ in $x_j$. Similarly, in 2017 fastText was introduced as an extension to skipgram in order to take into account morphology and improve the representation of rare words. In this case the embedding of a word also includes the embeddings of its character $n$-grams: where $w_j$ is the word embedding of $x_j$, $G_j$ is the number of character $n$-grams in $x_j$ and $g_k$ is the embedding of the $k$-th $n$-gram. cwe also proposed three alternatives to learn multiple embeddings per character and resolve ambiguities: (i) position-based character embeddings where each character has different embeddings depending on the position it appears in a word, i.e., beginning, middle or end (ii) cluster-based character embeddings where a character can have $K$ different cluster embeddings, and (iii) position-based cluster embeddings (CWE-LP) where for each position $K$ different embeddings are learned. We use the latter in our experiments with CWE but no positional embeddings are used with fastText. Finally, we consider a contextualized embedding architecture, BERT BIBREF4. BERT is a masked language model based on the highly efficient and parallelizable Transformer architecture BIBREF21 known to produce very rich contextualized representations for downstream NLP tasks. The architecture is trained by jointly conditioning on both left and right contexts in all the transformer layers using two unsupervised objectives: Masked LM and Next-sentence prediction. The representation of a word is therefore learned according to the context it is found in. Training contextual embeddings needs of huge amounts of corpora which are not available for low-resourced languages such as Yorùbá and Twi. However, Google provided pre-trained multilingual embeddings for 102 languages including Yorùbá (but not Twi).",Semantic Representations ::: Experiments ::: FastText Training and Evaluation,"As a first experiment, we compare the quality of fastText embeddings trained on (high-quality) curated data and (low-quality) massively extracted data for Twi and Yorùbá languages. Facebook released pre-trained word embeddings using fastText for 294 languages trained on Wikipedia BIBREF2 (F1 in tables) and for 157 languages trained on Wikipedia and Common Crawl BIBREF7 (F2). For Yorùbá, both versions are available but only embeddings trained on Wikipedia are available for Twi. We consider these embeddings the result of training on what we call massively-extracted corpora. Notice that training settings for both embeddings are not exactly the same, and differences in performance might come both from corpus size/quality but also from the background model. The 294-languages version is trained using skipgram, in dimension 300, with character $n$-grams of length 5, a window of size 5 and 5 negatives. The 157-languages version is trained using CBOW with position-weights, in dimension 300, with character $n$-grams of length 5, a window of size 5 and 10 negatives. We want to compare the performance of these embeddings with the equivalent models that can be obtained by training on the different sources verified by native speakers of Twi and Yorùbá; what we call curated corpora and has been described in Section SECREF4 For the comparison, we define 3 datasets according to the quality and quantity of textual data used for training: (i) Curated Small Dataset (clean), C1, about 1.6 million tokens for Yorùbá and over 735 k tokens for Twi. The clean text for Twi is the Bible and for Yoruba all texts marked under the C1 column in Table TABREF7. (ii) In Curated Small Dataset (clean + noisy), C2, we add noise to the clean corpus (Wikipedia articles for Twi, and BBC Yorùbá news articles for Yorùbá). This increases the number of training tokens for Twi to 742 k tokens and Yorùbá to about 2 million tokens. (iii) Curated Large Dataset, C3 consists of all available texts we are able to crawl and source out for, either clean or noisy. The addition of JW300 BIBREF22 texts increases the vocabulary to more than 10 k tokens in both languages. We train our fastText systems using a skipgram model with an embedding size of 300 dimensions, context window size of 5, 10 negatives and $n$-grams ranging from 3 to 6 characters similarly to the pre-trained models for both languages. Best results are obtained with minimum word count of 3. Table TABREF15 shows the Spearman correlation between human judgements and cosine similarity scores on the wordSim-353 test set. Notice that pre-trained embeddings on Wikipedia show a very low correlation with humans on the similarity task for both languages ($\rho $=$0.14$) and their performance is even lower when Common Crawl is also considered ($\rho $=$0.07$ for Yorùbá). An important reason for the low performance is the limited vocabulary. The pre-trained Twi model has only 935 tokens. For Yorùbá, things are apparently better with more than 150 k tokens when both Wikipedia and Common Crawl are used but correlation is even lower. An inspection of the pre-trained embeddings indicates that over 135 k words belong to other languages mostly English, French and Arabic. If we focus only on Wikipedia, we see that many texts are without diacritics in Yorùbá and often make use of mixed dialects and English sentences in Twi. The Spearman $\rho $ correlation for fastText models on the curated small dataset (clean), C1, improves the baselines by a large margin ($\rho =0.354$ for Twi and 0.322 for Yorùbá) even with a small dataset. The improvement could be justified just by the larger vocabulary in Twi, but in the case of Yorùbá the enhancement is there with almost half of the vocabulary size. We found out that adding some noisy texts (C2 dataset) slightly improves the correlation for Twi language but not for the Yorùbá language. The Twi language benefits from Wikipedia articles because its inclusion doubles the vocabulary and reduces the bias of the model towards religious texts. However, for Yorùbá, noisy texts often ignore diacritics or tonal marks which increases the vocabulary size at the cost of an increment in the ambiguity too. As a result, the correlation is slightly hurt. One would expect that training with more data would improve the quality of the embeddings, but we found out with the results obtained with the C3 dataset, that only high-quality data helps. The addition of JW300 boosts the vocabulary in both cases, but whereas for Twi the corpus mixes dialects and is noisy, for Yorùbá it is very clean and with full diacritics. Consequently, the best embeddings for Yorùbá are obtained when training with the C3 dataset, whereas for Twi, C2 is the best option. In both cases, the curated embeddings improve the correlation with human judgements on the similarity task a $\Delta \rho =+0.25$ or, equivalently, by an increment on $\rho $ of 170% (Twi) and 180% (Yorùbá).",Semantic Representations ::: Experiments ::: CWE Training and Evaluation,"The huge ambiguity in the written Twi language motivates the exploration of different approaches to word embedding estimations. In this work, we compare the standard fastText methodology to include sub-word information with the character-enhanced approach with position-based clustered embeddings (CWE-LP as introduced in Section SECREF17). With the latter, we expect to specifically address the ambiguity present in a language that does not translate the different oral tones on vowels into the written language. The character-enhanced word embeddings are trained using a skipgram architecture with cluster-based embeddings and an embedding size of 300 dimensions, context window-size of 5, and 5 negative samples. In this case, the best performance is obtained with a minimum word count of 1, and that increases the effective vocabulary that is used for training the embeddings with respect to the fastText experiments reported in Table TABREF15. We repeat the same experiments as with fastText and summarise them in Table TABREF16. If we compare the relative numbers for the three datasets (C1, C2 and C3) we observe the same trends as before: the performance of the embeddings in the similarity task improves with the vocabulary size when the training data can be considered clean, but the performance diminishes when the data is noisy. According to the results, CWE is specially beneficial for Twi but not always for Yorùbá. Clean Yorùbá text, does not have the ambiguity issues at character-level, therefore the $n$-gram approximation works better when enough clean data is used ($\rho ^{C3}_{CWE}=0.354$ vs. $\rho ^{C3}_{fastText}=0.391$) but it does not when too much noisy data (no diacritics, therefore character-level information would be needed) is used ($\rho ^{C2}_{CWE}=0.345$ vs. $\rho ^{C2}_{fastText}=0.302$). For Twi, the character-level information reinforces the benefits of clean data and the best correlation with human judgements is reached with CWE embeddings ($\rho ^{C2}_{CWE}=0.437$ vs. $\rho ^{C2}_{fastText}=0.388$).",Semantic Representations ::: Experiments ::: BERT Evaluation on NER Task,"In order to go beyond the similarity task using static word vectors, we also investigate the quality of the multilingual BERT embeddings by fine-tuning a named entity recognition task on the Yorùbá Global Voices corpus. One of the major advantages of pre-trained BERT embeddings is that fine-tuning of the model on downstream NLP tasks is typically computationally inexpensive, often with few number of epochs. However, the data the embeddings are trained on has the same limitations as that used in massive word embeddings. Fine-tuning involves replacing the last layer of BERT used optimizing the masked LM with a task-dependent linear classifier or any other deep learning architecture, and training all the model parameters end-to-end. For the NER task, we obtain the token-level representation from BERT and train a linear classifier for sequence tagging. Similar to our observations with non-contextualized embeddings, we find out that fine-tuning the pre-trained multilingual-uncased BERT for 4 epochs on the NER task gives an F1 score of 0. If we do the same experiment in English, F1 is 58.1 after 4 epochs. That shows how pre-trained embeddings by themselves do not perform well in downstream tasks on low-resource languages. To address this problem for Yorùbá, we fine-tune BERT representations on the Yorùbá corpus in two ways: (i) using the multilingual vocabulary, and (ii) using only Yorùbá vocabulary. In both cases diacritics are ignored to be consistent with the base model training. As expected, the fine-tuning of the pre-trained BERT on the Yorùbá corpus in the two configurations generates better representations than the base model. These models are able to achieve a better performance on the NER task with an average F1 score of over 47% (see Table TABREF26 for the comparative). The fine-tuned BERT model with only Yorùbá vocabulary further increases by more than 4% in F1 score obtained with the tuning that uses the multilingual vocabulary. Although we do not have enough data to train BERT from scratch, we observe that fine-tuning BERT on a limited amount of monolingual data of a low-resource language helps to improve the quality of the embeddings. The same observation holds true for high-resource languages like German and French BIBREF23.",Summary and Discussion,"In this paper, we present curated word and contextual embeddings for Yorùbá and Twi. For this purpose, we gather and select corpora and study the most appropriate techniques for the languages. We also create test sets for the evaluation of the word embeddings within a word similarity task (wordsim353) and the contextual embeddings within a NER task. Corpora, embeddings and test sets are available in github. In our analysis, we show how massively generated embeddings perform poorly for low-resourced languages as compared to the performance for high-resourced ones. This is due both to the quantity but also the quality of the data used. While the Pearson $\rho $ correlation for English obtained with fastText embeddings trained on Wikipedia (WP) and Common Crawl (CC) are $\rho _{WP}$=$0.67$ and $\rho _{WP+CC}$=$0.78$, the equivalent ones for Yorùbá are $\rho _{WP}$=$0.14$ and $\rho _{WP+CC}$=$0.07$. For Twi, only embeddings with Wikipedia are available ($\rho _{WP}$=$0.14$). By carefully gathering high-quality data and optimising the models to the characteristics of each language, we deliver embeddings with correlations of $\rho $=$0.39$ (Yorùbá) and $\rho $=$0.44$ (Twi) on the same test set, still far from the high-resourced models, but representing an improvement over $170\%$ on the task. In a low-resourced setting, the data quality, processing and model selection is more critical than in a high-resourced scenario. We show how the characteristics of a language (such as diacritization in our case) should be taken into account in order to choose the relevant data and model to use. As an example, Twi word embeddings are significantly better when training on 742 k selected tokens than on 16 million noisy tokens, and when using a model that takes into account single character information (CWE-LP) instead of $n$-gram information (fastText). Finally, we want to note that, even within a corpus, the quality of the data might depend on the language. Wikipedia is usually used as a high-quality freely available multilingual corpus as compared to noisier data such as Common Crawl. However, for the two languages under study, Wikipedia resulted to have too much noise: interference from other languages, text clearly written by non-native speakers, lack of diacritics and mixture of dialects. The JW300 corpus on the other hand, has been rated as high-quality by our native Yorùbá speakers, but as noisy by our native Twi speakers. In both cases, experiments confirm the conclusions.",Acknowledgements,"The authors thank Dr. Clement Odoje of the Department of Linguistics and African Languages, University of Ibadan, Nigeria and Olóyè Gbémisóyè Àrdèó for helping us with the Yorùbá translation of the WordSim-353 word pairs and Dr. Felix Y. Adu-Gyamfi and Ps. Isaac Sarfo for helping with the Twi translation. We also thank the members of the Niger-Volta Language Technologies Institute for providing us with clean Yorùbá corpus The project on which this paper is based was partially funded by the German Federal Ministry of Education and Research under the funding code 01IW17001 (Deeplee). Responsibility for the content of this publication is with the authors.",,,,,,,,,,,,,What turn out to be more important high volume or high quality data?,347e86893e8002024c2d10f618ca98e14689675f,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,,,"The Spearman $\rho $ correlation for fastText models on the curated small dataset (clean), C1, improves the baselines by a large margin ($\rho =0.354$ for Twi and 0.322 for Yorùbá) even with a small dataset. The improvement could be justified just by the larger vocabulary in Twi, but in the case of Yorùbá the enhancement is there with almost half of the vocabulary size. We found out that adding some noisy texts (C2 dataset) slightly improves the correlation for Twi language but not for the Yorùbá language. The Twi language benefits from Wikipedia articles because its inclusion doubles the vocabulary and reduces the bias of the model towards religious texts. However, for Yorùbá, noisy texts often ignore diacritics or tonal marks which increases the vocabulary size at the cost of an increment in the ambiguity too. As a result, the correlation is slightly hurt. One would expect that training with more data would improve the quality of the embeddings, but we found out with the results obtained with the C3 dataset, that only high-quality data helps. The addition of JW300 boosts the vocabulary in both cases, but whereas for Twi the corpus mixes dialects and is noisy, for Yorùbá it is very clean and with full diacritics. Consequently, the best embeddings for Yorùbá are obtained when training with the C3 dataset, whereas for Twi, C2 is the best option. In both cases, the curated embeddings improve the correlation with human judgements on the similarity task a $\Delta \rho =+0.25$ or, equivalently, by an increment on $\rho $ of 170% (Twi) and 180% (Yorùbá).","One would expect that training with more data would improve the quality of the embeddings, but we found out with the results obtained with the C3 dataset, that only high-quality data helps.",46dba8cddcfcbf57b2837040db3a5e9a5f7ceaa3,258ee4069f740c400c0049a2580945a1cc7f044c,False,high-quality,,,"The Spearman $\rho $ correlation for fastText models on the curated small dataset (clean), C1, improves the baselines by a large margin ($\rho =0.354$ for Twi and 0.322 for Yorùbá) even with a small dataset. The improvement could be justified just by the larger vocabulary in Twi, but in the case of Yorùbá the enhancement is there with almost half of the vocabulary size. We found out that adding some noisy texts (C2 dataset) slightly improves the correlation for Twi language but not for the Yorùbá language. The Twi language benefits from Wikipedia articles because its inclusion doubles the vocabulary and reduces the bias of the model towards religious texts. However, for Yorùbá, noisy texts often ignore diacritics or tonal marks which increases the vocabulary size at the cost of an increment in the ambiguity too. As a result, the correlation is slightly hurt. One would expect that training with more data would improve the quality of the embeddings, but we found out with the results obtained with the C3 dataset, that only high-quality data helps. The addition of JW300 boosts the vocabulary in both cases, but whereas for Twi the corpus mixes dialects and is noisy, for Yorùbá it is very clean and with full diacritics. Consequently, the best embeddings for Yorùbá are obtained when training with the C3 dataset, whereas for Twi, C2 is the best option. In both cases, the curated embeddings improve the correlation with human judgements on the similarity task a $\Delta \rho =+0.25$ or, equivalently, by an increment on $\rho $ of 170% (Twi) and 180% (Yorùbá).","One would expect that training with more data would improve the quality of the embeddings, but we found out with the results obtained with the C3 dataset, that only high-quality data helps.",b5922f00879502196670bb7b26b229547de5fec4,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,How much is model improved by massive data and how much by quality?,10091275f777e0c2890c3ac0fd0a7d8e266b57cf,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,True,,,,,863f554da4c30e1548dffc1da53632c9af7f005a,258ee4069f740c400c0049a2580945a1cc7f044c,What two architectures are used?,cbf1137912a47262314c94d36ced3232d5fa1926,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,,,"As a first experiment, we compare the quality of fastText embeddings trained on (high-quality) curated data and (low-quality) massively extracted data for Twi and Yorùbá languages. The huge ambiguity in the written Twi language motivates the exploration of different approaches to word embedding estimations. In this work, we compare the standard fastText methodology to include sub-word information with the character-enhanced approach with position-based clustered embeddings (CWE-LP as introduced in Section SECREF17). With the latter, we expect to specifically address the ambiguity present in a language that does not translate the different oral tones on vowels into the written language.","As a first experiment, we compare the quality of fastText embeddings trained on (high-quality) curated data and (low-quality) massively extracted data for Twi and Yorùbá languages. The huge ambiguity in the written Twi language motivates the exploration of different approaches to word embedding estimations. In this work, we compare the standard fastText methodology to include sub-word information with the character-enhanced approach with position-based clustered embeddings (CWE-LP as introduced in Section SECREF17).",0961f0f256f4d2c31bcc6e188931422f79883a5a,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3-Table1-1.png,"Table 1: Summary of the corpora used in the analysis. The last 3 columns indicate in which dataset (C1, C2 or C3) are the different sources included (see text, Section 5.2.).",4-Table2-1.png,Table 2: Number of tokens per named entity type in the Global Voices Yorùbá corpus.,5-Table3-1.png,"Table 3: FastText embeddings: Spearman ρ correlation between human judgements and similarity scores on the wordSim353 for the three datasets analysed (C1, C2 and C3). The comparison with massive fastText embeddings is shown in the top rows.",5-Table4-1.png,"Table 4: CWE embeddings: Spearman ρ correlation between human evaluation and embedding similarities for the three datasets analysed (C1, C2 and C3).",7-Table5-1.png,Table 5: NER F1 score on Global Voices Yorùbá corpus.,,,,,,,,,,,,fastText CWE-LP,,,,,,,,,,,,only high-quality data helps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improving Textual Network Embedding with Global Attention via Optimal Transport,"Constituting highly informative network embeddings is an important tool for network analysis. It encodes network topology, along with other useful side information, into low-dimensional node-based feature representations that can be exploited by statistical modeling. This work focuses on learning context-aware network embeddings augmented with text data. We reformulate the network-embedding problem, and present two novel strategies to improve over traditional attention mechanisms: ($i$) a content-aware sparse attention module based on optimal transport, and ($ii$) a high-level attention parsing module. Our approach yields naturally sparse and self-normalized relational inference. It can capture long-term interactions between sequences, thus addressing the challenges faced by existing textual network embedding schemes. Extensive experiments are conducted to demonstrate our model can consistently outperform alternative state-of-the-art methods.",Introduction," When performing network embedding, one maps network nodes into vector representations that reside in a low-dimensional latent space. Such techniques seek to encode topological information of the network into the embedding, such as affinity BIBREF0 , local interactions (e.g, local neighborhoods) BIBREF1 , and high-level properties such as community structure BIBREF2 . Relative to classical network-representation learning schemes BIBREF3 , network embeddings provide a more fine-grained representation that can be easily repurposed for other downstream applications (e.g., node classification, link prediction, content recommendation and anomaly detection). For real-world networks, one naturally may have access to rich side information about each node. Of particular interest are textual networks, where the side information comes in the form of natural language sequences BIBREF4 . For example, user profiles or their online posts on social networks (e.g., Facebook, Twitter), and documents in citation networks (e.g., Cora, arXiv). The integration of text information promises to significantly improve embeddings derived solely from the noisy, sparse edge representations BIBREF5 . Recent work has started to explore the joint embedding of network nodes and the associated text for abstracting more informative representations. BIBREF5 reformulated DeepWalk embedding as a matrix factorization problem, and fused text-embedding into the solution, while BIBREF6 augmented the network with documents as auxiliary nodes. Apart from direct embedding of the text content, one can first model the topics of the associated text BIBREF7 and then supply the predicted labels to facilitate embedding BIBREF8 . Many important downstream applications of network embeddings are context-dependent, since a static vector representation of the nodes adapts to the changing context less effectively BIBREF9 . For example, the interactions between social network users are context-dependent (e.g., family, work, interests), and contextualized user profiling can promote the specificity of recommendation systems. This motivates context-aware embedding techniques, such as CANE BIBREF9 , where the vector embedding dynamically depends on the context. For textual networks, the associated texts are natural candidates for context. CANE introduced a simple mutual attention weighting mechanism to derive context-aware dynamic embeddings for link prediction. Following the CANE setup, WANE BIBREF10 further improved the contextualized embedding, by considering fine-grained text alignment. Despite the promising results reported thus far, we identify three major limitations of existing context-aware network embedding solutions. First, mutual (or cross) attentions are computed from pairwise similarities between local text embeddings (word/phrase matching), whereas global sequence-level modeling is known to be more favorable across a wide range of NLP tasks BIBREF11 , BIBREF12 , BIBREF13 , BIBREF14 . Second, related to the above point, low-level affinity scores are directly used as mutual attention without considering any high-level parsing. Such an over-simplified operation denies desirable features, such as noise suppression and relational inference BIBREF15 , thereby compromising model performance. Third, mutual attention based on common similarity measures (e.g., cosine similarity) typically yields dense attention matrices, while psychological and computational evidence suggests a sparse attention mechanism functions more effectively BIBREF16 , BIBREF17 . Thus such naive similarity-based approaches can be suboptimal, since they are more likely to incorporate irrelevant word/phrase matching.  This work represents an attempt to improve context-aware textual network embedding, by addressing the above issues. Our contributions include: ( INLINEFORM0 ) We present a principled and more-general formulation of the network embedding problem, under reproducing kernel Hilbert spaces (RKHS) learning; this formulation clarifies aspects of the existing literature and provides a flexible framework for future extensions. ( INLINEFORM1 ) A novel global sequence-level matching scheme is proposed, based on optimal transport, which matches key concepts between text sequences in a sparse attentive manner. ( INLINEFORM2 ) We develop a high-level attention-parsing mechanism that operates on top of low-level attention, which is capable of capturing long-term interactions and allows relational inference for better contextualization. We term our model Global Attention Network Embedding (GANE). To validate the effectiveness of GANE, we benchmarked our models against state-of-the-art counterparts on multiple datasets. Our models consistently outperform competing methods.",Problem setup, We introduce basic notation and definitions used in this work. ,Proposed Method,,Model framework overview," To capture both the topological information (network structure INLINEFORM0 ) and the semantic information (text content INLINEFORM1 ) in the textual network embedding, we explicitly model two types of embeddings for each node INLINEFORM2 : ( INLINEFORM3 ) the topological embedding INLINEFORM4 , and ( INLINEFORM5 ) the semantic embedding INLINEFORM6 . The final embedding is constructed by concatenating the topological and semantic embeddings, i.e., INLINEFORM7 . We consider the topological embedding INLINEFORM8 as a static property of the node, fixed regardless of the context. On the other hand, the semantic embedding INLINEFORM9 dynamically depends on the context, which is the focus of this study. Motivated by the work of BIBREF9 , we consider the following probabilistic objective to train the network embeddings:  DISPLAYFORM0  where INLINEFORM0 represents sampled edges from the network and INLINEFORM1 is the collection of model parameters. The edge loss INLINEFORM2 is given by the cross entropy DISPLAYFORM0  where INLINEFORM0 denotes the conditional likelihood of observing a (weighted) link between nodes INLINEFORM1 and INLINEFORM2 , with the latter serving as the context. More specifically,  DISPLAYFORM0  where INLINEFORM0 is the normalizing constant and INLINEFORM1 is an inner product operation, to be defined momentarily. Note here we have suppressed the dependency on INLINEFORM2 to simplify notation. To capture both the topological and semantic information, along with their interactions, we propose to use the following decomposition for our inner product term:  DISPLAYFORM0  Here we use INLINEFORM0 to denote the inner product evaluation between the two feature embeddings INLINEFORM1 and INLINEFORM2 , which can be defined by a semi-positive-definite kernel function INLINEFORM3 BIBREF26 , e.g., Euclidean kernel, Gaussian RBF, IMQ kernel, etc. Note that for INLINEFORM4 , INLINEFORM5 and INLINEFORM6 do not reside on the same feature space. As such, embeddings are first mapped to the same feature space for inner product evaluation. In this study, we use the Euclidean kernel  INLINEFORM0  for inner product evaluation with INLINEFORM0 , and linear mapping  INLINEFORM0  for feature space realignment with INLINEFORM0 . Here INLINEFORM1 is a trainable parameter, and throughout this paper we omit the bias terms in linear maps to avoid notational clutter. Note that our solution differs from existing network-embedding models in that: ( INLINEFORM0 ) our objective is a principled likelihood loss, while prior works heuristically combine the losses of four different models BIBREF9 , which may fail to capture the non-trivial interactions between the fixed and dynamic embeddings; and ( INLINEFORM1 ) we present a formal derivation of network embedding in a reproducing kernel Hilbert space.  Direct optimization of ( EQREF9 ) requires summing over all nodes in the network, which can be computationally infeasible for large-scale networks. To alleviate this issue, we consider other more computationally efficient surrogate objectives. In particular, we adopt the negative sampling approach BIBREF27 , which replaces the bottleneck Softmax with a more tractable approximation given by  DISPLAYFORM0  where INLINEFORM0 is the sigmoid function, and INLINEFORM1 is a noise distribution over the nodes. Negative sampling can be considered as a special variant of noise contrastive estimation BIBREF28 , which seeks to recover the ground-truth likelihood by contrasting data samples with noise samples, thereby bypassing the need to compute the normalizing constant. As the number of noise samples INLINEFORM2 goes to infinity, this approximation becomes exact BIBREF29 . Following the practice of BIBREF27 , we set our noise distribution to INLINEFORM3 , where INLINEFORM4 denotes the out-degree of node INLINEFORM5 .  We argue that a key to the context-aware network embedding is the design of an effective attention mechanism, which cross-matches the relevant content between the node's associated text and the context. Over-simplified dot-product attention limits the potential of existing textual network embedding schemes. In the following sections, we present two novel, efficient attention designs that fulfill the desiderata listed in our Introduction. Our discussion follows the setup used in CANE BIBREF9 and WANE BIBREF10 , where the text from the interacting node is used as the context. Generalization to other forms of context is straightforward. ",Optimal-transport-based matching," We first consider reformulating content matching as an optimal transport problem, and then re-purpose the transport plan as our attention score to aggregate context-dependent information. More specifically, we see a node's text and context as two (discrete) distributions over the content space. Related content will be matched in the sense that they yield a higher weight in the optimal transport plan INLINEFORM0 . The following two properties make the optimal transport plan more appealing for use as attention score. ( INLINEFORM1 ) Sparsity: when solved exactly, INLINEFORM2 is a sparse matrix with at most INLINEFORM3 non-zero elements, where INLINEFORM4 is the number of contents ( BIBREF30 , INLINEFORM5 ); ( INLINEFORM6 ) Self-normalized: row-sum and column-sum equal the respective marginal distributions. Implementation-wise, we first feed embedded text sequence INLINEFORM0 and context sequence INLINEFORM1 into our OT solver to compute the OT plan,  DISPLAYFORM0  Note that here we treat pre-embedded sequence INLINEFORM0 as INLINEFORM1 point masses in the feature space, each with weight INLINEFORM2 , and similarly for INLINEFORM3 . Next we “transport” the semantic content from context INLINEFORM4 according to the estimated OT plan with matrix multiplication  DISPLAYFORM0  where we have treated INLINEFORM0 as a INLINEFORM1 matrix. Intuitively, this operation aligns the context with the target text sequence via averaging the context semantic embeddings with respect to the OT plan for each content element in INLINEFORM2 . To finalize the contextualized embedding, we aggregate the information from both INLINEFORM3 and the aligned INLINEFORM4 with an operator INLINEFORM5 ,  DISPLAYFORM0  In this case, we practice the following simple aggregation strategy: first concatenate INLINEFORM0 and the aligned INLINEFORM1 along the feature dimension, and then take max-pooling along the temporal dimension to reduce the feature vector into a INLINEFORM2 vector, followed by a linear mapping to project the embedding vector to the desired dimensionality. ",Attention parsing," Direct application of attention scores based on a low-level similarity-based matching criteria (e.g., dot-product attention) can be problematic in a number of ways: ( INLINEFORM0 ) low-level attention scores can be noisy (i.e., spurious matchings), and ( INLINEFORM1 ) similarity-matching does not allow relational inference. To better understand these points, consider the following cases. For ( INLINEFORM2 ), if the sequence embeddings used do not explicitly address the syntactic structure of the text, a relatively dense attention score matrix can be expected. For ( INLINEFORM3 ), consider the case when the context is a query, and the matching appears as a cue in the node's text data; then the information needed is actually in the vicinity rather than the exact matching location (e.g., shifted a few steps ahead). Inspired by the work of BIBREF31 , we propose a new mechanism called attention parsing to address the aforementioned issues. As the name suggests, attention parsing re-calibrates the raw low-level attention scores to better integrate the information. To this end, we conceptually treat the raw attention matrix INLINEFORM0 as a two-dimensional image and apply convolutional filters to it:  DISPLAYFORM0  where INLINEFORM0 denotes the filter banks with INLINEFORM1 and INLINEFORM2 respectively as window sizes and channel number. We can stack more convolutional layers, break sequence embedding dimensions to allow multi-group (channel) low-level attention as input, or introduce more-sophisticated model architectures (e.g., ResNet BIBREF32 , Transformer BIBREF18 , etc.) to enhance our model. For now, we focus on the simplest model described above, for the sake of demonstration. With INLINEFORM0 as the high-level representation of attention, our next step is to reduce it to a weight vector to align information from the context INLINEFORM1 . We apply a max-pooling operation with respect to the context dimension, followed by a linear map to get the logits INLINEFORM2 of the weights DISPLAYFORM0  where INLINEFORM0 is the projection matrix. Then the parsed attention weight INLINEFORM1 is obtained by DISPLAYFORM0  which is used to compute the aligned context embedding  DISPLAYFORM0  Note that here we compute a globally aligned context embedding vector INLINEFORM0 , rather than one for each location in INLINEFORM1 as described in the last section ( INLINEFORM2 ). In the subsequent aggregation operation, INLINEFORM3 is broadcasted to all the locations in INLINEFORM4 . We call this global alignment, to distinguish it from the local alignment strategy described in the last section. Both alignment strategies have their respective merits, and in practice they can be directly combined to produce the final context-aware embedding. ",Related Work,,Experiments,,Experimental setup," We consider three benchmark datasets: ( INLINEFORM0 ) Cora, a paper citation network with text information, built by BIBREF44 . We prune the dataset so that it only has papers on the topic of machine learning. ( INLINEFORM1 ) Hepth, a paper citation network from Arxiv on high energy physics theory, with paper abstracts as text information. ( INLINEFORM2 ) Zhihu, a Q&A network dataset constructed by BIBREF9 , which has 10,000 active users with text descriptions and their collaboration links. Summary statistics of these three datasets are summarized in Table . Pre-processing protocols from prior studies are used for data preparation BIBREF10 , BIBREF34 , BIBREF9 .  For quantitative evaluation, we tested our model on the following tasks: ( INLINEFORM0 ) Link prediction, where we deliberately mask out a portion of the edges to see if the embedding learned from the remaining edges can be used to accurately predict the missing edges. ( INLINEFORM1 ) Multi-label node classification, where we use the learned embedding to predict the labels associated with each node. Note that the label information is not used in our embedding. We also carried out ablation study to identify the gains. In addition to the quantitative results, we also visualized the embedding and the attention matrices to qualitatively verify our hypotheses.  For the link prediction task, we adopt the area under the curve (AUC) score to evaluate the performance, AUC is employed to measure the probability that vertices in existing edges are more similar than those in the nonexistent edge. For each training ratio, the experiment is executed 10 times and the mean AUC scores are reported, where higher AUC indicates better performance. For multi-label classification, we evaluate the performance with Macro-F1 scores. The experiment for each training ratio is also executed 10 times and the average Macro-F1 scores are reported, where a higher value indicates better performance.  To demonstrate the effectiveness of the proposed solutions, we evaluated our model along with the following strong baselines. ( INLINEFORM0 ) Topology only embeddings: MMB BIBREF45 , DeepWalk BIBREF1 , LINE BIBREF33 , Node2vec BIBREF46 . ( INLINEFORM1 ) Joint embedding of topology & text: Naive combination, TADW BIBREF5 , CENE BIBREF6 , CANE BIBREF9 , WANE BIBREF10 , DMTE BIBREF34 . A brief summary of these competing models is provided in the Supplementary Material (SM). ",Results," We consider two variants of our model, denoted as GANE-OT and GANE-AP. GANE-OT employs the most basic OT-based attention model, specifically, global word-by-word alignment model; while GANE-AP additionally uses a one-layer convolutional neural network for the attention parsing. Detailed experimental setups are described in the SM.  Tables and summarize the results from the link-prediction experiments on all three datasets, where a different ratio of edges are used for training. Results from models other than GANE are collected from BIBREF9 , BIBREF10 and BIBREF34 . We have also repeated these experiments on our own, and the results are consistent with the ones reported. Note that BIBREF34 did not report results on DMTE. Both GANE variants consistently outperform competing solutions. In the low-training-sample regime our solutions lead by a large margin, and the performance gap closes as the number of training samples increases. This indicates that our OT-based mutual attention framework can yield more informative textual representations than other methods. Note that GANE-AP delivers better results compared with GANE-OT, suggesting the attention parsing mechanism can further improve the low-level mutual attention matrix. More results on Cora and Hepth are provided in the SM.  To further evaluate the effectiveness of our model, we consider multi-label vertex classification. Following the setup described in BIBREF9 , we first computed all context-aware embeddings. Then we averaged over each node's context-aware embeddings with all other connected nodes, to obtain a global embedding for each node, i.e., INLINEFORM0 , where INLINEFORM1 denotes the degree of node INLINEFORM2 . A linear SVM is employed, instead of a sophisticated deep classifier, to predict the label attribute of a node. We randomly sample a portion of labeled vertices with embeddings ( INLINEFORM3 ) to train the classifier, using the rest of the nodes to evaluate prediction accuracy. We compare our results with those from other state-of-the-art models in Table . The GANE models delivered better results compared with their counterparts, lending strong evidence that the OT attention and attention parsing mechanism promise to capture more meaningful representations.  We further explore the effect of INLINEFORM0 -gram length in our model (i.e., the filter size for the covolutional layers used by the attention parsing module). In Figure FIGREF39 we plot the AUC scores for link prediction on the Cora dataset against varying INLINEFORM1 -gram length. The performance peaked around length 20, then starts to drop, indicating a moderate attention span is more preferable. Similar results are observed on other datasets (results not shown). Experimental details on the ablation study can be found in the SM. ",Qualitative Analysis," We employed t-SNE BIBREF47 to project the network embeddings for the Cora dataset in a two-dimensional space using GANE-OT, with each node color coded according to its label. As shown in Figure FIGREF40 , papers clustered together belong to the same category, with the clusters well-separated from each other in the network embedding space. Note that our network embeddings are trained without any label information. Together with the label classification results, this implies our model is capable of extracting meaningful information from both context and network topological.  To verify that our OT-based attention mechanism indeed produces sparse attention scores, we visualized the OT attention matrices and compared them with those simarlity-based attention matrices (e.g., WANE). Figure FIGREF44 plots one typical example. Our OT solver returns a sparse attention matrix, while dot-product-based WANE attention is effectively dense. This underscores the effectiveness of OT-based attention in terms of noise suppression. ",Conclusion," We have proposed a novel and principled mutual-attention framework based on optimal transport (OT). Compared with existing solutions, the attention mechanisms employed by our GANE model enjoys the following benefits: (i) it is naturally sparse and self-normalized, (ii) it is a global sequence matching scheme, and (iii) it can capture long-term interactions between two sentences. These claims are supported by experimental evidence from link prediction and multi-label vertex classification. Looking forward, our attention mechanism can also be applied to tasks such as relational networks BIBREF15 , natural language inference BIBREF11 , and QA systems BIBREF48 .",Acknowledgments,"This research was supported in part by DARPA, DOE, NIH, ONR and NSF. Appendix Competing models Topology only embeddings Mixed Membership Stochastic Blockmodel (MMB) BIBREF45 : a graphical model for relational data, each node randomly select a different ""topic"" when forming an edge. DeepWalk BIBREF1 : executes truncated random walks on the graph, and by treating nodes as tokens and random walks as natural language sequences, the node embedding are obtained using the SkipGram model BIBREF27 . Node2vec BIBREF46 : a variant of DeepWalk by executing biased random walks to explore the neighborhood (e.g., Breadth-first or Depth-first sampling). Large-scale Information Network Embedding (LINE) BIBREF33 : scalable network embedding scheme via maximizing the joint and conditional likelihoods. Joint embedding of topology & text Naive combination BIBREF9 : direct combination of the structure embedding and text embedding that best predicts edges. Text-Associated DeepWalk (TADW) BIBREF5 : reformulating embedding as a matrix factorization problem, and fused text-embedding into the solution. Content-Enhanced Network Embedding (CENE) BIBREF6 : treats texts as a special kind of nodes. Context-Aware Network Embedding (CANE) BIBREF9 : decompose the embedding into context-free and context-dependent part, use mutual attention to address the context-dependent embedding. Word-Alignment-based Network Embedding (WANE) BIBREF10 : Using fine-grained alignment to improve context-aware embedding. Diffusion Maps for Textual network Embedding (DMTE) BIBREF34 : using truncated diffusion maps to improve the context-free part embedding in CANE. Complete Link prediction results on Cora and Hepth The complete results for Cora and Hepth are listed in Tables and . Results from models other than GANE are collected from BIBREF9 , BIBREF10 , BIBREF34 . We have also repeated these experiments on our own, the results are consistent with the ones reported. Note that DMTE did not report results on Hepth BIBREF34 . Negative sampling approximation In this section we provide a quick justification for the negative sampling approximation. To this end, we first briefly review noise contrastive estimation (NCE) and how it connects to maximal likelihood estimation, then we establish the link to negative sampling. Interested readers are referred to BIBREF50 for a more thorough discussion on this topic. Noise contrastive estimation. NCE seeks to learn the parameters of a likelihood model INLINEFORM0 by optimizing the following discriminative objective: J() = uipd[p(y=1|ui,v) - K Eu'pn [p(y=0|u,v)]], where INLINEFORM1 is the label of whether INLINEFORM2 comes from the data distribution INLINEFORM3 or the tractable noise distribution INLINEFORM4 , and INLINEFORM5 is the context. Using the Monte Carlo estimator for the second term gives us J() = uipd[p(y=1|ui,v) - k=1K [p(y=0|uk,v)]], uk iid pn. Since the goal of INLINEFORM6 is to predict the label of a sample from a mixture distribution with INLINEFORM7 from INLINEFORM8 and INLINEFORM9 from INLINEFORM10 , plugging the model likelihood and noise likelihood into the label likelihood gives us p(y=1;u,v) = p(u|v)p(u|v) + K pn(u|v), p(y=0;u,v) = K pn(u|v)p(u|v) + K pn(u|v). Recall INLINEFORM11 takes the following softmax form DISPLAYFORM0 NCE treats INLINEFORM12 as an learnable parameter and optimized along with INLINEFORM13 . One key observation is that, in practice, one can safely clamp INLINEFORM14 to 1, and the NCE learned model ( INLINEFORM15 ) will self-normalize in the sense that INLINEFORM16 . As such, one can simply plug INLINEFORM17 into the above objective. Another key result is that, as INLINEFORM18 , the gradient of NCE objective recovers the gradient of softmax objective INLINEFORM19 BIBREF49 . Negative sampling as NCE. If we set INLINEFORM20 and let INLINEFORM21 be the uniform distribution on INLINEFORM22 , we have DISPLAYFORM1 where INLINEFORM23 is the sigmoid function. Plugging this back to the INLINEFORM24 covers the negative sampling objective Eqn (6) used in the paper. Combined with the discussion above, we know Eqn (6) provides a valid approximation to the INLINEFORM25 -likelihood in terms of the gradient directions, when INLINEFORM26 is sufficiently large. In this study, we use INLINEFORM27 negative sample for computational efficiency. Using more samples did not significantly improve our results (data not shown). Experiment Setup We use the same codebase from CANE BIBREF9 . The implementation is based on TensorFlow, all experiments are exectuted on a single NVIDIA TITAN X GPU. We set embedding dimension to INLINEFORM28 for all our experiments. To conduct a fair comparison with the baseline models, we follow the experiment setup from BIBREF10 . For all experiments, we set word embedding dimension as 100 trained from scratch. We train the model with Adam optimizer and set learning rate INLINEFORM29 . For GANE-AP model, we use best filte size INLINEFORM30 for convolution from our abalation study. Ablation study setup To test how the INLINEFORM31 -gram length affect our GANE-AP model performance, we re-run our model with different choices of INLINEFORM32 -gram length, namely, the window size in convolutional layer. Each experiment is repeated for 10 times and we report the averaged results to eliminate statistical fluctuations. ",,,,,,,,,,,,,,,,,Which of their proposed attention methods works better overall?,e9d882775a132e172eea68ab6ab4621a924bb6b8,,unfamiliar,no,,2cfd959e433f290bb50b55722370f0d22fe090b7,False,,,"Tables and summarize the results from the link-prediction experiments on all three datasets, where a different ratio of edges are used for training. Results from models other than GANE are collected from BIBREF9 , BIBREF10 and BIBREF34 . We have also repeated these experiments on our own, and the results are consistent with the ones reported. Note that BIBREF34 did not report results on DMTE. Both GANE variants consistently outperform competing solutions. In the low-training-sample regime our solutions lead by a large margin, and the performance gap closes as the number of training samples increases. This indicates that our OT-based mutual attention framework can yield more informative textual representations than other methods. Note that GANE-AP delivers better results compared with GANE-OT, suggesting the attention parsing mechanism can further improve the low-level mutual attention matrix. More results on Cora and Hepth are provided in the SM.","Note that GANE-AP delivers better results compared with GANE-OT, suggesting the attention parsing mechanism can further improve the low-level mutual attention matrix.",d00af826e460e561ede14ef183f8ee15dc7caf8a,71f73551e7aabf873649e8fe97aefc54e6dd14f8,,,,,,,,,Which dataset of texts do they use?,6367877c05beebfdbb31e83c1f25dfddf925b6b6,,unfamiliar,no,,2cfd959e433f290bb50b55722370f0d22fe090b7,False,,,"We consider three benchmark datasets: ( INLINEFORM0 ) Cora, a paper citation network with text information, built by BIBREF44 . We prune the dataset so that it only has papers on the topic of machine learning. ( INLINEFORM1 ) Hepth, a paper citation network from Arxiv on high energy physics theory, with paper abstracts as text information. ( INLINEFORM2 ) Zhihu, a Q&A network dataset constructed by BIBREF9 , which has 10,000 active users with text descriptions and their collaboration links. Summary statistics of these three datasets are summarized in Table . Pre-processing protocols from prior studies are used for data preparation BIBREF10 , BIBREF34 , BIBREF9 .","We consider three benchmark datasets: ( INLINEFORM0 ) Cora, a paper citation network with text information, built by BIBREF44 .  Hepth, a paper citation network from Arxiv on high energy physics theory, with paper abstracts as text information. ( INLINEFORM2 ) Zhihu, a Q&A network dataset constructed by BIBREF9 , which has 10,000 active users with text descriptions and their collaboration links. ",6d05c6fe60dce8049c6d89a85aec0d362cfff463,71f73551e7aabf873649e8fe97aefc54e6dd14f8,Do they measure how well they perform on longer sequences specifically?,d151327c93b67928313f8fad8079a4ff9ef89314,,unfamiliar,no,,2cfd959e433f290bb50b55722370f0d22fe090b7,False,True,,"We further explore the effect of INLINEFORM0 -gram length in our model (i.e., the filter size for the covolutional layers used by the attention parsing module). In Figure FIGREF39 we plot the AUC scores for link prediction on the Cora dataset against varying INLINEFORM1 -gram length. The performance peaked around length 20, then starts to drop, indicating a moderate attention span is more preferable. Similar results are observed on other datasets (results not shown). Experimental details on the ablation study can be found in the SM.","We further explore the effect of INLINEFORM0 -gram length in our model (i.e., the filter size for the covolutional layers used by the attention parsing module). In Figure FIGREF39 we plot the AUC scores for link prediction on the Cora dataset against varying INLINEFORM1 -gram length. ",d335e7b4a19f5c87dedf42949d11fd9e9730bf3e,71f73551e7aabf873649e8fe97aefc54e6dd14f8,,,,,,,,Which other embeddings do they compare against?,70f9358dc01fd2db01a6b165e0b4e83e4a9141a7,,unfamiliar,no,,2cfd959e433f290bb50b55722370f0d22fe090b7,False,,,"To demonstrate the effectiveness of the proposed solutions, we evaluated our model along with the following strong baselines. ( INLINEFORM0 ) Topology only embeddings: MMB BIBREF45 , DeepWalk BIBREF1 , LINE BIBREF33 , Node2vec BIBREF46 . ( INLINEFORM1 ) Joint embedding of topology & text: Naive combination, TADW BIBREF5 , CENE BIBREF6 , CANE BIBREF9 , WANE BIBREF10 , DMTE BIBREF34 . A brief summary of these competing models is provided in the Supplementary Material (SM).","Topology only embeddings: MMB BIBREF45 , DeepWalk BIBREF1 , LINE BIBREF33 , Node2vec BIBREF46 . ( INLINEFORM1 ) Joint embedding of topology & text: Naive combination, TADW BIBREF5 , CENE BIBREF6 , CANE BIBREF9 , WANE BIBREF10 , DMTE BIBREF34 . ",4b124917aed874862aefcbf04bdaae295ec4f85d,71f73551e7aabf873649e8fe97aefc54e6dd14f8,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4-Figure1-1.png,"Figure 1: Schematic of the proposed mutual attention mechanism. In this setup, bag-of-words feature matchings are explicitly abstracted to infer the relationship between vertices.",6-Table1-1.png,Table 1: Dataset statistics.,7-Table2-1.png,Table 2: AUC scores for link prediction on the Cora and Hepth dataset.,7-Table3-1.png,Table 3: AUC scores for link prediction on the Zhihu dataset.,8-Figure4-1.png,Figure 4: Mutual attention between two nodes in Cora. Left: WANE attention. Right: OT attention (ours).,8-Table4-1.png,Table 4: Test Macro-F1 scores for multi-label node classification on Cora.,,,,,,,,,Cora Hepth Zhihu,,8-Figure2-1.png,Figure 2: n-gram length VS AUC on Cora.,,,,,,,,,,attention parsing,,,,,,,,,MMB DeepWalk LINE  Node2vec TADW CENE CANE WANE DMTE,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A Label Semantics Approach to Linguistic Hedges,"We introduce a model for the linguistic hedges `very' and `quite' within the label semantics framework, and combined with the prototype and conceptual spaces theories of concepts. The proposed model emerges naturally from the representational framework we use and as such, has a clear semantic grounding. We give generalisations of these hedge models and show that they can be composed with themselves and with other functions, going on to examine their behaviour in the limit of composition.",Introduction,"The modelling of natural language relies on the idea that languages are compositional, i.e. that the meaning of a sentence is a function of the meanings of the words in the sentence, as proposed by BIBREF0 . Whether or not this principle tells the whole story, it is certainly important as we undoubtedly manage to create and understand novel combinations of words. Fuzzy set theory has long been considered a useful framework for the modelling of natural language expressions, as it provides a functional calculus for concept combination BIBREF1 , BIBREF2 . A simple example of compositionality is hedged concepts. Hedges are words such as `very', `quite', `more or less', `extremely'. They are usually modelled as transforming the membership function of a base concept to either narrow or broaden the extent of application of that concept. So, given a concept `short', the term `very short' applies to fewer objects than `short', and `quite short' to more. Modelling a hedge as a transformation of a concept allows us to determine membership of an object in the hedged concept as a function of its membership in the base concept, rather than building the hedged concept from scratch BIBREF3 . Linguistic hedges have been widely applied, including in fuzzy classifiers BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 and database queries BIBREF8 , BIBREF9 . Using linguistic hedges in these applications allows increased accuracy in rules or queries whilst maintaining human interpretability of results BIBREF10 , BIBREF11 . This motivates the need for a semantically grounded account of linguistic hedges: if hedged results are more interpretable then the hedges used must themselves be meaningful. In the following we provide an account of linguistic hedges that is both functional, and semantically grounded. In its most basic formulation, the operation requires no additional parameters, although we also show that the formulae can be generalised if necessary. Our account of linguistic hedges uses the label semantics framework to model concepts BIBREF12 . This is a random set approach which quantifies an agent's subjective uncertainty about the extent of application of a concept. We refer to this uncertainty as semantic uncertainty BIBREF13 to emphasise that it concerns the definition of concepts and categories, in contrast to stochastic uncertainty which concerns the state of the world. In BIBREF13 the label semantics approach is combined with conceptual spaces BIBREF14 and prototype theory BIBREF15 , to give a formalisation of concepts as based on a prototype and a threshold, located in a conceptual space. This approach is discussed in detail in section ""Conceptual Spaces"" . An outline of the paper is then as follows: section ""Approaches to linguistic hedges"" discusses different approaches to linguistic hedges from the literature, and compares these with our model. Subsequently, in section ""Label semantics approach to linguistic hedges"" , we give formulations of the hedges `very' and `quite'. These are formed by considering the dependence of the threshold of a hedged concept on the threshold of the original concept. We give a basic model and two generalisations, show that the models can be composed and investigate the behaviour in the limit of composition. Section ""Discussion"" compares our results to those in the literature and proposes further lines of research.",Prototype theory and fuzzy set theory,"Prototype theory views concepts as being defined in terms of prototypes, rather than by a set of necessary and sufficient conditions. Elements from an underlying metric space then have graded membership in a concept depending on their similarity to a prototype for the concept. There is some evidence that humans use natural categories in this way, as shown in experiments reported in BIBREF15 . Fuzzy set theory BIBREF1 was proposed as a calculus for combining and modifying concepts with graded membership, and extended these ideas in BIBREF2 to linguistic variables as variables taking words as values, rather than numbers. For example, `height' can be viewed as a linguistic variable taking values `short,' `tall', `very tall', etc. The variable relates to an underlying universe of discourse $\Omega $ , which for the concept `tall' could be $\mathbb {R}^+$ . Then each value $L$ of the variable is associated with a fuzzy subset of $\Omega $ , and a function $\mu _L:\Omega \rightarrow [0,1]$ associates with each $x \in \Omega $ the value of its membership in $L$ . Prototype theory gives a semantic basis to fuzzy sets through the notion of similarity to a prototype, as described in BIBREF16 . In this context, concepts are represented by fuzzy sets and membership of an element in a concept is quantified by its similarity to the prototype. In this situation the fuzziness of the concept is seen as inherent to the concept. An alternative interpretation for fuzzy sets is random set theory, see BIBREF16 for an exposition. Here, the fuzziness of a set comes from uncertainty about a crisp set, i.e. semantic uncertainty, rather than fuzziness inherent in the world. This second approach is the stance taken by BIBREF13 , and which we now adopt in this paper.",Conceptual Spaces,"Conceptual spaces are proposed by Gärdenfors in BIBREF14 as a framework for representing information at the conceptual level. Gärdenfors contrasts his theory with both a symbolic, logical approach to concepts, and an associationist approach where concepts are represented as associations between different kinds of basic information elements. Rather, conceptual spaces are geometrical structures based on quality dimensions such as weight, height, hue, brightness, etc. It is assumed that conceptual spaces are metric spaces, with an associated distance measure. This might be Euclidean distance, or any other appropriate metric. The distance measure can be used to formulate a measure of similarity, as needed for prototype theory - similar objects are close together in the conceptual space, very different objects are far apart. To develop the conceptual space framework, Gärdenfors also introduces the notion of integral and separable dimensions. Dimensions are integral if assignment of a value in one dimension implies assignment of a value in another, such as depth and breadth. Conversely, separable dimensions are those where there is no such implication, such as height and sweetness. A domain is then defined as a set of quality dimensions that are separable from all other dimensions, and a conceptual space is defined as a collection of one or more domains. Gärdenfors goes on to define a property as a convex region of a domain in a conceptual space. A concept is defined as a set of such regions that are related via a set of salience weights. This casting of (at least) properties as convex regions of a domain sits very well with prototype theory, as Gärdenfors points out. If properties are convex regions of a space, then it is possible to say that an object is more or less central to that region. Because the region is convex, its centroid will lie within the region, and this centroid can be seen as the prototype of the property.",Label Semantics,"The label semantics framework was proposed by BIBREF12 and related to prototype theory and conceptual spaces in BIBREF13 . In this framework, agents use a set of labels $LA = \lbrace L_1, L_2, ..., L_n\rbrace $ to describe an underlying conceptual space $\Omega $ which has a distance metric $d(x,y)$ between points. In fact, it is sufficient that $d(x,y)$ be a pseudo-distance. When $x$ or $y$ is a set, say $Y$ , we take $d(x,Y) = \text{min}\lbrace d(x,y): y \in Y\rbrace $ . In this case, the set $Y$ is seen as an ontic set, i.e., a set where all elements are jointly prototypes, as opposed to an epistemic set describing a precise but unknown prototype, as described in BIBREF17 . Each label $L_i$ is associated with firstly a set of prototype values $\Omega $0 , and secondly a threshold $\Omega $1 , about which the agents are uncertain. The thresholds $\Omega $2 are drawn from probability distributions $\Omega $3 . Labels $\Omega $4 are associated with neighbourhoods $\Omega $5 . The neighbourhood can be seen as the extension of the concept $\Omega $6 . The intuition here is that $\Omega $7 captures the idea of being sufficiently close to prototypes $\Omega $8 . In other words, $\Omega $9 is sufficiently close to $d(x,y)$0 to be appropriately labelled as $d(x,y)$1 providing that $d(x,y)$2 . Given an element $x \in \Omega $ , we can ask how appropriate a given label is to describe it. This is quantified by an appropriateness measure, denoted $\mu _{L_i}(x)$ . We are intentionally using the same notation as for the membership function of a fuzzy set. This quantity is the probability that the distance from $x$ to $P_i$ , the prototype of $L_i$ , is less than the threshold $\varepsilon _i$ , as given by: $
\mu _{L_i}(x) = P(\varepsilon _i : x \in \mathcal {N}^{\varepsilon _i}_{L_i}) = P(\varepsilon _i : d(x, P_i) \le \varepsilon _i) = \int _{d(x, P_i)}^\infty \delta _{\varepsilon _i}(\varepsilon _i) \mathrm {d}\varepsilon _i
$  We also use the notation $\int _{d}^\infty \delta _{\varepsilon _i} (\varepsilon _i)\mathrm {d}\varepsilon _i = \Delta _i(d)$ , according to which $\mu _{L_i}(x) = \Delta _i(d(x, P_i))$ . The above formulation provides a link to the random set interpretation of fuzzy sets. Random sets are random variables taking sets as values. If we view $\mathcal {N}^{\varepsilon _i}_{L_i}$ as a random set from $\mathbb {R}^+$ into $2^\Omega $ , then $\mu _{L_i}(x)$ is the single point coverage function of $\mathcal {N}^{\varepsilon _i}_{L_i}$ , as defined in BIBREF18 , and also commonly called a contour function BIBREF19 . Labels can often be semantically related to each other. For example, the label `pet fish' is semantically related to the labels `pet' and `fish', and the label `very tall' related to the label `tall'. This prompts two questions: firstly, how the prototypes of each concept are related to each other, and secondly, how the thresholds of each concept are related. Two simple models for the relationships between the thresholds are given in BIBREF13 . The consonant model takes all thresholds as being dependent on one common underlying threshold. So, all thresholds have the same distance metric $d$ and are related to a base threshold $\varepsilon $ by the dependency that $\varepsilon _i = f_i(\varepsilon )$ for increasing functions $f_i$ . In contrast, the independence model takes all thresholds as being independent of each other. This might hold when labels are taken from different conceptual spaces. Between these two extremes, we model dependencies between thresholds as a Bayesian network - i.e., a directed acyclic graph whose edges encode conditional dependence between variables. The key property of this type of network is that the joint distribution of all variables can be broken into factors that depend only on each individual variable and its parents. So, for example, the network in figure 1 can be factorised as $\delta (\varepsilon _1,\varepsilon _2, \varepsilon _3, \varepsilon _4, \varepsilon _5) = \delta _{\varepsilon _1}(\varepsilon _1)\delta _{\varepsilon _2}(\varepsilon _2)\delta _{\varepsilon _3|\varepsilon _1, \varepsilon _2}(\varepsilon _3|\varepsilon _1,\varepsilon _2)\delta _{\varepsilon _4|\varepsilon _2}(\varepsilon _4|\varepsilon _2)\delta _{\varepsilon _5|\varepsilon _3}(\varepsilon _5|\varepsilon _3)$ . This enables calculation of the joint distribution and therefore marginal distributions in an efficient manner. One intuitively easy example is where the dependency of one threshold $\varepsilon _2$ on another $\varepsilon _1$ is that $\varepsilon _2 \le \varepsilon _1$ . This could be taken to model the dependency of the threshold of the concept `very tall' on the threshold of `tall'. The label `very tall' should be appropriate to describe fewer people than the label `tall'. Therefore, the threshold for describing someone as `very tall' will be narrower than the threshold for describing someone as `tall', i.e. $\varepsilon _{\text{very tall}} \le \varepsilon _{\text{tall}}$ . This simple model will form part of the approach to modelling linguistic hedges, as outlined in the sequel.",Approaches to linguistic hedges,"Linguistic hedges have been given varying treatments in the literature. In this section we summarise these different approaches and state the approach that we wish to take, discussing properties that hedge modifiers may need. We give two specific approaches from the literature with which we will compare our results. In BIBREF3 the idea of linguistic hedges as operators modifying fuzzy sets was introduced, so that the membership function $\mu _{hL}(x)$ of a hedged concept, $hL$ , is a function of the membership of the base concept $L$ , i.e. $\mu _{hL}(x) = f(\mu _L(x))$ . Furthermore, truth can be considered as a linguistic variable and hence a fuzzy set BIBREF2 , so that the application of a hedge can be seen as modifying the truth value of a sentence using that concept BIBREF20 , BIBREF21 , BIBREF2 . This second view is useful in approximate reasoning, and allows for an algebraic approach to investigating the properties of linguistic hedges, as introduced in BIBREF2 , and expanded upon in BIBREF22 , BIBREF20 , BIBREF21 . The approach we take, however, is to view a hedge as modifying the fuzzy set associated with a concept directly, as taken by BIBREF23 , BIBREF10 , BIBREF24 , BIBREF25 . Rather than examining the algebraic properties of hedges or their role in reasoning, we look at how hedges are semantically grounded and argue that our approach provides a particularly clear semantics. We will propose a set of operations that may be used for both expansion and refinement of single concepts. This is in contrast to the work presented in BIBREF26 in which information coarsening is effected by taking disjunctions of labels. The idea of a hedged concept has some similarities to that of the bipolar model of concepts described in BIBREF27 , since if it is appropriate to describe someone as `very tall', it must be appropriate to describe them as `tall', and similarly describing someone as `quite tall' implies that it is not entirely inappropriate to describe them as `tall'. However, we see the concepts derived by application of hedges as labels in their own right which can be used to describe data or objects. Zadeh divides hedges into two types. A type 1 hedge can be seen as an operator acting on a single fuzzy set. Examples are `very', `more or less', `quite', or `extremely' BIBREF3 . Type 2 hedges are more complicated and include modifiers such as `technically' or `practically'. In BIBREF3 concepts are considered as made up of various different components, with the membership function a weighted sum of the memberships of the individual components. Type 1 hedges operate on all components equally, whereas type 2 hedges differentiate between components. For example, the hedge `essentially' might give more weight to the most important components in a concept. Type 2 hedges are further explored in BIBREF28 , BIBREF29 , where components of a concept are categorised as definitional, primary or secondary, and the hedges `technically', `strictly speaking' and `loosely speaking' are analysed in terms of these categories. Although in the following we restrict ourselves to consideration of type 1 hedges only, the treatment of concepts as having different components is mirrored by the conceptual spaces view, where each component might be seen as a dimension in the conceptual space. Further development of the framework may therefore allow a treatment of type 2 hedges. A further distinction between types of hedge lies in the difference between powering or shifting modifiers. Powering modifiers are of the form $\mu _{hL}(x) = (\mu _L(x))^k$ , where $hL$ refers to the hedged concept and $k$ is some real value, and shifting modifiers are of the form $\mu _{hL}(x) = (\mu _L(x-a))$ . Zadeh introduces both types of modifier in his discussion of type 1 hedges BIBREF3 , however his powering modifiers are most frequently cited. These are the concentration operator $CON(\mu _{\text{tall}}(x)) = (\mu _{\text{tall}}(x))^2$ , and the dilation operator $DIL(\mu _{\text{tall}}(x)) = (\mu _{\text{tall}}(x))^\frac{1}{2}$ , which are often taken to implement the hedges `very' and `quite', (alternatively `more or less'), respectively.  The operators $CON$ and $DIL$ leave the core, $\lbrace x \in \Omega : \mu _L(x) = 1\rbrace $ , and support $\lbrace x \in \Omega : \mu _L(x) \ne 0\rbrace $ , of the fuzzy sets unchanged, which is often argued to be undesirable BIBREF9 , BIBREF23 , BIBREF25 , BIBREF30 . In particular, BIBREF9 argue that in a fuzzy database, if a concentrating hedge is being used to refine a query that is returning too many objects, the hedge needs to reduce the number of objects returned, and hence narrow down the core. Furthermore, BIBREF7 find that classifiers using the $CON$ and $DIL$ operators (classical hedges) do not perform as well as those with hedges that modify the core and support of the fuzzy sets. In contrast, Zadeh himself argues that the core should not be altered. The application of a modifier `very' to a property given by a crisp set should leave that property unchanged: `very square' is the same as `square'. A fuzzy set is made up of a non-fuzzy part, the core, and a fuzzy part, $\lbrace x \in \Omega : 0 < \mu _L(x) <1\rbrace $ . Since the core of a fuzzy set is a crisp set, it should be left unchanged. The use of classical hedges does improve performance over non-hedged fuzzy rules in expert systems BIBREF4 , BIBREF5 , BIBREF6 , so the argument against classical hedges is a matter of degree. The use of the $CON$ and $DIL$ operators to model the hedges `very' and `quite' is further criticised on the basis that the modifiers are arbitrary and semantically ungrounded. No justification is given for these modifiers other than that they have what seem to be intuitively the right properties BIBREF23 , BIBREF31 , BIBREF25 . Grounding hedges semantically is important for a theoretical account of what happens when we use terms like `very' and also for retaining interpretability in fuzzy systems. BIBREF23 , BIBREF31 both ground modifiers using a resemblance relation which takes into account how objects in the universe are similar to each other. BIBREF25 takes a horizon shifting approach. In BIBREF25 the class of finite numbers is used as an example of the horizon shifting approach. Some numbers are certainly finite, however as numbers get larger, finiteness becomes impossible to verify. Mapping this idea onto the concept `small', we can say that there is a class of numbers that are definitely small, say $[0, c]$ . As numbers get larger than $c$ we approach the horizon past which the concept `small' no longer applies, expressed as $1 - \epsilon (x)(x-c)$ . So: $
\mu _{\text{small}}(x) =
{\left\lbrace \begin{array}{ll}
1 & \text{if } x \in [0, c] \\
1 - \epsilon (x)(x-c) & \text{if } x \ge c
\end{array}\right.}
$  Now, to implement the hedge `very', the horizon $c$ is shifted by a factor $\sigma $ and the membership function altered thus: $
\mu _{\text{very small}}(x) =
{\left\lbrace \begin{array}{ll}
1 & \text{if } x \in [0, \sigma c] \\
1 - \epsilon (x, \sigma )(x- \sigma c) & \text{if } x \ge \sigma c
\end{array}\right.}
$  In BIBREF25 , examples of different kinds of membership functions that might be used to implement this idea are given. A linear membership function gives $\epsilon (x) = \frac{1}{a-c}$ where $a$ is the upper limit of the membership function. To implement the hedge, the function $\epsilon (x, \sigma ) = \frac{1}{\sigma (a-c)}$ is introduced, giving $
\mu _{\text{small}}(x) =
{\left\lbrace \begin{array}{ll}
1 & \text{if } x \in [0, c] \\
1 - \frac{x-c}{a-c} & \text{if } x \in [c, a]\\
0 & \text{otherwise}
\end{array}\right.}
$  and $
\mu _{\text{very small}}(x) =
{\left\lbrace \begin{array}{ll}
1 & \text{if } x \in [0, \sigma c] \\
1 - \frac{x- \sigma c}{\sigma (a-c)} & \text{if } x \in [\sigma c, \sigma a]\\
0 & \text{otherwise}
\end{array}\right.}
$   BIBREF23 , BIBREF31 both ground their approaches in the idea of looking at the elements near a fuzzy set in order to contract or dilate the set. The two approaches are similar, so we restrict ourselves to that of BIBREF23 . This approach introduces a fuzzy resemblance relation on the universe of discourse, and either a $T$ -norm in the case of dilation, or a fuzzy implicator for concentration. The modifier is then implemented as follows. Consider a fuzzy set $F$ and a proximity relation $E^Z$ which is approximate equality, parametrised by a fuzzy set $Z$ . As described in BIBREF23 , $E$ is modelled by $(u,v) \rightarrow E(u,v) = Z(u - v)$ , where $Z$ is a fuzzy interval centred on 0 with finite support. In terms of a trapezoidal membership function, $Z$ can be expressed as $(-z - a, -z, z, z+a)$ . Therefore, if $|u -v| \le z$ , $F$0 and $F$1 are judged to be approximately equal, i.e. $F$2 . The set $F$3 is dilated by $F$4 , where $F$5 is any $F$6 -norm, $F$7 being the standard. To understand the effect that this has on a fuzzy set $F$ , suppose that $F$ has a trapezoidal membership function $(A, B, C, D)$ where $[B, C]$ is the core of $F$ and $[A,B]$ , $[C,D]$ the support, and that $Z$ similarly is $(-z - a, -z, z, z+a)$ , with the T-norm $min$ used. Then $F$0 . Concentration is effected in a similar way: $E_Z(F)(s) = \text{inf}_{r \in \Omega } I(F(r), E^Z(s,r))$ , where I is a fuzzy implication. If $F$ and $Z$ are as above with the condition that $C - B \ge 2z$ , and $I$ is the Gödel implication, then $E_Z(F)$ = $(A + z + a, B + z, C - z, D - z - a)$ . For example, suppose we start with a set $F$ described in trapezoidal notation as $F = (A, B, C, D) = (2,4,6,8)$ , and an approximate equality function parametrised by $Z =(-z-a, -z, z, z + a) = (-1, -0.5, 0.5, 1)$ . The dilation of the set $F$ using T-norm $min$ is then: $
E^Z(F) = (A - z - a, B - z, C +z, D +z +a) = (1, 3.5, 6.5, 9)
$  The concentration of the set $F$ using the Gödel implication is: $
E_Z(F) = (A + z + a, B + z, C - z, D - z - a) = (3, 4.5, 5.5, 7)
$  These effects are illustrated in figure 2 . The intuitive idea behind this approach is that if an object $x_1$ resembles another object $x_2$ that is $L$ , then $x_1$ can be said to be `quite $L$ '. Conversely, object $x_2$ that is $L$ can be said to be `very $L$ ' only if all the objects $x$ that resemble it can be said to be $L$ . This formulation alters both the core and support of the fuzzy set $x_2$0 , which has been argued to be a desirable effect. Following BIBREF23 , BIBREF31 , BIBREF25 , we will propose linguistic modifiers that are semantically grounded rather than attempting to show their utility in classifiers, reasoning or to examine the algebra of modifiers. Our approach to linguistic modifiers arises very naturally from the label semantics framework, and the primary result does not require any parameters additional to the original membership function of the concept. We also show similarities between our model and the two detailed above.",Label semantics approach to linguistic hedges,"We present three formulations of linguistic hedges with increasing levels of generality. The first assumes that prototypes are equal. Secondly, we show that an analogue holds where prototypes are not equal, and thirdly that these hold in the case where the second threshold is a function of the first. We go on to show similarities between our model and those of BIBREF23 , BIBREF31 , BIBREF25 . Furthermore, we show that hedges are compositional, and look at their behaviour in the limit of composition. As described in section ""Approaches to linguistic hedges"" , $LA$ denotes a finite set of labels $\lbrace L_i\rbrace $ that agents use to describe basic categories. $\Omega $ is the underlying domain of discourse, with prototypes $P_i \in \Omega $ and thresholds $\varepsilon _i$ , drawn from a distribution $\delta _{\varepsilon _i}$ . As before, the appropriateness $\mu _{L_i}(x) = \Delta _i(d(x, P_i)) = \int _{d(x, P_i)}^\infty \delta _{\varepsilon _i}(\varepsilon _i)\mathrm {d}\varepsilon _i$ . We use the notation $L_i = <P_i, d, \delta _{\varepsilon _i}>$ . A concept $L_1$ can be narrowed or broadened to a second concept $L_2$ using the linguistic hedges `very' and `quite' respectively, i.e. $L_2$ is defined as `quite $L_1$ '. The directed acyclic graph illustrating this dependency is given in figure 3 . In this case, the threshold $\varepsilon _2$ associated with $L_2$ is dependent on $\varepsilon _1$ in that $\varepsilon _2 \ge \varepsilon _1$ . In the case of `very', we have that $\varepsilon _2 \le \varepsilon _1$ . Essentially, for `quite', we are saying that however wide a margin of certainty we apply the label `tall' with, the margin for `quite tall' will be wider, and conversely for `very'.",Hedges with unmodified prototypes,"Definition 1 (Dilation and Concentration) A label $L_2 = <P_2, d, \delta _{\varepsilon _2}>$ is a dilation of a label $L_1 = <P_1, d, \delta _{\varepsilon _1}>$ when $\varepsilon _2$ is dependent on $\varepsilon _1$ such that $\varepsilon _2 \ge \varepsilon _1$ . $L_2$ is a concentration of $L_1$ when $\varepsilon _2$ is dependent on $\varepsilon _1$ such that $\varepsilon _2 \le \varepsilon _1$ . Theorem 2 ( $L_2 = $ quite $L_1$ ) Suppose $L_2 = <P_2, d, \delta _{\varepsilon _2}>$ is a dilation of $L_1 = <P_1, d, \delta _{\varepsilon _1}>$ , so that $\varepsilon _2 \ge \varepsilon _1$ . Suppose also that $P_1 = P_2 = P$ , and that the marginal (unconditional) distribution of $\varepsilon _2$ , before conditioning on the knowledge that $\varepsilon _2 \ge \varepsilon _1$ , is identical to $\delta _{\varepsilon _1}$ , since $L_2$ is a dilation of $L_1$0 . Then $L_1$1 , $L_1$2 . $L_1$3  and hence, $
\delta (\varepsilon _1, \varepsilon _2) &= \delta _{\varepsilon _1}(\varepsilon _1)\delta _{\varepsilon _2|\varepsilon _1}(\varepsilon _2|\varepsilon _1)
=
{\left\lbrace \begin{array}{ll}
\frac{\delta _{\varepsilon _1}(\varepsilon _1)\delta _{\varepsilon _1}(\varepsilon _2)}{\Delta _1(\varepsilon _1)} & \text{if } \varepsilon _2 \ge \varepsilon _1\\
0 & \text{otherwise}
\end{array}\right.}
$  Then since $\varepsilon _2 \ge \varepsilon _1$ we have that $
\mu _{L_2}(x) &= \int _0^\infty \int _{max(\varepsilon _1, d(x,P))}^\infty \delta (\varepsilon _1, \varepsilon _2) d\varepsilon _2 d\varepsilon _1 = \int _0^\infty \int _{max(\varepsilon _1, d(x,P))}^\infty \frac{\delta _{\varepsilon _1}(\varepsilon _1)\delta _{\varepsilon _1}(\varepsilon _2)}{\Delta _1(\varepsilon _1)} d\varepsilon _2 d\varepsilon _1\\
&= \int _0^{d(x,P)}\frac{\delta _{\varepsilon _1}(\varepsilon _1)}{\Delta _1(\varepsilon _1)}\int _{d(x,P)}^\infty \delta _{\varepsilon _1}(\varepsilon _2) d\varepsilon _2 d\varepsilon _1 + \int _{d(x,P)}^\infty \frac{\delta _{\varepsilon _1}(\varepsilon _1)}{\Delta _1(\varepsilon _1)}\int _{\varepsilon _1}^\infty \delta _{\varepsilon _1}(\varepsilon _2) d\varepsilon _2 d\varepsilon _1\\
&= \mu _{L_1}(x)\int _0^{d(x,P)}\frac{\delta _{\varepsilon _1}(\varepsilon _1)}{\Delta (\varepsilon _1)} d\varepsilon _1 + \int _{d(x,P)}^\infty \delta _{\varepsilon _1}(\varepsilon _1) d\varepsilon _1 = \mu _{L_1}(x) - \mu _{L_1}(x)\ln (\mu _{L_1}(x))\nonumber $   The following example gives an illustration of the effect of applying this hedge, in comparison with the standard dilation hedge $(\mu _L(x))^{1/2}$ . Example 3 Suppose our conceptual space $\Omega = \mathbb {R}$ with Euclidean distance and that a label $L$ has prototype $P = 5$ , and threshold $\varepsilon \sim $ Uniform $[0,3]$ . Then $
\mu _L(x) =
{\left\lbrace \begin{array}{ll}
1 - \frac{|x - 5|}{3} & \text{ if } x \in [2, 8]\\
0 & otherwise
\end{array}\right.}
$  We can then form a new label $qL$ with prototype $P_q = P = 5$ and threshold $\varepsilon _q \ge \varepsilon $ . Then, according to theorem ""Theorem 2 (L 2 =L_2 =  quite L 1 L_1)"" , $\mu _{qL}(x) = \mu _L(x) - \mu _L(x)\ln \mu _L(x)$ . The effect of applying a dilation hedge to $L$ can be seen in figure 4 . The dilation hedge given above is contrasted with Zadeh's dilation hedge $(\mu _L(x))^{1/2}$ . Theorem 4 ( $L_2 = $ very $L_1$ ) Suppose $L_2 = <P_2, d, \delta _{\varepsilon _2}>$ is a concentration of $L_1 = <P_1, d, \delta _{\varepsilon _1}>$ , so that $\varepsilon _2 \le \varepsilon _1$ . Suppose also that $P_1 = P_2 = P$ , and that the marginal (unconditional) distribution of $\varepsilon _2$ , before conditioning on the knowledge that $\varepsilon _2 \le \varepsilon _1$ , is identical to $\delta _{\varepsilon _1}$ , since $L_2$ is a concentration of $L_1$0 . Then $L_1$1 , $L_1$2 . $L_1$3  and hence, $
\delta (\varepsilon _1, \varepsilon _2) &= \delta _{\varepsilon _1}(\varepsilon _1)\delta _{\varepsilon _2|\varepsilon _1}(\varepsilon _2|\varepsilon _1)
=
{\left\lbrace \begin{array}{ll}
\frac{\delta _{\varepsilon _1}(\varepsilon _1)\delta _{\varepsilon _1}(\varepsilon _2)}{1-\Delta _1(\varepsilon _1)} & \text{if } \varepsilon _2 \le \varepsilon _1\\
0 & \text{otherwise}
\end{array}\right.}
$  So since $\varepsilon _2 \le \varepsilon _1$ we have that: $
\mu _{L_2}(x) &= \int _0^\infty \int _{min(\varepsilon _1, d(x,P))}^{\varepsilon _1} \delta (\varepsilon _1, \varepsilon _2) d\varepsilon _2 d\varepsilon _1 = \int _0^\infty \int _{min(\varepsilon _1, d(x,P))}^{\varepsilon _1} \frac{\delta _{\varepsilon _1}(\varepsilon _1)\delta _{\varepsilon _1}(\varepsilon _2)}{1-\Delta _1(\varepsilon _1)} d\varepsilon _2 d\varepsilon _1\\
&= \int _{d(x,P)}^\infty \frac{\delta _{\varepsilon _1}(\varepsilon _1)}{1 - \Delta _1(\varepsilon _1)} \int _{d(x,P)}^{\varepsilon _1} \delta _{\varepsilon _1}(\varepsilon _2) d\varepsilon _2 d\varepsilon _1\\
&= \int _{d(x,P)}^\infty \frac{\delta _{\varepsilon _1}(\varepsilon _1)}{1 - \Delta _1(\varepsilon _1)} \left( \int _0^{\varepsilon _1} \delta _{\varepsilon _1}(\varepsilon _2) d\varepsilon _2 - \int _0^{d(x,P)} \delta _{\varepsilon _1}(\varepsilon _2) d\varepsilon _2 \right) d\varepsilon _1\\
&= \mu _{L_1}(x) - (1-\mu _{L_1}(x))\int _{d(x,P)}^\infty \frac{\delta _{\varepsilon _1}(\varepsilon _1)}{1 - \Delta _1(\varepsilon _1)} d\varepsilon _1 = \mu _{L_1}(x) + (1-\mu _{L_1}(x))\ln (1 - \mu _{L_1}(x))
$  The effect of these hedges are illustrated in the following example. Example 5 Suppose the label $L$ is as described in example ""Example 3"" . We can form a new label $vL$ with prototype $P_v = P = 5$ and threshold $\varepsilon _v \le \varepsilon $ . Then, according to theorem ""Theorem 4 (L 2 =L_2 =  very L 1 L_1)"" , $\mu _{vL}(x) = \mu _L(x) + (1 - \mu _L(x))\ln (1- \mu _L(x))$ The effect of applying this contraction hedge is seen in figure 5 , and again, this concentration hedge is contrasted with Zadeh's concentration hedge $(\mu _L(x))^2$ . These hedges can also be applied across multiple dimensions, demonstrated in the example below. Example 6 Suppose we have two labels `tall' and `thin'. `Tall' has prototype $P_{\text{tall}} = 6.5$ ft and `thin' has prototype $P_{\text{thin}} = 24$ in. The appropriateness of each label is defined by: $
\mu _{\text{tall}}(x_1) =
{\left\lbrace \begin{array}{ll}
1 & if x_1 > 6.5\\
1 - (P_{\text{tall}} - x_1) & if x_1 \in [5.5, 6.5]\\
0 & \text{otherwise}
\end{array}\right.}
$  where the variable $x_1$ measures height $
\mu _{\text{thin}}(x_2) =
{\left\lbrace \begin{array}{ll}
1 & if x_2 < 24\\
1 - \frac{(x_2 - P_{\text{thin}})}{4} & if x_2 \in [24, 28]\\
0 & \text{otherwise}
\end{array}\right.}
$  where $x_2$ measures waist size. Suppose further that being tall and being thin are independent of each other. The appropriateness of the label `tall and thin' could then be defined by: $
\mu _{\text{tall and thin}}(x_1, x_2) = \mu _{\text{tall}}(x_1)\mu _{\text{thin}}(x_2)
$  If `tall' and `thin' are independent, we can treat their hedges separately, so the appropriateness of a label `very tall and quite thin' is: $
\mu _{\text{very tall and quite thin}}(x_1, x_2) &= \mu _{\text{very tall}}(x_1)\mu _{\text{quite thin}}(x_2)\\
& = (\mu _{\text{tall}}(x_1)+ (1 - \mu _{\text{tall}}(x_1)\ln (1 - \mu _{\text{tall}}(x_1))) (\mu _{\text{thin}}(x_2) - \mu _{\text{thin}}(x_2)\ln (\mu _{\text{thin}}(x_2)))
$  This is illustrated in figure 6 ",Hedges with differing prototypes,"As they stand, the hedges proposed leave the core and support of the fuzzy sets unchanged, which is often argued to be undesirable BIBREF23 , BIBREF9 , BIBREF25 , BIBREF30 . A slight modification yields models of hedges in which the core, or prototype, of the concept has been changed. Theorem 7 (Dilation) Suppose that $L_2 = $ quite $L_1$ , as in theorem ""Theorem 2 (L 2 =L_2 =  quite L 1 L_1)"" , but that $P_2 \ne P_1$ . Then $\mu _{L_2}(x) = \Delta _1(d(x,P_2)) - \Delta _1(d(x, P_2))\ln (\Delta _1(d(x, P_2)))$ . Substitute $\Delta _1(d(x, P_2))$ for $\mu _{L_1}(x)$ throughout proof of theorem ""Theorem 2 (L 2 =L_2 =  quite L 1 L_1)""  Theorem 8 (Concentration) Suppose that $L_2 = $ very $L_1$ , as in theorem ""Theorem 4 (L 2 =L_2 =  very L 1 L_1)"" , but that $P_2 \ne P_1$ . Then $\mu _{L_2}(x) = \Delta _1(d(x,P_2)) + (1 - \Delta _1(d(x, P_2)))\ln (1 - \Delta _1(d(x, P_2)))$ . As above. Corollary 9 If $\varepsilon _2 \ge \varepsilon _1$ and $P_2 \supseteq P_1$ then $\mu _{L_2}(x) \ge \mu _{L_1}(x) - \mu _{L_1}(x)\ln (\mu _{L_1}(x))$ , and if $\varepsilon _2 \le \varepsilon _1$ and $P_2 \subseteq P_1$ , then $\mu _{L_2}(x) \le \mu _{L_1}(x) + (1-\mu _{L_1}(x))\ln (1-\mu _{L_1}(x))$ .  $\mu _{L_2}(x) = \Delta _1(d(x,P_2)) - \Delta _1(d(x, P_2))\ln (\Delta _1(d(x, P_2)))$ , but since $P_2 \supseteq P_1$ , $d(x, P_2) \le d(x, P_1)$ $\forall x \in \Omega $ , and so $\Delta _1(d(x, P_2)) \ge \Delta _1(d(x, P_1)) = \mu _{L_1}(x)$ $\forall x \in \Omega $ . Hence, $\mu _{L_2}(x) \ge \mu _{L_1}(x) - \mu _{L_1}(x)\ln (\mu _{L_1}(x))$ . A similar argument shows that $\mu _{L_2}(x) \le \mu _{L_1}(x) + (1-\mu _{L_1}(x))\ln (1-\mu _{L_1}(x))$ . Example 10 Suppose our conceptual space $\Omega = \mathbb {R}$ with Euclidean distance and that a label $L$ has prototype $P = [4.5,5.5]$ , and threshold $\varepsilon \sim $ Uniform $[0,3]$ . Then $
\mu _L(x) =
{\left\lbrace \begin{array}{ll}
1 & \text{ if } x \in [4.5,5.5]\\
1 - \frac{|x - 5|}{3} & \text{ if } x \in [1.5, 4.5] \text{ or } x \in [5.5, 8.5] \\
0 & otherwise
\end{array}\right.}•
$  We form the concept $qL$ by setting the prototype to be $P_q = [4,6]$ , and $\varepsilon _q \ge \varepsilon $ . The effect of applying our dilation hedge is illustrated in figure 7 . Conversely, suppose that a label $L$ has prototype $P = [4, 6]$ and threshold $\varepsilon \sim $ Uniform $[0,3]$ . Then $
\mu _L(x) =
{\left\lbrace \begin{array}{ll}
1 & \text{ if } x \in [4,6]\\
1 - \frac{|x - 5|}{3} & \text{ if } x \in [1, 4] \text{ or } x \in [6, 9]\\
0 & otherwise
\end{array}\right.}•
$  We now form the concept $vL$ by contracting the prototype to $P_v = [4.5,5.5]$ and setting $\varepsilon _v \le \varepsilon $ . The effect of applying the contraction hedge is illustrated in figure 8 .",Functions of thresholds,"It may be the case that the threshold of a given concept is greater than or less than a function of the original threshold. This could hold when a hedged concept is a very expanded or restricted version of the original concept, such as when the hedge `loosely' or `extremely' is used. Our formulae can also take account of this. Theorem 11 Suppose $L_2 = <P_2, d, \delta _{\varepsilon _2}>$ is a dilation of $L_1 = <P_1, d, \delta _{\varepsilon _1}>$ with $P_2 \ne P_1$ and $\varepsilon _2 \ge f(\varepsilon _1)$ , where $f: \mathbb {R} \rightarrow \mathbb {R}$ is strictly increasing or decreasing. Then $
\mu _{L_2}(x) = \Delta _1(f^{-1}(d(x,P_2))) - \Delta _1(f^{-1}(d(x,P_2)))\ln (\Delta _1(f^{-1}(d(x,P_2))))
$  Rewrite $\varepsilon _2 \ge f(\varepsilon _1)$ as $\varepsilon _2 \ge \varepsilon = f(\varepsilon _1)$ , where $\varepsilon \sim \delta $ and is associated with a label $L$ with prototype $P$ . Then: $
\mu _{L_2}(x) = \Delta (d(x,P_2)) - \Delta (d(x, P_2))\ln (\Delta (d(x, P_2)))
$  as above Since $f: \mathbb {R} \rightarrow \mathbb {R}$ is strictly monotone, $f^{-1}$ exists, and $\Delta (d(x, P)) = P(d(x,P) \le \varepsilon ) = P(f^{-1}(d(x,P)) \le \varepsilon _1) = \Delta _1(f^{-1}(d(x,P)))$ . So $
\mu _{L_2}(x) = \Delta _1(f^{-1}(d(x,P_2))) - \Delta _1(f^{-1}(d(x,P_2)))\ln (\Delta _1(f^{-1}(d(x,P_2))))
$  as required. Theorem 12 Suppose $L_2 = <P_2, d, \delta _{\varepsilon _2}>$ is a concentration of $L_1 = <P_1, d, \delta _{\varepsilon _1}>$ with $P_2 \ne P_1$ and $\varepsilon _2 \le f(\varepsilon _1)$ , where $f: \mathbb {R} \rightarrow \mathbb {R}$ is strictly increasing or decreasing. Then $
\mu _{L_2}(x) = \Delta _1(f^{-1}(d(x,P_2))) +(1- \Delta _1(f^{-1}(d(x,P_2))))\ln (1 -\Delta _1(f^{-1}(d(x,P_2))))
$  The proof is entirely similar to that of theorem ""Theorem 11"" ",Links to other models of hedges,"It is possible to specify the dependence of the threshold of the hedged concept on the threshold of the unhedged concept purely deterministically, i.e. by $\varepsilon _2 = f(\varepsilon _1)$ , rather than $\varepsilon _2 \le f(\varepsilon _1)$ . In this case, we can show links to other models of hedges from the literature. A simple example of a deterministic dependency is given below. Example 13 Suppose $\Omega = \mathbb {R}$ , $d$ is Euclidean distance and that $L_1$ has prototype $P = 5$ and $\varepsilon \sim $ Uniform $[0,3]$ . Then as before, $
\mu _L(x) =
{\left\lbrace \begin{array}{ll}
1 - \frac{|x - 5|}{3} & \text{ if } x \in [2, 8]\\
0 & otherwise
\end{array}\right.}•
$  To implement a dilation hedge, we would form a new label $qL$ with $P_q = P = 5$ and $\varepsilon _q = k_q\varepsilon $ with $k_q > 1$ . For a contraction hedge, we would form the label $vL$ by setting $P_v = P = 5$ and $\varepsilon _v = k_v\varepsilon $ with $k_v <1$ . Then, $
\mu _{hL}(x) =
{\left\lbrace \begin{array}{ll}
1 - \frac{|x - 5|}{3k} & \text{ if } x \in [5 - 3k, 5+ 3k]\\
0 & otherwise
\end{array}\right.}•
$  where $h = q$ or $v$ , $k = k_q$ or $k_v$ respectively. The effect of implementing these hedges is illustrated in figure 9 . Using this approach, we can also create an effect similar to that of changing the prototype. Suppose that a label $L$ in a conceptual space $\Omega $ has a single point $P$ as a prototype, but that the minimum value of the threshold $\varepsilon $ is greater than 0, for example, $\varepsilon \sim $ Uniform $[c,a]$ . Then $
\mu _{L}(x) =
{\left\lbrace \begin{array}{ll}
1 & \text{ if } d(x,P) < c\\
\frac{a}{a - c} - \frac{|x - P|}{a - c} & \text{ if } d(x, P) \in [a,c]\\
0 & otherwise
\end{array}\right.}•
$  Suppose that a hedged concept $hL$ is formed from $L$ by the dependency $\varepsilon _h = k\varepsilon $ where $k$ is a constant. Then $
\mu _{L}(x) = \Delta (\frac{|x-P|}{k}) =
{\left\lbrace \begin{array}{ll}
1 & \text{ if } \frac{|x-P|}{k} < c\\
\frac{a}{(a - c)} - \frac{|x - P|}{k(a - c)} & \text{ if } \frac{|x-P|}{k} \in [a,c]\\
0 & otherwise
\end{array}\right.}• =
{\left\lbrace \begin{array}{ll}
1 & \text{ if } |x - P| < kc\\
\frac{ka - |x - P|}{k(a - c)} & \text{ if } |x-P| \in [ka,kc]\\
0 & otherwise
\end{array}\right.}•
$  This effect is illustrated in the example below. Example 14 Suppose that the conceptual space $\Omega = \mathbb {R}$ and that a label $L$ has prototype $P = 5$ and threshold $\varepsilon \sim $ Uniform $[1, 2]$ . Then $
\mu _{L}(x) =
{\left\lbrace \begin{array}{ll}
1 & \text{ if } |x - 5| < 1\\
2 - |x - 5| & \text{ if } |x - 5| \in [1,2]\\
0 & otherwise
\end{array}\right.}•
$  Forming a new label $qL$ by applying the hedge $\varepsilon _q = 2\varepsilon $ gives appropriateness measure $
\mu _{qL}(x) =
{\left\lbrace \begin{array}{ll}
1 & \text{ if } |x - 5| < 2\\
\frac{4 - |x - 5|}{2} & \text{ if } |x - 5| \in [2,4]\\
0 & otherwise
\end{array}\right.}•
$  Forming a new label $vL$ by applying the hedge $\varepsilon _v = 0.5\varepsilon $ gives appropriateness measure $
\mu _{vL}(x) =
{\left\lbrace \begin{array}{ll}
1 & \text{ if } |x - 5| < 0.5\\
\frac{1 - |x - 5|}{0.5} & \text{ if } |x - 5| \in [0.5,1]\\
0 & otherwise
\end{array}\right.}•
$  These are illustrated in figure 10 . Notice that if we set $\Omega = \mathbb {R}^+$ and label $L$ specified by $P = 0$ , $\varepsilon \sim $ Uniform $[c, a]$ , this is identical to the linear membership model given in BIBREF25 . Specifically, we have $
\mu _{L}(x) =
{\left\lbrace \begin{array}{ll}
1 & \text{ if } x < c\\
\frac{a - x}{a-c} & \text{ if } x \in [c,a]\\
0 & \text{otherwise}
\end{array}\right.}
$  Forming a hedged concept $hL$ by setting $P_{hL}=P=0$ and $\varepsilon _{hL} = k\varepsilon $ gives $
\mu _{hL}(x) =
{\left\lbrace \begin{array}{ll}
1 & \text{ if } x < kc\\
\frac{ka - x}{ka-kc} & \text{ if } x \in [kc,ka]\\
0 & \text{otherwise}
\end{array}\right.}
$  Comparing this with the model given in section ""Approaches to linguistic hedges"" , we see that this is precisely the model proposed by BIBREF25 in the linear case. Similarity between the hedging effects illustrated in figure 9 and the effects implemented in the model proposed in BIBREF23 , illustrated in figure 2 , can clearly be seen. To derive the model given in BIBREF23 , we describe the fuzzy sets associated with labels $L$ and $hL$ in trapezoidal notation. Notice that $L= (P-a, P - c, P+c, P+a)$ and $hL = (P-ka, P- kc, P+kc, P+ka)$ . We can render this transformation in the terms employed by BIBREF23 . Consider labels $L$ and $hL$ as fuzzy sets characterised by the appropriateness measure $\mu _{L}(x)$ and $\mu _{hL}(x)$ . Then, in the case of dilation, we have: $
qL = E^Z(L)(s) = \text{sup}_{r \in \Omega }T(\mu _{L}(s), E^Z(s,r))
$  and for contraction, $
vL = E_Z(L)(s) = \text{inf}_{r \in \Omega }I(\mu _{L}(s), E^Z(s,r))
$  When $T$ is the $T$ -norm $min$ , I is the Gödel implication and $Z = (-z-\alpha , -z, z, z+\alpha )$ (with the restriction that $c < z$ to ensure a well-defined set), the approach in BIBREF23 gives $qL = (P-a - z - \alpha , P - c - z, P+c + z, P+a + z + \alpha )$ . If we set $z = (k - 1)c$ and $\alpha = (k - 1)(a - c)$ , this is equal to $qL = (P-ka, P- kc, P+kc, P+ka)$ . However, we also require that $vL = (P-ka, P- kc, P+kc, P+ka)$ . The approach in BIBREF23 gives $vL = (P-a + z + \alpha , P - c + z, P+c - z, P+a - z - \alpha )$ , and we therefore need to set $z = (1-k)c$ and $\alpha = (1-k)(a-c)$  This formulation is not as general as given in BIBREF23 , however, note that it only uses one additional parameter and no additional operators, rather than the two parameters and either a $T$ -norm or implication used by BIBREF23 . Two more key models from the literature are the powering and shifting modifiers proposed in BIBREF3 . Recall that powering modifiers are of the form $\mu _{hL}(x) = (\mu _L(x))^k$ and shifting modifiers are of the form $\mu _{hL}(x) = (\mu _L(x-a))$ . Shifting modifiers are easy to implement within our model, simply by shifting the prototype by the quantity $a$ . Powering modifiers can be expressed as a function of the threshold $\varepsilon $ given a particular distribution of the threshold $\delta $ . Suppose $\Omega = \mathbb {R}$ , $\varepsilon \sim U[0,c]$ , giving $
\mu _L(x) =
{\left\lbrace \begin{array}{ll}
1 - \frac{d(x, P)}{b} & \text{ if } x \in [P-b, P+b]\\
0 & otherwise
\end{array}\right.}•
$  and suppose a new label $hL$ is formed with prototype $P$ and threshold $\varepsilon _h = f(\varepsilon )$ such that $\mu _{hL}(x) = \mu _L(x)^k$ . Then $\mu _{hL}(x) = \Delta (f^{-1}(d(x,P))) = (\Delta (d(x,P)))^k$ , so $
f^{-1}(d(x,P)) = \Delta ^{-1}((\Delta (d(x,P)))^k) = b - b(\frac{(b - d(x, P))^k}{b^k}) = b - \frac{(b - d(x,P))^k}{b^{k-1}}
$  and hence $
\varepsilon _{hL} = f(\varepsilon ) = b - (b^{k-1}(b - \varepsilon ))^{1/k}
$  This expression seems surprisingly complicated, and there may be better ways of deriving the powering hedges that are not as a function of the threshold $\varepsilon $ . In this section we have shown that our general model can capture some of the many approaches found in the literature as special cases. We now go on to look at the property of compositionality that is exhibited by a number of models.",Compositionality,"One of the features of hedges seen in BIBREF23 , BIBREF31 , BIBREF25 , BIBREF3 is that they can be applied multiple times. Within the label semantics framework, this consists in expanding or reducing the threshold of a concept a number of times. The directed acyclic graph corresponding to this is shown in figure 11 . We show below that expressions for `very' and `quite' as given in theorems ""Theorem 2 (L 2 =L_2 =  quite L 1 L_1)"" and ""Theorem 4 (L 2 =L_2 =  very L 1 L_1)"" are compositional, and that the appropriateness of a concept after $n$ applications of a hedge can be expressed purely in terms of the appropriateness after $n -1$ applications. We also derive expressions for the composition of deterministic hedges as described in section ""Links to other models of hedges"" . Theorem 15 Suppose that labels $L_1, L_2, ... , L_n$ are defined by prototypes $P_1 = P_2 = ... = P_n = P$ , thresholds $\varepsilon _1 \ge \varepsilon _2 \ge ... \ge \varepsilon _n$ and with a distance metric $d$ common to all labels. Then $\mu _{L_n}(x) = \mu _{L_{n-1}}(x) + (1-\mu _{L_{n-1}}(x))\ln (1 - \mu _{L_{n-1}}(x))$  We proceed by induction on $n$ . Theorem ""Theorem 2 (L 2 =L_2 =  quite L 1 L_1)"" proves this for $n=2$ . Assuming true for $n=k$ , we have $
\mu _{L_{k+1}}(x) &= \int _0^\infty \int _0^\infty ... \int _{max(d(x,P), \varepsilon _k)}^\infty \delta (\varepsilon _1, \varepsilon _2,...,\varepsilon _{k+1}) \mathrm {d}\varepsilon _{k+1}...\mathrm {d}\varepsilon _1 \\
&= \int _0^\infty \frac{\delta _{\varepsilon _1}(\varepsilon _1)}{\Delta _1(\varepsilon _1)} \int _0^\infty \frac{\delta _{\varepsilon _1}(\varepsilon _2)}{\Delta _2(\varepsilon _2)} ... \int _0^\infty \frac{\delta _{\varepsilon _{k-1}}(\varepsilon _k)}{\Delta _k(\varepsilon _k)} \int _{max(d(x,P), \varepsilon _{k})}^\infty \delta _{\varepsilon _k}(\varepsilon _{k+1}) \mathrm {d}\varepsilon _{k+1}...\mathrm {d}\varepsilon _1 \\
&=\int _0^\infty \frac{\delta _{\varepsilon _1}(\varepsilon _1)}{\Delta _1(\varepsilon _1)} \int _0^\infty \frac{\delta _{\varepsilon _1}(\varepsilon _2)}{\Delta _2(\varepsilon _2)} ... \int _{max(d(x, P), \varepsilon _{k-1})}^\infty \frac{\delta _{\varepsilon _{k-1}}(\varepsilon _k)}{\Delta _k(\varepsilon _k)} \overbrace{\int _{\varepsilon _k}^\infty \delta _{\varepsilon _k}(\varepsilon _{k+1}) \mathrm {d}\varepsilon _{k+1}}^{= \Delta _k(\varepsilon _k)} \mathrm {d}\varepsilon _k...\mathrm {d}\varepsilon _1 \\
& \quad +\int _0^{d(x,P)} \frac{\delta _{\varepsilon _1}(\varepsilon _1)}{\Delta _1(\varepsilon _1)} \int _{\varepsilon _1}^{d(x,P)} \frac{\delta _{\varepsilon _1}(\varepsilon _2)}{\Delta _2(\varepsilon _2)} ... \int _{\varepsilon _{k-1}}^{d(x,P)} \frac{\delta _{\varepsilon _{k-1}}(\varepsilon _k)}{\Delta _k(\varepsilon _k)} \overbrace{\int _{d(x,P)}^\infty \delta _{\varepsilon _k}(\varepsilon _{k+1}) \mathrm {d}\varepsilon _{k+1}}^{= \mu _{L_k}(x)} \mathrm {d}\varepsilon _k...\mathrm {d}\varepsilon _1 \\
&= \overbrace{\int _0^\infty \frac{\delta _{\varepsilon _1}(\varepsilon _1)}{\Delta _1(\varepsilon _1)} \int _0^\infty \frac{\delta _{\varepsilon _1}(\varepsilon _2)}{\Delta _2(\varepsilon _2)} ... \int _{max(d(x, P), \varepsilon _k)}^\infty \delta _{\varepsilon _{k-1}}(\varepsilon _k)\mathrm {d}\varepsilon _k...\mathrm {d}\varepsilon _1 }^{= \mu _{L_k}(x) \text{ by ind. hyp.}}\\
& \quad + \mu _{L_k}(x) \int _0^{d(x,P)} \frac{\delta _{\varepsilon _1}(\varepsilon _1)}{\Delta _1(\varepsilon _1)} \int _{\varepsilon _1}^{d(x,P)} \frac{\delta _{\varepsilon _1}(\varepsilon _2)}{\Delta _2(\varepsilon _2)} ... \int _{\varepsilon _{k-1}}^{d(x,P)} \frac{\delta _{\varepsilon _{k-1}}(\varepsilon _k)}{\Delta _k(\varepsilon _k)} \mathrm {d}\varepsilon _k...\mathrm {d}\varepsilon _1\\
&= \mu _{L_k}(x) + \mu _{L_k}(x) \int _0^{d(x,P)} \frac{\delta _{\varepsilon _{k-1}}(\varepsilon _k)}{\Delta _k(\varepsilon _k)} ... \int _0^{\varepsilon _3} \frac{\delta _{\varepsilon _1}(\varepsilon _2)}{\Delta _2(\varepsilon _2)} \int _0^{\varepsilon _2} \frac{\delta _{\varepsilon _1}(\varepsilon _1)}{\Delta _1(\varepsilon _1)} \mathrm {d}\varepsilon _1...\mathrm {d}\varepsilon _k\\
&= \mu _{L_k}(x) + \mu _{L_k}(x) \overbrace{\int _0^{d(x,P)} \frac{\delta _{\varepsilon _{k-1}}(\varepsilon _k)}{\Delta _k(\varepsilon _k)} ... \int _0^{\varepsilon _3} \frac{-\delta _{\varepsilon _1}(\varepsilon _2)\ln (\Delta _1(\varepsilon _2))}{\Delta _2(\varepsilon _2)} \mathrm {d}\varepsilon _2...\mathrm {d}\varepsilon _k}^{=A}
$  By the inductive hypothesis, $\forall i = 0...k$ $
\delta _{\varepsilon _i}(\varepsilon _i) &= -\frac{\mathrm {d}}{\mathrm {d\varepsilon _i}}\Delta _i(\varepsilon _i)\\ \nonumber &= -\frac{\mathrm {d}}{\mathrm {d\varepsilon _i}}(\Delta _{i-1}(\varepsilon _i) - \Delta _{i-1}(\varepsilon _i)\ln (\Delta _{i-1}(\varepsilon _i)))\\
&= -\delta _{\varepsilon _{i-1}}(\varepsilon _i)\ln (\Delta _{i-1}(\varepsilon _i))
$  Recursively substituting in $A$ , we obtain $
\mu _{L_{k+1}}(x) &= \mu _{L_k}(x) + \mu _{L_k}(x) \int _0^{d(x,P)} \frac{\delta _{\varepsilon _{k-1}}(\varepsilon _k)}{\Delta _k(\varepsilon _k)} ... \int _0^{\varepsilon _3} \frac{\delta _{\varepsilon _2}(\varepsilon _2)}{\Delta _2(\varepsilon _2)} \mathrm {d}\varepsilon _2...\mathrm {d}\varepsilon _k\\
&= \mu _{L_k}(x) +\mu _{L_k}(x) \int _0^{d(x, P)}\frac{\delta _{\varepsilon _k}(\varepsilon _k)}{\Delta _k(\varepsilon _k)} \mathrm {d}\varepsilon _k\\
&= \mu _{L_k}(x) -\mu _{L_k}(x)\ln (\mu _{L_k}(x))
$  Theorem 16 Suppose labels $L_1, L_2, ..., L_n$ are defined by prototypes $P_1 = P_2 = ... = P_n = P$ , thresholds $\varepsilon _1 \le \varepsilon _2 \le ... \le \varepsilon _n$ , and that distance metric $d$ is common to all. Then $\mu _{L_n}(x) = \mu _{L_{n-1}}(x) - \mu _{L_{n-1}}(x)\ln (\mu _{L_{n-1}}(x))$  Similar to proof of theorem ""Theorem 15"" . We can also derive expressions for the composition of deterministic hedges. Theorem 17 Suppose labels $L_1, L_2, ..., L_n$ are defined by prototypes $P_1 = P_2 = ... = P_n = P$ , thresholds $\varepsilon _n = f(\varepsilon _{n-1}), \varepsilon _{n-1} = f(\varepsilon _{n-2}), ..., \varepsilon _2 = f(\varepsilon _1)$ , where $f$ is monotone increasing or decreasing, and that distance metric $d$ is common to all. Then $\mu _{L_n}(x) = \Delta _1(f^{-(n-1)}(d(x,P))$ , where $f^{-k}$ signifies $f^{-1}$ composed $k$ times.  $\mu _{L_2}(x) = \Delta _1(f^{-1}(d(x,P))$ . Suppose that $\mu _{L_k}(x) = \Delta _k(d(x,P)) = \Delta _1(f^{-(k-1)}(d(x,P))$ . Since $\varepsilon _{k+1} = f(\varepsilon _k)$ , we have $\mu _{L_{k+1}}(x) = \Delta _k(f^{-1}(d(x,P)) = \Delta _1(f^{-k}(d(x,P)))$ . Therefore $\mu _{L_n}(x) = \Delta _1(f^{-(n-1)}(d(x,P))$ by induction. Since labels can be composed in this way, we can model different degrees of emphasis corresponding to the composition of multiple hedges. So, for example, we could model `extremely L' as `very, very L'. This is illustrated in example ""Example 18"" . Example 18 Suppose the label $L$ is as described in example ""Example 3"" , i.e. $L$ has prototype $P = 5$ , and threshold $\varepsilon \sim $ Uniform $[0,3]$ . We can form a new label $vL$ with prototype $P_v = P = 5$ and threshold $\varepsilon _v \le \varepsilon $ , which has appropriateness $\mu _{vL}(x) = \mu _L(x) + (1 - \mu _L(x))\ln (1- \mu _L(x))$ as shown in theorem ""Theorem 4 (L 2 =L_2 =  very L 1 L_1)"" . We may then form another new label $vvL$ with prototype $L$0 and threshold $L$1 with appropriateness $L$2 as described in theorem ""Theorem 15"" . The effect of applying this contraction hedge is seen in figure 12 . We have contrasted the effect of the composed hedges with $L$3 . Since these hedges can be composed only an integral number of times, we cannot obtain the differences in grade that could be achieved with using various powers in a powering modifier, e.g. $\mu _L(x)^{1.73}$ . However, in section ""Functions of thresholds"" we discuss how to tune the intensity of hedges by using dependencies on functions of thresholds. We have further shown in section ""Links to other models of hedges"" how to derive powering and shifting modifiers within our framework. It would be interesting to explore how other examples of hedges can be expressed in this framework. We have shown that when multiple hedges of the forms seen in theorems ""Theorem 2 (L 2 =L_2 =  quite L 1 L_1)"" and ""Theorem 4 (L 2 =L_2 =  very L 1 L_1)"" are used, $\mu _{L_n}(x)$ can be expressed purely in terms of the appropriateness of the label directly preceding it. We have not been able to find a closed form solution for this recurrence, however, we can investigate the fixed points of the recurrence and examine what happens to the values of $\mu _{L_n}(x)$ as $n \rightarrow \infty $ . We have also shown that deterministic hedges can be composed, and we go on to look at their behaviour in the limit of composition.",Limits of Compositions.,"The following results examine the behaviour of $\mu _{L_n}(x)$ as $n \rightarrow \infty $  Theorem 19 Suppose $L_1, ..., L_n$ are labels obtained by repeated application of the dilation operator. Then $\mu _{L_n}$ has a limit $M^+$ and $M^+ = 1$ $\forall x \in \Omega $ such that $\mu _{L_1}(x) \ne 0$ , and $M^+ = 0$ otherwise.  $\mu _{L_{i+1}}(x) = \mu _{L_i}(x) - \mu _{L_i}(x)\ln (\mu _{L_i}(x))$ , $i = 1,.., n-1$ . If $\mu _{L_1}(x) = 1$ then $\mu _{L_i}(x) = 1$ $\forall i = 1,..., n$ . Also, if $\mu _{L_1}(x) = 0$ then $\mu _{L_i}(x) = 0$ $\forall i = 1,..., n$ . Suppose $\mu _{L_i}(x) \in (0,1)$ . Then $\mu _{L_{i+1}}(x) > \mu _{L_i}(x)$ , and so for $i = 1,.., n-1$0 , $i = 1,.., n-1$1 is a strictly increasing sequence. If a limit $M^+$ exists, then we will have $M^+ = M^+ - M^+\ln (M^+)$ , so either $M^+ = 0$ or $\ln (M^+) = 0$ . We can't have $M^+ = 0$ , since we assume that $\mu _{L_1}(x) \in (0,1)$ and the sequence is strictly increasing. Therefore, we must have $\ln (M^+) = 0$ and therefore $M^+ = 1$ . So $
\mu _{L_\infty }(x) = \left\lbrace 
\begin{array}{l l}
1 & \quad \mu _{L_1}(x) \in (0,1]\\
0 & \quad \mu _{L_1}(x) = 0\\
\end{array} \right.
$  Theorem 20 Suppose $L_1,..., L_n$ are labels obtained by repeated application of the contraction operator. Then $\mu _{L_n}$ has a limit $M^-$ and $M^- = 0$ $\forall x \in \Omega $ such that $\mu _{L_1}(x) \ne 1$ , and $M^- = 1$ otherwise.  $\mu _{L_{i+1}}(x) = \mu _{L_i}(x) + (1- \mu _{L_i}(x))\ln (1 - \mu _{L_i}(x))$ , $i = 1,..., n-1$ . Again, if $\mu _{L_1}(x) = 1$ then $\mu _{L_i}(x) = 1$ $\forall i = 1,..., n$ . Also, if $\mu _{L_1}(x) = 0$ then $\mu _{L_i}(x) = 0$ $\forall i = 1,..., n$ , and for $\mu _{L_1}(x) \in (0,1)$ , $\mu _{L_1}(x) >...> \mu _{L_n}(x)$ is a strictly decreasing sequence. If a limit $M^-$ exists, then $
M^- &= M^- + (1 - M^-)\ln (1- M^-)\\
\ln (1 - M^-) &= M^-\ln (1- M^-)
$  So either $M^- = 0$ or $\ln (1- M^-) = 0$ . If $\ln (1- M^-) = 0$ then $M^- = 1$ , which is impossible since $\mu _{L_1}(x) \in (0,1)$ and the sequence of $\mu _{L_i}(x)$ is strictly decreasing. Therefore $
\mu _{L_\infty }(x) = \left\lbrace 
\begin{array}{l l}
0 & \quad \mu _{L_1}(x) \in [0,1)\\
1 & \quad \mu _{L_1}(x) = 1\\
\end{array} \right.
$  We have shown here that in the limit, the result of applying dilation or contraction modifiers multiple times is to create a crisp set. In the case of dilation, the crisp set includes the whole support of the fuzzy set associated with the original label, whereas in the case of contraction, the concept reduces to include only its prototype. When deterministic hedges are used, i.e. $\varepsilon _2 = f(\varepsilon _1)$ , the behaviour of the limit depends on the behaviour of the function $f$ and its properties in the limit as $n \rightarrow \infty $ of $f^{-n}$ . Example 21 Suppose $f(\varepsilon ) = 0.5\varepsilon $ . Applying this hedge multiple times will result in $\mu _{L_n}(x) = \Delta _1(2^n d(x,P))$ . As $n\rightarrow \infty $ , $2^n d(x,P) \rightarrow \infty $ , except where $d(x,P)=0$ . Therefore, $
\mu _{L_\infty }(x) = \left\lbrace 
\begin{array}{l l}
0 & \quad d(x,P) > 0\\
1 & \quad d(x,P) = 0\\
\end{array} \right.
$  On the other hand, if $f(\varepsilon ) = 2\varepsilon $ , $\mu _{L_n}(x) = \Delta _1(2^{-n}d(x,P))$ . As $n\rightarrow \infty $ , $2^{-n} d(x,P) \rightarrow 0$ , and hence $\mu _{L_\infty }(x) = 1$ $\forall x \in \Omega $ . The behaviour of the hedges given in example ""Example 21"" is therefore different from those in theorems ""Theorem 19"" and ""Theorem 20"" , since the concept either shrinks to a single point, in the case of contraction, or, in the case of dilation, expands to fill the entire space $\Omega $ .",Discussion,"We have presented formulae for linguistic hedges which are both functional and semantically grounded. The modifiers presented arise naturally from the label semantics framework, in which concepts are represented by a prototype and threshold. Our hedges have an intuitive meaning: if I think that the threshold for a concept `small' is of a certain width, then the threshold for the concept `very small' will be narrower. On the other hand, the threshold for the concept `quite small' will be broader. The hedges proposed are examples of `type 1' hedges, i.e. they operate equally across all dimensions of the fuzzy set associated with a concept. The first result presented is somewhat similar to a powering modifier since the core and support of the set remain the same. In BIBREF9 , BIBREF7 , it is argued that this property is undesirable for hedges used in fuzzy expert systems, since if a query is returning too large a set of answers, this type of contraction hedge does not reduce this overabundance. However, although the hedges we propose do not at their simplest address the overabundance issue, we argue that they address another problem associated with powering hedges, in that they have a clear semantic grounding that the powering modifiers lack.  BIBREF23 , BIBREF31 also propose modifiers that are semantically grounded, using the idea of resemblance to nearby objects. Their formulations have the properties that the core and support of the fuzzy set are both changed, thereby addressing the issue of overabundant answers BIBREF9 , BIBREF7 . In our most specific case, since the prototype is not altered, the core and support of the fuzzy set representing the concept remain the same. However, our initial proposal can be generalised, as in section ""Hedges with differing prototypes"" , to apply to the case where $P_1 \ne P_2$ . Specifying a semantically meaningful way of altering the boundaries of the prototype would answer the objection that the core and support of a set should change under a linguistic hedge. The most general result (section ""Functions of thresholds"" ) shows that the formula still applies when $\varepsilon _2 \le f(\varepsilon _1)$ , or $\varepsilon _2 \ge f(\varepsilon _1)$ . Combined with a distribution $\delta $ such that the lower bound of the distribution is not zero, the core and support of the fuzzy set are modified. With the condition $\varepsilon _2 = f(\varepsilon _1)$ , we are able to recreate the result given in BIBREF25 for linear membership functions, and show how the model proposed by BIBREF23 has strong similarities to our own. In this case we have introduced additional parameters, so the simplicity of the original result is lost. However, the further parameters introduced are no more than those introduced by BIBREF25 , and arguably fewer than those introduced by BIBREF23 , who require that a resemblance relation be specified, using two additional parameters, and also, that a $T$ -norm or fuzzy implication need to be specified. There are various choices of operator that could be used for either of these, and it is not obvious that any one is better than the others. We have also shown that the basic case operators `very' and `quite' can be composed, which is not immediately obvious from the formulae (section ""Compositionality"" ). Further, we show that in the limit of composition the membership of any object in the fuzzy part of $L$ , i.e. $\lbrace x: 0 < \mu _L(x) < 1\rbrace $ , increases to 1 in the case of `quite' or decreases to 0 in the case of `very' (section ""Limits of Compositions."" ). This is similar to the limit of applying the powering modifiers, but differs from what would happen with the modifiers proposed by BIBREF23 . In that case, the limit of `very' would shrink to a single point and the limit of `quite' would expand to encompass the whole universe of discourse. This can be modelled using the deterministic hedges described in section ""Links to other models of hedges"" . Although behaviour differs slightly, in fact human discourse does not apply modifiers infinitely, so the difference in behaviour is arguably not important. Our formulation has the benefit that it can be applied in more situations than simply linguistic hedges. For example, the concept `apple green' has a prototype different to that of just green, and the threshold for `apple green' is likely to be smaller than the threshold for simply `green'. Our model can take account of this.",Conclusions and further work,"We have presented formulae for two simple linguistic hedges, `very' and `quite'. These formulae are functional, hence easy to compute, but also semantically grounded, in that they arise naturally from the conceptual framework of label semantics combined with prototype theory and conceptual spaces theory, and in the most specific case require no additional parameters. We have also shown that two other formulations BIBREF23 , BIBREF31 , BIBREF25 , can be derived from this framework with equal or fewer parameters. We have shown that the hedges can be composed and have described their behaviour in the limit of composition. Further work could look at testing the utility of these hedges in particular classifiers to compare their performance with the classical hedges and with the hedges used by e.g. BIBREF9 , BIBREF7 , and also to examine a trade-off between accuracy and the number of parameters used. Alternatively, investigating semantically grounded ways of expanding or reducing prototypes could have a similar impact. The model could also be extended to the more complicated type 2 hedges such as `essentially', or `technically', by treating dimensions of the conceptual space heterogeneously. This requires using some type of weighting or necessity measure on the dimensions, work which is currently ongoing.",Acknowledgements,Martha Lewis gratefully acknowledges support from EPSRC Grant No. EP/E501214/1,,,,,,,,,,,,,Does this paper address the variation among English dialects regarding these hedges?,320d72a9cd19b52c29dda9ddecd520c9938a717f,infinity,unfamiliar,no,semantics,50d8b4a941c26b89482c94ab324b5a274f9ced66,False,False,,,,aba922a2695a5c7602e5e59bed9fda244f5ada81,35491e1e579f6d147f4793edce4c1a80ab2410e7,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4-Figure1-1.png,Figure 1: Example of a Bayesian network of thresholds. Dependencies between thresholds εi are represented by arrows.,6-Figure2-1.png,"Figure 2: Illustration of the expansion and contraction modifiers proposed in [2]. The original set F can be described in trapezoidal set notation as (2, 4, 6, 8). The approximate equality function is parametrised by a set Z = (−1,−0.5, 0.5, 1). Dilating the set as described in [2] gives the set EZ(F ) = (1, 3.5, 6.5, 9). Concentrating the set as described results in the set EZ(F ) = (3, 4.5, 5.5, 7)",8-Figure4-1.png,"Figure 4: Plots of µL(x), µL(x) 1/2, and µqL(x) = µL(x) − µL(x) lnµL(x). Values of µqL(x) near to the prototype P = 5 are greater than equivalent values of µL(x) 1/2.",9-Figure5-1.png,"Figure 5: Plots of µL(x), µL(x) 2, and µvL(x) = µL(x)+(1−µL(x)) ln(1−µL(x)). Values of µvL(x) decrease more quickly as we move from the prototype P = 5 than do equivalent values of µL(x) 2.",10-Figure6-1.png,"Figure 6: Plot of µvery tall and quite thin(x1, x2) = µvery tall(x1)µquite thin(x2). Notice how appropriateness drops off quickly as the variable x1, i.e. height, decreases. In contrast, appropriateness decreases slowly as we increase the variable x2, or waist size",11-Figure7-1.png,"Figure 7: Plots of µL(x) and µqL(x) = ∆(d(x, Pq)) − ∆(d(x, Pq)) ln(∆(d(x, Pq))). The prototype of L has been expanded.",,,,,,,,,,,12-Figure8-1.png,"Figure 8: Plots of µL(x) and µvL(x) = ∆(d(x, Pv) + (1−∆(d(x, Pv)) ln(1−∆(d(x, Pv))). The prototype of L has been reduced.",13-Figure9-1.png,"Figure 9: Plots of µL(x), µqL(x) and µvL(x). The prototype, a single point, remains constant on application of the hedges.",14-Figure10-1.png,"Figure 10: Plots of µL(x), µqL(x) and µvL(x). In this case, the set {x ∈ Ω : µhL(x) = 1}, where h = v, q or nothing, expands or contracts on application of the hedges.",18-Figure12-1.png,"Figure 12: Plots of µL(x), µL(x) 4, and µvvL(x). Values of µvvL(x) decrease more quickly as we move from the prototype P = 5 than do equivalent values of µL(x) 4.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Katecheo: A Portable and Modular System for Multi-Topic Question Answering,"We introduce a modular system that can be deployed on any Kubernetes cluster for question answering via REST API. This system, called Katecheo, includes four configurable modules that collectively enable identification of questions, classification of those questions into topics, a search of knowledge base articles, and reading comprehension. We demonstrate the system using publicly available, pre-trained models and knowledge base articles extracted from Stack Exchange sites. However, users can extend the system to any number of topics, or domains, without the need to modify any of the model serving code. All components of the system are open source and available under a permissive Apache 2 License.",Introduction,"When people interact with chatbots, smart speakers or digital assistants (e.g., Siri), one of their primary modes of interaction is information retrieval BIBREF0 . Thus, those that build dialog systems often have to tackle the problem of question answering. Developers could support question answering using publicly available chatbot platforms, such as Watson Assistant or DialogFlow. To do this, a user would need to program an intent for each anticipated question with various examples of the question and one or more curated responses. This approach has the advantage of generating high quality answers, but it is limited to those questions anticipated by developers. Moreover, the management burden of such a system might be prohibitive as the number of questions that needs to be supported is likely to increase over time. To overcome the burden of programming intents, developers might look towards more advanced question answering systems that are built using open domain question and answer data (e.g., from Stack Exchange or Wikipedia), reading comprehension models, and knowledge base searches. In particular, BIBREF1 previously demonstrated a two step system, called DrQA, that matches an input question to a relevant article from a knowledge base and then uses a recurrent neural network (RNN) based comprehension model to detect an answer within the matched article. This more flexible method was shown to produce promising results for questions related to Wikipedia articles and it performed competitively on the SQuAD benchmark BIBREF2 . However, if developers wanted to integrate this sort of reading comprehension based methodology into their applications, how would they currently go about this? They would need to wrap pre-trained models in their own custom code and compile similar knowledge base articles at the very least. At the most, they may need to re-train reading comprehension models on open domain question and answer data (e.g., SQuAD) and/or implement their own knowledge base search algorithms. In this paper we present Katecheo, a portable and modular system for reading comprehension based question answering that attempts to ease this development burden. The system provides a quickly deployable and easily extendable way for developers to integrate question answering functionality into their applications. Katecheo includes four configurable modules that collectively enable identification of questions, classification of those questions into topics, a search of knowledge base articles, and reading comprehension. The modules are tied together in a single inference graph that can be invoked via a REST API call. We demonstrate the system using publicly available, pre-trained models and knowledge base articles extracted from Stack Exchange sites. However, users can extend the system to any number of topics, or domains, without the need to modify the model serving code. All components of the system are open source and publicly available under a permissive Apache 2 License. The rest of the paper is organized as follows. In the next section, we provide an overview of the system logic and its modules. In Section 3, we outline the architecture and configuration of Katecheo, including extending the system to an arbitrary number of topics. In Section 4, we report some results using example pre-trained models and public knowledge base articles. Then in conclusion, we summarize the system, its applicability, and future development work.",System Overview,"Katecheo is partially inspired by the work of BIBREF1 on DrQA. That previously developed method has two primary phases of question answering: document retrieval and reading comprehension. Together these functionalities enable open domain question answering. However, many dialog systems are not completely open domain. For example, developers might want to create a chatbot that has targeted conversations about restaurant reservations and movie times. It would be advantageous for such a chatbot to answer questions about food and entertainment, but the developers might not want to allow the conversation to stray into other topics. With Katecheo, one of our goals was to create a question answering system that is more flexible than those relying on curated responses while remaining more targeted than a completely open domain question answering system. The system includes document retrieval (or what we refer to as “knowledge base search”) and reading comprehension, but only within sets of curated knowledge base articles each corresponding to a particular topic (e.g., food or entertainment). When a question text is input into the Katecheo system, it is processed through four modules: (1) question identification, (2) topic classification, (3) knowledge base search, and (4) reading comprehension. This overall logic is depicted in Figure FIGREF6 .",Question Identification,"The first module in Katecheo, question identification, determines if the input text (labeled Q in Figure FIGREF6 ) is actually a question. In our experience, users of dialog systems provide a huge number of unexpected inputs. Some of these unexpected inputs are questions and some are just statements. Before going to the trouble of matching a knowledge base article and generating an answer, Katecheo completes this initial step to ensure that the input is a question. If the input is a question, the question identification module (henceforth the “question identifier"") passes a positive indication/flag to the next module indicating that it should continue processing the question. Otherwise, it passes a negative flag to end the processing. The question identifier uses a rule-based approach to question identification. As suggested in BIBREF3 , we utilize the presence of question marks and 5W1H words to determine if the input is a question. Based on our testing, this provides quite high performance (90%+ accuracy) and is not a blocker to overall performance.",Topic Classification,"To reach our goal of a question answering system that would be more targeted than previous open domain question answering, we decided to allow the user of the system to define one or more topics. The topic classification module of the system (henceforth the “topic classifier"") will attempt to classify the input question into one of the topics and then select a knowledge base article from a set of knowledge base articles corresponding to that topic. One way we could enable this topic classification is by training a text classifier that would classify the input text into one of the user supplied topics. However, this approach would require (i) the user to provide both the topic and many example questions within that topic, and (ii) the system to retrain its classification model any time a new topic was added. We wanted to prioritize the ease of deployment, modularity and extensibility of the system, and, thus, we decided to take a slightly more naive approach. Along with each topic, the user supplies the system with a pre-trained Named Entity Recognition (NER) model that identifies entities within that topic. The topic classifier then utilizes these pre-trained models to determine if the input question includes entities from one of the user supplied topics. If so, the topic classifier classifies the question into that topic. When two of the topics conflict, the system currently suspends processing and returns a null answer. The system accepts NER models that are compatible with spaCy BIBREF4 . As discussed further below, the user can supply a link to a zip file that contains each topic NER model. Note, it might be possible to remove the dependence on NER models in the future. We are currently exploring the use of other topic modeling techniques including non-negative matrix factorization and/or Latent Dirichlet Allocation (LDA). These techniques could enable the system to automatically match the input question to most appropriate topical knowledge base, and thus only rely on the user to supply knowledge base articles.",Knowledge Base Search,"Once the topic has been identified, a search is made to match the question with an appropriate knowledge base article from a set of user supplied knowledge base articles corresponding to the user supplied topic. This matched article will be utilized in the next stage of processing to generate an answer. The user supplied sets of knowledge base articles for each topic are in a JSON format and include a title and body text for each article. The system assumes that the knowledge base articles are in the form of a question and answer knowledge base (e.g., like a Stack Exchange site), rather than any arbitrarily structured articles. In this way, we are able to utilize the titles of the articles (i.e., the questions) in matching to user input questions. In the knowledge base search module of Katecheo (henceforth the “KB Search"" module), we use the Python package FuzzyWuzzy to perform string matching between the input question and the knowledge base article titles. FuzzyWuzzy uses Levenshtein Distance BIBREF5 match the input string to one or more input candidate strings. We eventually plan to update this knowledge base search to an approach similar to that of BIBREF1 using bigram hashing and TF-IDF. However, the fuzzy string matching approach works reasonably well as long as the supplied knowledge bases are of a type where many of the article titles are in the form of topical questions.",Reading Comprehension,"The final module of the Katecheo system is the reading comprehension (or just “comprehension"") module. This module takes as input the original input question plus the matched knowledge base article body text and uses a reading comprehension model to select an appropriate answer from within the article. The current release of Katecheo uses a Bi-Directional Attention Flow, or BiDAF, model for reading comprehension BIBREF6 . This BiDAF model includes a Convolutional Neural Network (CNN) based character level embedding layer, a word embedding layer that uses pre-trained GloVE embeddings, a Long Short-Term Memory Network (LSTM) based contextual embedding layer, an “attention flow layer"", and a modeling layer include bi-directional LSTMs. We are using a pre-trained version of BiDAF available in the AllenNLP BIBREF7 library. Future releases of Katecheo will include the ability to swap out the reading comprehension model for newer architectures based on, e.g., BERT BIBREF8 or XLNet BIBREF9 or custom trained models.",Architecture and Configuration,"All four of the Katecheo modules are containerized with Docker BIBREF10 and are deployed as pods on top of Kubernetes BIBREF11 (see Figure FIGREF12 ). In this way, Katecheo is completely portable to any standard Kubernetes cluster including hosted versions in AWS, GCP, Digital Ocean, Azure, etc. and on-premises version that use vanilla Kubernetes, OpenShift, CaaS, etc. To provide developers with a familiar interface to the question answering system, we provide a REST API interface. Developers can call Katecheo via a single endpoint with ingress to the system provided by Ambassador, a Kubernetes-native API Gateway. Seldon-core is used to simplify the routing between the four modules, create the REST API, and manage deployments. To create the Seldon deployment of the four modules, as depicted in Figure FIGREF12 , we: (1) create a Python class for each module that contains standardized Seldon-specified methods and that loads the various models for making predictions; (2) wrap that Python class in a standard, containerized Seldon model server using a public Seldon Docker image and s2i ; (3) push the wrapped Python code to DockerHub ; (4) create a Seldon inference graph that links the modules in a Directed Acyclic Graph (DAG); and (5) deploy the inference graph to Kubernetes. After all of these steps are complete, a single REST API endpoint is exposed. When a user calls this single API endpoint the Seldon inference graph is invoked and the modules are executed using the specified routing logic. To specify the topic names, topic NER models, and topic knowledge base JSON files (as mentioned in reference to Figure FIGREF6 ), the user need only fill out a JSON configuration file template in the following format: [  {  ""name"": ""topic 1 name"",  ""ner_model"": ""<link>"",  ""kb_file"": ""<link>""  },  {  ""name"": ""topic 2 name"",  ""ner_model"": ""<link>"",  ""kb_file"": ""<link>""  },  etc... ]  where each INLINEFORM0 would be replaced with a respective URL containing the NER model or knowledge base JSON file. The linked NER models need to be spaCy compatible and compressed into a single zip file, and the linked knowledge base JSON files need to include both titles and bodies as specified in the Katecheo GitHub repository README file. Once this configuration file is created, a deploy script can be executed to automatically deploy all of the Katecheo modules.",Example Usage,"We demonstrated the utility of Katecheo by deploying the system for question answering in two topics, Medical Sciences and Christianity. These topics are diverse enough that they would warrant different curated sets of knowledge base articles, and we can easily retrieve knowledge base articles for each of these subjects from the Medical Sciences and Christianity Stack Exchange sites, respectively. We also have access to NER models for both of these topics. For the Medical Sciences NER model, we utilized the en_ner_bc5cdr_md model from scispaCy BIBREF12 , which is trained on the BC5CDR corpus BIBREF13 . For the Christianity topic, we utilize a custom spaCy NER model trained on annotated data from the GotQuestions website. Example inputs and outputs of the system are included in Table TABREF17 . As can be seen, the system is able to match many questions with an appropriate topic and subsequently generate an answer using the BiDAF comprehension model. Not all of the answers would fit into conversational question answering in terms of naturalness, but others show promise. There were cases in which the system was not able to classify an input question into an appropriate topic, even when there would have been a closely matching knowledge base article. In particular when testing the system on the Medical Sciences topic, we noticed a higher number of these cases (see the fourth and fifth rows of Table TABREF17 ). This is due to the fact that the pre-trained Medical Sciences NER model from scispaCy is primarily intended to recognize chemical and disease entities within text, not general medical sciences terminology. On the other hand, the NER model utilized for the Christianity topic is more generally applicable within that topic.",Conclusions,"In conclusion, Katecheo is a portable and modular system for reading comprehension based question answering. It is portable because it is built on cloud native technologies (i.e., Docker and Kubernetes) and can be deployed to any cloud or on-premise environment. It is modular because it is composed of four configurable modules that collectively enable identification of questions, classification of those questions into topics, a search of knowledge base articles, and reading comprehension. Initial usage of the system indicates that it provides a flexible and developer friendly way to enable question answering functionality for multiple topics or domains via REST API. That being said, the current configurations of Katecheo are limited to answering from knowledge bases constructed in a question and answer format, and the current topic classification relies on topical NER models that are compatible with spaCy. In the future, we plan to overcome these limitations by extending our knowledge base search methodology, enabling usage of a wider variety of pre-trained models, and exploring other topic matching/modeling techniques to remove our NER model dependency. The complete source code, configuration information, deployment scripts, and examples for Katecheo are available at https://github.com/cvdigitalai/katecheo. A screencast demonstration of Katecheo is available at https://youtu.be/g51t6eRX2Y8.",,,,,,,,,,,,,,,,,,,,,,,,,how many domains did they experiment with?,4d5e2a83b517e9c082421f11a68a604269642f29,,,,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,2,"We demonstrated the utility of Katecheo by deploying the system for question answering in two topics, Medical Sciences and Christianity. These topics are diverse enough that they would warrant different curated sets of knowledge base articles, and we can easily retrieve knowledge base articles for each of these subjects from the Medical Sciences and Christianity Stack Exchange sites, respectively.","We demonstrated the utility of Katecheo by deploying the system for question answering in two topics, Medical Sciences and Christianity.",7f8619d9280a918743612bc1fbcef60ffeeb55e6,34c35a1877e453ecaebcf625df3ef788e1953cc4,,,,,,,,,what pretrained models were used?,2c3b2c3bab6d18cb0895462e3cfd91cd0dee7f7d,,,,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,,"The current release of Katecheo uses a Bi-Directional Attention Flow, or BiDAF, model for reading comprehension BIBREF6 . This BiDAF model includes a Convolutional Neural Network (CNN) based character level embedding layer, a word embedding layer that uses pre-trained GloVE embeddings, a Long Short-Term Memory Network (LSTM) based contextual embedding layer, an “attention flow layer"", and a modeling layer include bi-directional LSTMs. We are using a pre-trained version of BiDAF available in the AllenNLP BIBREF7 library. Future releases of Katecheo will include the ability to swap out the reading comprehension model for newer architectures based on, e.g., BERT BIBREF8 or XLNet BIBREF9 or custom trained models. Architecture and Configuration","The current release of Katecheo uses a Bi-Directional Attention Flow, or BiDAF, model for reading comprehension BIBREF6 . Future releases of Katecheo will include the ability to swap out the reading comprehension model for newer architectures based on, e.g., BERT BIBREF8 or XLNet BIBREF9 or custom trained models.

Architecture and Configuration",4d93055ba78109cc42b609ccac07e763855869ce,34c35a1877e453ecaebcf625df3ef788e1953cc4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3-Figure1-1.png,"Figure 1: The overall processing flow in Katecheo. Q represents the input question text, the dashed lines represent a flag passed between modules indicating whether the next module should proceed with processing, and the cylinders represent various data inputs to the modules.",4-Figure2-1.png,Figure 2: The overall Katecheo architecture. Each node in Kubernetes may be a cloud instance or on-premise machine.,6-Table1-1.png,"Table 1: Example inputs, outputs, and matched topics from a Katecheo system deployed to provide question answering on two topics, Medical Sciences and Christianity.",,,,,,,,,,,,,,,BiDAF BERT ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
INFODENS: An Open-source Framework for Learning Text Representations,"The advent of representation learning methods enabled large performance gains on various language tasks, alleviating the need for manual feature engineering. While engineered representations are usually based on some linguistic understanding and are therefore more interpretable, learned representations are harder to interpret. Empirically studying the complementarity of both approaches can provide more linguistic insights that would help reach a better compromise between interpretability and performance. We present INFODENS, a framework for studying learned and engineered representations of text in the context of text classification tasks. It is designed to simplify the tasks of feature engineering as well as provide the groundwork for extracting learned features and combining both approaches. INFODENS is flexible, extensible, with a short learning curve, and is easy to integrate with many of the available and widely used natural language processing tools.",Introduction,"Linear classifiers in combination with the right features achieve good performance on text classification tasks BIBREF0 . Those hand-crafted features provide baselines for evaluating deep learning methods and are sometimes difficult to beat BIBREF1 , BIBREF2 . In some cases, hand-crafted features can even be combined with learned features to improve performance on a given task BIBREF3 , BIBREF4 highlighting some complementarity in the information captured by each approach. Conducting empirical experiments to study such complementarity would be beneficial, and the reasons are threefold: Firstly, this enables us to compare the performance of both hand crafted and learned representations and make design decisions regarding the trade-offs between speed and accuracy on a specific dataset. Secondly, it helps in investigating where the performance gaps are and whether these methods can complement each other and how they can be combined to improve performance. Finally, it allows us to derive new linguistic hypotheses as in many cases, deep learning methods are great engineering tools but they operate as black box methods and it is difficult to extract from them linguistic insights. In this paper we present INFODENS a framework aimed at studying hand-crafted and learned representations. We first explain how INFODENS can be used to simplify the tasks of feature engineering, feature learning, and evaluation. We then validate the framework on sentiment analysis and topic classification tasks and showcase that in many cases, hand-crafted features can be complementary to learned representations.",Framework Design and Architecture,"The framework is designed in a modular and developer-friendly manner to encourage changes and extensions. The source code is accompanied by a user and a developer guide, and we give a brief overview of the architecture in this section, summarized in Figure FIGREF2 . The framework consists of the following frozen and hot spots:",Frozen spots,"These are the modules of the framework that need not be changed for extending the functionality in typical use cases. is the callable module and centerpiece of the framework. It instantiates the other modules, calls their APIs, and handles the communication between them. provides the APIs for accessing the input text, preprocessed versions of it, and external resources. It also handles the building of language models and the unsupervised learning of word-embeddings. dynamically detects the available feature extractors and manages the multi-threaded feature extraction process. It handles merging the extracted and given feature matrices and generating feature descriptors. is the module that exports the extracted features in a chosen format. This can also be extended with other existing or custom formats via the Format writer.  manages the training and evaluation of the different classifiers or regressors. Like the feature manager, it also detects the classifiers dynamically at run time.",Hot spots,"These are the modules which developers can modify and extend with their code to add new functionality. is used to integrate different NLP tools (taggers, tokenizers.. etc) without changing the Preprocessor APIs. It can also be called to do on-the-fly preprocessing of feature-specific input files. handles the definition and extraction of configuration parameters from configuration files. extract and return vector representations of text, whether learned or engineered. Researchers can write their own feature extractor methods which are detected dynamically at run-time and called by the feature manager. are trained on the extracted features to build a model that is then used to evaluate the features. Their design is inspired by the scikit-learn BIBREF7 approach. Similar to the feature extractors, they are detected dynamically by the classifier manager. implements the feature output formats. It can be extended to support other formats by adding new methods to the class.",Usage,"The framework can be used as a standalone toolkit without any modifications given the implemented features and classifiers. For example, it can be used to extract features for usage with other machine learning tools, or to evaluate given features with the existing classifiers or regressors. Extending the framework with new feature extractors or classifiers is as simple as a drag and drop placement of the new code files into the feature_extractor and classifer directories respectively. The framework will then detect the new extensions dynamically at runtime. In this section we explore how each use case is handled.",Feature Extraction and Evaluation,"The framework is run by invoking the Python script infodens.py with an INI configuration file consisting of five sections specifying the input files, the output parameters, the general settings, the requested features and their arguments, and finally, the classifiers. Figure FIGREF17 shows an example of a configuration file. All the parameters are described in the README file on the repository.",Feature Development,"Since a main use case for the framework is extracting engineered and learned features, it was designed such that developing a new feature extractor would require minimal effort. Figure FIGREF19 demonstrates a simple feature extractor that retrieves the sentence length. More complicated features and learned features are provided in the repository which can be used as a guide for developers. Documentation for adding classifiers and format writers is described in the Wiki of the repository but is left out of this paper due to the limited space.",Evaluation and Results,"In this section, we evaluate the performance of the framework used out of the box. We first detail the datasets used, then the set of hand-crafted and learned representations, along with the classifiers, all of which are available as part of the released code.",Datasets and External Resources,"We use the the datasets provided by Zhang et al. zhang2015character, three of which are topic classification datasets: AG's news, DBpedia, and Yahoo! Answers, and four are for sentiment analysis: Yelp review polarity, Yelp review full, Amazon review polarity, and Amazon review full. We exclude the Sougu News dataset, which is a transliterated Chinese text, as we only utilize English language models and word embeddings for the purposes of this demonstration. The results gathered by BIBREF10 , comparing different convolutional models and the fastText approach, are used as baselines. External resources required to extract INLINEFORM0 -gram probabilities and word embeddings, namely a 5-gram modified Kneser-Ney smoothed language model BIBREF11 and a set of skip-gram based word embeddings with 256 dimensions BIBREF12 , are trained on a subset of the News Shuffle corpus containing approx. 200M sentences and INLINEFORM1 unique tokens BIBREF13 .",Hand-crafted Features,"We extract 5 Surface and Lexical features, namely sequence length in number of tokens, average word length, type-token ratio, and lexical to tokens ratio (ratio of adjectives, verbs, nouns, and adverbs to tokens). Bag of INLINEFORM0 -grams features are extracted on the word and POS level. We use frequency cut-offs of INLINEFORM1 for INLINEFORM2 -grams from 1 to 5 respectively for the smaller datasets and ten times higher for the Yahoo! and Amazon datasets. For POS INLINEFORM3 -grams we use cut-offs 10 for unigrams and 20 for bigrams and higher. For the Yahoo! and Amazon datasets we use cut-offs of INLINEFORM4 . The INLINEFORM5 -grams features are then also extracted using the hashing trick with the same cut-offs to reduce the final feature vector size when combined with other features. scikit-learn's BIBREF14 FeatureHasher is used with output vectors sizes of INLINEFORM6 INLINEFORM7 INLINEFORM8 for ngrams from INLINEFORM9 respectively and INLINEFORM10 INLINEFORM11 INLINEFORM12 are used for POS ngrams. We extract lexical and POS level Language model features based on external language models, namely sentence log probabilities, perplexities, and surprisal in units of bits. Building the language model and extracting the features is done by providing the path to the compiled binaries for kenlm BIBREF15 . Finally we extract N-gram Frequency Quantile Distribution features with the same cut-offs as in the bag of ngrams features, with 4 quantiles and an OOV quantile. NLTK BIBREF16 is used for tokenization and POS tagging.",Learned Features,"We extracted two features that use a learned representation: Firstly, we get a sentence embedding feature that is built by averaging the word embeddings of an input sentence. Secondly, we extract a fastText representation using the fastText library with the same parameters as reported in Joulin et al. joulin2016bag.",Classifiers,"The linear SVC from scikit-learn BIBREF14 which is based on LIBLINEAR BIBREF17 is trained as a baseline for evaluating each feature type as well as the concatenated features. A grid search for INLINEFORM0 is performed with 10 values in the log scale ranging from INLINEFORM1 to INLINEFORM2 . Performance is then also compared to feeding the concatenated features into a feed-forward neural network. We report the results on two settings, a network with a single fully-connected hidden layer of size 100 and another network with two fully-connected hidden layers of sizes 100 and 50 respectively. Both networks use a softmax output layer. The implementation is done using Keras BIBREF18 with the TensorFlow BIBREF19 backend. The two smaller datasets and the Amazon datasets are trained for 2 epochs and the remaining datasets are trained for 5 epochs. We use with the Adam optimizer with a learning rate of INLINEFORM3 , and dropout with rate INLINEFORM4 . A single NVIDIA Titan X GPU is used for all experiments and time per epoch ranges from a few seconds for a small number of features on the smallest datasets to 5 hours on the full feature set on the largest datasets. These settings were not chosen to optimize accuracy, but only for the purpose of evaluating the framework due to the large number of experiments presented. Users are encouraged to experiment with different hyper-parameters values and network sizes, as well as modify the code to build more sophisticated neural network models. Experimenting with the other classifiers available in the framework, such as logistic regression, can provide additional insightful comparisons.",Results and Discussion,"We present the results in Table TABREF20 . In Zhang et al. zhang2015character it was noted that the performance of ngram features degrades for larger datasets. However, we have seen in our baseline experiments that this effect can be reduced by using suitable frequency cut-offs. We have also seen that in many cases, the ngram features can solely outperform the neural approaches. For the two smaller datasets, linear classifiers tend to perform better, while for the larger datasets performance increases with increasing the non-linear layers even for hand-crafted representations. Combining hand-crafted and learned features is often beneficial, but not always, especially with the linear classifier. What is clear is that different datasets benefit from different representations and model parameters and it is difficult to find a representation that consistently performs well across all datasets. This necessitates repeated experimentation to understand which approaches and parameters would provide more consistent improvements.",Related Work,"While there exist toolkits such as FEXTOR BIBREF20 , EDISON BIBREF21 , Learning Based Java BIBREF22 , and NLP frameworks such as GATE BIBREF23 that facilitate feature extraction, INFODENS differs in that it integrates feature learning in the extraction pipeline along with customizable feature evaluation. Additionally, a main design goal of INFODENS is to require little to no programming experience to be used as a standalone toolkit, and minimal programming effort to develop new features and classifiers. This is accomplished as the framework is developed fully in Python, taking advantage of the plethora of libraries available for deep learning and natural language processing. And due to the interpreted nature of Python, extensions to the library require no recompilation and, by design, are discovered dynamically at runtime.",Conclusions and Future work,"We presented INFODENS, a framework aimed at learning text representations and showed how combining hand-crafted and learned representations can be beneficial. The framework provides flexible usage and extension scenarios enabling rapid evaluation of different text representations on different tasks. We aim to integrate more learned representations of text, namely convolutional features, and additionally, the next iteration of the framework will focus on allowing features to be combined differently, for example to be fed into different neural network layers, such as to an embedding or a convolutional layer instead of vanilla fully connected layers. Finally, a module to visualize the learned feature weights will be developed in order to understand which combination of features lead to a better classification decision.",Acknowledgments,This work is funded by the German Research Foundation (Deutsche Forschungsgemeinschaft) under grant SFB1102: Information Density and Linguistic Encoding.,,,,,,,,,,,Do they differentiate insights where they are dealing with learned or engineered representations?,d13efa7dee280c7c2f6dc451c4fbbf0240fc2efa,infinity,unfamiliar,no,,2cfd959e433f290bb50b55722370f0d22fe090b7,False,True,,"We extract 5 Surface and Lexical features, namely sequence length in number of tokens, average word length, type-token ratio, and lexical to tokens ratio (ratio of adjectives, verbs, nouns, and adverbs to tokens). Bag of INLINEFORM0 -grams features are extracted on the word and POS level. We use frequency cut-offs of INLINEFORM1 for INLINEFORM2 -grams from 1 to 5 respectively for the smaller datasets and ten times higher for the Yahoo! and Amazon datasets. For POS INLINEFORM3 -grams we use cut-offs 10 for unigrams and 20 for bigrams and higher. For the Yahoo! and Amazon datasets we use cut-offs of INLINEFORM4 . The INLINEFORM5 -grams features are then also extracted using the hashing trick with the same cut-offs to reduce the final feature vector size when combined with other features. scikit-learn's BIBREF14 FeatureHasher is used with output vectors sizes of INLINEFORM6 INLINEFORM7 INLINEFORM8 for ngrams from INLINEFORM9 respectively and INLINEFORM10 INLINEFORM11 INLINEFORM12 are used for POS ngrams. We extract lexical and POS level Language model features based on external language models, namely sentence log probabilities, perplexities, and surprisal in units of bits. Building the language model and extracting the features is done by providing the path to the compiled binaries for kenlm BIBREF15 . Finally we extract N-gram Frequency Quantile Distribution features with the same cut-offs as in the bag of ngrams features, with 4 quantiles and an OOV quantile. NLTK BIBREF16 is used for tokenization and POS tagging. We extracted two features that use a learned representation: Firstly, we get a sentence embedding feature that is built by averaging the word embeddings of an input sentence. Secondly, we extract a fastText representation using the fastText library with the same parameters as reported in Joulin et al. joulin2016bag.","We extract 5 Surface and Lexical features, namely sequence length in number of tokens, average word length, type-token ratio, and lexical to tokens ratio (ratio of adjectives, verbs, nouns, and adverbs to tokens). We extracted two features that use a learned representation: Firstly, we get a sentence embedding feature that is built by averaging the word embeddings of an input sentence. Secondly, we extract a fastText representation using the fastText library with the same parameters as reported in Joulin et al. joulin2016bag.",98b948dd21528345e257396586f0a2ae495232c0,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,Do they show an example of usage for INFODENS?,6cd01609c8afb425fbed941441a2528123352940,infinity,unfamiliar,no,,2cfd959e433f290bb50b55722370f0d22fe090b7,False,True,,"The framework can be used as a standalone toolkit without any modifications given the implemented features and classifiers. For example, it can be used to extract features for usage with other machine learning tools, or to evaluate given features with the existing classifiers or regressors. Extending the framework with new feature extractors or classifiers is as simple as a drag and drop placement of the new code files into the feature_extractor and classifer directories respectively. The framework will then detect the new extensions dynamically at runtime. In this section we explore how each use case is handled. Since a main use case for the framework is extracting engineered and learned features, it was designed such that developing a new feature extractor would require minimal effort. Figure FIGREF19 demonstrates a simple feature extractor that retrieves the sentence length. More complicated features and learned features are provided in the repository which can be used as a guide for developers. Documentation for adding classifiers and format writers is described in the Wiki of the repository but is left out of this paper due to the limited space.","For example, it can be used to extract features for usage with other machine learning tools, or to evaluate given features with the existing classifiers or regressors. Figure FIGREF19 demonstrates a simple feature extractor that retrieves the sentence length.",c38d3b4d2ae1a03cc49d778e5102ae370e0c4062,258ee4069f740c400c0049a2580945a1cc7f044c,What kind of representation exploration does INFODENS provide?,7a70fb11cb3449749f0c2c06101965bf5d02c54a,infinity,unfamiliar,no,,2cfd959e433f290bb50b55722370f0d22fe090b7,True,,,,,6016a9cd561bc440d4719bbd30e5417d87027640,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2-Figure1-1.png,Figure 1: Overview of the framework’s architecture.,3-Figure2-1.png,Figure 2: Configuration file with all the sections and some parameters,4-Figure3-1.png,"Figure 3: Example feature extractor. A feature class that inherits from Feature extractor is defined which can then contain multiple feature extractors. A feature extractor is a method within that class with a decorator @featid that assigns it a unique numeric ID. The extractor is given 2 parameters, a string for arguments passed to it in the configuration file, and a flag for a preprocessing run. The extractor must first check if this call to it is a preprocessor call (preprocessReq is True), which is used to gather the different preprocessing requests from all extractors, so as not to repeat the requests. It then returns. When the preprocessing request is false, the extractor must then proceed to call the preprocessor APIs and retrieve the required train and test data to fill in and return 2 SciPy sparse matrices with sizes (n1, x) and (n2, x) where x is the length of the feature vector, and n1 and n2 are the number of sentences in the train and test sets respectively. The extractor also returns as a third parameter a string describing the feature vector.",5-Table1-1.png,Table 1: Accuracy [%] results on the test sets.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
What we write about when we write about causality: Features of causal statements across large-scale social discourse,"Identifying and communicating relationships between causes and effects is important for understanding our world, but is affected by language structure, cognitive and emotional biases, and the properties of the communication medium. Despite the increasing importance of social media, much remains unknown about causal statements made online. To study real-world causal attribution, we extract a large-scale corpus of causal statements made on the Twitter social network platform as well as a comparable random control corpus. We compare causal and control statements using statistical language and sentiment analysis tools. We find that causal statements have a number of significant lexical and grammatical differences compared with controls and tend to be more negative in sentiment than controls. Causal statements made online tend to focus on news and current events, medicine and health, or interpersonal relationships, as shown by topic models. By quantifying the features and potential biases of causality communication, this study improves our understanding of the accuracy of information and opinions found online.",Introduction,"Social media and online social networks now provide vast amounts of data on human online discourse and other activities BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . With so much communication taking place online and with social media being capable of hosting powerful misinformation campaigns BIBREF7 such as those claiming vaccines cause autism BIBREF8 , BIBREF9 , it is more important than ever to better understand the discourse of causality and the interplay between online communication and the statement of cause and effect. Causal inference is a crucial way that humans comprehend the world, and it has been a major focus of philosophy, statistics, mathematics, psychology, and the cognitive sciences. Philosophers such as Hume and Kant have long argued whether causality is a human-centric illusion or the discovery of a priori truth BIBREF10 , BIBREF11 . Causal inference in science is incredibly important, and researchers have developed statistical measures such as Granger causality BIBREF12 , mathematical and probabilistic frameworks BIBREF13 , BIBREF14 , BIBREF15 , BIBREF16 , and text mining procedures BIBREF17 , BIBREF18 , BIBREF19 to better infer causal influence from data. In the cognitive sciences, the famous perception experiments of Michotte et al. led to a long line of research exploring the cognitive biases that humans possess when attempting to link cause and effect BIBREF20 , BIBREF21 , BIBREF22 . How humans understand and communicate cause and effect relationships is complicated, and is influenced by language structure BIBREF23 , BIBREF24 , BIBREF25 , BIBREF26 and sentiment or valence BIBREF27 . A key finding is that the perceived emphasis or causal weight changes between the agent (the grammatical construct responsible for a cause) and the patient (the construct effected by the cause) depending on the types of verbs used to describe the cause and effect. Researchers have hypothesized BIBREF28 that this is because of the innate weighting property of the verbs in the English language that humans use to attribute causes and effects. Another finding is the role of a valence bias: the volume and intensity of causal reasoning may increase due to negative feedback or negative events BIBREF27 . Despite these long lines of research, causal attributions made via social media or online social networks have not been well studied. The goal of this paper is to explore the language and topics of causal statements in a large corpus of social media taken from Twitter. We hypothesize that language and sentiment biases play a significant role in these statements, and that tools from natural language processing and computational linguistics can be used to study them. We do not attempt to study the factual correctness of these statements or offer any degree of verification, nor do we exhaustively identify and extract all causal statements from these data. Instead, here we focus on statements that are with high certainty causal statements, with the goal to better understand key characteristics about causal statements that differ from everyday online communication. The rest of this paper is organized as follows: In Sec. ""Materials and Methods"" we discuss our materials and methods, including the dataset we studied, how we preprocessed that data and extracted a `causal' corpus and a corresponding `control' corpus, and the details of the statistical and language analysis tools we studied these corpora with. In Sec. ""Results"" we present results using these tools to compare the causal statements to control statements. We conclude with a discussion in Sec. ""Discussion"" .","Dataset, filtering, and corpus selection","Data was collected from a 10% uniform sample of Twitter posts made during 2013, specifically the Gardenhose API. Twitter activity consists of short posts called tweets which are limited to 140 characters. Retweets, where users repost a tweet to spread its content, were not considered. (The spread of causal statements will be considered in future work.) We considered only English-language tweets for this study. To avoid cross-language effects, we kept only tweets with a user-reported language of `English' and, as a second constraint, individual tweets needed to match more English stopwords than any other language's set of stopwords. Stopwords considered for each language were determined using NLTK's database BIBREF29 . A tweet will be referred to as a `document' for the rest of this work. All document text was processed the same way. Punctuation, XML characters, and hyperlinks were removed, as were Twitter-specific “at-mentions” and “hashtags” (see also the Appendix). There is useful information here, but it is either not natural language text, or it is Twitter-specific, or both. Documents were broken into individual words (unigrams) on whitespace. Casing information was retained, as we will use it for our Named Entity analysis, but otherwise all words were considered lowercase only (see also the Appendix). Stemming BIBREF30 and lemmatization BIBREF31 were not performed. Causal documents were chosen to contain one occurrence only of the exact unigrams: `caused', `causing', or `causes'. The word `cause' was not included due to its use as a popular contraction for `because'. One `cause-word' per document restricted the analysis to single relationships between two relata. Documents that contain bidirectional words (`associate', `relate', `connect', `correlate', and any of their stems) were also not selected for analysis. This is because our focus is on causality, an inherently one-sided relationship between two objects. We also did not consider additional synonyms of these cause words, although that could be pursued for future work. Control documents were also selected. These documents did not contain any of `caused', `causing', or `causes', nor any bidirectional words, and are further matched temporally to obtain the same number of control documents as causal documents in each fifteen-minute period during 2013. Control documents were otherwise selected randomly; causal synonyms may be present. The end result of this procedure identified 965,560 causal and 965,560 control documents. Each of the three “cause-words”, `caused', `causes', and `causing' appeared in 38.2%, 35.0%, and 26.8% of causal documents, respectively.",Tagging and corpus comparison,"Documents were further studied by annotating their unigrams with Parts-of-Speech (POS) and Named Entities (NE) tags. POS tagging was done using NLTK v3.1 BIBREF29 which implements an averaged perceptron classifier BIBREF32 trained on the Brown Corpus BIBREF33 . (POS tagging is affected by punctuation; we show in the Appendix that our results are relatively robust to the removal of punctuation.) POS tags denote the nouns, verbs, and other grammatical constructs present in a document. Named Entity Recognition (NER) was performed using the 4-class, distributional similarity tagger provided as part of the Stanford CoreNLP v3.6.0 toolkit BIBREF34 . NER aims to identify and classify proper words in a text. The NE classifications considered were: Organization, Location, Person, and Misc. The Stanford NER tagger uses a conditional random field model BIBREF35 trained on diverse sets of manually-tagged English-language data (CoNLL-2003) BIBREF34 . Conditional random fields allow dependencies between words so that `New York' and `New York Times', for example, are classified separately as a location and organization, respectively. These taggers are commonly used and often provide reasonably accurate results, but there is always potential ambiguity in written text and improving upon these methods remains an active area of research. Unigrams, POS, and NEs were compared between the cause and control corpora using odds ratios (ORs):  $$\operatorname{OR}(x) = \frac{p_C(x)/ (1-p_C(x))}{p_N(x) / (1-p_N(x))},$$   (Eq. 1)   where $p_C(x)$ and $p_N(x)$ are the probabilities that a unigram, POS, or NE $x$ occurs in the causal and control corpus, respectively. These probabilities were computed for each corpus separately as $p(x) = f(x) / \sum _{x^{\prime } \in V} f(x^{\prime })$ , where $f(x)$ is the total number of occurrences of $x$ in the corpus and $V$ is the relevant set of unigrams, POS, or NEs. Confidence intervals for the ORs were computed using Wald's methodology BIBREF36 . As there are many unique unigrams in the text, when computing unigram ORs we focused on the most meaningful unigrams within each corpus by using the following filtering criteria: we considered only the $\operatorname{OR}$ s of the 1500 most frequent unigrams in that corpus that also have a term-frequency-inverse-document-frequency (tf-idf) score above the 90th percentile for that corpus BIBREF37 . The tf-idf was computed as  $$\mbox{tf-idf}(w) = \log f(w) \times \log \left(D̑{\mathit {df}(w)} \right) ,$$   (Eq. 2)  where $D$ is the total number of documents in the corpus, and $\mathit {df}(w)$ is the number of documents in the corpus containing unigram $w$ . Intuitively, unigrams with higher tf-idf scores appear frequently, but are not so frequent that they are ubiquitous through all documents. Filtering via tf-idf is standard practice in the information retrieval and data mining fields.",Cause-trees,"For a better understanding of the higher-order language structure present in text phrases, cause-trees were constructed. A cause-tree starts with a root cause word (either `caused', `causing' or `causes'), then the two most probable words following (preceding) the root are identified. Next, the root word plus one of the top probable words is combined into a bigram and the top two most probable words following (preceding) this bigram are found. Repeatedly applying this process builds a binary tree representing the $n$ -grams that begin with (terminate at) the root word. This process can continue until a certain $n$ -gram length is reached or until there are no more documents long enough to search.",Sentiment analysis,"Sentimental analysis was applied to estimate the emotional content of documents. Two levels of analysis were used: a method where individual unigrams were given crowdsourced numeric sentiment scores, and a second method involving a trained classifier that can incorporate document-level phrase information. For the first sentiment analysis, each unigram $w$ was assigned a crowdsourced “labMT” sentiment score $s(w)$ BIBREF5 . (Unlike BIBREF5 , scores were recentered by subtracting the mean, $s(w) \leftarrow s(w)-\left<s\right>$ .) Unigrams determined by volunteer raters to have a negative emotional sentiment (`hate',`death', etc.) have $s(w) < 0$ , while unigrams determined to have a positive emotional sentiment (`love', `happy', etc.) tend to have $s(w) > 0$ . Unigrams that have labMT scores and are above the 90th percentile of tf-idf for the corpus form the set $\tilde{V}$ . (Unigrams in $\tilde{V}$ need not be among the 1500 most frequent unigrams.) The set $\tilde{V}$ captures 87.9% (91.5%) of total unigrams in the causal (control) corpus. Crucially, the tf-idf filtering ensures that the words `caused', `causes', and `causing', which have a slight negative sentiment, are not included and do not introduce a systematic bias when comparing the two corpora. This sentiment measure works on a per-unigram basis, and is therefore best suited for large bodies of text, not short documents BIBREF5 . Instead of considering individual documents, the distributions of labMT scores over all unigrams for each corpus was used to compare the corpora. In addition, a single sentiment score for each corpus was computed as the average sentiment score over all unigrams in that corpus, weighed by unigram frequency: $\sum _{w \in \tilde{V}} {f(w) s(w)} \Big / \sum _{w^{\prime } \in \tilde{V}} f(w^{\prime })$ . To supplement this sentiment analysis method, we applied a second method capable of estimating with reasonable accuracy the sentiment of individual documents. We used the sentiment classifier BIBREF38 included in the Stanford CoreNLP v3.6.0 toolkit to documents in each corpus. Documents were individually classified into one of five categories: very negative, negative, neutral, positive, very positive. The data used to train this classifier is taken from positive and negative reviews of movies (Stanford Sentiment Treebank v1.0) BIBREF38 .",Topic modeling,"Lastly, we applied topic modeling to the causal corpus to determine what are the topical foci most discussed in causal statements. Topics were built from the causal corpus using Latent Dirichlet Allocation (LDA) BIBREF39 . Under LDA each document is modeled as a bag-of-words or unordered collection of unigrams. Topics are considered as mixtures of unigrams by estimating conditional distributions over unigrams: $P(w|T)$ , the probability of unigram $w$ given topic $T$ and documents are considered as mixtures of topics via $P(T|d)$ , the probability of topic $T$ given document $d$ . These distributions are then found via statistical inference given the observed distributions of unigrams across documents. The total number of topics is a parameter chosen by the practitioner. For this study we used the MALLET v2.0.8RC3 topic modeling toolkit BIBREF40 for model inference. By inspecting the most probable unigrams per topic (according to $P(w|T)$ ), we found 10 topics provided meaningful and distinct topics.",Results,"We have collected approximately 1M causal statements made on Twitter over the course of 2013, and for a control we gathered the same number of statements selected at random but controlling for time of year (see Methods). We applied Parts-of-Speech (POS) and Named Entity (NE) taggers to all these texts. Some post-processed and tagged example documents, both causal and control, are shown in Fig. 1 A. We also applied sentiment analysis methods to these documents (Methods) and we have highlighted very positive and very negative words throughout Fig. 1 . In Fig. 1 B we present odds ratios for how frequently unigrams (words), POS, or NE appear in causal documents relative to control documents. The three unigrams most strongly skewed towards causal documents were `stress', `problems', and `trouble', while the three most skewed towards control documents were `photo', `ready', and `cute'. While these are only a small number of the unigrams present, this does imply a negative sentiment bias among causal statements (we return to this point shortly). Figure 1 B also presents odds ratios for POS tags, to help us measure the differences in grammatical structure between causal and control documents (see also the Appendix for the effects of punctuation and casing on these odds ratios). The causal corpus showed greater odds for plural nouns (Penn Treebank tag: NNS), plural proper nouns (NNPS), Wh-determiners/pronouns (WDT, WP$) such as `whichever',`whatever', `whose', or `whosever', and predeterminers (PDT) such as `all' or `both'. Predeterminers quantify noun phrases such as `all' in `after all the events that caused you tears', showing that many causal statements, despite the potential brevity of social media, can encompass or delineate classes of agents and/or patients. On the other hand, the causal corpus has lower odds than the control corpus for list items (LS), proper singular nouns (NNP), and interjections (UH). Lastly, Fig. 1 B contains odds ratios for NE tags, allowing us to quantify the types of proper nouns that are more or less likely to appear in causal statements. Of the four tags, only the “Person” tag is less likely in the causal corpus than the control. (This matches the odds ratio for the proper singular noun discussed above.) Perhaps surprisingly, these results together imply that causal statements are less likely to involve individual persons than non-causal statements. There is considerable celebrity news and gossip on social media BIBREF4 ; discussions of celebrities may not be especially focused on attributing causes to these celebrities. All other NE tags, Organization, Location, and Miscellaneous, occur more frequently in the causal corpus than the control. All the odds ratios in Fig. 1 B were significant at the $\alpha = 0.05$ level except the List item marker (LS) POS tag. The unigram analysis in Fig. 1 does not incorporate higher-order phrase structure present in written language. To explore these structures specifically in the causal corpus, we constructed “cause-trees”, shown in Fig. 2 . Inspired by association mining BIBREF41 , a cause-tree is a binary tree rooted at either `caused', `causes', or `causing', that illustrates the most frequently occurring $n$ -grams that either begin or end with that root cause word (see Methods for details). The “causes” tree shows the focused writing (sentence segments) that many people use to express either the relationship between their own actions and a cause-and-effect (“even if it causes”), or the uncontrollable effect a cause may have on themselves: “causes me to have” shows a person's inability to control a causal event (“[...] i have central heterochromia which causes me to have dual colors in both eyes”). The `causing' tree reveals our ability to confine causal patterns to specific areas, and also our ability to be affected by others causal decisions. Phrases like “causing a scene in/at” and “causing a ruckus in/at” (from documents like “causing a ruckus in the hotel lobby typical [...]”) show people commonly associate bounds on where causal actions take place. The causing tree also shows people's tendency to emphasize current negativity: Phrases like “pain this is causing” coming from documents like “cant you see the pain you are causing her” supports the sentiment bias that causal attribution is more likely for negative cause-effect associations. Finally, the `caused' tree focuses heavily on negative events and indicates people are more likely to remember negative causal events. Documents with phrases from the caused tree (“[...] appalling tragedy [...] that caused the death”, “[...] live with this pain that you caused when i was so young [...]”) exemplify the negative events that are focused on are large-scale tragedies or very personal negative events in one's life. Taken together, the popularity of negative sentiment unigrams (Fig. 1 ) and $n$ -grams (Fig. 2 ) among causal documents shows that emotional sentiment or “valence” may play a role in how people perform causal attribution BIBREF27 . The “if it bleeds, it leads” mentality among news media, where violent and negative news are more heavily reported, may appeal to this innate causal association mechanism. (On the other hand, many news media themselves use social media for reporting.) The prevalence of negative sentiment also contrasts with the “better angels of our nature” evidence of Pinker BIBREF42 , illustrating one bias that shows why many find the results of Ref. BIBREF42 surprising. Given this apparent sentiment skew, we further studied sentiment (Fig. 3 ). We compared the sentiment between the corpora in four different ways to investigate the observation (Figs. 1 B and 2 ) that people focus more about negative concepts when they discuss causality. First, we computed the mean sentiment score of each corpus using crowdsourced “labMT” scores weighted by unigram frequency (see Methods). We also applied tf-idf filtering (Methods) to exclude very common words, including the three cause-words, from the mean sentiment score. The causal corpus text was slightly negative on average while the control corpus was slightly positive (Fig. 3 A). The difference in mean sentiment score was significant (t-test: $p < 0.01$ ). Second, we moved from the mean score to the distribution of sentiment across all (scored) unigrams in the causal and control corpora (Fig. 3 B). The causal corpus contained a large group of negative sentiment unigrams, with labMT scores in the approximate range $-3 < s < -1/2$ ; the control corpus had significantly fewer unigrams in this score range. Third, in Fig. 3 C we used POS tags to categorize scored unigrams into nouns, verbs, and adjectives. Studying the distributions for each, we found that nouns explain much of the overall difference observed in Fig. 3 B, with verbs showing a similar but smaller difference between the two corpora. Adjectives showed little difference. The distributions in Fig. 3 C account for 87.8% of scored text in the causal corpus and 77.2% of the control corpus. The difference in sentiment between corpora was significant for all distributions (t-test: $p < 0.01$ ). Fourth, to further confirm that the causal documents tend toward negative sentiment, we applied a separate, independent sentiment analysis using the Stanford NLP sentiment toolkit BIBREF38 to classify the sentiment of individual documents not unigrams (see Methods). Instead of a numeric sentiment score, this classifier assigns documents to one of five categories ranging from very negative to very positive. The classifier showed that the causal corpus contains more negative and very negative documents than the control corpus, while the control corpus contains more neutral, positive, and very positive documents (Fig. 3 D). We have found language (Figs. 1 and 2 ) and sentiment (Fig. 3 ) differences between causal statements made on social media compared with other social media statements. But what is being discussed? What are the topical foci of causal statements? To study this, for our last analysis we applied topic models to the causal statements. Topic modeling finds groups of related terms (unigrams) by considering similarities between how those terms co-occur across a set of documents. We used the popular topic modeling method Latent Dirichlet Allocation (LDA) BIBREF39 . We ranked unigrams by how strongly associated they were with the topic. Inspecting these unigrams we found that a 10-topic model discovered meaningful topics. See Methods for full details. The top unigrams for each topic are shown in Tab. 1 . Topics in the causal corpus tend to fall into three main categories: (i) news, covering current events, weather, etc.; (ii) medicine and health, covering cancer, obesity, stress, etc.; and (iii) relationships, covering problems, stress, crisis, drama, sorry, etc. While the topics are quite different, they are all similar in their use of negative sentiment words. The negative/global features in the `news' topic are captured in the most representative words: damage, fire, power, etc. Similar to news, the `accident' topic balances the more frequent day-to-day minor frustrations with the less frequent but more severe impacts of car accidents. The words `traffic' and `delays' are the most probable words for this topic, and are common, low-impact occurrences. On the contrary, `crash', `car', `accident' and `death' are the next most probable words for the accident topic, and generally show a focus on less-common but higher-impact events. The `medical' topic also focused on negative words; highly probable words for this topic included `cancer', `break', `disease', `blood', etc. Meanwhile, the `body' topic contained words like: `stress', `lose', and `weight', giving a focus on on our more personal struggles with body image. Besides body image, the `injuries' topic uses specific pronouns (`his', `him', `her') in references to a person's own injuries or the injuries of others such as athletes. Aside from more factual information, social information is well represented in causal statements. The `problems' topic shows people attribute their problems to many others with terms like: `dont', `people', `they', `them'. The `stress' topic also uses general words such as `more', `than', or `people' to link stress to all people, and in the same vein, the `crisis' topic focuses on problems within organizations such as governments. The `drama' and `sorry' topics tend towards more specific causal statements. Drama used the words: `like', `she', and `her' while documents in the sorry topic tended to address other people. The topics of causal documents discovered by LDA showed that both general and specific statements are made regarding news, medicine, and relationships when individuals make causal attributions online.",Discussion,"The power of online communication is the speed and ease with which information can be propagated by potentially any connected users. Yet these strengths come at a cost: rumors and misinformation also spread easily. Causal misattribution is at the heart of many rumors, conspiracy theories, and misinformation campaigns. Given the central role of causal statements, further studies of the interplay of information propagation and online causal attributions are crucial. Are causal statements more likely to spread online and, if so, in which ways? What types of social media users are more or less likely to make causal statements? Will a user be more likely to make a causal statement if they have recently been exposed to one or more causal statements from other users? The topics of causal statements also bring forth important questions to be addressed: how timely are causal statements? Are certain topics always being discussed in causal statements? Are there causal topics that are very popular for only brief periods and then forgotten? Temporal dynamics of causal statements are also interesting: do time-of-day or time-of-year factors play a role in how causal statements are made? Our work here focused on a limited subset of causal statements, but more generally, these results may inform new methods for automatically detecting causal statements from unstructured, natural language text BIBREF17 . Better computational tools focused on causal statements are an important step towards further understanding misinformation campaigns and other online activities. Lastly, an important but deeply challenging open question is how, if it is even possible, to validate the accuracy of causal statements. Can causal statements be ranked by some confidence metric(s)? We hope to pursue these and other questions in future research. Parts-of-speech tagging depends on punctuation and casing, which we filtered in our data, so a study of how robust the POS algorithm is to punctuation and casing removal is important. We computed POS tags for the corpora with and without casing as well as with and without punctuation (which includes hashtags, links and at-symbols). Two tags mentioned in Fig. 1 B, NNPS and LS (which was not significant), were affected by punctuation removal. Otherwise, there is a strong correlation (Fig. 4 ) between Odds Ratios (causal vs. control) with punctuation and without punctuation, including casing and without casing ( $\rho = 0.71$ and $0.80$ , respectively), indicating the POS differences between the corpora were primarily not due to the removal of punctuation or casing.",Acknowledgments,We thank R. Gallagher for useful comments and gratefully acknowledge the resources provided by the Vermont Advanced Computing Core. This material is based upon work supported by the National Science Foundation under Grant No. ISS-1447634.,,,,,,,,,,,,,,,,,,,,,,,,,How do they extract causality from text?,4c822bbb06141433d04bbc472f08c48bc8378865,infinity,familiar,no,social,50d8b4a941c26b89482c94ab324b5a274f9ced66,False,,"They identify documents that contain the unigrams 'caused', 'causing', or 'causes'","Causal documents were chosen to contain one occurrence only of the exact unigrams: `caused', `causing', or `causes'. The word `cause' was not included due to its use as a popular contraction for `because'. One `cause-word' per document restricted the analysis to single relationships between two relata. Documents that contain bidirectional words (`associate', `relate', `connect', `correlate', and any of their stems) were also not selected for analysis. This is because our focus is on causality, an inherently one-sided relationship between two objects. We also did not consider additional synonyms of these cause words, although that could be pursued for future work. Control documents were also selected. These documents did not contain any of `caused', `causing', or `causes', nor any bidirectional words, and are further matched temporally to obtain the same number of control documents as causal documents in each fifteen-minute period during 2013. Control documents were otherwise selected randomly; causal synonyms may be present. The end result of this procedure identified 965,560 causal and 965,560 control documents. Each of the three “cause-words”, `caused', `causes', and `causing' appeared in 38.2%, 35.0%, and 26.8% of causal documents, respectively.","Causal documents were chosen to contain one occurrence only of the exact unigrams: `caused', `causing', or `causes'.",f286d3a109fe0b38fcee6121e231001a4704e9c8,057bf5a20e4406f1f05cf82ecd49cf4f227dd287,,,,,,,,,"What is the source of the ""control"" corpus?",1baf87437b70cc0375b8b7dc2cfc2830279bc8b5,infinity,familiar,no,social,50d8b4a941c26b89482c94ab324b5a274f9ced66,False,,"Randomly selected from a Twitter dump, temporally matched to causal documents","Data was collected from a 10% uniform sample of Twitter posts made during 2013, specifically the Gardenhose API. Twitter activity consists of short posts called tweets which are limited to 140 characters. Retweets, where users repost a tweet to spread its content, were not considered. (The spread of causal statements will be considered in future work.) We considered only English-language tweets for this study. To avoid cross-language effects, we kept only tweets with a user-reported language of `English' and, as a second constraint, individual tweets needed to match more English stopwords than any other language's set of stopwords. Stopwords considered for each language were determined using NLTK's database BIBREF29 . A tweet will be referred to as a `document' for the rest of this work. Causal documents were chosen to contain one occurrence only of the exact unigrams: `caused', `causing', or `causes'. The word `cause' was not included due to its use as a popular contraction for `because'. One `cause-word' per document restricted the analysis to single relationships between two relata. Documents that contain bidirectional words (`associate', `relate', `connect', `correlate', and any of their stems) were also not selected for analysis. This is because our focus is on causality, an inherently one-sided relationship between two objects. We also did not consider additional synonyms of these cause words, although that could be pursued for future work. Control documents were also selected. These documents did not contain any of `caused', `causing', or `causes', nor any bidirectional words, and are further matched temporally to obtain the same number of control documents as causal documents in each fifteen-minute period during 2013. Control documents were otherwise selected randomly; causal synonyms may be present. The end result of this procedure identified 965,560 causal and 965,560 control documents. Each of the three “cause-words”, `caused', `causes', and `causing' appeared in 38.2%, 35.0%, and 26.8% of causal documents, respectively.","Data was collected from a 10% uniform sample of Twitter posts made during 2013, specifically the Gardenhose API. Control documents were also selected. These documents did not contain any of `caused', `causing', or `causes', nor any bidirectional words, and are further matched temporally to obtain the same number of control documents as causal documents in each fifteen-minute period during 2013. Control documents were otherwise selected randomly; causal synonyms may be present.",b2733052258dc2ad74edbb76c3f152740e30bdbc,057bf5a20e4406f1f05cf82ecd49cf4f227dd287,"What are the selection criteria for ""causal statements""?",0b31eb5bb111770a3aaf8a3931d8613e578e07a8,infinity,familiar,no,social,50d8b4a941c26b89482c94ab324b5a274f9ced66,False,,"Presence of only the exact unigrams 'caused', 'causing', or 'causes'","Causal documents were chosen to contain one occurrence only of the exact unigrams: `caused', `causing', or `causes'. The word `cause' was not included due to its use as a popular contraction for `because'. One `cause-word' per document restricted the analysis to single relationships between two relata. Documents that contain bidirectional words (`associate', `relate', `connect', `correlate', and any of their stems) were also not selected for analysis. This is because our focus is on causality, an inherently one-sided relationship between two objects. We also did not consider additional synonyms of these cause words, although that could be pursued for future work. Control documents were also selected. These documents did not contain any of `caused', `causing', or `causes', nor any bidirectional words, and are further matched temporally to obtain the same number of control documents as causal documents in each fifteen-minute period during 2013. Control documents were otherwise selected randomly; causal synonyms may be present. The end result of this procedure identified 965,560 causal and 965,560 control documents. Each of the three “cause-words”, `caused', `causes', and `causing' appeared in 38.2%, 35.0%, and 26.8% of causal documents, respectively.","Causal documents were chosen to contain one occurrence only of the exact unigrams: `caused', `causing', or `causes'.",ae22aca6f06a3c10293e77feb2defd1a052ebf47,057bf5a20e4406f1f05cf82ecd49cf4f227dd287,,,,,,,,"Do they use expert annotations, crowdsourcing, or only automatic methods to analyze the corpora?",7348e781b2c3755b33df33f4f0cab4b94fcbeb9b,infinity,familiar,no,social,50d8b4a941c26b89482c94ab324b5a274f9ced66,False,,Only automatic methods,"The rest of this paper is organized as follows: In Sec. ""Materials and Methods"" we discuss our materials and methods, including the dataset we studied, how we preprocessed that data and extracted a `causal' corpus and a corresponding `control' corpus, and the details of the statistical and language analysis tools we studied these corpora with. In Sec. ""Results"" we present results using these tools to compare the causal statements to control statements. We conclude with a discussion in Sec. ""Discussion"" .","In Sec. ""Materials and Methods"" we discuss our materials and methods, including the dataset we studied, how we preprocessed that data and extracted a `causal' corpus and a corresponding `control' corpus, and the details of the statistical and language analysis tools we studied these corpora with.",0ce98e42cf869d3feab61c966335792e98d16ad0,057bf5a20e4406f1f05cf82ecd49cf4f227dd287,,,,,,,,how do they collect the comparable corpus?,f68bd65b5251f86e1ed89f0c858a8bb2a02b233a,two,unfamiliar,yes,social,50d8b4a941c26b89482c94ab324b5a274f9ced66,False,,,Randomly from a Twitter dump,"Data was collected from a 10% uniform sample of Twitter posts made during 2013, specifically the Gardenhose API. Twitter activity consists of short posts called tweets which are limited to 140 characters. Retweets, where users repost a tweet to spread its content, were not considered. (The spread of causal statements will be considered in future work.) We considered only English-language tweets for this study. To avoid cross-language effects, we kept only tweets with a user-reported language of `English' and, as a second constraint, individual tweets needed to match more English stopwords than any other language's set of stopwords. Stopwords considered for each language were determined using NLTK's database BIBREF29 . A tweet will be referred to as a `document' for the rest of this work. Causal documents were chosen to contain one occurrence only of the exact unigrams: `caused', `causing', or `causes'. The word `cause' was not included due to its use as a popular contraction for `because'. One `cause-word' per document restricted the analysis to single relationships between two relata. Documents that contain bidirectional words (`associate', `relate', `connect', `correlate', and any of their stems) were also not selected for analysis. This is because our focus is on causality, an inherently one-sided relationship between two objects. We also did not consider additional synonyms of these cause words, although that could be pursued for future work. Control documents were also selected. These documents did not contain any of `caused', `causing', or `causes', nor any bidirectional words, and are further matched temporally to obtain the same number of control documents as causal documents in each fifteen-minute period during 2013. Control documents were otherwise selected randomly; causal synonyms may be present. The end result of this procedure identified 965,560 causal and 965,560 control documents. Each of the three “cause-words”, `caused', `causes', and `causing' appeared in 38.2%, 35.0%, and 26.8% of causal documents, respectively.","Data was collected from a 10% uniform sample of Twitter posts made during 2013, specifically the Gardenhose API. Control documents were also selected. These documents did not contain any of `caused', `causing', or `causes', nor any bidirectional words, and are further matched temporally to obtain the same number of control documents as causal documents in each fifteen-minute period during 2013. Control documents were otherwise selected randomly; causal synonyms may be present.",34a0794200f1e29c3849bfa03a4f6128de26733b,057bf5a20e4406f1f05cf82ecd49cf4f227dd287,How do they collect the control corpus?,e111925a82bad50f8e83da274988b9bea8b90005,two,unfamiliar,yes,social,50d8b4a941c26b89482c94ab324b5a274f9ced66,False,,Randomly from Twitter,"Data was collected from a 10% uniform sample of Twitter posts made during 2013, specifically the Gardenhose API. Twitter activity consists of short posts called tweets which are limited to 140 characters. Retweets, where users repost a tweet to spread its content, were not considered. (The spread of causal statements will be considered in future work.) We considered only English-language tweets for this study. To avoid cross-language effects, we kept only tweets with a user-reported language of `English' and, as a second constraint, individual tweets needed to match more English stopwords than any other language's set of stopwords. Stopwords considered for each language were determined using NLTK's database BIBREF29 . A tweet will be referred to as a `document' for the rest of this work. Causal documents were chosen to contain one occurrence only of the exact unigrams: `caused', `causing', or `causes'. The word `cause' was not included due to its use as a popular contraction for `because'. One `cause-word' per document restricted the analysis to single relationships between two relata. Documents that contain bidirectional words (`associate', `relate', `connect', `correlate', and any of their stems) were also not selected for analysis. This is because our focus is on causality, an inherently one-sided relationship between two objects. We also did not consider additional synonyms of these cause words, although that could be pursued for future work. Control documents were also selected. These documents did not contain any of `caused', `causing', or `causes', nor any bidirectional words, and are further matched temporally to obtain the same number of control documents as causal documents in each fifteen-minute period during 2013. Control documents were otherwise selected randomly; causal synonyms may be present. The end result of this procedure identified 965,560 causal and 965,560 control documents. Each of the three “cause-words”, `caused', `causes', and `causing' appeared in 38.2%, 35.0%, and 26.8% of causal documents, respectively.","Data was collected from a 10% uniform sample of Twitter posts made during 2013, specifically the Gardenhose API. Control documents were also selected. These documents did not contain any of `caused', `causing', or `causes', nor any bidirectional words, and are further matched temporally to obtain the same number of control documents as causal documents in each fifteen-minute period during 2013. Control documents were otherwise selected randomly; causal synonyms may be present. ",d3219ac0de3157cec4bf78b9f020c264071b86a8,057bf5a20e4406f1f05cf82ecd49cf4f227dd287,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4-Figure1-1.png,"Fig. 1. Measuring the differences between causal and control documents. (A) Examples of processed documents tagged by Parts-of-Speech (POS) or Named Entities (NEs). Unigrams highlighted in red (yellow) are in the bottom 10% (top 10%) of the labMT sentiment scores. (B) Log Odds ratios with 95% Wald confidence intervals for the most heavily skewed unigrams, POS, and all NEs between the causal and control corpus. POS tags that are plural and use Wh-pronouns (that, what, which, ...) are more common in the causal corpus, while singular nouns and list items are more common in the controls. Finally, the ‘Person’ tag is the only NE less likely in the causal corpus. Certain unigrams were censored for presentation only, not analysis. All shown odds ratios were significant at the α = 0.05 level except LS (List item markers). See also the Appendix.",5-Figure2-1.png,"Fig. 2. “Cause-trees” containing the most probable n-grams terminating at (left) or beginning with (right) a chosen root cause-word (see Methods). Line widths are log proportional to their corresponding n-gram frequency and bar plots measure the 4-gram per-document rate N(4-gram)/D. Most trees express negative sentiment consistent with the unigram analysis (Fig. 1). The ‘causes’ tree shows (i) people think in terms of causal probability (“you know what causes [. . . ]”), and (ii) people use causal language when they are directly affected or being affected by another (“causes you”, “causes me”). The ‘causing’ tree is more global (“causing a ruckus/scene”) and ego-centric (“pain you are causing”). The ‘caused’ tree focuses on negative sentiment and alludes to humans retaining negative causal thoughts in the past.",6-Figure3-1.png,"Fig. 3. Sentiment analysis revealed differences between the causal and control corpora. (A) The mean unigram sentiment score (see Methods), computed from crowdsourced “labMT” scores [6], was more negative for the causal corpus than for the control. This held whether or not tf-idf filtering was applied. (B) The distribution of unigram sentiment scores for the two corpora showed more negative unigrams (with scores in the approximate range −3 < s < −1/2) in the causal corpus compared with the control corpus. (C) Breaking the sentiment distribution down by Parts-of-Speech, nouns show the most pronounced difference in sentiment between cause and control; verbs and adjectives are also more negative in the causal corpus than the control but with less of a difference than nouns. POS tags corresponding to nouns, verbs, and adjectives together account for 87.8% and 77.2% of the causal and control corpus text, respectively. (D) Applying a different sentiment analysis tool—a trained sentiment classifier [39] that assigns individual documents to one of five categories—the causal corpus had an overabundance of negative sentiment documents and fewer positive sentiment documents than the control. This shift from very positive to very negative documents further supports the tendency for causal statements to be negative.",7-TableI-1.png,"TABLE I TOPICAL FOCI OF CAUSAL DOCUMENTS. EACH COLUMN LISTS THE UNIGRAMS MOST HIGHLY ASSOCIATED (IN DESCENDING ORDER) WITH A TOPIC, COMPUTED FROM A 10-TOPIC LATENT DIRICHLET ALLOCATION MODEL. THE TOPICS GENERALLY FALL INTO THREE BROAD CATEGORIES: NEWS, MEDICINE, AND RELATIONSHIPS. MANY TOPICS PLACE AN EMPHASIS ON NEGATIVE SENTIMENT TERMS. TOPIC NAMES WERE DETERMINED MANUALLY. WORDS ARE HIGHLIGHTED ACCORDING TO SENTIMENT SCORE AS IN FIG. 1.",8-Figure4-1.png,"Fig. 4. Comparison of Odds Ratios for all Parts-of-Speech (POS) tags with punctuation retained and removed for documents with and without casing. Tags Cardinal number (CD), List item marker (LS), and Proper noun plural (NNPS) were most affected by removing punctuation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Regularizing Output Distribution of Abstractive Chinese Social Media Text Summarization for Improved Semantic Consistency,"Abstractive text summarization is a highly difficult problem, and the sequence-to-sequence model has shown success in improving the performance on the task. However, the generated summaries are often inconsistent with the source content in semantics. In such cases, when generating summaries, the model selects semantically unrelated words with respect to the source content as the most probable output. The problem can be attributed to heuristically constructed training data, where summaries can be unrelated to the source content, thus containing semantically unrelated words and spurious word correspondence. In this paper, we propose a regularization approach for the sequence-to-sequence model and make use of what the model has learned to regularize the learning objective to alleviate the effect of the problem. In addition, we propose a practical human evaluation method to address the problem that the existing automatic evaluation method does not evaluate the semantic consistency with the source content properly. Experimental results demonstrate the effectiveness of the proposed approach, which outperforms almost all the existing models. Especially, the proposed approach improves the semantic consistency by 4\% in terms of human evaluation.",Introduction,"Abstractive test summarization is an important text generation task. With the applying of the sequence-to-sequence model and the publication of large-scale datasets, the quality of the automatic generated summarization has been greatly improved BIBREF0 , BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 . However, the semantic consistency of the automatically generated summaries is still far from satisfactory. The commonly-used large-scale datasets for deep learning models are constructed based on naturally-annotated data with heuristic rules BIBREF1 , BIBREF3 , BIBREF4 . The summaries are not written for the source content specifically. It suggests that the provided summary may not be semantically consistent with the source content. For example, the dataset for Chinese social media text summarization, namely LCSTS, contains more than 20% text-summary pairs that are not related, according to the statistics of the manually checked data BIBREF1 . Table TABREF1 shows an example of semantic inconsistency. Typically, the reference summary contains extra information that cannot be understood from the source content. It is hard to conclude the summary even for a human. Due to the inconsistency, the system cannot extract enough information in the source text, and it would be hard for the model to learn to generate the summary accordingly. The model has to encode spurious correspondence of the summary and the source content by memorization. However, this kind of correspondence is superficial and is not actually needed for generating reasonable summaries. Moreover, the information is harmful to generating semantically consistent summaries, because unrelated information is modeled. For example, the word UTF8gbsn“利益” (benefits) in the summary is not related to the source content. Thus, it has to be remembered by the model, together with the source content. However, this correspondence is spurious, because the word UTF8gbsn“利益” is not related to any word in the source content. In the following, we refer to this problem as Spurious Correspondence caused by the semantically inconsistent data. In this work, we aim to alleviate the impact of the semantic inconsistency of the current dataset. Based on the sequence-to-sequence model, we propose a regularization method to heuristically show down the learning of the spurious correspondence, so that the unrelated information in the dataset is less represented by the model. We incorporate a new soft training target to achieve this goal. For each output time in training, in addition to the gold reference word, the current output also targets at a softened output word distribution that regularizes the current output word distribution. In this way, a more robust correspondence of the source content and the output words can be learned, and potentially, the output summary will be more semantically consistent. To obtain the softened output word distribution, we propose two methods based on the sequence-to-sequence model. More detailed explanation is introduced in Section SECREF2 . Another problem for abstractive text summarization is that the system summary cannot be easily evaluated automatically. ROUGE BIBREF9 is widely used for summarization evaluation. However, as ROUGE is designed for extractive text summarization, it cannot deal with summary paraphrasing in abstractive text summarization. Besides, as ROUGE is based on the reference, it requires high-quality reference summary for a reasonable evaluation, which is also lacking in the existing dataset for Chinese social media text summarization. We argue that for proper evaluation of text generation task, human evaluation cannot be avoided. We propose a simple and practical human evaluation for evaluating text summarization, where the summary is evaluated against the source content instead of the reference. It handles both of the problems of paraphrasing and lack of high-quality reference. The contributions of this work are summarized as follows:",Proposed Method,"Base on the fact that the spurious correspondence is not stable and its realization in the model is prone to change, we propose to alleviate the issue heuristically by regularization. We use the cross-entropy with an annealed output distribution as the regularization term in the loss so that the little fluctuation in the distribution will be depressed and more robust and stable correspondence will be learned. By correspondence, we mean the relation between (a) the current output, and (b) the source content and the partially generated output. Furthermore, we propose to use an additional output layer to generate the annealed output distribution. Due to the same fact, the two output layers will differ more in the words that superficially co-occur, so that the output distribution can be better regularized.",Regularizing the Neural Network with Annealed Distribution,"Typically, in the training of the sequence-to-sequence model, only the one-hot hard target is used in the cross-entropy based loss function. For an example in the training set, the loss of an output vector is DISPLAYFORM0  where INLINEFORM0 is the output vector, INLINEFORM1 is the one-hot hard target vector, and INLINEFORM2 is the number of labels. However, as INLINEFORM3 is the one-hot vector, all the elements are zero except the one representing the correct label. Hence, the loss becomes DISPLAYFORM0  where INLINEFORM0 is the index of the correct label. The loss is then summed over the output sentences and across the minibatch and used as the source error signal in the backpropagation. The hard target could cause several problems in the training. Soft training methods try to use a soft target distribution to provide a generalized error signal to the training. For the summarization task, a straight-forward way would be to use the current output vector as the soft target, which contains the knowledge learned by the current model, i.e., the correspondence of the source content and the current output word: DISPLAYFORM0  Then, the two losses are combined as the new loss function: DISPLAYFORM0  where INLINEFORM0 is the index of the true label and INLINEFORM1 is the strength of the soft training loss. We refer to this approach as Self-Train (The left part of Figure FIGREF6 ). The output of the model can be seen as a refined supervisory signal for the learning of the model. The added loss promotes the learning of more stable correspondence. The output not only learns from the one-hot distribution but also the distribution generated by the model itself. However, during the training, the output of the neural network can become too close to the one-hot distribution. To solve this, we make the soft target the soften output distribution. We apply the softmax with temperature INLINEFORM2 , which is computed by DISPLAYFORM0  This transformation keeps the relative order of the labels, and a higher temperature will make the output distributed more evenly. The key motivation is that if the model is still not confident how to generate the current output word under the supervision of the reference summary, it means the correspondence can be spurious and the reference output is unlikely to be concluded from the source content. It makes no sense to force the model to learn such correspondence. The regularization follows that motivation, and in such case, the error signal will be less significant compared to the one-hot target. In the case where the model is extremely confident how to generate the current output, the annealed distribution will resemble the one-hot target. Thus, the regularization is not effective. In all, we make use of the model itself to identify the spurious correspondence and then regularize the output distribution accordingly.",Dual Output Layers,"However, the aforementioned method tries to regularize the output word distribution based on what it has already learned. The relative order of the output words is kept. The self-dependency may not be desirable for regularization. It may be better if more correspondence that is spurious can be identified. In this paper, we further propose to obtain the soft target from a different view of the model, so that different knowledge of the dataset can be used to mitigate the overfitting problem. An additional output layer is introduced to generate the soft target. The two output layers share the same hidden representation but have independent parameters. They could learn different knowledge of the data. We refer to this approach as Dual-Train. For clarity, the original output layer is denoted by INLINEFORM0 and the new output layer INLINEFORM1 . Their outputs are denoted by INLINEFORM2 and INLINEFORM3 , respectively. The output layer INLINEFORM4 acts as the original output layer. We apply soft training using the output from INLINEFORM5 to this output layer to increase its ability of generalization. Suppose the correct label is INLINEFORM6 . The target of the output INLINEFORM7 includes both the one-hot distribution and the distribution generated from INLINEFORM8 : DISPLAYFORM0  The new output layer INLINEFORM0 is trained normally using the originally hard target. This output layer is not used in the prediction, and its only purpose is to generate the soft target to facilitate the soft training of INLINEFORM1 . Suppose the correct label is INLINEFORM2 . The target of the output INLINEFORM3 includes only the one-hot distribution: DISPLAYFORM0  Because of the random initialization of the parameters in the output layers, INLINEFORM0 and INLINEFORM1 could learn different things. The diversified knowledge is helpful when dealing with the spurious correspondence in the data. It can also be seen as an online kind of ensemble methods. Several different instances of the same model are softly aggregated into one to make classification. The right part of Figure FIGREF6 shows the architecture of the proposed Dual-Train method.",Experiments,"We evaluate the proposed approach on the Chinese social media text summarization task, based on the sequence-to-sequence model. We also analyze the output text and the output label distribution of the models, showing the power of the proposed approach. Finally, we show the cases where the correspondences learned by the proposed approach are still problematic, which can be explained based on the approach we adopt.",Dataset,"Large-Scale Chinese Short Text Summarization Dataset (LCSTS) is constructed by BIBREF1 . The dataset consists of more than 2.4 million text-summary pairs in total, constructed from a famous Chinese social media microblogging service Weibo. The whole dataset is split into three parts, with 2,400,591 pairs in PART I for training, 10,666 pairs in PART II for validation, and 1,106 pairs in PART III for testing. The authors of the dataset have manually annotated the relevance scores, ranging from 1 to 5, of the text-summary pairs in PART II and PART III. They suggested that only pairs with scores no less than three should be used for evaluation, which leaves 8,685 pairs in PART II, and 725 pairs in PART III. From the statistics of the PART II and PART III, we can see that more than 20% of the pairs are dropped to maintain semantic quality. It indicates that the training set, which has not been manually annotated and checked, contains a huge quantity of unrelated text-summary pairs.",Experimental Settings,"We use the sequence-to-sequence model BIBREF10 with attention BIBREF11 , BIBREF12 , BIBREF13 , BIBREF14 as the Baseline. Both the encoder and decoder are based on the single layer LSTM BIBREF15 . The word embedding size is 400, and the hidden state size of the LSTM unit is 500. We conduct experiments on the word level. To convert the character sequences into word sequences, we use Jieba to segment the words, the same with the existing work BIBREF1 , BIBREF6 . Self-Train and Dual-Train are implemented based on the baseline model, with two more hyper-parameters, the temperature INLINEFORM0 and the soft training strength INLINEFORM1 . We use a very simple setting for all tasks, and set INLINEFORM2 , INLINEFORM3 . We pre-train the model without applying the soft training objective for 5 epochs out of total 10 epochs. We use the Adam optimizer BIBREF16 for all the tasks, using the default settings with INLINEFORM4 , INLINEFORM5 , and INLINEFORM6 . In testing, we use beam search to generate the summaries, and the beam size is set to 5. We report the test results at the epoch that achieves the best score on the development set.",Evaluation Protocol,"For text summarization, a common automatic evaluation method is ROUGE BIBREF9 . The generated summary is evaluated against the reference summary, based on unigram recall (ROUGE-1), bigram recall (ROUGE-2), and recall of longest common subsequence (ROUGE-L). To facilitate comparison with the existing systems, we adopt ROUGE as the automatic evaluation method. The ROUGE is calculated on the character level, following the previous work BIBREF1 . However, for abstractive text summarization, the ROUGE is sub-optimal, and cannot assess the semantic consistency between the summary and the source content, especially when there is only one reference for a piece of text. The reason is that the same content may be expressed in different ways with different focuses. Simple word match cannot recognize the paraphrasing. It is the case for all of the existing large-scale datasets. Besides, as aforementioned, ROUGE is calculated on the character level in Chinese text summarization, making the metrics favor the models on the character level in practice. In Chinese, a word is the smallest semantic element that can be uttered in isolation, not a character. In the extreme case, the generated text could be completely intelligible, but the characters could still match. In theory, calculating ROUGE metrics on the word level could alleviate the problem. However, word segmentation is also a non-trivial task for Chinese. There are many kinds of segmentation rules, which will produce different ROUGE scores. We argue that it is not acceptable to introduce additional systematic bias in automatic evaluations, and automatic evaluation for semantically related tasks can only serve as a reference. To avoid the deficiencies, we propose a simple human evaluation method to assess the semantic consistency. Each summary candidate is evaluated against the text rather than the reference. If the candidate is irrelevant or incorrect to the text, or the candidate is not understandable, the candidate is labeled bad. Otherwise, the candidate is labeled good. Then, we can get an accuracy of the good summaries. The proposed evaluation is very simple and straight-forward. It focuses on the relevance between the summary and the text. The semantic consistency should be the major consideration when putting the text summarization methods into practice, but the current automatic methods cannot judge properly. For detailed guidelines in human evaluation, please refer to Appendix SECREF6 . In the human evaluation, the text-summary pairs are dispatched to two human annotators who are native speakers of Chinese. As in our setting the summary is evaluated against the reference, the number of the pairs needs to be manually evaluated is four times the number of the pairs in the test set, because we need to compare four systems in total. To decrease the workload and get a hint about the annotation quality at the same time, we adopt the following procedure. We first randomly select 100 pairs in the validation set for the two human annotators to evaluate. Each pair is annotated twice, and the inter-annotator agreement is checked. We find that under the protocol, the inter-annotator agreement is quite high. In the evaluation of the test set, a pair is only annotated once to accelerate evaluation. To further maintain consistency, summaries of the same source content will not be distributed to different annotators.",Experimental Results,"First, we show the results for human evaluation, which focuses on the semantic consistency of the summary with its source content. We evaluate the systems implemented by us as well as the reference. We cannot conduct human evaluations for the existing systems from other work, because the output summaries needed are not available for us. Besides, the baseline system we implemented is very competitive in terms of ROUGE and achieves better performance than almost all the existing systems. The results are listed in Table TABREF24 . It is surprising to see that the accuracy of the reference summaries does not reach 100%. It means that the test set still contains text-summary pairs of poor quality even after removing the pairs with relevance scores lower than 3 as suggested by the authors of the dataset. As we can see, Dual-Train improves the accuracy by 4%. Due to the rigorous definition of being good, the results mean that 4% more of the summaries are semantically consistent with their source content. However, Self-Train has a performance drop compared to the baseline. After investigating its generated summaries, we find that the major reason is that the generated summaries are not grammatically complete and often stop too early, although the generated part is indeed more related to the source content. Because the definition of being good, the improved relevance does not make up the loss on intelligibility. Then, we compare the automatic evaluation results in Table TABREF25 . As we can see, only applying soft training without adaptation (Self-Train) hurts the performance. With the additional output layer (Dual-Train), the performance can be greatly improved over the baseline. Moreover, with the proposed method the simple baseline model is second to the best compared with the state-of-the-art models and even surpasses in ROUGE-2. It is promising that applying the proposed method to the state-of-the-art model could also improve its performance. The automatic evaluation is done on the original test set to facilitate comparison with existing work. However, a more reasonable setting would be to exclude the 52 test instances that are found bad in the human evaluation, because the quality of the automatic evaluation depends on the reference summary. As the existing methods do not provide their test output, it is a non-trivial task to reproduce all their results of the same reported performance. Nonetheless, it does not change the fact that ROUGE cannot handle the issues in abstractive text summarization properly.",Experimental Analysis,"To examine the effect of the proposed method and reveal how the proposed method improves the consistency, we compare the output of the baseline with Dual-Train, based on both the output text and the output label distribution. We also conduct error analysis to discover room for improvements. To gain a better understanding of the results, we analyze the summaries generated by the baseline model and our proposed model. Some of the summaries are listed in Table TABREF28 . As shown in the table, the summaries generated by the proposed method are much better than the baseline, and we believe they are more precise and informative than the references. In the first one, the baseline system generates a grammatical but unrelated summary, while the proposed method generates a more informative summary. In the second one, the baseline system generates a related but ungrammatical summary, while the proposed method generates a summary related to the source content but different from the reference. We believe the generated summary is actually better than the reference because the focus of the visit is not the event itself but its purpose. In the third one, the baseline system generates a related and grammatical summary, but the facts stated are completely incorrect. The summary generated by the proposed method is more comprehensive than the reference, while the reference only includes the facts in the last sentence of the source content. In short, the generated summary of the proposed method is more consistent with the source content. It also exhibits the necessity of the proposed human evaluation. Because when the generated summary is evaluated against the reference, it may seem redundant or wrong, but it is actually true to the source content. While it is arguable that the generated summary is better than the reference, there is no doubt that the generated summary of the proposed method is better than the baseline. However, the improvement cannot be properly shown by the existing evaluation methods. Furthermore, the examples suggest that the proposed method does learn better correspondence. The highlighted words in each example in Table TABREF28 share almost the same previous words. However, in the first one, the baseline considers “UTF8gbsn停” (stop) as the most related words, which is a sign of noisy word relations learned from other training examples, while the proposed method generates “UTF8gbsn进站” (to the platform), which is more related to what a human thinks. It is the same with the second example, where a human selects “UTF8gbsn专家” (expert) and Dual-Train selects “UTF8gbsn工作者” (worker), while the baseline selects “UTF8gbsn钻研” (research) and fails to generate a grammatical sentence later. In the third one, the reference and the baseline use the same word, while Dual-Train chooses a word of the same meaning. It can be concluded that Dual-Train indeed learns better word relations that could generalize to the test set, and good word relations can guide the decoder to generate semantically consistent summaries. To show why the generated text of the proposed method is more related to the source content, we further analyze the label distribution, i.e., the word distribution, generated by the (first) output layer, from which the output word is selected. To illustrate the relationship, we calculate a representation for each word based on the label distributions. Each representation is associated with a specific label (word), denoted by INLINEFORM0 , and each dimension INLINEFORM1 shows how likely the label indexed by INLINEFORM2 will be generated instead of the label INLINEFORM3 . To get such representation, we run the model on the training set and get the output vectors in the decoder, which are then averaged with respect to their corresponding labels to form a representation. We can obtain the most related words of a word by simply selecting the highest values from its representation. Table TABREF30 lists some of the labels and the top 4 labels that are most likely to replace each of the labels. It is a hint about the correspondence learned by the model. From the results, it can be observed that Dual-Train learns the better semantic relevance of a word compared to the baseline because the spurious word correspondence is alleviated by regularization. For example, the possible substitutes of the word “UTF8gbsn多长时间” (how long) considered by Dual-Train include “UTF8gbsn多少” (how many), “UTF8gbsn多久” (how long) and “UTF8gbsn时间” (time). However, the relatedness is learned poorly in the baseline, as there is “UTF8gbsn知道” (know), a number, and two particles in the possible substitutes considered by the baseline. Another representative example is the word “UTF8gbsn图像” (image), where the baseline also includes two particles in its most related words. The phenomenon shows that the baseline suffers from spurious correspondence in the data, and learns noisy and harmful relations, which rely too much on the co-occurrence. In contrast, the proposed method can capture more stable semantic relatedness of the words. For text summarization, grouping the words that are in the same topic together can help the model to generate sentences that are more coherent and can improve the quality of the summarization and the relevance to the source content. Although the proposed method resolves a large number of the noisy word relations, there are still cases that the less related words are not eliminated. For example, the top 4 most similar words of “UTF8gbsn期货业” (futures industry) from the proposed method include “UTF8gbsn改革” (reform). It is more related than “2013” from the baseline, but it can still be harmful to text summarization. The problem could arise from the fact that words as “UTF8gbsn期货业” rarely occur in the training data, and their relatedness is not reflected in the data. Another issue is that there are some particles, e.g., “UTF8gbsn的” (DE) in the most related words. A possible explanation is that particles show up too often in the contexts of the word, and it is hard for the models to distinguish them from the real semantically-related words. As our proposed approach is based on regularization of the less common correspondence, it is reasonable that such kind of relation cannot be eliminated. The first case can be categorized into data sparsity, which usually needs the aid of knowledge bases to solve. The second case is due to the characteristics of natural language. However, as such words are often closed class words, the case can be resolved by manually restricting the relatedness of these words.",Related Work,Related work includes efforts on designing models for the Chinese social media text summarization task and the efforts on obtaining soft training target for supervised learning.,Systems for Chinese Social Media Text Summarization,"The Large-Scale Chinese Short Text Summarization dataset was proposed by BIBREF1 . Along with the datasets, BIBREF1 also proposed two systems to solve the task, namely RNN and RNN-context. They were two sequence-to-sequence based models with GRU as the encoder and the decoder. The difference between them was that RNN-context had attention mechanism while RNN did not. They conducted experiments both on the character level and on the word level. RNN-distract BIBREF5 was a distraction-based neural model, where the attention mechanism focused on different parts of the source content. CopyNet BIBREF6 incorporated a copy mechanism to allow part of the generated summary to be copied from the source content. The copy mechanism also explained that the results of their word-level model were better than the results of their character-level model. SRB BIBREF17 was a sequence-to-sequence based neural model to improve the semantic relevance between the input text and the output summary. DRGD BIBREF8 was a deep recurrent generative decoder model, combining the decoder with a variational autoencoder.",Methods for Obtaining Soft Training Target,"Soft target aims to refine the supervisory signal in supervised learning. Related work includes soft target for traditional learning algorithms and model distillation for deep learning algorithms. The soft label methods are typically for binary classification BIBREF18 , where the human annotators not only assign a label for an example but also give information on how confident they are regarding the annotation. The main difference from our method is that the soft label methods require additional annotation information (e.g., the confidence information of the annotated labels) of the training data, which is costly in the text summarization task. There have also been prior studies on model distillation in deep learning that distills big models into a smaller one. Model distillation BIBREF19 combined different instances of the same model into a single one. It used the output distributions of the previously trained models as the soft target distribution to train a new model. A similar work to model distillation is the soft-target regularization method BIBREF20 for image classification. Instead of using the outputs of other instances, it used an exponential average of the past label distributions of the current instance as the soft target distribution. The proposed method is different compared with the existing model distillation methods, in that the proposed method does not require additional models or additional space to record the past soft label distributions. The existing methods are not suitable for text summarization tasks, because the training of an additional model is costly, and the additional space is huge due to the massive number of data. The proposed method uses its current state as the soft target distribution and eliminates the need to train additional models or to store the history information.",Conclusions,"We propose a regularization approach for the sequence-to-sequence model on the Chinese social media summarization task. In the proposed approach, we use a cross-entropy based regularization term to make the model neglect the possible unrelated words. We propose two methods for obtaining the soft output word distribution used in the regularization, of which Dual-Train proves to be more effective. Experimental results show that the proposed method can improve the semantic consistency by 4% in terms of human evaluation. As shown by the analysis, the proposed method achieves the improvements by eliminating the less semantically-related word correspondence. The proposed human evaluation method is effective and efficient in judging the semantic consistency, which is absent in previous work but is crucial in the accurate evaluation of the text summarization systems. The proposed metric is simple to conduct and easy to interpret. It also provides an insight on how practicable the existing systems are in the real-world scenario.",Standard for Human Evaluation,"For human evaluation, the annotators are asked to evaluate the summary against the source content based on the goodness of the summary. If the summary is not understandable, relevant or correct according to the source content, the summary is considered bad. More concretely, the annotators are asked to examine the following aspects to determine whether the summary is good: If a rule is not met, the summary is labeled bad, and the following rules do not need to be checked. In Table TABREF33 , we give examples for cases of each rule. In the first one, the summary is not fluent, because the patient of the predicate UTF8gbsn“找” (seek for) is missing. The second summary is fluent, but the content is not related to the source, in that we cannot determine if Lei Jun is actually fighting the scalpers based on the source content. In the third one, the summary is fluent and related to the source content, but the facts are wrong, as the summary is made up by facts of different people. The last one met all the three rules, and thus it is considered good. This work is supported in part by the GS501100001809National Natural Science Foundation of Chinahttp://dx.doi.org/10.13039/501100001809 under Grant No. GS50110000180961673028. ",,,,,,,,,,,,,Are results reported only for English data?,ddd6ba43c4e1138156dd2ef03c25a4c4a47adad0,five,unfamiliar,no,irony,e8b24c3133e0bec0a6465e1f13acd3a5ed816b37,False,False,,"We evaluate the proposed approach on the Chinese social media text summarization task, based on the sequence-to-sequence model. We also analyze the output text and the output label distribution of the models, showing the power of the proposed approach. Finally, we show the cases where the correspondences learned by the proposed approach are still problematic, which can be explained based on the approach we adopt. Large-Scale Chinese Short Text Summarization Dataset (LCSTS) is constructed by BIBREF1 . The dataset consists of more than 2.4 million text-summary pairs in total, constructed from a famous Chinese social media microblogging service Weibo. The whole dataset is split into three parts, with 2,400,591 pairs in PART I for training, 10,666 pairs in PART II for validation, and 1,106 pairs in PART III for testing. The authors of the dataset have manually annotated the relevance scores, ranging from 1 to 5, of the text-summary pairs in PART II and PART III. They suggested that only pairs with scores no less than three should be used for evaluation, which leaves 8,685 pairs in PART II, and 725 pairs in PART III. From the statistics of the PART II and PART III, we can see that more than 20% of the pairs are dropped to maintain semantic quality. It indicates that the training set, which has not been manually annotated and checked, contains a huge quantity of unrelated text-summary pairs.","We evaluate the proposed approach on the Chinese social media text summarization task, based on the sequence-to-sequence model. Large-Scale Chinese Short Text Summarization Dataset (LCSTS) is constructed by BIBREF1 . The dataset consists of more than 2.4 million text-summary pairs in total, constructed from a famous Chinese social media microblogging service Weibo. ",ef018f67e48219b1f4fc4590d2a1400f87e49645,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,,,,,,,,,Which existing models does this approach outperform?,bd99aba3309da96e96eab3e0f4c4c8c70b51980a,five,unfamiliar,no,irony,e8b24c3133e0bec0a6465e1f13acd3a5ed816b37,False,,"RNN-context, SRB, CopyNet, RNN-distract, DRGD",FLOAT SELECTED: Table 3. Comparisons with the Existing Models in Terms of ROUGE Metrics,FLOAT SELECTED: Table 3. Comparisons with the Existing Models in Terms of ROUGE Metrics,625346e5a184bef42b0c6f2029d5be4697001a72,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,What human evaluation method is proposed?,73bb8b7d7e98ccb88bb19ecd2215d91dd212f50d,five,unfamiliar,no,irony,e8b24c3133e0bec0a6465e1f13acd3a5ed816b37,False,,comparing the summary with the text instead of the reference and labeling the candidate bad if it is incorrect or irrelevant,"More detailed explanation is introduced in Section SECREF2 . Another problem for abstractive text summarization is that the system summary cannot be easily evaluated automatically. ROUGE BIBREF9 is widely used for summarization evaluation. However, as ROUGE is designed for extractive text summarization, it cannot deal with summary paraphrasing in abstractive text summarization. Besides, as ROUGE is based on the reference, it requires high-quality reference summary for a reasonable evaluation, which is also lacking in the existing dataset for Chinese social media text summarization. We argue that for proper evaluation of text generation task, human evaluation cannot be avoided. We propose a simple and practical human evaluation for evaluating text summarization, where the summary is evaluated against the source content instead of the reference. It handles both of the problems of paraphrasing and lack of high-quality reference. The contributions of this work are summarized as follows: For text summarization, a common automatic evaluation method is ROUGE BIBREF9 . The generated summary is evaluated against the reference summary, based on unigram recall (ROUGE-1), bigram recall (ROUGE-2), and recall of longest common subsequence (ROUGE-L). To facilitate comparison with the existing systems, we adopt ROUGE as the automatic evaluation method. The ROUGE is calculated on the character level, following the previous work BIBREF1 . However, for abstractive text summarization, the ROUGE is sub-optimal, and cannot assess the semantic consistency between the summary and the source content, especially when there is only one reference for a piece of text. The reason is that the same content may be expressed in different ways with different focuses. Simple word match cannot recognize the paraphrasing. It is the case for all of the existing large-scale datasets. Besides, as aforementioned, ROUGE is calculated on the character level in Chinese text summarization, making the metrics favor the models on the character level in practice. In Chinese, a word is the smallest semantic element that can be uttered in isolation, not a character. In the extreme case, the generated text could be completely intelligible, but the characters could still match. In theory, calculating ROUGE metrics on the word level could alleviate the problem. However, word segmentation is also a non-trivial task for Chinese. There are many kinds of segmentation rules, which will produce different ROUGE scores. We argue that it is not acceptable to introduce additional systematic bias in automatic evaluations, and automatic evaluation for semantically related tasks can only serve as a reference. To avoid the deficiencies, we propose a simple human evaluation method to assess the semantic consistency. Each summary candidate is evaluated against the text rather than the reference. If the candidate is irrelevant or incorrect to the text, or the candidate is not understandable, the candidate is labeled bad. Otherwise, the candidate is labeled good. Then, we can get an accuracy of the good summaries. The proposed evaluation is very simple and straight-forward. It focuses on the relevance between the summary and the text. The semantic consistency should be the major consideration when putting the text summarization methods into practice, but the current automatic methods cannot judge properly. For detailed guidelines in human evaluation, please refer to Appendix SECREF6 . In the human evaluation, the text-summary pairs are dispatched to two human annotators who are native speakers of Chinese. As in our setting the summary is evaluated against the reference, the number of the pairs needs to be manually evaluated is four times the number of the pairs in the test set, because we need to compare four systems in total. To decrease the workload and get a hint about the annotation quality at the same time, we adopt the following procedure. We first randomly select 100 pairs in the validation set for the two human annotators to evaluate. Each pair is annotated twice, and the inter-annotator agreement is checked. We find that under the protocol, the inter-annotator agreement is quite high. In the evaluation of the test set, a pair is only annotated once to accelerate evaluation. To further maintain consistency, summaries of the same source content will not be distributed to different annotators.","We propose a simple and practical human evaluation for evaluating text summarization, where the summary is evaluated against the source content instead of the reference. It handles both of the problems of paraphrasing and lack of high-quality reference.  To avoid the deficiencies, we propose a simple human evaluation method to assess the semantic consistency. Each summary candidate is evaluated against the text rather than the reference. If the candidate is irrelevant or incorrect to the text, or the candidate is not understandable, the candidate is labeled bad. Otherwise, the candidate is labeled good. Then, we can get an accuracy of the good summaries. The proposed evaluation is very simple and straight-forward. It focuses on the relevance between the summary and the text. The semantic consistency should be the major consideration when putting the text summarization methods into practice, but the current automatic methods cannot judge properly. For detailed guidelines in human evaluation, please refer to Appendix SECREF6 . ",d76d0cd894e1434896b72b8211bf1fa0e2db7703,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,5-Figure1-1.png,Fig. 1. Illustration of the proposed methods. Left: Self-Train. Right: Dual-Train.,8-Table2-1.png,"Table 2. Results of the Human Evaluation on the Test Set, Showing How Many Summaries are Semantically Consistent with Their Source Content",8-Table3-1.png,Table 3. Comparisons with the Existing Models in Terms of ROUGE Metrics,10-Table5-1.png,Table 5. Examples of the Labels and Their Top Four Most Related Labels,13-Table6-1.png,Table 6. Examples for Each Case in the Human Evaluation,14-Table7-1.png,Table 7. Results of the Inter-Annotator Agreement,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Team Papelo: Transformer Networks at FEVER,"We develop a system for the FEVER fact extraction and verification challenge that uses a high precision entailment classifier based on transformer networks pretrained with language modeling, to classify a broad set of potential evidence. The precision of the entailment classifier allows us to enhance recall by considering every statement from several articles to decide upon each claim. We include not only the articles best matching the claim text by TFIDF score, but read additional articles whose titles match named entities and capitalized expressions occurring in the claim text. The entailment module evaluates potential evidence one statement at a time, together with the title of the page the evidence came from (providing a hint about possible pronoun antecedents). In preliminary evaluation, the system achieves .5736 FEVER score, .6108 label accuracy, and .6485 evidence F1 on the FEVER shared task test set.",Introduction,"The release of the FEVER fact extraction and verification dataset BIBREF0 provides a large-scale challenge that tests a combination of retrieval and textual entailment capabilities. To verify a claim in the dataset as supported, refuted, or undecided, a system must retrieve relevant articles and sentences from Wikipedia. Then it must decide whether each of those sentences, or some combination of them, entails or refutes the claim, which is an entailment problem. Systems are evaluated on the accuracy of the claim predictions, with credit only given when correct evidence is submitted. As entailment data, premises in FEVER data differ substantially from those in the image caption data used as the basis for the Stanford Natural Language Inference (SNLI) BIBREF1 dataset. Sentences are longer (31 compared to 14 words on average), vocabulary is more abstract, and the prevalence of named entities and out-of-vocabulary terms is higher. The retrieval aspect of FEVER is not straightforward either. A claim may have small word overlap with the relevant evidence, especially if the claim is refuted by the evidence. Our approach to FEVER is to fix the most obvious shortcomings of the baseline approaches to retrieval and entailment, and to train a sharp entailment classifier that can be used to filter a broad set of retrieved potential evidence. For the entailment classifier we compare Decomposable Attention BIBREF2 , BIBREF3 as implemented in the official baseline, ESIM BIBREF4 , and a transformer network with pre-trained weights BIBREF5 . The transformer network naturally supports out-of-vocabulary words and gives substantially higher performance than the other methods.",Transformer network,"The core of our system is an entailment module based on a transformer network. Transformer networks BIBREF6 are deep networks applied to sequential input data, with each layer implementing multiple heads of scaled dot product attention. This attention mechanism allows deep features to be compared across positions in the input. Many entailment networks have two sequence inputs, but the transformer is designed with just one. A separator token divides the premise from the hypothesis. We use a specific transformer network released by OpenAI BIBREF5 that has been pre-trained for language modeling. The network consists of twelve blocks. Each block consists of a multi-head masked self-attention layer, layer normalization BIBREF7 , a feed forward network, and another layer normalization. After the twelfth block, two branches exist. In one branch, matrix multiplication and softmax layers are applied at the terminal sequence position to predict the entailment classification. In the other branch, a hidden state is multiplied by each token embedding and a softmax is taken to predict the next token. The language modeling branch has been pre-trained on the BookCorpus dataset BIBREF8 . We take the pre-trained model and train both branches on examples from FEVER.",Reframing entailment,"The baseline FEVER system BIBREF0 ran the AllenNLP BIBREF3 implementation of Decomposable Attention BIBREF2 to classify a group of five premise statements concatenated together against the claim. These five premise statements were fixed by the retrieval module and not considered individually. In our system, premise statements are individually evaluated. We collect training data as the five sentences with the highest TFIDF score against the claim, taken from the Wikipedia pages selected by the retrieval module. If any ground truth evidence group for a claim requires more than one sentence, the claim is dropped from the training set. Otherwise, each sentence is labeled with the truth value of the claim if it is in the ground truth evidence set, and labeled as neutral if not. The resulting data forms an entailment problem that we call “FEVER One.” For comparison, we form “FEVER Five” and “FEVER Five Oracle” by concatenating all five retrieved sentences, as in the baseline. In FEVER Five Oracle, the ground truth is the claim ground truth (if verifiable), but in FEVER Five, ground truth depends on whether the retrieved evidence is in the ground truth evidence set. Several FEVER claims require multiple statements as evidence in order to be supported or refuted. The number of such claims is relatively small: in the first half of the development set, only 623 of 9999 claims were verifiable and had no singleton evidence groups. Furthermore, we disagreed with many of these annotations and thought that less evidence should have sufficed. Thus we chose not to develop a strategy for multiple evidence statements. To compare results on FEVER Five to FEVER One, we must aggregate decisions about individual sentences of possible evidence to a decision about the claim. We do this by applying the following rules: We resolve conflicts between supporting and refuting information in favor of the supporting information, because we observed cases in the development data where information was retrieved for different entities with the same name. For example, Ann Richards appeared both as a governor of Texas and as an Australian actress. Information that would be a contradiction regarding the actress should not stop evidence that would support a claim about the politician. Even if a sentence is in the evidence set, it might not be possible for the classifier to correctly determine whether it supports the claim, because the sentence could have pronouns with antecedents outside the given sentence. Ideally, a coreference resolution system could add this information to the sentence, but running one could be time consuming and introduce its own errors. As a cheap alternative, we make the classifier aware of the title of the Wikipedia page. We convert any undersores in the page title to spaces, and insert the title between brackets before the rest of each premise sentence. The dataset constructed in this way is called “FEVER Title One.” The FEVER baseline system works by solving FEVER Five Oracle. Using Decomposable Attention, it achieves .505 accuracy on the test half of the development set. Swapping in the Enhanced Sequential Inference Model (ESIM) BIBREF4 to solve FEVER Five Oracle results in an accuracy of .561. Because ESIM uses a single out-of-vocabulary (OOV) token for all unknown words, we expect it to confuse named entities. Thus we extend the model by allocating 10,000 indices for out-of-vocabulary words with randomly initialized embeddings, and taking a hash of each OOV word to select one of these indices. With extended ESIM, the accuracy is .586. Therefore, we run most later comparisons with extended ESIM or transformer networks as the entailment module, rather than Decomposable Attention. The FEVER One dataset is highly unbalanced in favor of neutral statements, so that the majority class baseline would achieve 93.0% on this data. In fact it makes training ESIM a challenge, as the model only learns the trivial majority class predictor if the natural training distribution is followed. We reweight the examples in FEVER One for ESIM so that each class contributes to the loss equally. Then, we use Cohen's Kappa rather than the accuracy to evaluate a model's quality, so that following the bias with purely random agreement is not rewarded in the evaluation. In Table 1 we compare FEVER One to FEVER Title One, both at the level of classifying individual support statements and of classifying the claim by aggregating these decisions as described above. On a support basis, we find a 52% increase in Kappa by adding the titles. When ESIM is replaced by the transformer network, class reweighting is not necessary. The network naturally learns to perform in excess of the majority class baseline. Cohen's Kappa is 68% higher than that for ESIM. The possibility of training on oracle labels for a concatenated set of evidence allows a classifier to simply guess whether the hypothesis is true and supported somewhere, rather than having to consider the relationship between hypothesis and premise. For example, it is possible to classify 67% of SNLI examples correctly without reading the premise BIBREF9 . As we show in Table 2 , for ESIM, we find that this kind of guessing makes the FEVER Title Five Oracle performance better than FEVER Title Five. The Transformer model is accurate enough that oracle guessing does not help. Both models perform best when classifying each bit of evidence separately and then aggregating.",Improving retrieval,"Regardless of how strong the entailment classifier is, FEVER score is limited by whether the document and sentence retrieval modules, which produce the input to the entailment classifier, find the right evidence. In Table 3 , we examine the percentage of claims for which correct evidence is retrieved, before filtering with the entailment classifier. For this calculation, we skip any claim with an evidence group with multiple statements, and count a claim as succesfully retrieved if it is not verifiable or if the statement in one of the evidence groups is retrieved. The baseline system retrieves the five articles with the highest TFIDF score, and then extracts the five sentences from that collection with the highest TFIDF score against the claim. It achieves 66.1% evidence retrieval. Our first modification simply adds the title to each premise statement when computing its TFIDF against the claim, so that statements from a relevant article get credit even if the subject is not repeated. This raises evidence retrieval to 68.3%. A more significant boost comes from retrieving additional Wikipedia pages based on named entity recognition (NER). We start with phrases tagged as named entities by SpaCy BIBREF10 , but these tags are not very reliable, so we include various capitalized phrases. We retrieve Wikipedia pages whose title exactly matches one of these phrases. The named entity retrieval strategy boosts the evidence retrieval rate to 80.8%, while less than doubling the processing time. However, sometimes the named entity page thus retrieved is only a Wikipedia disambiguation page with no useful information. Noticing a lot of questions about films in the development set, we modify the strategy to also retrieve a page titled “X (film)” if it exists, whenever “X” is retrieved. The film retrievals raise evidence retrieval to 81.2%. Finally, we eliminate the TFIDF sentence ranking to expand sentence retrieval from five sentences to entire articles, up to the first fifty sentences from each. Thus we obtain 2.6 million statements to classify regarding the 19,998 claims in the shared task development set, for an average of 128 premises per claim. The evidence retrieval rate, including all these premises, increases to 90.1%. We continue to apply the entailment module trained with only five premise retrievals. Running the entailment module on this batch using a machine with three NVIDIA GeForce GTX 1080Ti GPU cards takes on the order of six hours. Retrieving more than five sentences means that we can no longer submit all retrieved evidence as support for the claims. Instead, we follow the aggregation strategy from Section ""Reframing entailment"" to decide the claim label, and only submit statements whose classification matches. Limiting evidence in this way when only five statements are retrieved (“narrow evidence” in Table 4 ) pushes FEVER score down very little, to .5550 from .5617 on the development set, so we have confidence that the extra retrieval will make up for the loss. Indeed, when the system reviews the extra evidence, FEVER score goes up to .5844 on the development set. Table 4 compares the end-to-end performance of systems that evaluate five retrieved statements together, evaluate five retrieved statements separately, and evaluate all statements from entire articles separately. Evaluating the statements separately gives better performance. We submit the systems that retrieve five statements and entire articles for evaluation on the test set, achieving preliminary FEVER scores of .5539 and .5736 respectively (label accuracy of .5754 and .6108, evidence recall of .6245 and .5002, evidence F1 of .2542 and .6485). In preliminary standings, the latter system ranks fourth in FEVER score and first in evidence F1.",Discussion,"Our approach to FEVER involves a minimum of heuristics and relies mainly on the strength of the Transformer Network based entailment classification. The main performance gains come from adding retrievals that resolve named entities rather than matching the claim text only, filtering fewer of the retrievals, and making the entailment classifier somewhat aware of the topic of what it is reading by including the title. If higher quality and more plentiful multi-evidence claims would be constructed, it would be nice to incorporate dynamic retrievals into the system, allowing the classifier to decide that it needs more information about keywords it encountered during reading.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,How big is their training set?,9efd025cfa69c6ff2777528bd158f79ead9353d1,infinity,familiar,no,transformer,50d8b4a941c26b89482c94ab324b5a274f9ced66,True,,,,,0efbcf10ffd60b7ac765e797acb4188b6fb548c7,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,,,,,,,,,What baseline do they compare to?,559c1307610a15427caeb8aff4d2c01ae5c9de20,infinity,familiar,no,transformer,50d8b4a941c26b89482c94ab324b5a274f9ced66,False,,,"Our approach to FEVER is to fix the most obvious shortcomings of the baseline approaches to retrieval and entailment, and to train a sharp entailment classifier that can be used to filter a broad set of retrieved potential evidence. For the entailment classifier we compare Decomposable Attention BIBREF2 , BIBREF3 as implemented in the official baseline, ESIM BIBREF4 , and a transformer network with pre-trained weights BIBREF5 . The transformer network naturally supports out-of-vocabulary words and gives substantially higher performance than the other methods.","Our approach to FEVER is to fix the most obvious shortcomings of the baseline approaches to retrieval and entailment, and to train a sharp entailment classifier that can be used to filter a broad set of retrieved potential evidence. For the entailment classifier we compare Decomposable Attention BIBREF2 , BIBREF3 as implemented in the official baseline, ESIM BIBREF4 , and a transformer network with pre-trained weights BIBREF5 . The transformer network naturally supports out-of-vocabulary words and gives substantially higher performance than the other methods.",dca8d216296bceafacb89fa8c0e8e3404ad2f298,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,Which pre-trained transformer do they use?,4ecb6674bcb4162bf71aea8d8b82759255875df3,infinity,familiar,no,transformer,50d8b4a941c26b89482c94ab324b5a274f9ced66,False,,,"Our approach to FEVER is to fix the most obvious shortcomings of the baseline approaches to retrieval and entailment, and to train a sharp entailment classifier that can be used to filter a broad set of retrieved potential evidence. For the entailment classifier we compare Decomposable Attention BIBREF2 , BIBREF3 as implemented in the official baseline, ESIM BIBREF4 , and a transformer network with pre-trained weights BIBREF5 . The transformer network naturally supports out-of-vocabulary words and gives substantially higher performance than the other methods.","For the entailment classifier we compare Decomposable Attention BIBREF2 , BIBREF3 as implemented in the official baseline, ESIM BIBREF4 , and a transformer network with pre-trained weights BIBREF5 . ",dfd6ac4bdae8afaa4796cb91975e84117cd7f088,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,,,,,,,,What is the FEVER task?,eacc1eb65daad055df934e0e878f417b73b2ecc1,infinity,familiar,no,transformer,50d8b4a941c26b89482c94ab324b5a274f9ced66,False,,,"The release of the FEVER fact extraction and verification dataset BIBREF0 provides a large-scale challenge that tests a combination of retrieval and textual entailment capabilities. To verify a claim in the dataset as supported, refuted, or undecided, a system must retrieve relevant articles and sentences from Wikipedia. Then it must decide whether each of those sentences, or some combination of them, entails or refutes the claim, which is an entailment problem. Systems are evaluated on the accuracy of the claim predictions, with credit only given when correct evidence is submitted. As entailment data, premises in FEVER data differ substantially from those in the image caption data used as the basis for the Stanford Natural Language Inference (SNLI) BIBREF1 dataset. Sentences are longer (31 compared to 14 words on average), vocabulary is more abstract, and the prevalence of named entities and out-of-vocabulary terms is higher. The retrieval aspect of FEVER is not straightforward either. A claim may have small word overlap with the relevant evidence, especially if the claim is refuted by the evidence.","The release of the FEVER fact extraction and verification dataset BIBREF0 provides a large-scale challenge that tests a combination of retrieval and textual entailment capabilities. To verify a claim in the dataset as supported, refuted, or undecided, a system must retrieve relevant articles and sentences from Wikipedia. Then it must decide whether each of those sentences, or some combination of them, entails or refutes the claim, which is an entailment problem. Systems are evaluated on the accuracy of the claim predictions, with credit only given when correct evidence is submitted. As entailment data, premises in FEVER data differ substantially from those in the image caption data used as the basis for the Stanford Natural Language Inference (SNLI) BIBREF1 dataset. Sentences are longer (31 compared to 14 words on average), vocabulary is more abstract, and the prevalence of named entities and out-of-vocabulary terms is higher. The retrieval aspect of FEVER is not straightforward either. A claim may have small word overlap with the relevant evidence, especially if the claim is refuted by the evidence.",723b977f0074c3cb287db7a362930b75459cfc32,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3-Table1-1.png,Table 1: Effect of adding titles to premises.,3-Table2-1.png,Table 2: Concatenating evidence or not.,3-Table3-1.png,Table 3: Percentage of evidence retrieved from first half of development set. Single-evidence claims only.,3-Table4-1.png,Table 4: FEVER Score of various systems. All use NE+Film retrieval.,,,,,,,,,,,,,"For the entailment classifier we compare Decomposable Attention BIBREF2 , BIBREF3 as implemented in the official baseline, ESIM BIBREF4 , and a transformer network with pre-trained weights BIBREF5 .",BIBREF5,,,,,,,,,,,,,,,,,,,,,"tests a combination of retrieval and textual entailment capabilities. To verify a claim in the dataset as supported, refuted, or undecided, a system must retrieve relevant articles and sentences from Wikipedia. Then it must decide whether each of those sentences, or some combination of them, entails or refutes the claim, which is an entailment problem",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Visualizing and Measuring the Geometry of BERT,"Transformer architectures show significant promise for natural language processing. Given that a single pretrained model can be fine-tuned to perform well on many different tasks, these networks appear to extract generally useful linguistic features. A natural question is how such networks represent this information internally. This paper describes qualitative and quantitative investigations of one particularly effective model, BERT. At a high level, linguistic features seem to be represented in separate semantic and syntactic subspaces. We find evidence of a fine-grained geometric representation of word senses. We also present empirical descriptions of syntactic representations in both attention matrices and individual word embeddings, as well as a mathematical argument to explain the geometry of these representations.",Introduction,"Neural networks for language processing have advanced rapidly in recent years. A key breakthrough was the introduction of transformer architectures BIBREF0 . One recent system based on this idea, BERT BIBREF1 , has proven to be extremely flexible: a single pretrained model can be fine-tuned to achieve state-of-the-art performance on a wide variety of NLP applications. This suggests the model is extracting a set of generally useful features from raw text. It is natural to ask, which features are extracted? And how is this information represented internally? Similar questions have arisen with other types of neural nets. Investigations of convolutional neural networks BIBREF2 , BIBREF3 have shown how representations change from layer to layer BIBREF4 ; how individual units in a network may have meaning BIBREF5 ; and that “meaningful” directions exist in the space of internal activation BIBREF6 . These explorations have led to a broader understanding of network behavior. Analyses on language-processing models (e.g., BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 ) point to the existence of similarly rich internal representations of linguistic structure. Syntactic features seem to be extracted by RNNs (e.g., BIBREF7 , BIBREF9 ) as well as in BERT BIBREF11 , BIBREF12 , BIBREF13 , BIBREF10 . Inspirational work from Hewitt and Manning BIBREF8 found evidence of a geometric representation of entire parse trees in BERT's activation space. Our work extends these explorations of the geometry of internal representations. Investigating how BERT represents syntax, we describe evidence that attention matrices contain grammatical representations. We also provide mathematical arguments that may explain the particular form of the parse tree embeddings described in BIBREF8 . Turning to semantics, using visualizations of the activations created by different pieces of text, we show suggestive evidence that BERT distinguishes word senses at a very fine level. Moreover, much of this semantic information appears to be encoded in a relatively low-dimensional subspace.",Context and related work,"Our object of study is the BERT model introduced in BIBREF1 . To set context and terminology, we briefly describe the model's architecture. The input to BERT is based on a sequence of tokens (words or pieces of words). The output is a sequence of vectors, one for each input token. We will often refer to these vectors as context embeddings because they include information about a token's context. BERT's internals consist of two parts. First, an initial embedding for each token is created by combining a pre-trained wordpiece embedding with position and segment information. Next, this initial sequence of embeddings is run through multiple transformer layers, producing a new sequence of context embeddings at each step. (BERT comes in two versions, a 12-layer BERT-base model and a 24-layer BERT-large model.) Implicit in each transformer layer is a set of attention matrices, one for each attention head, each of which contains a scalar value for each ordered pair $(token_i, token_j)$ .",Language representation by neural networks,"Sentences are sequences of discrete symbols, yet neural networks operate on continuous data–vectors in high-dimensional space. Clearly a successful network translates discrete input into some kind of geometric representation–but in what form? And which linguistic features are represented? The influential Word2Vec system BIBREF14 , for example, has been shown to place related words near each other in space, with certain directions in space correspond to semantic distinctions. Grammatical information such as number and tense are also represented via directions in space. Analyses of the internal states of RNN-based models have shown that they represent information about soft hierarchical syntax in a form that can be extracted by a one-hidden-layer network BIBREF9 . One investigation of full-sentence embeddings found a wide variety of syntactic properties could be extracted not just by an MLP, but by logistic regression BIBREF15 . Several investigations have focused on transformer architectures. Experiments suggest context embeddings in BERT and related models contain enough information to perform many tasks in the traditional “NLP pipeline” BIBREF12 –tagging part-of-speech, co-reference resolution, dependency labeling, etc.–with simple classifiers (linear or small MLP models) BIBREF11 , BIBREF10 . Qualitative, visualization-based work BIBREF16 suggests attention matrices may encode important relations between words. A recent and fascinating discovery by Hewitt and Manning BIBREF8 , which motivates much of our work, is that BERT seems to create a direct representation of an entire dependency parse tree. The authors find that (after a single global linear transformation, which they term a “structural probe”) the square of the distance between context embeddings is roughly proportional to tree distance in the dependency parse. They ask why squaring distance is necessary; we address this question in the next section. The work cited above suggests that language-processing networks create a rich set of intermediate representations of both semantic and syntactic information. These results lead to two motivating questions for our research. Can we find other examples of intermediate representations? And, from a geometric perspective, how do all these different types of information coexist in a single vector?",Geometry of syntax,"We begin by exploring BERT's internal representation of syntactic information. This line of inquiry builds on the work by Hewitt and Manning in two ways. First, we look beyond context embeddings to investigate whether attention matrices encode syntactic features. Second, we provide a simple mathematical analysis of the tree embeddings that they found.",Attention probes and dependency representations,"As in BIBREF8 , we are interested in finding representations of dependency grammar relations BIBREF17 . While BIBREF8 analyzed context embeddings, another natural place to look for encodings is in the attention matrices. After all, attention matrices are explicitly built on the relations between pairs of words. To formalize what it means for attention matrices to encode linguistic features, we use an attention probe, an analog of edge probing BIBREF11 . An attention probe is a task for a pair of tokens, $(token_i, token_j)$ where the input is a model-wide attention vector formed by concatenating the entries $a_{ij}$ in every attention matrix from every attention head in every layer. The goal is to classify a given relation between the two tokens. If a linear model achieves reliable accuracy, it seems reasonable to say that the model-wide attention vector encodes that relation. We apply attention probes to the task of identifying the existence and type of dependency relation between two words. The data for our first experiment is a corpus of parsed sentences from the Penn Treebank BIBREF18 . This dataset has the constituency grammar for the sentences, which was translated to a dependency grammar using the PyStanfordDependencies library BIBREF19 . The entirety of the Penn Treebank consists of 3.1 million dependency relations; we filtered this by using only examples of the 30 dependency relations with more than 5,000 examples in the data set. We then ran each sentence through BERT-base, and obtained the model-wide attention vector (see Figure 1 ) between every pair of tokens in the sentence, excluding the $[SEP]$ and $[CLS]$ tokens. This and subsequent experiments were conducted using PyTorch on MacBook machines. With these labeled embeddings, we trained two L2 regularized linear classifiers via stochastic gradient descent, using BIBREF20 . The first of these probes was a simple linear binary classifier to predict whether or not an attention vector corresponds to the existence of a dependency relation between two tokens. This was trained with a balanced class split, and 30% train/test split. The second probe was a multiclass classifier to predict which type of dependency relation exists between two tokens, given the dependency relation’s existence. This probe was trained with distributions outlined in table 2 . The binary probe achieved an accuracy of 85.8%, and the multiclass probe achieved an accuracy of 71.9%. Our real aim, again, is not to create a state-of-the-art parser, but to gauge whether model-wide attention vectors contain a relatively simple representation of syntactic features. The success of this simple linear probe suggests that syntactic information is in fact encoded in the attention vectors.",Geometry of parse tree embeddings,"Hewitt and Manning's result that context embeddings represent dependency parse trees geometrically raises several questions. Is there a reason for the particular mathematical representation they found? Can we learn anything by visualizing these representations? Hewitt and Manning ask why parse tree distance seems to correspond specifically to the square of Euclidean distance, and whether some other metric might do better BIBREF8 . We describe mathematical reasons why squared Euclidean distance may be natural. First, one cannot generally embed a tree, with its tree metric $d$ , isometrically into Euclidean space (Appendix ""Embedding trees in Euclidean space"" ). Since an isometric embedding is impossible, motivated by the results of BIBREF8 we might ask about other possible representations. Definition 1 (power- $p$ embedding) Let $M$ be a metric space, with metric $d$ . We say $f: M \rightarrow \mathbb {R}^n$ is a power- $p$ embedding if for all $x, y \in M$ , we have $||f(x) - f(y)||^p = d(x, y)$  In these terms, we can say BIBREF8 found evidence of a power-2 embedding for parse trees. It turns out that power-2 embeddings are an especially elegant mapping. For one thing, it is easy to write down an explicit model–a mathematical idealization–for a power-2 embedding for any tree. Theorem 1 Any tree with $n$ nodes has a power-2 embedding into $\mathbb {R}^{n-1}$ . Let the nodes of the tree be $t_0, ..., t_{n-1}$ , with $t_0$ being the root node. Let $\lbrace e_1, ..., e_{n-1}\rbrace $ be orthogonal unit basis vectors for $\mathbb {R}^{n-1}$ . Inductively, define an embedding $f$ such that: $f(t_0) = 0$ $f(t_i) = e_i + f(parent(t_i))$  Given two distinct tree nodes $x$ and $y$ , where $m$ is the tree distance $d(x, y)$ , it follows that we can move from $f(x)$ to $f(y)$ using $m$ mutually perpendicular unit steps. Thus $||f(x) - f(y)||^2 = m = d(x, y)$  Remark 1 This embedding has a simple informal description: at each embedded vertex of the graph, all line segments to neighboring embedded vertices are unit-distance segments, orthogonal to each other and to every other edge segment. (It's even easy to write down a set of coordinates for each node.) By definition any two power-2 embeddings of the same tree are isometric; with that in mind, we refer to this as the canonical power-2 embedding. In the proof of Theorem 1, instead of choosing basis vectors in advance, one can choose random unit vectors. Because two random vectors will be nearly orthogonal in high-dimensional space, the power-2 embedding condition will approximately hold. This means that in space that is sufficiently high-dimensional (compared to the size of the tree) it is possible to construct an approximate power-2 embedding with essentially “local” information, where a tree node is connected to its children via random unit-length branches. We refer to this type of embedding as a random branch embedding. (See Appendix ""Ideal vs. actual parse tree embeddings"" for a visualization of these various embeddings.) In addition to these appealing aspects of power-2 embeddings, it is worth noting that power- $p$ embeddings will not necessarily even exist when $p < 2$ . (See Appendix ""Embedding trees in Euclidean space"" for the proof.) Theorem 2 For any $p < 2$ , there is a tree which has no power- $p$ embedding. Remark 2 On the other hand, the existence result for power-2 embeddings, coupled with results of BIBREF22 , implies that power- $p$ tree embeddings do exist for any $p > 2$ . The simplicity of power-2 tree embeddings, as well as the fact that they may be approximated by a simple random model, suggests they may be a generally useful alternative to approaches to tree embeddings that require hyperbolic geometry BIBREF23 . How do parse tree embeddings in BERT compare to exact power-2 embeddings? To explore this question, we created a simple visualization tool. The input to each visualization is a sentence from the Penn Treebank with associated dependency parse trees (see Section ""Geometry of word senses"" ). We then extracted the token embeddings produced by BERT-large in layer 16 (following BIBREF8 ), transformed by the Hewitt and Manning’s “structural probe” matrix $B$ , yielding a set of points in 1024-dimensional space. We used PCA to project to two dimensions. (Other dimensionality-reduction methods, such as t-SNE and UMAP BIBREF24 , were harder to interpret.) To visualize the tree structure, we connected pairs of points representing words with a dependency relation. The color of each edge indicates the deviation from true tree distance. We also connected, with dotted line, pairs of words without a dependency relation but whose positions (before PCA) were far closer than expected. The resulting image lets us see both the overall shape of the tree embedding, and fine-grained information on deviation from a true power-2 embedding. Two example visualizations are shown in Figure 2 , next to traditional diagrams of their underlying parse trees. These are typical cases, illustrating some common patterns; for instance, prepositions are embedded unexpectedly close to words they relate to. (Figure 7 shows additional examples.) A natural question is whether the difference between these projected trees and the canonical ones is merely noise, or a more interesting pattern. By looking at the average embedding distances of each dependency relation (see Figure 3 ) , we can see that they vary widely from around 1.2 ( $compound:prt$ , $advcl$ ) to 2.5 ( $mwe$ , $parataxis$ , $auxpass$ ). Such systematic differences suggest that BERT's syntactic representation has an additional quantitative aspect beyond traditional dependency grammar.",Geometry of word senses,"BERT seems to have several ways of representing syntactic information. What about semantic features? Since embeddings produced by transformer models depend on context, it is natural to speculate that they capture the particular shade of meaning of a word as used in a particular sentence. (E.g., is “bark” an animal noise or part of a tree?) We explored geometric representations of word sense both qualitatively and quantitatively.",Visualization of word senses,"Our first experiment is an exploratory visualization of how word sense affects context embeddings. For data on different word senses, we collected all sentences used in the introductions to English-language Wikipedia articles. (Text outside of introductions was frequently fragmentary.) We created an interactive application, which we plan to make public. A user enters a word, and the system retrieves 1,000 sentences containing that word. It sends these sentences to BERT-base as input, and for each one it retrieves the context embedding for the word from a layer of the user's choosing. The system visualizes these 1,000 context embeddings using UMAP BIBREF24 , generally showing clear clusters relating to word senses. Different senses of a word are typically spatially separated, and within the clusters there is often further structure related to fine shades of meaning. In Figure 4 , for example, we not only see crisp, well-separated clusters for three meanings of the word “die,” but within one of these clusters there is a kind of quantitative scale, related to the number of people dying. See Appendix ""Additional word sense visualizations"" for further examples. The apparent detail in the clusters we visualized raises two immediate questions. First, is it possible to find quantitative corroboration that word senses are well-represented? Second, how can we resolve a seeming contradiction: in the previous section, we saw how position represented syntax; yet here we see position representing semantics.",Measurement of word sense disambiguation capability,"The crisp clusters seen in visualizations such as Figure 4 suggest that BERT may create simple, effective internal representations of word senses, putting different meanings in different locations. To test this hypothesis quantitatively, we test whether a simple classifier on these internal representations can perform well at word-sense disambiguation (WSD). We follow the procedure described in BIBREF10 , which performed a similar experiment with the ELMo model. For a given word with $n$ senses, we make a nearest-neighbor classifier where each neighbor is the centroid of a given word sense's BERT-base embeddings in the training data. To classify a new word we find the closest of these centroids, defaulting to the most commonly used sense if the word was not present in the training data. We used the data and evaluation from BIBREF25 : the training data was SemCor BIBREF26 (33,362 senses), and the testing data was the suite described in BIBREF25 (3,669 senses). The simple nearest-neighbor classifier achieves an F1 score of 71.1, higher than the current state of the art (Table 1 ), with the accuracy monotonically increasing through the layers. This is a strong signal that context embeddings are representing word-sense information. Additionally, an even higher score of 71.5 was obtained using the technique described in the following section. We hypothesized that there might also exist a linear transformation under which distances between embeddings would better reflect their semantic relationships–that is, words of the same sense would be closer together and words of different senses would be further apart. To explore this hypothesis, we trained a probe following Hewitt and Manning's methodology. We initialized a random matrix $B\in {R}^{k\times m}$ , testing different values for $m$ . Loss is, roughly, defined as the difference between the average cosine similarity between embeddings of words with different senses, and that between embeddings of the same sense. However, we clamped the cosine similarity terms to within $\pm 0.1$ of the pre-training averages for same and different senses. (Without clamping, the trained matrix simply ended up taking well-separated clusters and separating them further. We tested values between $0.05$ and $0.2$ for the clamping range and $0.1$ had the best performance.) Our training corpus was the same dataset from 4.1.2., filtered to include only words with at least two senses, each with at least two occurrences (for 8,542 out of the original 33,362 senses). Embeddings came from BERT-base (12 layers, 768-dimensional embeddings). We evaluate our trained probes on the same dataset and WSD task used in 4.1.2 (Table 1 ). As a control, we compare each trained probe against a random probe of the same shape. As mentioned in 4.1.2, untransformed BERT embeddings achieve a state-of-the-art accuracy rate of 71.1%. We find that our trained probes are able to achieve slightly improved accuracy down to $m=128$ . Though our probe achieves only a modest improvement in accuracy for final-layer embeddings, we note that we were able to more dramatically improve the performance of embeddings at earlier layers (see Appendix for details: Figure 10 ). This suggests there is more semantic information in the geometry of earlier-layer embeddings than a first glance might reveal. Our results also support the idea that word sense information may be contained in a lower-dimensional space. This suggests a resolution to the seeming contradiction mentioned above: a vector encodes both syntax and semantics, but in separate complementary subspaces.",Embedding distance and context: a concatenation experiment,"If word sense is affected by context, and encoded by location in space, then we should be able to influence context embedding positions by systematically varying their context. To test this hypothesis, we performed an experiment based on a simple and controllable context change: concatenating sentences where the same word is used in different senses. We picked 25,096 sentence pairs from SemCor, using the same keyword in different senses. E.g.: A: ""He thereupon went to London and spent the winter talking to men of wealth."" went: to move from one place to another. B: ""He went prone on his stomach, the better to pursue his examination."" went: to enter into a specified state. We define a matching and an opposing sense centroid for each keyword. For sentence A, the matching sense centroid is the average embedding for all occurrences of “went” used with sense A. A's opposing sense centroid is the average embedding for all occurrences of “went” used with sense B. We gave each individual sentence in the pair to BERT-base and recorded the cosine similarity between the keyword embeddings and their matching sense centroids. We also recorded the similarity between the keyword embeddings and their opposing sense centroids. We call the ratio between the two similarities the individual similarity ratio. Generally this ratio is greater than one, meaning that the context embedding for the keyword is closer to the matching centroid than the opposing one. We joined each sentence pair with the word ""and"" to create a single new sentence. We gave these concatenations to BERT and recorded the similarities between the keyword embeddings and their matching/opposing sense centroids. Their ratio is the concatenated similarity ratio. Our hypothesis was that the keyword embeddings in the concatenated sentence would move towards their opposing sense centroids. Indeed, we found that the average individual similarity ratio was higher than the average concatenated similarity ratio at every layer (see Figure 5 ). Concatenating a random sentence did not change the individual similarity ratios. If the ratio is less than one for any sentence, that means BERT has misclassified its keyword sense. We found that the misclassification rate was significantly higher for final-layer embeddings in the concatenated sentences compared to the individual sentences: 8.23% versus 2.43% respectively. We also measured the effect of projecting the final-layer keyword embeddings into the semantic subspace discussed in 4.1.3. After multiplying each embedding by our trained semantic probe, we obtained an average concatenated similarity ratio of 1.578 and individual similarity ratio of 1.875, which suggests that the transformed embeddings are closer to their matching sense centroids than the original embeddings (the original concatenated similarity ratio is 1.284 and the individual similarity ratio is 1.430). We also measured lower average misclassification rates for the transformed embeddings: 7.31% for concatenated sentences and 2.27% for individual sentences.",Conclusion and future work,"We have presented a series of experiments that shed light on BERT's internal representations of linguistic information. We have found evidence of syntactic representation in attention matrices, with certain directions in space representing particular dependency relations. We have also provided a mathematical justification for the squared-distance tree embedding found by Hewitt and Manning. Meanwhile, we have shown that just as there are specific syntactic subspaces, there is evidence for subspaces that represent semantic information. We also have shown how mistakes in word sense disambiguation may correspond to changes in internal geometric representation of word meaning. Our experiments also suggest an answer to the question of how all these different representations fit together. We conjecture that the internal geometry of BERT may be broken into multiple linear subspaces, with separate spaces for different syntactic and semantic information. Investigating this kind of decomposition is a natural direction for future research. What other meaningful subspaces exist? After all, there are many types of linguistic information that we have not looked for. A second important avenue of exploration is what the internal geometry can tell us about the specifics of the transformer architecture. Can an understanding of the geometry of internal representations help us find areas for improvement, or refine BERT's architecture? Acknowledgments: We would like to thank David Belanger, Tolga Bolukbasi, Jasper Snoek, and Ian Tenney for helpful feedback and discussions.",Embedding trees in Euclidean space,"Here we provide additional detail on the existence of various forms of tree embeddings. Isometric embeddings of a tree (with its intrinsic tree metric) into Euclidean space are rare. Indeed, such an embedding is impossible even a four-point tree $T$ , consisting of a root node $R$ with three children $C_1, C_2, C_3$ . If $f:T \rightarrow \mathbb {R}^n$ is a tree isometry then $||f(R) - f(C_1)) || = ||f(R) - f(C_2)) || = 1$ , and $||f(C_1) - f(C_2)) || = 2$ . It follows that $f(R)$ , $f(C_1)$ , $f(C_2)$ are collinear. The same can be said of $f(R)$ , $R$0 , and $R$1 , meaning that $R$2 . Since this four-point tree cannot be embedded, it follows the only trees that can be embedded are simply chains. Not only are isometric embeddings generally impossible, but power- $p$ embeddings may also be unavailable when $p < 2$ , as the following argument shows. Proof of Theorem ""Theorem 2"" We covered the case of $p = 1$ above. When $p < 1$ , even a tree of three points is impossible to embed without violating the triangle inequality. To handle the case when $1 < p < 2$ , consider a “star-shaped” tree of one root node with $k$ children; without loss of generality, assume the root node is embedded at the origin. Then in any power- $p$ embedding the other vertices will be sent to unit vectors, and for each pair of these unit vectors we have $||v_i - v_j||^p = 2$ . On the other hand, a well-known folk theorem (e.g., see BIBREF27 ) says that given $k$ unit vectors $v_1, ..., v_k$ at least one pair of distinct vectors has $v_i \cdot v_j \ge -1/(k - 1)$ . By the law of cosines, it follows that $||v_i - v_j|| \le \sqrt{2 + \frac{2}{k-1}}$ . For any $p < 2$ , there is a sufficiently large $k$ such that $||v_i - v_j||^p \le (\sqrt{2 + \frac{2}{k-1}})^p = (2 + \frac{2}{k-1})^{p/2} < 2$ . Thus for any $p < 2$ a large enough star-shaped tree cannot have a power- $p$ embedding.",Ideal vs. actual parse tree embeddings,"Figure 2 shows (left) a visualization of a BERT parse tree embedding (as defined by the context embeddings for individual words in a sentence). We compare with PCA projections of the canonical power-2 embedding of the same tree structure, as well as a random branch embedding. Finally, we display a completely randomly embedded tree as a control. The visualizations show a clear visual similarity between the BERT embedding and the two mathematical idealizations.",Additional BERT parse tree visualizations,Figure 7 shows four additional examples of PCA projections of BERT parse tree embeddings.,Additional word sense visualizations,"We provide two additional examples of word sense visualizations, hand-annotated to show key clusters. See Figure 8 and Figure 9 .",,,,,,,,,,,,,How were the feature representations evaluated?,2ba0c7576eb5b84463a59ff190d4793b67f40ccc,infinity,familiar,no,,50d8b4a941c26b89482c94ab324b5a274f9ced66,False,,,"Our work extends these explorations of the geometry of internal representations. Investigating how BERT represents syntax, we describe evidence that attention matrices contain grammatical representations. We also provide mathematical arguments that may explain the particular form of the parse tree embeddings described in BIBREF8 . Turning to semantics, using visualizations of the activations created by different pieces of text, we show suggestive evidence that BERT distinguishes word senses at a very fine level. Moreover, much of this semantic information appears to be encoded in a relatively low-dimensional subspace. To formalize what it means for attention matrices to encode linguistic features, we use an attention probe, an analog of edge probing BIBREF11 . An attention probe is a task for a pair of tokens, $(token_i, token_j)$ where the input is a model-wide attention vector formed by concatenating the entries $a_{ij}$ in every attention matrix from every attention head in every layer. The goal is to classify a given relation between the two tokens. If a linear model achieves reliable accuracy, it seems reasonable to say that the model-wide attention vector encodes that relation. We apply attention probes to the task of identifying the existence and type of dependency relation between two words.","Turning to semantics, using visualizations of the activations created by different pieces of text, we show suggestive evidence that BERT distinguishes word senses at a very fine level.  We apply attention probes to the task of identifying the existence and type of dependency relation between two words.",ca25dba72eccb56455e1b8cab552e6ea250bfb5c,35491e1e579f6d147f4793edce4c1a80ab2410e7,,,,,,,,,What linguistic features were probed for?,c58e60b99a6590e6b9a34de96c7606b004a4f169,infinity,familiar,no,,50d8b4a941c26b89482c94ab324b5a274f9ced66,False,,,"To formalize what it means for attention matrices to encode linguistic features, we use an attention probe, an analog of edge probing BIBREF11 . An attention probe is a task for a pair of tokens, $(token_i, token_j)$ where the input is a model-wide attention vector formed by concatenating the entries $a_{ij}$ in every attention matrix from every attention head in every layer. The goal is to classify a given relation between the two tokens. If a linear model achieves reliable accuracy, it seems reasonable to say that the model-wide attention vector encodes that relation. We apply attention probes to the task of identifying the existence and type of dependency relation between two words. Our first experiment is an exploratory visualization of how word sense affects context embeddings. For data on different word senses, we collected all sentences used in the introductions to English-language Wikipedia articles. (Text outside of introductions was frequently fragmentary.) We created an interactive application, which we plan to make public. A user enters a word, and the system retrieves 1,000 sentences containing that word. It sends these sentences to BERT-base as input, and for each one it retrieves the context embedding for the word from a layer of the user's choosing.",We apply attention probes to the task of identifying the existence and type of dependency relation between two words. Our first experiment is an exploratory visualization of how word sense affects context embeddings. ,6c0b689316f4eecb5c2309217c65dce64d03e392,35491e1e579f6d147f4793edce4c1a80ab2410e7,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3-Figure1-1.png,Figure 1: A model-wide attention vector for an ordered pair of tokens contains the scalar attention values for that pair in all attention heads and layers. Shown: BERT-base.,5-Figure2-1.png,Figure 2: Visualizing embeddings of two sentences after applying the Hewitt-Manning probe. We compare the parse tree (left images) with a PCA projection of context embeddings (right images).,5-Figure3-1.png,Figure 3: The average squared edge length between two words with a given dependency.,6-Figure4-1.png,"Figure 4: Embeddings for the word ""die"" in different contexts, visualized with UMAP. Sample points are annotated with corresponding sentences. Overall annotations (blue text) are added as a guide.",7-Table1-1.png,Table 1: [Left] F1 scores for WSD task. [Right] Semantic probe % accuracy on final-layer BERT-base embeddings,8-Figure5-1.png,Figure 5: Average ratio of similarity to sense A vs. similarity to sense B.,,,,,,,,,dependency relation between two words word sense,,11-Figure6-1.png,"Figure 6: PCA projection of the context embeddings for the sentence “The field has reserves of 21 million barrels.” transformed by Hewitt and Manning’s “structural probe” matrix, compared to the canonical power-2 embedding, a random branch embedding, and a completely random embedding.",12-Figure7-1.png,"Figure 7: Additional examples of BERT parse trees. In each pair, at left is a drawing of the abstract tree; at right is a PCA view of the embeddings. Colors are the same as in Figure 6.",13-Figure8-1.png,Figure 8: Context embeddings for “lie” as used in different sentences.,13-Figure9-1.png,Figure 9: Context embeddings for “lie” as used in different sentences.,14-Table2-1.png,"Table 2: Per-dependency results of multiclass linear classifier trained on attention vectors, with 300,000 training examples and 150,000 test examples.",,attention probes using visualizations of the activations created by different pieces of text,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,15-Figure10-1.png,Figure 10: Change in classification accuracy by layer for different probe dimensionalities.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deep Contextualized Acoustic Representations For Semi-Supervised Speech Recognition,"We propose a novel approach to semi-supervised automatic speech recognition (ASR). We first exploit a large amount of unlabeled audio data via representation learning, where we reconstruct a temporal slice of filterbank features from past and future context frames. The resulting deep contextualized acoustic representations (DeCoAR) are then used to train a CTC-based end-to-end ASR system using a smaller amount of labeled audio data. In our experiments, we show that systems trained on DeCoAR consistently outperform ones trained on conventional filterbank features, giving 42% and 19% relative improvement over the baseline on WSJ eval92 and LibriSpeech test-clean, respectively. Our approach can drastically reduce the amount of labeled data required; unsupervised training on LibriSpeech then supervision with 100 hours of labeled data achieves performance on par with training on all 960 hours directly.",Introduction,"Current state-of-the-art models for speech recognition require vast amounts of transcribed audio data to attain good performance. In particular, end-to-end ASR models are more demanding in the amount of training data required when compared to traditional hybrid models. While obtaining a large amount of labeled data requires substantial effort and resources, it is much less costly to obtain abundant unlabeled data. For this reason, semi-supervised learning (SSL) is often used when training ASR systems. The most commonly-used SSL approach in ASR is self-training BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4. In this approach, a smaller labeled set is used to train an initial seed model, which is applied to a larger amount of unlabeled data to generate hypotheses. The unlabeled data with the most reliable hypotheses are added to the training data for re-training. This process is repeated iteratively. However, self-training is sensitive to the quality of the hypotheses and requires careful calibration of the confidence measures. Other SSL approaches include: pre-training on a large amount of unlabeled data with restricted Boltzmann machines (RBMs) BIBREF5; entropy minimization BIBREF6, BIBREF7, BIBREF8, where the uncertainty of the unlabeled data is incorporated as part of the training objective; and graph-based approaches BIBREF9, where the manifold smoothness assumption is exploited. Recently, transfer learning from large-scale pre-trained language models (LMs) BIBREF10, BIBREF11, BIBREF12 has shown great success and achieved state-of-the-art performance in many NLP tasks. The core idea of these approaches is to learn efficient word representations by pre-training on massive amounts of unlabeled text via word completion. These representations can then be used for downstream tasks with labeled data. Inspired by this, we propose an SSL framework that learns efficient, context-aware acoustic representations using a large amount of unlabeled data, and then applies these representations to ASR tasks using a limited amount of labeled data. In our implementation, we perform acoustic representation learning using forward and backward LSTMs and a training objective that minimizes the reconstruction error of a temporal slice of filterbank features given previous and future context frames. After pre-training, we fix these parameters and add output layers with connectionist temporal classification (CTC) loss for the ASR task. The paper is organized as follows: in Section SECREF2, we give a brief overview of related work in acoustic representation learning and SSL. In Section SECREF3, we describe an implementation of our SSL framework with DeCoAR learning. We describe the experimental setup in Section SECREF4 and the results on WSJ and LibriSpeech in Section SECREF5, followed by our conclusions in Section SECREF6.",Related work,"While semi-supervised learning has been exploited in a plethora of works in hybrid ASR system, there are very few work done in the end-to-end counterparts BIBREF3, BIBREF13, BIBREF14. In BIBREF3, an intermediate representation of speech and text is learned via a shared encoder network. To train these representation, the encoder network was trained to optimize a combination of ASR loss, text-to-text autoencoder loss and inter-domain loss. The latter two loss functions did not require paired speech and text data. Learning efficient acoustic representation can be traced back to restricted Boltzmann machine BIBREF15, BIBREF16, BIBREF17, which allows pre-training on large amounts of unlabeled data before training the deep neural network acoustic models. More recently, acoustic representation learning has drawn increasing attention BIBREF18, BIBREF19, BIBREF20, BIBREF21, BIBREF22, BIBREF23 in speech processing. For example, an autoregressive predictive coding model (APC) was proposed in BIBREF20 for unsupervised speech representation learning and was applied to phone classification and speaker verification. WaveNet auto-encoders BIBREF21 proposed contrastive predictive coding (CPC) to learn speech representations and was applied on unsupervised acoustic unit discovery task. Wav2vec BIBREF22 proposed a multi-layer convolutional neural network optimized via a noise contrastive binary classification and was applied to WSJ ASR tasks. Unlike the speech representations described in BIBREF22, BIBREF20, our representations are optimized to use bi-directional contexts to auto-regressively reconstruct unseen frames. Thus, they are deep contextualized representations that are functions of the entire input sentence. More importantly, our work is a general semi-supervised training framework that can be applied to different systems and requires no architecture change.",DEep COntextualized Acoustic Representations ::: Representation learning from unlabeled data,"Our approach is largely inspired by ELMo BIBREF10. In ELMo, given a sequence of $T$ tokens $(w_1,w_2,...,w_T)$, a forward language model (implemented with an LSTM) computes its probability using the chain rule decomposition: Similarly, a backward language model computes the sequence probability by modeling the probability of token $w_t$ given its future context $w_{t+1},\cdots , w_T$ as follows: ELMo is trained by maximizing the joint log-likelihood of both forward and backward language model probabilities: where $\Theta _x$ is the parameter for the token representation layer, $\Theta _s$ is the parameter for the softmax layer, and $\overrightarrow{\Theta }_{\text{LSTM}}$, $\overleftarrow{\Theta }_{\text{LSTM}}$ are the parameters of forward and backward LSTM layers, respectively. As the word representations are learned with neural networks that use past and future information, they are referred to as deep contextualized word representations. For speech processing, predicting a single frame $\mathbf {x}_t$ may be a trivial task, as it could be solved by exploiting the temporal smoothness of the signal. In the APC model BIBREF20, the authors propose predicting a frame $K$ steps ahead of the current one. Namely, the model aims to minimize the $\ell _1$ loss between an acoustic feature vector $\mathbf {x}$ at time $t+K$ and a reconstruction $\mathbf {y}$ predicted at time $t$: $\sum _{t=1}^{T-K} |\mathbf {x}_{t+K} - \mathbf {y}_t|$. They conjectured this would induce the model to learn more global structure rather than simply leveraging local information within the signal. We propose combining the bidirectionality of ELMo and the reconstruction objective of APC to give deep contextualized acoustic representations (DeCoAR). We train the model to predict a slice of $K$ acoustic feature vectors, given past and future acoustic vectors. As depicted on the left side of Figure FIGREF1, a stack of forward and backward LSTMs are applied to the entire unlabeled input sequence $\mathbf {X} = (\mathbf {x}_1,\cdots ,\mathbf {x}_T)$. The network computes a hidden representation that encodes information from both previous and future frames (i.e. $\overrightarrow{\mathbf {z}}_t, \overleftarrow{\mathbf {z}}_t$) for each frame $\mathbf {x}_t$. Given a sequence of acoustic feature inputs $(\mathbf {x}_1, ..., \mathbf {x}_{T}) \in \mathbb {R}^d$, for each slice $(\mathbf {x}_t, \mathbf {x}_{t+1}, ..., \mathbf {x}_{t+K})$ starting at time step $t$, our objective is defined as follows: where $[\overrightarrow{\mathbf {z}}_t; \overleftarrow{\mathbf {z}}_{t}] \in \mathbb {R}^{2h}$ are the concatenated forward and backward states from the last LSTM layer, and is a position-dependent feed-forward network with 512 hidden dimensions. The final loss $\mathcal {L}$ is summed over all possible slices in the entire sequence: Note this can be implemented efficiently as a layer which predicts these $(K+1)$ frames at each position $t$, all at once. We compare with the use of unidirectional LSTMs and various slice sizes in Section SECREF5.",DEep COntextualized Acoustic Representations ::: End-to-end ASR training with labeled data,"After we have pre-trained the DeCoAR on unlabeled data, we freeze the parameters in the architecture. To train an end-to-end ASR system using labeled data, we remove the reconstruction layer and add two BLSTM layers with CTC loss BIBREF24, as illustrated on the right side of Figure FIGREF1. The DeCoAR vectors induced by the labeled data in the forward and backward layers are concatenated. We fine-tune the parameters of this ASR-specific new layer on the labeled data. While we use LSTMs and CTC loss in our implementation, our SSL approach should work for other layer choices (e.g. TDNN, CNN, self-attention) and other downstream ASR models (e.g. hybrid, seq2seq, RNN transducers) as well.",Experimental Setup ::: Data,"We conducted our experiments on the WSJ and LibriSpeech datasets, pre-training by using one of the two training sets as unlabeled data. To simulate the SSL setting in WSJ, we used 30%, 50% as well as 100% of labeled data for ASR training, consisting of 25 hours, 40 hours, and 81 hours, respectively. We used dev93 for validation and eval92 and evaluation. For LibriSpeech, the amount of training data used varied from 100 hours to the entire 960 hours. We used dev-clean for validation and test-clean, test-other for evaluation.",Experimental Setup ::: ASR systems,"Our experiments consisted of three different setups: 1) a fully-supervised system using all labeled data; 2) an SSL system using wav2vec features; 3) an SSL system using our proposed DeCoAR features. All models used were based on deep BLSTMs with the CTC loss criterion. In the supervised ASR setup, we used conventional log-mel filterbank features, which were extracted with a 25ms sliding window at a 10ms frame rate. The features were normalized via mean subtraction and variance normalization on a per-speaker basis. The model had 6 BLSTM layers, with 512 cells in each direction. We found that increasing the number of cells to a larger number did not further improve the performance and thus used it as our best supervised ASR baseline. The output CTC labels were 71 phonemes plus one blank symbol. In the SSL ASR setup, we pre-trained a 4-layer BLSTM (1024 cells per sub-layer) to learn DeCoAR features according to the loss defined in Equation DISPLAY_FORM4 and use a slice size of 18. We optimized the network with SGD and use a Noam learning rate schedule, where we started with a learning rate of 0.001, gradually warm up for 500 updates, and then perform inverse square-root decay. We grouped the input sequences by length with a batch size of 64, and trained the models on 8 GPUs. After the representation network was trained, we froze the parameters, and added a projection layer, followed by 2-layer BLSTM with CTC loss on top it. We fed the labeled data to the network. For comparison, we obtained 512-dimensional wav2vec representations BIBREF22 from the wav2vec-large model. Their model was pre-trained on 960-hour LibriSpeech data with constrastive loss and had 12 convolutional layers with skip connections. For evaluation purposes, we applied WFST-based decoding using EESEN BIBREF25. We composed the CTC labels, lexicons and language models (unpruned trigram LM for WSJ, 4-gram for LibriSpeech) into a decoding graph. The acoustic model score was set to $0.8$ and $1.0$ for WSJ and LibriSpeech, respectively, and the blank symbol prior scale was set to $0.3$ for both tasks. We report the performance in word error rate (WER).",Results ::: Semi-supervised WSJ results,"Table TABREF14 shows our results on semi-supervised WSJ. We demonstrate that DeCoAR feature outperforms filterbank and wav2vec features, with a relative improvement of 42% and 20%, respectively. The lower part of the table shows that with smaller amounts of labeled data, the DeCoAR features are significantly better than the filterbank features: Compared to the system trained on 100% labeled data with filterbank features, we achieve comparable results on eval92 using 30% of the labeled data and better performance on eval92 using 50% of the labeled data.",Results ::: Semi-supervised LibriSpeech results,"Table TABREF7 shows the results on semi-supervised LibriSpeech. Both our representations and wav2vecBIBREF22 are trained on 960h LibriSpeech data. We conduct our semi-supervised experiments using 100h (train-clean-100), 360h (train-clean-360), 460h, and 960h of training data. Our approach outperforms both the baseline and wav2vec model in each SSL scenario. One notable observation is that using only 100 hours of transcribed data achieves very similar performance to the system trained on the full 960-hour data with filterbank features. On the more challenging test-other dataset, we also achieve performance on par with the filterbank baseline using a 360h subset. Furthermore, training with with our DeCoAR features approach improves the baseline even when using the exact same training data (960h). Note that while BIBREF26 introduced SpecAugment to significantly improve LibriSpeech performance via data augmentation, and BIBREF27 achieved state-of-the-art results using both hybrid and end-to-end models, our approach focuses on the SSL case with less labeled training data via our DeCoAR features.",Results ::: Ablation Study and Analysis ::: Context window size,"We study the effect of the context window size during pre-training. Table TABREF20 shows that masking and predicting a larger slice of frames can actually degrade performance, while increasing training time. A similar effect was found in SpanBERT BIBREF28, another deep contextual word representation which found that masking a mean span of 3.8 consecutive words was ideal for their word reconstruction objective.",Results ::: Ablation Study and Analysis ::: Unidirectional versus bidirectional context,"Next, we study the importance of bidirectional context by training a unidirectional LSTM, which corresponds to only using $\overrightarrow{\mathbf {z}}_t$ to predict $\mathbf {x}_t, \cdots , \mathbf {x}_{t+K}$. Table TABREF22 shows that this unidirectional model achieves comparable performance to the wav2vec model BIBREF22, suggesting that bidirectionality is the largest contributor to DeCoAR's improved performance.",Results ::: Ablation Study and Analysis ::: DeCoAR as denoiser,"Since our model is trained by predicting masked frames, DeCoAR has the side effect of learning decoder feed-forward networks $\text{FFN}_i$ which reconstruct the $(t+i)$-th filterbank frame from contexts $\overrightarrow{\mathbf {z}}_t$ and $\overleftarrow{\mathbf {z}}_{t+K}$. In this section, we consider the spectrogram reconstructed by taking the output of $\text{FFN}_i$ at all times $t$. The qualitative result is depicted in Figure FIGREF15 where the slice size is 18. We see that when $i=0$ (i.e., when reconstructing the $t$-th frame from $[\overrightarrow{\mathbf {z}}_t; \overleftarrow{\mathbf {z}}_{t+K}]$), the reconstruction is almost perfect. However, as soon as one predicts unseen frames $i=1, 4, 8$ (of 16), the reconstruction becomes more simplistic, but not by much. Background energy in the silent frames 510-550 is zeroed out. By $i=8$ artifacts begin to occur, such as an erroneous sharp band of energy being predicted around frame 555. This behavior is compatible with recent NLP works that interpret contextual word representations as denoising autoencoders BIBREF12. The surprising ability of DeCoAR to broadly reconstruct a frame $\overrightarrow{\mathbf {x}}_{t+{K/2}}$ in the middle of a missing 16-frame slice suggests that its representations $[\overrightarrow{\mathbf {z}}_t; \overleftarrow{\mathbf {z}}_{t+K}]$ capture longer-term phonetic structure during unsupervised pre-training, as with APC BIBREF20. This motivates its success in the semi-supervised ASR task with only two additional layers, as it suggests DeCoAR learns phonetic representations similar to those likely learned by the first 4 layers of a corresponding end-to-end ASR model.",Conclusion,"In this paper, we introduce a novel semi-supervised learning approach for automatic speech recognition. We first propose a novel objective for a deep bidirectional LSTM network, where large amounts of unlabeled data are used to learn deep contextualized acoustic representations (DeCoAR). These DeCoAR features are used as the representations of labeled data to train a CTC-based end-to-end ASR model. In our experiments, we show a 42% relative improvement on WSJ compared to a baseline trained on log-mel filterbank features. On LibriSpeech, we achieve similar performance to training on 960 hours of labeled by pretraining then using only 100 hours of labeled data. While we use BLSTM-CTC as our ASR model, our approach can be applied to other end-to-end ASR models.",,,,,,,,,,,,,,,,,,,What are baseline models on WSJ eval92 and LibriSpeech test-clean?,04cab3325e20c61f19846674bf9a2c46ea60c449,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,,,"More recently, acoustic representation learning has drawn increasing attention BIBREF18, BIBREF19, BIBREF20, BIBREF21, BIBREF22, BIBREF23 in speech processing. For example, an autoregressive predictive coding model (APC) was proposed in BIBREF20 for unsupervised speech representation learning and was applied to phone classification and speaker verification. WaveNet auto-encoders BIBREF21 proposed contrastive predictive coding (CPC) to learn speech representations and was applied on unsupervised acoustic unit discovery task. Wav2vec BIBREF22 proposed a multi-layer convolutional neural network optimized via a noise contrastive binary classification and was applied to WSJ ASR tasks. Our experiments consisted of three different setups: 1) a fully-supervised system using all labeled data; 2) an SSL system using wav2vec features; 3) an SSL system using our proposed DeCoAR features. All models used were based on deep BLSTMs with the CTC loss criterion.",Wav2vec BIBREF22 proposed a multi-layer convolutional neural network optimized via a noise contrastive binary classification and was applied to WSJ ASR tasks. Our experiments consisted of three different setups: 1) a fully-supervised system using all labeled data; 2) an SSL system using wav2vec features; 3) an SSL system using our proposed DeCoAR features. All models used were based on deep BLSTMs with the CTC loss criterion.,c1287a391c6963744a30102954f020b545a51409,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2-Figure1-1.png,Fig. 1. Illustration of our semi-supervised speech recognition system.,3-Table1-1.png,Table 1. Semi-supervised LibriSpeech results.,3-Table2-1.png,"Table 2. Semi-supervised WSJ results. Unlabeled indicates the amount of unlabeled data used for acoustic representation learning, and Labeled indicates the amount of labeled data in ASR training.",4-Figure2-1.png,Fig. 2. The spectrograms for a portion of LibriSpeech dev-clean utterance 2428-83699-0034 reconstructed by generated by taking the i-th frame prediction (slice size 18) at each time step. The reconstruction becomes less noisy but more simplistic when predicting further into the masked slice.,4-Table4-1.png,Table 4. Comparison of WERs on WSJ after pre-training using unidirectional or bidirectional context on LibriSpeech.,4-Table3-1.png,Table 3. Comparison of WERs on WSJ after pre-training with different slice window sizes (K + 1) on LibriSpeech.,,,,,,,,,,,,,,,,,,,,,,Wav2vec BIBREF22 a fully-supervised system using all labeled data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
User Generated Data: Achilles' heel of BERT,"Pre-trained language models such as BERT are known to perform exceedingly well on various NLP tasks and have even established new State-Of-The-Art (SOTA) benchmarks for many of these tasks. Owing to its success on various tasks and benchmark datasets, industry practitioners have started to explore BERT to build applications solving industry use cases. These use cases are known to have much more noise in the data as compared to benchmark datasets. In this work we systematically show that when the data is noisy, there is a significant degradation in the performance of BERT. Specifically, we performed experiments using BERT on popular tasks such sentiment analysis and textual similarity. For this we work with three well known datasets - IMDB movie reviews, SST-2 and STS-B to measure the performance. Further, we examine the reason behind this performance drop and identify the shortcomings in the BERT pipeline.",Introduction,"In recent times, pre-trained contextual language models have led to significant improvement in the performance for many NLP tasks. Among the family of these models, the most popular one is BERT BIBREF0, which is also the focus of this work. The strength of the BERT model FIGREF2 stems from its transformerBIBREF1 based encoder architectureFIGREF1. While it is still not very clear as to why BERT along with its embedding works so well for downstream tasks when it is fine tuned, there has been some work in this direction that that gives some important cluesBIBREF2, BIBREF3. At a high level, BERT’s pipelines looks as follows: given a input sentence, BERT tokenizes it using wordPiece tokenizerBIBREF4. The tokens are then fed as input to the BERT model and it learns contextualized embeddings for each of those tokens. It does so via pre-training on two tasks - Masked Language Model (MLM)BIBREF0 and Next Sentence Prediction (NSP)BIBREF0. The focus of this work is to understand the issues that a practitioner can run into while trying to use BERT for building NLP applications in industrial settings. It is a well known fact that NLP applications in industrial settings often have to deal with the noisy data. There are different kinds of possible noise namely non-canonical text such as spelling mistakes, typographic errors, colloquialisms, abbreviations, slang, internet jargon, emojis, embedded metadata (such as hashtags, URLs, mentions), non standard syntactic constructions and spelling variations, grammatically incorrect text, mixture of two or more languages to name a few. Such noisy data is a hallmark of user generated text content and commonly found on social media, chats, online reviews, web forums to name a few. Owing to this noise a common issue that NLP models have to deal with is Out Of Vocabulary (OOV) words. These are words that are found in test and production data but not part of training data. In this work we highlight how BERT fails to handle Out Of Vocabulary(OOV) words, given its limited vocabulary. We show that this negatively impacts the performance of BERT when working with user generated text data and evaluate the same. This evaluation is motivated from the business use case we are solving where we are building a dialogue system to screen candidates for blue collar jobs. Our candidate user base, coming from underprivileged backgrounds, are often high school graduates. This coupled with ‘fat finger’ problem over a mobile keypad leads to a lot of typos and spelling mistakes in the responses sent to the dialogue system. Hence, for this work we focus on spelling mistakes as the noise in the data. While this work is motivated from our business use case, our findings are applicable across various use cases in industry - be it be sentiment classification on twitter data or topic detection of a web forum. To simulate noise in the data, we begin with a clean dataset and introduce spelling errors in a fraction of words present in it. These words are chosen randomly. We will explain this process in detail later. Spelling mistakes introduced mimic the typographical errors in the text introduced by our users. We then use the BERT model for tasks using both clean and noisy datasets and compare the results. We show that the introduction of noise leads to a significant drop in performance of the BERT model for the task at hand as compared to clean dataset. We further show that as we increase the amount of noise in the data, the performance degrades sharply.",Related Work,"In recent years pre-trained language models ((e.g. ELMoBIBREF5, BERTBIBREF0) have made breakthroughs in several natural language tasks. These models are trained over large corpora that are not human annotated and are easily available. Chief among these models is BERTBIBREF0. The popularity of BERT stems from its ability to be fine-tuned for a variety of downstream NLP tasks such as text classification, regression, named-entity recognition, question answeringBIBREF0, machine translationBIBREF6 etc. BERT has been able to establish State-of-the-art (SOTA) results for many of these tasks. People have been able to show how one can leverage BERT to improve searchBIBREF7. Owing to its success, researchers have started to focus on uncovering drawbacks in BERT, if any. BIBREF8 introduce TEXTFOOLER, a system to generate adversarial text. They apply it to NLP tasks of text classification and textual entailment to attack the BERT model. BIBREF9 evaluate three models - RoBERTa, XLNet, and BERT in Natural Language Inference (NLI) and Question Answering (QA) tasks for robustness. They show that while RoBERTa, XLNet and BERT are more robust than recurrent neural network models to stress tests for both NLI and QA tasks; these models are still very fragile and show many unexpected behaviors. BIBREF10 discuss length-based and sentence-based misclassification attacks for the Fake News Detection task trained using a context-aware BERT model and they show 78% and 39% attack accuracy respectively. Our contribution in this paper is to answer that can we use large language models like BERT directly over user generated data.",Experiment,"For our experiments, we use pre-trained BERT implementation as given by huggingface transformer library. We use the BERTBase uncased model. We work with three datasets namely - IMDB movie reviewsBIBREF11, Stanford Sentiment Treebank (SST-2) BIBREF12 and Semantic Textual Similarity (STS-B) BIBREF13. IMDB dataset is a popular dataset for sentiment analysis tasks, which is a binary classification problem with equal number of positive and negative examples. Both STS-B and SST-2 datasets are a part of GLUE benchmark[2] tasks . In STS-B too, we predict positive and negative sentiments. In SST-2 we predict textual semantic similarity between two sentences. It is a regression problem where the similarity score varies between 0 to 5. To evaluate the performance of BERT we use standard metrics of F1-score for imdb and STS-B, and Pearson-Spearman correlation for SST-2. In Table TABREF5, we give the statistics for each of the datasets. We take the original datasets and add varying degrees of noise (i.e. spelling errors to word utterances) to create datasets for our experiments. From each dataset, we create 4 additional datasets each with varying percentage levels of noise in them. For example from IMDB, we create 4 variants, each having 5%, 10%, 15% and 20% noise in them. Here, the number denotes the percentage of words in the original dataset that have spelling mistakes. Thus, we have one dataset with no noise and 4 variants datasets with increasing levels of noise. Likewise, we do the same for SST-2 and STS-B. All the parameters of the BERTBase model remain the same for all 5 experiments on the IMDB dataset and its 4 variants. This also remains the same across other 2 datasets and their variants. For all the experiments, the learning rate is set to 4e-5, for optimization we use Adam optimizer with epsilon value 1e-8. We ran each of the experiments for 10 and 50 epochs.",Results,"Let us discuss the results from the above mentioned experiments. We show the plots of accuracy vs noise for each of the tasks. For IMDB, we fine tune the model for the sentiment analysis task. We plot F1 score vs % of error, as shown in Figure FIGREF6. Figure FIGREF6imdba shows the performance after fine tuning for 10 epochs, while Figure FIGREF6imdbb shows the performance after fine tuning for 50 epochs. Similarly, Figure FIGREF9ssta and Figure FIGREF9sstb) shows F1 score vs % of error for Sentiment analysis on SST-2 dataset after fine tuning for 10 and 50 epochs respectively. Figure FIGREF12stsa and FIGREF12stsb shows Pearson-Spearman correlation vs % of error for textual semantic similarity on STS-B dataset after fine tuning for 10 and 50 epochs respectively.",Results ::: Key Findings,"It is clear from the above plots that as we increase the percentage of error, for each of the three tasks, we see a significant drop in BERT’s performance. Also, from the plots it is evident that the reason for this drop in performance is introduction of noise (spelling mistakes). After all we get very good numbers, for each of the three tasks, when there is no error (0.0 % error). To understand the reason behind the drop in performance, first we need to understand how BERT processes input text data. BERT uses WordPiece tokenizer to tokenize the text. WordPiece tokenizer utterances based on the longest prefix matching algorithm to generate tokens . The tokens thus obtained are fed as input of the BERT model. When it comes to tokenizing noisy data, we see a very interesting behaviour from WordPiece tokenizer. Owing to the spelling mistakes, these words are not directly found in BERT’s dictionary. Hence WordPiece tokenizer tokenizes noisy words into subwords. However, it ends up breaking them into subwords whose meaning can be very different from the meaning of the original word. Often, this changes the meaning of the sentence completely, therefore leading to substantial dip in the performance. To understand this better, let us look into two examples, one each from the IMDB and STS-B datasets respectively, as shown below. Here, (a) is the sentence as it appears in the dataset ( before adding noise) while (b) is the corresponding sentence after adding noise. The mistakes are highlighted with italics. The sentences are followed by the corresponding output of the WordPiece tokenizer on these sentences: In the output ‘##’ is WordPiece tokenizer’s way of distinguishing subwords from words. ‘##’ signifies subwords as opposed to words. Example 1 (imdb example): “that loves its characters and communicates something rather beautiful about human nature” (0% error) “that loves 8ts characters abd communicates something rathee beautiful about human natuee” (5% error) Output of wordPiece tokenizer: ['that', 'loves', 'its', 'characters', 'and', 'communicate', '##s', 'something', 'rather', 'beautiful', 'about', 'human','nature'] (0% error IMDB example) ['that', 'loves', '8', '##ts', 'characters', 'abd', 'communicate','##s', 'something','rat', '##hee', 'beautiful', 'about', 'human','nat', '##ue', '##e'] (5% error IMDB example) Example 2(STS example): “poor ben bratt could n't find stardom if mapquest emailed himpoint-to-point driving directions.” (0% error) “poor ben bratt could n't find stardom if mapquest emailed him point-to-point drivibg dirsctioge.” (5% error) Output of wordPiece tokenizer: ['poor', 'ben', 'brat', '##t', 'could', 'n', ""'"", 't', 'find','star', '##dom', 'if', 'map', '##quest', 'email', '##ed', 'him','point', '-', 'to', '-', 'point', 'driving', 'directions', '.'] (0% error STS example) ['poor', 'ben', 'brat', '##t', 'could', 'n', ""'"", 't', 'find','star', '##dom', 'if', 'map', '##quest', 'email', '##ed', 'him', 'point', '-', 'to', '-', 'point', 'dr', '##iv', '##ib','##g','dir','##sc', '##ti', '##oge', '.'] (5% error STS example) In example 1, the tokenizer splits communicates into [‘communicate’, ‘##s’] based on longest prefix matching because there is no exact match for “communicates” in BERT vocabulary. The longest prefix in this case is “communicate” and left over is “s” both of which are present in the vocabulary of BERT. We have contextual embeddings for both “communicate” and “##s”. By using these two embeddings, one can get an approximate embedding for “communicates”. However, this approach goes for a complete toss when the word is misspelled. In example 1(b) the word natuee (‘nature’ is misspelled) is split into ['nat', '##ue', '##e'] based on the longest prefix match. Combining the three embeddings one cannot approximate the embedding of nature. This is because the word nat has a very different meaning (it means ‘a person who advocates political independence for a particular country’). This misrepresentation in turn impacts the performance of downstream subcomponents of BERT bringing down the overall performance of BERT model. Hence, as we systematically introduce more errors, the quality of output of the tokenizer degrades further, resulting in the overall performance drop. Our results and analysis shows that one cannot apply BERT blindly to solve NLP problems especially in industrial settings. If the application you are developing gets data from channels that are known to introduce noise in the text, then BERT will perform badly. Examples of such scenarios are applications working with twitter data, mobile based chat system, user comments on platforms like youtube, reddit to name a few. The reason for the introduction of noise could vary - while for twitter, reddit it's often deliberate because that is how users prefer to write, while for mobile based chat it often suffers from ‘fat finger’ typing error problem. Depending on the amount of noise in the data, BERT can perform well below expectations. We further conducted experiments with different tokenizers other than WordPiece tokenizer. For this we used stanfordNLP WhiteSpace BIBREF14 and Character N-gram BIBREF15 tokenizers. WhiteSpace tokenizer splits text into tokens based on white space. Character N-gram tokenizer splits words that have more than n characters in them. Thus, each token has at most n characters in them. The resultant tokens from the respective tokenizer are fed to BERT as inputs. For our case, we work with n = 6. Results of these experiments are presented in Table TABREF25. Even though wordPiece tokenizer has the issues stated earlier, it is still performing better than whitespace and character n-gram tokenizer. This is primarily because of the vocabulary overlap between STS-B dataset and BERT vocabulary.",Conclusion and Future Work,"In this work we systematically studied the effect of noise (spelling mistakes) in user generated text data on the performance of BERT. We demonstrated that as the noise increases, BERT’s performance drops drastically. We further investigated the BERT system to understand the reason for this drop in performance. We show that the problem lies with how misspelt words are tokenized to create a representation of the original word. There are 2 ways to address the problem - either (i) preprocess the data to correct spelling mistakes or (ii) incorporate ways in BERT architecture to make it robust to noise. The problem with (i) is that in most industrial settings this becomes a separate project in itself. We leave (ii) as a future work to fix the issues.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,What is the performance change of the textual semantic similarity task when no error and maximum errors (noise) are present?,7f9bc06cfa81a4e3f7df4c69a1afef146ed5a1cf,five,research,somewhat,,34c35a1877e453ecaebcf625df3ef788e1953cc4,False,,"10 Epochs: pearson-Spearman correlation  drops  60 points when error increase by 20%
50 Epochs: pearson-Spearman correlation  drops  55 points when error increase by 20%",Figure FIGREF12stsa and FIGREF12stsb shows Pearson-Spearman correlation vs % of error for textual semantic similarity on STS-B dataset after fine tuning for 10 and 50 epochs respectively. FLOAT SELECTED: Figure 5: Pearson-Spearman correlation vs % of error for textual semantic similarity on STS-B dataset,Figure FIGREF12stsa and FIGREF12stsb shows Pearson-Spearman correlation vs % of error for textual semantic similarity on STS-B dataset after fine tuning for 10 and 50 epochs respectively. FLOAT SELECTED: Figure 5: Pearson-Spearman correlation vs % of error for textual semantic similarity on STS-B dataset,4586a85a0c6b7494ed0d33cf42a776425ebf3db4,a0b403873302db7cada39008f04d01155ef68f4f,,,,,,,,,Which sentiment analysis data set has a larger performance drop when a 10% error is introduced?,58a340c338e41002c8555202ef9adbf51ddbb7a1,five,research,somewhat,,34c35a1877e453ecaebcf625df3ef788e1953cc4,False,,SST-2 dataset,"Let us discuss the results from the above mentioned experiments. We show the plots of accuracy vs noise for each of the tasks. For IMDB, we fine tune the model for the sentiment analysis task. We plot F1 score vs % of error, as shown in Figure FIGREF6. Figure FIGREF6imdba shows the performance after fine tuning for 10 epochs, while Figure FIGREF6imdbb shows the performance after fine tuning for 50 epochs. Similarly, Figure FIGREF9ssta and Figure FIGREF9sstb) shows F1 score vs % of error for Sentiment analysis on SST-2 dataset after fine tuning for 10 and 50 epochs respectively. FLOAT SELECTED: Figure 3: F1 score vs % of error for Sentiment analysis on IMDB dataset","We show the plots of accuracy vs noise for each of the tasks. For IMDB, we fine tune the model for the sentiment analysis task. We plot F1 score vs % of error, as shown in Figure FIGREF6. Figure FIGREF6imdba shows the performance after fine tuning for 10 epochs, while Figure FIGREF6imdbb shows the performance after fine tuning for 50 epochs. Similarly, Figure FIGREF9ssta and Figure FIGREF9sstb) shows F1 score vs % of error for Sentiment analysis on SST-2 dataset after fine tuning for 10 and 50 epochs respectively. FLOAT SELECTED: Figure 3: F1 score vs % of error for Sentiment analysis on IMDB dataset",a9d965a94a17909ee4147998cf85a6d5c9d09f43,a0b403873302db7cada39008f04d01155ef68f4f,What kind is noise is present in typical industrial data?,0ca02893bda50007f7a76e7c8804101718fbb01c,five,research,somewhat,,34c35a1877e453ecaebcf625df3ef788e1953cc4,False,,,"The focus of this work is to understand the issues that a practitioner can run into while trying to use BERT for building NLP applications in industrial settings. It is a well known fact that NLP applications in industrial settings often have to deal with the noisy data. There are different kinds of possible noise namely non-canonical text such as spelling mistakes, typographic errors, colloquialisms, abbreviations, slang, internet jargon, emojis, embedded metadata (such as hashtags, URLs, mentions), non standard syntactic constructions and spelling variations, grammatically incorrect text, mixture of two or more languages to name a few. Such noisy data is a hallmark of user generated text content and commonly found on social media, chats, online reviews, web forums to name a few. Owing to this noise a common issue that NLP models have to deal with is Out Of Vocabulary (OOV) words. These are words that are found in test and production data but not part of training data. In this work we highlight how BERT fails to handle Out Of Vocabulary(OOV) words, given its limited vocabulary. We show that this negatively impacts the performance of BERT when working with user generated text data and evaluate the same.","It is a well known fact that NLP applications in industrial settings often have to deal with the noisy data. There are different kinds of possible noise namely non-canonical text such as spelling mistakes, typographic errors, colloquialisms, abbreviations, slang, internet jargon, emojis, embedded metadata (such as hashtags, URLs, mentions), non standard syntactic constructions and spelling variations, grammatically incorrect text, mixture of two or more languages to name a few. Such noisy data is a hallmark of user generated text content and commonly found on social media, chats, online reviews, web forums to name a few.",6e57952c2e8ac59bc53d4ec46781d590cc27444c,a0b403873302db7cada39008f04d01155ef68f4f,,,,,,,,What is the reason behind the drop in performance using BERT for some popular task?,751aa2b1531a17496536887288699cc8d5c3cec9,five,research,somewhat,,34c35a1877e453ecaebcf625df3ef788e1953cc4,False,,,"It is clear from the above plots that as we increase the percentage of error, for each of the three tasks, we see a significant drop in BERT’s performance. Also, from the plots it is evident that the reason for this drop in performance is introduction of noise (spelling mistakes). After all we get very good numbers, for each of the three tasks, when there is no error (0.0 % error). To understand the reason behind the drop in performance, first we need to understand how BERT processes input text data. BERT uses WordPiece tokenizer to tokenize the text. WordPiece tokenizer utterances based on the longest prefix matching algorithm to generate tokens . The tokens thus obtained are fed as input of the BERT model. When it comes to tokenizing noisy data, we see a very interesting behaviour from WordPiece tokenizer. Owing to the spelling mistakes, these words are not directly found in BERT’s dictionary. Hence WordPiece tokenizer tokenizes noisy words into subwords. However, it ends up breaking them into subwords whose meaning can be very different from the meaning of the original word. Often, this changes the meaning of the sentence completely, therefore leading to substantial dip in the performance.","To understand the reason behind the drop in performance, first we need to understand how BERT processes input text data. BERT uses WordPiece tokenizer to tokenize the text. WordPiece tokenizer utterances based on the longest prefix matching algorithm to generate tokens . The tokens thus obtained are fed as input of the BERT model. When it comes to tokenizing noisy data, we see a very interesting behaviour from WordPiece tokenizer. Owing to the spelling mistakes, these words are not directly found in BERT’s dictionary. Hence WordPiece tokenizer tokenizes noisy words into subwords. However, it ends up breaking them into subwords whose meaning can be very different from the meaning of the original word. Often, this changes the meaning of the sentence completely, therefore leading to substantial dip in the performance.",738deeb7036bbc3cc85bee72b7fc2d39aa62c29b,a0b403873302db7cada39008f04d01155ef68f4f,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2-Figure1-1.png,Figure 1: BERT architecture [1],2-Figure2-1.png,Figure 2: The Transformer model architecture [2],3-Table1-1.png,Table 1: Number of utterances in each datasets,4-Figure3-1.png,Figure 3: F1 score vs % of error for Sentiment analysis on IMDB dataset,4-Figure4-1.png,Figure 4: F1 score vs % of error for Sentiment analysis on SST-2 data,5-Figure5-1.png,Figure 5: Pearson-Spearman correlation vs % of error for textual semantic similarity on STS-B dataset,,,,,,,,,," non-canonical text such as spelling mistakes, typographic errors, colloquialisms, abbreviations, slang, internet jargon, emojis, embedded metadata (such as hashtags, URLs, mentions), non standard syntactic constructions and spelling variations, grammatically incorrect text, mixture of two or more languages",6-Table2-1.png,Table 2: Comparative results on STS-B dataset with different tokenizers,,,,,,,,,,,,,,,,,,,"Hence WordPiece tokenizer tokenizes noisy words into subwords. However, it ends up breaking them into subwords whose meaning can be very different from the meaning of the original word. Often, this changes the meaning of the sentence completely, therefore leading to substantial dip in the performance.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Universal Dependency Parsing for Hindi-English Code-switching,"Code-switching is a phenomenon of mixing grammatical structures of two or more languages under varied social constraints. The code-switching data differ so radically from the benchmark corpora used in NLP community that the application of standard technologies to these data degrades their performance sharply. Unlike standard corpora, these data often need to go through additional processes such as language identification, normalization and/or back-transliteration for their efficient processing. In this paper, we investigate these indispensable processes and other problems associated with syntactic parsing of code-switching data and propose methods to mitigate their effects. In particular, we study dependency parsing of code-switching data of Hindi and English multilingual speakers from Twitter. We present a treebank of Hindi-English code-switching tweets under Universal Dependencies scheme and propose a neural stacking model for parsing that efficiently leverages part-of-speech tag and syntactic tree annotations in the code-switching treebank and the preexisting Hindi and English treebanks. We also present normalization and back-transliteration models with a decoding process tailored for code-switching data. Results show that our neural stacking parser is 1.5% LAS points better than the augmented parsing model and our decoding process improves results by 3.8% LAS points over the first-best normalization and/or back-transliteration.",Introduction,"Code-switching (henceforth CS) is the juxtaposition, within the same speech utterance, of grammatical units such as words, phrases, and clauses belonging to two or more different languages BIBREF0 . The phenomenon is prevalent in multilingual societies where speakers share more than one language and is often prompted by multiple social factors BIBREF1 . Moreover, code-switching is mostly prominent in colloquial language use in daily conversations, both online and offline. Most of the benchmark corpora used in NLP for training and evaluation are based on edited monolingual texts which strictly adhere to the norms of a language related, for example, to orthography, morphology, and syntax. Social media data in general and CS data, in particular, deviate from these norms implicitly set forth by the choice of corpora used in the community. This is the reason why the current technologies often perform miserably on social media data, be it monolingual or mixed language data BIBREF2 , BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 . CS data offers additional challenges over the monolingual social media data as the phenomenon of code-switching transforms the data in many ways, for example, by creating new lexical forms and syntactic structures by mixing morphology and syntax of two languages making it much more diverse than any monolingual corpora BIBREF4 . As the current computational models fail to cater to the complexities of CS data, there is often a need for dedicated techniques tailored to its specific characteristics. Given the peculiar nature of CS data, it has been widely studied in linguistics literature BIBREF8 , BIBREF0 , BIBREF1 , and more recently, there has been a surge in studies concerning CS data in NLP as well BIBREF9 , BIBREF9 , BIBREF3 , BIBREF10 , BIBREF11 , BIBREF12 , BIBREF13 , BIBREF14 , BIBREF15 , BIBREF16 . Besides the individual computational works, a series of shared-tasks and workshops on preprocessing and shallow syntactic analysis of CS data have also been conducted at multiple venues such as Empirical Methods in NLP (EMNLP 2014 and 2016), International Conference on NLP (ICON 2015 and 2016) and Forum for Information Retrieval Evaluation (FIRE 2015 and 2016). Most of these works have attempted to address preliminary tasks such as language identification, normalization and/or back-transliteration as these data often need to go through these additional processes for their efficient processing. In this paper, we investigate these indispensable processes and other problems associated with syntactic parsing of code-switching data and propose methods to mitigate their effects. In particular, we study dependency parsing of Hindi-English code-switching data of multilingual Indian speakers from Twitter. Hindi-English code-switching presents an interesting scenario for the parsing community. Mixing among typologically diverse languages will intensify structural variations which will make parsing more challenging. For example, there will be many sentences containing: (1) both SOV and SVO word orders, (2) both head-initial and head-final genitives, (3) both prepositional and postpositional phrases, etc. More importantly, none among the Hindi and English treebanks would provide any training instance for these mixed structures within individual sentences. In this paper, we present the first code-switching treebank that provides syntactic annotations required for parsing mixed-grammar syntactic structures. Moreover, we present a parsing pipeline designed explicitly for Hindi-English CS data. The pipeline comprises of several modules such as a language identification system, a back-transliteration system, and a dependency parser. The gist of these modules and our overall research contributions are listed as follows:",Preliminary Tasks,"As preliminary steps before parsing of CS data, we need to identify the language of tokens and normalize and/or back-transliterate them to enhance the parsing performance. These steps are indispensable for processing CS data and without them the performance drops drastically as we will see in Results Section. We need normalization of non-standard word forms and back-transliteration of Romanized Hindi words for addressing out-of-vocabulary problem, and lexical and syntactic ambiguity introduced due to contracted word forms. As we will train separate normalization and back-transliteration models for Hindi and English, we need language identification for selecting which model to use for inference for each word form separately. Moreover, we also need language information for decoding best word sequences.",Language Identification,"For language identification task, we train a multilayer perceptron (MLP) stacked on top of a recurrent bidirectional LSTM (Bi-LSTM) network as shown in Figure ""Results"" .  skip=0.5em figureLanguage identification network  We represent each token by a concatenated vector of its English embedding, back-transliterated Hindi embedding, character Bi-LSTM embedding and flag embedding (English dictionary flag and word length flag with length bins of 0-3, 4-6, 7-10, and 10-all). These concatenated vectors are passed to a Bi-LSTM network to generate a sequence of hidden representations which encode the contextual information spread across the sentence. Finally, output layer uses the feed-forward neural network with a softmax function for a probability distribution over the language tags. We train the network on our CS training set concatenated with the data set provided in ICON 2015 shared task (728 Facebook comments) on language identification and evaluate it on the datasets from bhat-EtAl:2017:EACLshort. We achieved the state-of-the-art performance on both development and test sets BIBREF13 . The results are shown in Table ""Results"" .  skip=0.5em tableLanguage Identification results on CS test set. ",Normalization and Back-transliteration,"We learn two separate but similar character-level models for normalization-cum-transliteration of noisy Romanized Hindi words and normalization of noisy English words. We treat both normalization and back-transliteration problems as a general sequence to sequence learning problem. In general, our goal is to learn a mapping for non-standard English and Romanized Hindi word forms to standard forms in their respective scripts. In case of Hindi, we address the problem of normalization and back-transliteration of Romanized Hindi words using a single model. We use the attention-based encoder-decoder model of Luong BIBREF17 with global attention for learning. For Hindi, we train the model on the transliteration pairs (87,520) from the Libindic transliteration project and Brahmi-Net BIBREF18 which are further augmented with noisy transliteration pairs (1,75,668) for normalization. Similarly, for normalization of noisy English words, we train the model on noisy word forms (4,29,715) synthetically generated from the English vocabulary. We use simple rules such as dropping non-initial vowels and replacing consonants based on their phonological proximity to generate synthetic data for normalization. Figure ""Supplemental Material"" shows some of the noisy forms generated from standard word forms using simple and finite rules which include vowel elision (please $\rightarrow $ pls), interchanging similar consonants and vowels (cousin $\rightarrow $ couzin), replacing consonant or vowel clusters with a single letter (Twitter $\rightarrow $ Twiter), etc. From here onwards, we will refer to both normalization and back-transliteration as normalization.  figureSynthetic normalization pairs generated for a sample of English words using hand crafted rules.  At inference time, our normalization models will predict the most likely word form for each input word. However, the single-best output from the model may not always be the best option considering an overall sentential context. Contracted word forms in social media content are quite often ambiguous and can represent different standard word forms. For example, noisy form `pt' can expand to different standard word forms such as `put', `pit', `pat', `pot' and `pet'. The choice of word selection will solely depend on the sentential context. To select contextually relevant forms, we use exact search over n-best normalizations from the respective models extracted using beam-search decoding. The best word sequence is selected using the Viterbi decoding over $b^n$ word sequences scored by a trigram language model. $b$ is the size of beam-width and $n$ is the sentence length. The language models are trained on the monolingual data of Hindi and English using KenLM toolkit BIBREF19 . For each word, we extract five best normalizations ( $b$ =5). Decoding the best word sequence is a non-trivial problem for CS data due to lack of normalized and back-transliterated CS data for training a language model. One obvious solution is to apply decoding on individual language fragments in a CS sentence BIBREF20 . One major problem with this approach is that the language models used for scoring are trained on complete sentences but are applied on sentence fragments. Scoring individual CS fragments might often lead to wrong word selection due to incomplete context, particularly at fragment peripheries. We solve this problem by using a 3-step decoding process that works on two separate versions of a CS sentence, one in Hindi, and one in English. In the first step, we replace first-best back-transliterated forms of Hindi words by their translation equivalents using a Hindi-English bilingual lexicon. An exact search is used over the top `5' normalizations of English words, the translation equivalents of Hindi words and the actual word itself. In the second step, we decode best word sequence over Hindi version of the sentence by replacing best English word forms decoded from the first step by their translation equivalents. An exact search is used over the top `5' normalizations of Hindi words, the dictionary equivalents of decoded English words and the original words. In the final step, English and Hindi words are selected from their respective decoded sequences using the predicted language tags from the language identification system. Note that the bilingual mappings are only used to aid the decoding process by making the CS sentences lexically monolingual so that the monolingual language models could be used for scoring. They are not used in the final decoded output. The overall decoding process is shown in Figure 1 . Both of our normalization and back-transliteration systems are evaluated on the evaluation set of bhat-EtAl:2017:EACLshort. Results of our systems are reported in Table ""Supplemental Material"" with a comparison of accuracies based on the nature of decoding used. The results clearly show the significance of our 3-step decoding over first-best and fragment-wise decoding.  skip=0.5em tableNormalization accuracy based on the number of noisy tokens in the evaluation set. FB = First Best, and FW = Fragment Wise ",Universal Dependencies for Hindi-English,"Recently bhat-EtAl:2017:EACLshort provided a CS dataset for the evaluation of their parsing models which they trained on the Hindi and English Universal Dependency (UD) treebanks. We extend this dataset by annotating 1,448 more sentences. Following bhat-EtAl:2017:EACLshort we first sampled CS data from a large set of tweets of Indian language users that we crawled from Twitter using Tweepy–a Twitter API wrapper. We then used a language identification system trained on ICON dataset (see Section ""Preliminary Tasks"" ) to filter Hindi-English CS tweets from the crawled Twitter data. Only those tweets were selected that satisfied a minimum ratio of 30:70(%) code-switching. From this dataset, we manually selected 1,448 tweets for annotation. The selected tweets are thoroughly checked for code-switching ratio. For POS tagging and dependency annotation, we used Version 2 of Universal dependency guidelines BIBREF21 , while language tags are assigned based on the tag set defined in BIBREF22 , BIBREF23 . The dataset was annotated by two expert annotators who have been associated with annotation projects involving syntactic annotations for around 10 years. Nonetheless, we also ensured the quality of the manual annotations by carrying an inter-annotator agreement analysis. We randomly selected a dataset of 150 tweets which were annotated by both annotators for both POS tagging and dependency structures. The inter-annotator agreement has a 96.20% accuracy for POS tagging and a 95.94% UAS and a 92.65% LAS for dependency parsing. We use our dataset for training while the development and evaluation sets from bhat-EtAl:2017:EACLshort are used for tuning and evaluation of our models. Since the annotations in these datasets follow version 1.4 of the UD guidelines, we converted them to version 2 by using carefully designed rules. The statistics about the data are given in Table ""Supplemental Material"" .  skip=0.5em tableData Statistics. Dev set is used for tuning model parameters, while Test set is used for evaluation. ",Dependency Parsing,"We adapt Kiperwasser and Goldberg kiperwasser2016simple transition-based parser as our base model and incorporate POS tag and monolingual parse tree information into the model using neural stacking, as shown in Figures ""Parsing Algorithm"" and ""Stacking Models"" .",Parsing Algorithm,"Our parsing models are based on an arc-eager transition system BIBREF24 . The arc-eager system defines a set of configurations for a sentence w $_1$ ,...,w $_n$ , where each configuration C = (S, B, A) consists of a stack S, a buffer B, and a set of dependency arcs A. For each sentence, the parser starts with an initial configuration where S = [ROOT], B = [w $_1$ ,...,w $_n$ ] and A = $\emptyset $ and terminates with a configuration C if the buffer is empty and the stack contains the ROOT. The parse trees derived from transition sequences are given by A. To derive the parse tree, the arc-eager system defines four types of transitions ( $t$ ): Shift, Left-Arc, Right-Arc, and Reduce. We use the training by exploration method of goldberg2012dynamic for decoding a transition sequence which helps in mitigating error propagation at evaluation time. We also use pseudo-projective transformations of nivre2005 to handle a higher percentage of non-projective arcs in the CS data ( $\sim $ 2%). We use the most informative scheme of head+path to store the transformation information.  skip=0.5em figurePOS tagging and parsing network based on stack-propagation model proposed in BIBREF25 . ",Base Models,"Our base model is a stack of a tagger network and a parser network inspired by stack-propagation model of zhang-weiss:2016:P16-1. The parameters of the tagger network are shared and act as a regularization on the parsing model. The model is trained by minimizing a joint negative log-likelihood loss for both tasks. Unlike zhang-weiss:2016:P16-1, we compute the gradients of the log-loss function simultaneously for each training instance. While the parser network is updated given the parsing loss only, the tagger network is updated with respect to both tagging and parsing losses. Both tagger and parser networks comprise of an input layer, a feature layer, and an output layer as shown in Figure ""Parsing Algorithm"" . Following zhang-weiss:2016:P16-1, we refer to this model as stack-prop. The input layer of the tagger encodes each input word in a sentence by concatenating a pre-trained word embedding with its character embedding given by a character Bi-LSTM. In the feature layer, the concatenated word and character representations are passed through two stacked Bi-LSTMs to generate a sequence of hidden representations which encode the contextual information spread across the sentence. The first Bi-LSTM is shared with the parser network while the other is specific to the tagger. Finally, output layer uses the feed-forward neural network with a softmax function for a probability distribution over the Universal POS tags. We only use the forward and backward hidden representations of the focus word for classification. Similar to the tagger network, the input layer encodes the input sentence using word and character embeddings which are then passed to the shared Bi-LSTM. The hidden representations from the shared Bi-LSTM are then concatenated with the dense representations from the feed-forward network of the tagger and passed through the Bi-LSTM specific to the parser. This ensures that the tagging network is penalized for the parsing error caused by error propagation by back-propagating the gradients to the shared tagger parameters BIBREF25 . Finally, we use a non-linear feed-forward network to predict the labeled transitions for the parser configurations. From each parser configuration, we extract the top node in the stack and the first node in the buffer and use their hidden representations from the parser specific Bi-LSTM for classification.  skip=0.5em figureCode-switching tweet showing grammatical fragments from Hindi and English.",Stacking Models,"It seems reasonable that limited CS data would complement large monolingual data in parsing CS data and a parsing model which leverages both data would significantly improve parsing performance. While a parsing model trained on our limited CS data might not be enough to accurately parse the individual grammatical fragments of Hindi and English, the preexisting Hindi and English treebanks are large enough to provide sufficient annotations to capture their structure. Similarly, parsing model(s) trained on the Hindi and English data may not be able to properly connect the divergent fragments of the two languages as the model lacks evidence for such mixed structures in the monolingual data. This will happen quite often as Hindi and English are typologicalls very diverse (see Figure UID16 ).  skip=0.5em figureNeural Stacking-based parsing architecture for incorporating monolingual syntactic knowledge.  As we discussed above, we adapted feature-level neural stacking BIBREF25 , BIBREF26 for joint learning of POS tagging and parsing. Similarly, we also adapt this stacking approach for incorporating the monolingual syntactic knowledge into the base CS model. Recently, wang-EtAl:2017:Long6 used neural stacking for injecting syntactic knowledge of English into a graph-based Singlish parser which lead to significant improvements in parsing performance. Unlike wang-EtAl:2017:Long6, our base stacked models will allow us to transfer the POS tagging knowledge as well along the parse tree knowledge. As shown in Figure ""Stacking Models"" , we transfer both POS tagging and parsing information from the source model trained on augmented Hindi and English data. For tagging, we augment the input layer of the CS tagger with the MLP layer of the source tagger. For transferring parsing knowledge, hidden representations from the parser specific Bi-LSTM of the source parser are augmented with the input layer of the CS parser which already includes the hidden layer of the CS tagger, word and character embeddings. In addition, we also add the MLP layer of the source parser to the MLP layer of the CS parser. The MLP layers of the source parser are generated using raw features from CS parser configurations. Apart from the addition of these learned representations from the source model, the overall CS model remains similar to the base model shown in Figure ""Parsing Algorithm"" . The tagging and parsing losses are back-propagated by traversing back the forward paths to all trainable parameters in the entire network for training and the whole network is used collectively for inference.",Experiments,"We train all of our POS tagging and parsing models on training sets of the Hindi and English UD-v2 treebanks and our Hindi-English CS treebank. For tuning and evaluation, we use the development and evaluation sets from bhat-EtAl:2017:EACLshort. We conduct multiple experiments in gold and predicted settings to measure the effectiveness of the sub-modules of our parsing pipeline. In predicted settings, we use the POS taggers separately trained on the Hindi, English and CS training sets. All of our models use word embeddings from transformed Hindi and English embedding spaces to address the problem of lexical differences prevalent in CS sentences.",Hyperparameters,"For language identification, POS tagging and parsing models, we include the lexical features in the input layer of our neural networks using 64-dimension pre-trained word embeddings, while we use randomly initialized embeddings within a range of $[-0.1$ , $+0.1]$ for non-lexical units such as POS tags and dictionary flags. We use 32-dimensional character embeddings for all the three models and 32-dimensional POS tag embeddings for pipelined parsing models. The distributed representation of Hindi and English vocabulary are learned separately from the Hindi and English monolingual corpora. The English monolingual data contains around 280M sentences, while the Hindi data is comparatively smaller and contains around 40M sentences. The word representations are learned using Skip-gram model with negative sampling which is implemented in word2vec toolkit BIBREF27 . We use the projection algorithm of artetxe2016learning to transform the Hindi and English monolingual embeddings into same semantic space using a bilingual lexicon ( $\sim $ 63,000 entries). The bilingual lexicon is extracted from ILCI and Bojar Hindi-English parallel corpora BIBREF28 , BIBREF29 . For normalization models, we use 32-dimensional character embeddings uniformly initialized within a range of $[-0.1, +0.1]$ . The POS tagger specific Bi-LSTMs have 128 cells while the parser specific Bi-LSTMs have 256 cells. The Bi-LSTM in the language identification model has 64 cells. The character Bi-LSTMs have 32 cells for all three models. The hidden layer of MLP has 64 nodes for the language identification network, 128 nodes for the POS tagger and 256 nodes for the parser. We use hyperbolic tangent as an activation function in all tasks. In the normalization models, we use single layered Bi-LSTMs with 512 cells for both encoding and decoding of character sequences. For language identification, POS tagging and parsing networks, we use momentum SGD for learning with a minibatch size of 1. The LSTM weights are initialized with random orthonormal matrices as described in BIBREF30 . We set the dropout rate to 30% for POS tagger and parser Bi-LSTM and MLP hidden states while for language identification network we set the dropout to 50%. All three models are trained for up to 100 epochs, with early stopping based on the development set. In case of normalization, we train our encoder-decoder models for 25 epochs using vanilla SGD. We start with a learning rate of $1.0$ and after 8 epochs reduce it to half for every epoch. We use a mini-batch size of 128, and the normalized gradient is rescaled whenever its norm exceeds 5. We use a dropout rate of 30% for the Bi-LSTM. Language identification, POS tagging and parsing code is implemented in DyNet BIBREF31 and for normalization without decoding, we use Open-NMT toolkit for neural machine translation BIBREF32 . All the code is available at https://github.com/irshadbhat/nsdp-cs and the data is available at https://github.com/CodeMixedUniversalDependencies/UD_Hindi_English.",Results,"In Table ""Results"" , we present the results of our main model that uses neural stacking for learning POS tagging and parsing and also for knowledge transfer from the Bilingual model. Transferring POS tagging and syntactic knowledge using neural stacking gives 1.5% LAS improvement over a naive approach of data augmentation. The Bilingual model which is trained on the union of Hindi and English data sets is least accurate of all our parsing models. However, it achieves better or near state-of-the-art results on the Hindi and English evaluation sets (see Table ""Results"" ). As compared to the best system in CoNLL 2017 Shared Task on Universal Dependencies BIBREF33 , BIBREF34 , our results for English are around 3% better in LAS, while for Hindi only 0.5% LAS points worse. The CS model trained only on the CS training data is slightly more accurate than the Bilingual model. Augmenting the CS data to Hindi-English data complements their syntactic structures relevant for parsing mixed grammar structures which are otherwise missing in the individual datasets. The average improvements of around $\sim $ 5% LAS clearly show their complementary nature.  skip=0.5em tableAccuracy of different parsing models on the evaluation set. POS tags are jointly predicted with parsing. LID = Language tag, TRN = Transliteration/normalization.  Table ""Results"" summarizes the POS tagging results on the CS evaluation set. The tagger trained on the CS training data is 2.5% better than the Bilingual tagger. Adding CS training data to Hindi and English train sets further improves the accuracy by 1%. However, our stack-prop tagger achieves the highest accuracy of 90.53% by leveraging POS information from Bilingual tagger using neural stacking.  skip=0.5em tablePOS and parsing results for Hindi and English monolingual test sets using pipeline and stack-prop models.   skip=0.5em tablePOS tagging accuracies of different models on CS evaluation set. SP = stack-prop. ",Conclusion,"In this paper, we have presented a dependency parser designed explicitly for Hindi-English CS data. The parser uses neural stacking architecture of zhang-weiss:2016:P16-1 and chen-zhang-liu:2016:EMNLP2016 for learning POS tagging and parsing and for knowledge transfer from Bilingual models trained on Hindi and English UD treebanks. We have also presented normalization and back-transliteration models with a decoding process tailored for CS data. Our neural stacking parser is 1.5% LAS points better than the augmented parsing model and 3.8% LAS points better than the one which uses first-best normalizations.",,,,,,,,,,,,,,,,,How big is the provided treebank?,df2839dbd68ed9d5d186e6c148fa42fce60de64f,infinity,unfamiliar,no,,50d8b4a941c26b89482c94ab324b5a274f9ced66,False,,"1448 sentences more than the dataset from Bhat et al., 2017","Recently bhat-EtAl:2017:EACLshort provided a CS dataset for the evaluation of their parsing models which they trained on the Hindi and English Universal Dependency (UD) treebanks. We extend this dataset by annotating 1,448 more sentences. Following bhat-EtAl:2017:EACLshort we first sampled CS data from a large set of tweets of Indian language users that we crawled from Twitter using Tweepy–a Twitter API wrapper. We then used a language identification system trained on ICON dataset (see Section ""Preliminary Tasks"" ) to filter Hindi-English CS tweets from the crawled Twitter data. Only those tweets were selected that satisfied a minimum ratio of 30:70(%) code-switching. From this dataset, we manually selected 1,448 tweets for annotation. The selected tweets are thoroughly checked for code-switching ratio. For POS tagging and dependency annotation, we used Version 2 of Universal dependency guidelines BIBREF21 , while language tags are assigned based on the tag set defined in BIBREF22 , BIBREF23 . The dataset was annotated by two expert annotators who have been associated with annotation projects involving syntactic annotations for around 10 years. Nonetheless, we also ensured the quality of the manual annotations by carrying an inter-annotator agreement analysis. We randomly selected a dataset of 150 tweets which were annotated by both annotators for both POS tagging and dependency structures. The inter-annotator agreement has a 96.20% accuracy for POS tagging and a 95.94% UAS and a 92.65% LAS for dependency parsing.","Recently bhat-EtAl:2017:EACLshort provided a CS dataset for the evaluation of their parsing models which they trained on the Hindi and English Universal Dependency (UD) treebanks. We extend this dataset by annotating 1,448 more sentences.",988165c00acb794b1771513009c5973b17a5a2e4,2d5eb3b62fdd55fc54d7803863a493e795720f0d,,,,,,,,,What is LAS metric?,3996438cef34eb7bedaa6745b190c69553cf246b,infinity,unfamiliar,no,,50d8b4a941c26b89482c94ab324b5a274f9ced66,True,,,,,7937d4de6d6d634561edf7034ddad867fb3b2bee,fe09bc2ef2737a3258f978e26226dcbac1b3f948,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,20-Figure2.1-1.png,Figure 2.1: Dependency tree of Example sentence 3.,23-Figure2.2-1.png,Figure 2.2: Transition sequence for Example sentence 3 based on Arc-eager algorithm.,29-Table3.1-1.png,Table 3.1: Universal dependency relations,30-Table3.2-1.png,Table 3.2: UD lables with their meaning.,32-Figure3.1-1.png,Figure 3.1: Few examples trees our Hindi-English Code-Mixed dependency treebank.,32-Table3.3-1.png,"Table 3.3: Statistics on training, testing and development sets used in all the experiments reported in this thesis.",,,,,,,,,,,35-Figure4.1-1.png,Figure 4.1: Language identification network,36-Table4.1-1.png,Table 4.1: Language Identification results on CS development set and test set.,37-Figure4.2-1.png,Figure 4.2: Synthetic normalization pairs generated for a sample of English words using hand crafted rules.,39-Figure4.3-1.png,Figure 4.3: The figure shows a 3-step decoding process for the sentence “Yar cn anyone tel me k twitr account bnd ksy krty hn plz” (Friend can anyone tell me how to close twitter account please).,39-Table4.2-1.png,"Table 4.2: Normalization accuracy based on the number of noisy tokens in the evaluation set. FB = First Best, and FW = Fragment Wise",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,41-Figure4.4-1.png,Figure 4.4: Devanagari to Roman character mapping table,44-Figure5.2-1.png,Figure 5.2: Resolving structural ambiguity problem using a token-level language tag.,,,,,,,,45-Figure5.3-1.png,Figure 5.3: First Pass: Parse individual fragments using their respective parsing models. Second Pass: Parse the root nodes of the parsed fragments by the matrix language parsing model.,45-Figure5.4-1.png,Figure 5.4: Example case of an imperfect segmentation,46-Figure5.5-1.png,Figure 5.5: First Pass: Parse subordinate language first. Second Pass: Parse the roots of the subordinate fragments with the fragments of matrix language using the matrix language parser.,46-Figure5.6-1.png,Figure 5.6: Example case of an imperfect segmentation,,,,,,,,,,,,,,,,,,,,,,,,,48-Table5.1-1.png,"Table 5.1: POS Tagging accuracies for monolingual and multilingual models. LID = Language tag, G = Gold LID, A = Auto LID.",48-Table5.2-1.png,Table 5.2: Accuracy of different parsing strategies on Code-switching as well as Hindi and English evaluation sets. Multipass f |s = fragment-wise and subordinate-first parsing methods.,50-Table5.3-1.png,Table 5.3: Parsing accuracies with exact search and k-best search (k = 5),54-Figure6.1-1.png,Figure 6.1: POS tagging and parsing network based on stack-propagation model proposed in [82].,55-Figure6.2-1.png,Figure 6.2: Code-switching tweet showing grammatical fragments from Hindi and English.,56-Figure6.3-1.png,Figure 6.3: Neural Stacking-based parsing architecture for incorporating monolingual syntactic knowledge.,59-Table6.1-1.png,"Table 6.1: Accuracy of different parsing models on the evaluation set. POS tags are jointly predicted with parsing. LID = Language tag, TRN = Transliteration/normalization.",59-Table6.2-1.png,Table 6.2: POS and parsing results for Hindi and English monolingual test sets using pipeline and stack-prop models.,60-Table6.3-1.png,Table 6.3: POS tagging accuracies of different models on CS evaluation set. SP = stack-prop.,60-Table6.4-1.png,"Table 6.4: Accuracy of different parsing models on the test set using predicted language tags, normalized/back-transliterated words and predicted POS tags. POS tags are predicted separately before parsing. In Neural Stacking model, only parsing knowledge from the Bilingual model is transferred.",61-Table6.5-1.png,Table 6.5: Impact of normalization and back-transliteration on POS tagging and parsing models.,61-Table6.6-1.png,Table 6.6: Impact of monolingual and cross-lingual embeddings on stacking model performance.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CM-Net: A Novel Collaborative Memory Network for Spoken Language Understanding,"Spoken Language Understanding (SLU) mainly involves two tasks, intent detection and slot filling, which are generally modeled jointly in existing works. However, most existing models fail to fully utilize co-occurrence relations between slots and intents, which restricts their potential performance. To address this issue, in this paper we propose a novel Collaborative Memory Network (CM-Net) based on the well-designed block, named CM-block. The CM-block firstly captures slot-specific and intent-specific features from memories in a collaborative manner, and then uses these enriched features to enhance local context representations, based on which the sequential information flow leads to more specific (slot and intent) global utterance representations. Through stacking multiple CM-blocks, our CM-Net is able to alternately perform information exchange among specific memories, local contexts and the global utterance, and thus incrementally enriches each other. We evaluate the CM-Net on two standard benchmarks (ATIS and SNIPS) and a self-collected corpus (CAIS). Experimental results show that the CM-Net achieves the state-of-the-art results on the ATIS and SNIPS in most of criteria, and significantly outperforms the baseline models on the CAIS. Additionally, we make the CAIS dataset publicly available for the research community.",Introduction,"Spoken Language Understanding (SLU) is a core component in dialogue systems. It typically aims to identify the intent and semantic constituents for a given utterance, which are referred as intent detection and slot filling, respectively. Past years have witnessed rapid developments in diverse deep learning models BIBREF0, BIBREF1 for SLU. To take full advantage of supervised signals of slots and intents, and share knowledge between them, most of existing works apply joint models that mainly based on CNNs BIBREF2, BIBREF3, RNNs BIBREF4, BIBREF5, and asynchronous bi-model BIBREF6. Generally, these joint models encode words convolutionally or sequentially, and then aggregate hidden states into a utterance-level representation for the intent prediction, without interactions between representations of slots and intents. Intuitively, slots and intents from similar fields tend to occur simultaneously, which can be observed from Figure FIGREF2 and Table TABREF3. Therefore, it is beneficial to generate the representations of slots and intents with the guidance from each other. Some works explore enhancing the slot filling task unidirectionally with the guidance from intent representations via gating mechanisms BIBREF7, BIBREF8, while the predictions of intents lack the guidance from slots. Moreover, the capsule network with dynamic routing algorithms BIBREF9 is proposed to perform interactions in both directions. However, there are still two limitations in this model. The one is that the information flows from words to slots, slots to intents and intents to words in a pipeline manner, which is to some extent limited in capturing complicated correlations among words, slots and intents. The other is that the local context information which has been shown highly useful for the slot filling BIBREF10, is not explicitly modeled. In this paper, we try to address these issues, and thus propose a novel $\mathbf {C}$ollaborative $\mathbf {M}$emory $\mathbf {N}$etwork, named CM-Net. The main idea is to directly capture semantic relationships among words, slots and intents, which is conducted simultaneously at each word position in a collaborative manner. Specifically, we alternately perform information exchange among the task-specific features referred from memories, local context representations and global sequential information via the well-designed block, named CM-block, which consists of three computational components: Deliberate Attention: Obtaining slot-specific and intent-specific representations from memories in a collaborative manner. Local Calculation: Updating local context representations with the guidances of the referred slot and intent representations in the previous Deliberate Attention.  Global Recurrence: Generating specific (slot and intent) global sequential representations based on local context representations from the previous Local Calculation. Above components in each CM-block are conducted consecutively, which are responsible for encoding information from different perspectives. Finally, multiple CM-blocks are stacked together, and construct our CM-Net. We firstly conduct experiments on two popular benchmarks, SNIPS BIBREF11 and ATIS BIBREF12, BIBREF13. Experimental results show that the CM-Net achieves the state-of-the-art results in 3 of 4 criteria (e.g., intent detection accuracy on ATIS) on both benchmarks. Additionally, trials on our self-collected dataset, named CAIS, demonstrate the effectiveness and generalizability of the CM-Net. Our main contributions are as follows: We propose a novel CM-Net for SLU, which explicitly captures semantic correlations among words, slots and intents in a collaborative manner, and incrementally enriches the specific features, local context representations and global sequential representations through stacked CM-blocks. Our CM-Net achieves the state-of-the-art results on two major SLU benchmarks (ATIS and SNIPS) in most of criteria. We contribute a new corpus CAIS with manual annotations of slot tags and intent labels to the research community.",Background,"In principle, the slot filling is treated as a sequence labeling task, and the intent detection is a classification problem. Formally, given an utterance $X = \lbrace x_1, x_2, \cdots , x_N \rbrace $ with $N$ words and its corresponding slot tags $Y^{slot} = \lbrace y_1, y_2, \cdots , y_N \rbrace $, the slot filling task aims to learn a parameterized mapping function $f_{\theta } : X \rightarrow Y $ from input words to slot tags. For the intent detection, it is designed to predict the intent label $\hat{y}^{int}$ for the entire utterance $X$ from the predefined label set $S^{int}$. Typically, the input utterance is firstly encoded into a sequence of distributed representations $\mathbf {X} = \lbrace \mathbf {x}_1, \mathbf {x}_2, \cdots , \mathbf {x}_N\rbrace $ by character-aware and pre-trained word embeddings. Afterwards, the following bidirectional RNNs are applied to encode the embeddings $\mathbf {X}$ into context-sensitive representations $\mathbf {H} = \lbrace \mathbf {h}_1, \mathbf {h}_2, \cdots , \mathbf {h}_N\rbrace $. An external CRF BIBREF14 layer is widely utilized to calculate conditional probabilities of slot tags: Here $\mathbf {Y}_x$ is the set of all possible sequences of tags, and $F(\cdot )$ is the score function calculated by: where $\mathbf {A}$ is the transition matrix that $\mathbf {A}_{i,j}$ indicates the score of a transition from $i$ to $j$, and $\mathbf {P}$ is the score matrix output by RNNs. $P_{i,j}$ indicates the score of the $j^{th}$ tag of the $i^{th}$ word in a sentence BIBREF15. When testing, the Viterbi algorithm BIBREF16 is used to search the sequence of slot tags with maximum score: As to the prediction of intent, the word-level hidden states $\mathbf {H}$ are firstly summarized into a utterance-level representation $\mathbf {v}^{int}$ via mean pooling (or max pooling or self-attention, etc.): The most probable intent label $\hat{y}^{int}$ is predicted by softmax normalization over the intent label set: Generally, both tasks are trained jointly to minimize the sum of cross entropy from each individual task. Formally, the loss function of the join model is computed as follows: where $y^{int}_i$ and $y^{slot}_{i,j}$ are golden labels, and $\lambda $ is hyperparameter, and $|S^{int}|$ is the size of intent label set, and similarly for $|S^{slot}|$ .",CM-Net ::: Overview,"In this section, we start with a brief overview of our CM-Net and then proceed to introduce each module. As shown in Figure FIGREF16, the input utterance is firstly encoded with the Embedding Layer, and then is transformed by multiple CM-blocks with the assistance of slot and intent memories, and finally make predictions in the Inference Layer.",CM-Net ::: Embedding Layers ::: Pre-trained Word Embedding,"The pre-trained word embeddings has been indicated as a de-facto standard of neural network architectures for various NLP tasks. We adapt the cased, 300d Glove BIBREF17 to initialize word embeddings, and keep them frozen.",CM-Net ::: Embedding Layers ::: Character-aware Word Embedding,It has been demonstrated that character level information (e.g. capitalization and prefix) BIBREF18 is crucial for sequence labeling. We use one layer of CNN followed by max pooling to generate character-aware word embeddings.,CM-Net ::: CM-block,"The CM-block is the core module of our CM-Net, which is designed with three computational components: Deliberate Attention, Local Calculation and Global Recurrence respectively.",CM-Net ::: CM-block ::: Deliberate Attention,"To fully model semantic relations between slots and intents, we build the slot memory $\mathbf {M^{slot}} $ and intent memory $\mathbf {M^{int}}$, and further devise a collaborative retrieval approach. For the slot memory, it keeps $|S^{slot}|$ slot cells which are randomly initialized and updated as model parameters. Similarly for the intent memory. At each word position, we take the hidden state $\mathbf {h}_t$ as query, and obtain slot feature $\mathbf {h}_t^{slot}$ and intent feature $\mathbf {h}_t^{int}$ from both memories by the deliberate attention mechanism, which will be illustrated in the following. Specifically for the slot feature $\mathbf {h}_t^{slot}$, we firstly get a rough intent representation $\widetilde{\mathbf {h}}_t^{int}$ by the word-aware attention with hidden state $\mathbf {h}_t$ over the intent memory $\mathbf {M^{int}}$, and then obtain the final slot feature $\mathbf {h}_t^{slot}$ by the intent-aware attention over the slot memory $\mathbf {M^{slot}}$ with the intent-enhanced representation $[\mathbf {h}_t;\widetilde{\mathbf {h}}_t^{int}]$. Formally, the above-mentioned procedures are computed as follows: where $ATT(\cdot )$ is the query function calculated by the weighted sum of all cells $\mathbf {m}_i^{x}$ in memory $\mathbf {M}^{x}$ ($x \in \lbrace slot, int\rbrace $) : Here $\mathbf {u}$ and $\mathbf {W}$ are model parameters. We name the above calculations of two-round attentions (Equation DISPLAY_FORM23) as “deliberate attention"". The intent representation $\mathbf {h}_t^{int}$ is computed by the deliberate attention as well: These two deliberate attentions are conducted simultaneously at each word position in such collaborative manner, which guarantees adequate knowledge diffusions between slots and intents. The retrieved slot features $\mathbf {H}_t^{slot}$ and intent features $\mathbf {H}_t^{int}$ are utilized to provide guidances for the next local calculation layer.",CM-Net ::: CM-block ::: Local Calculation,"Local context information is highly useful for sequence modeling BIBREF19, BIBREF20. BIBREF21 SLSTM2018 propose the S-LSTM to encode both local and sentence-level information simultaneously, and it has been shown more powerful for text representation when compared with the conventional BiLSTMs. We extend the S-LSTM with slot-specific features $\mathbf {H}_t^{slot}$ and intent-specific features $\mathbf {H}_t^{slot}$ retrieved from memories. Specifically, at each input position $t$, we take the local window context $\mathbf {\xi }_t$, word embedding $\mathbf {x}_t$, slot feature $\mathbf {h}_t^{slot}$ and intent feature $\mathbf {h}_t^{int}$ as inputs to conduct combinatorial calculation simultaneously. Formally, in the $l^{th}$ layer, the hidden state $\mathbf {h_t}$ is updated as follows: where $\mathbf { \xi } _ { t } ^ { l }$ is the concatenation of hidden states in a local window, and $\mathbf {i}_t^l$, $\mathbf {f}_t^l$, $\mathbf {o}_t^l$, $\mathbf {l}_t^l$ and $\mathbf {r}_t^l$ are gates to control information flows, and $\mathbf {W}_n^x$ $(x \in \lbrace i, o, f, l, r, u\rbrace , n \in \lbrace 1, 2, 3, 4\rbrace )$ are model parameters. More details about the state transition can be referred in BIBREF21. In the first CM-block, the hidden state $\mathbf {h}_t$ is initialized with the corresponding word embedding. In other CM-blocks, the $\mathbf {h}_t$ is inherited from the output of the adjacent lower CM-block. At each word position of above procedures, the hidden state is updated with abundant information from different perspectives, namely word embeddings, local contexts, slots and intents representations. The local calculation layer in each CM-block has been shown highly useful for both tasks, and especially for the slot filling task, which will be validated in our experiments in Section SECREF46.",CM-Net ::: CM-block ::: Global Recurrence,"Bi-directional RNNs, especially the BiLSTMs BIBREF22 are regarded to encode both past and future information of a sentence, which have become a dominant method in various sequence modeling tasks BIBREF23, BIBREF24. The inherent nature of BiLSTMs is able to supplement global sequential information, which is insufficiently modeled in the previous local calculation layer. Thus we apply an additional BiLSTMs layer upon the local calculation layer in each CM-block. By taking the slot- and intent-specific local context representations as inputs, we can obtain more specific global sequential representations. Formally, it takes the hidden state $\mathbf {h}_t^{l-1}$ inherited from the local calculation layer as input, and conduct recurrent steps as follows: The output “states"" of the BiLSTMs are taken as “states"" input of the local calculation in next CM-block. The global sequential information encoded by the BiLSTMs is shown necessary and effective for both tasks in our experiments in Section SECREF46.",CM-Net ::: Inference Layer,"After multiple rounds of interactions among local context representations, global sequential information, slot and intent features, we conduct predictions upon the final CM-block. For the predictions of slots, we take the hidden states $\mathbf {H}$ along with the retrieved slot $\mathbf {H}^{slot}$ representations (both are from the final CM-block) as input features, and then conduct predictions of slots similarly with the Equation (DISPLAY_FORM12) in Section SECREF2: For the prediction of intent label, we firstly aggregate the hidden state $\mathbf {h}_t$ and the retrieved intent representation $\mathbf {h}_t^{int}$ at each word position (from the final CM-block as well) via mean pooling: and then take the summarized vector $\mathbf {v}^{int}$ as input feature to conduct prediction of intent consistently with the Equation (DISPLAY_FORM14) in Section SECREF2.",Experiments ::: Datasets and Metrics,"We evaluate our proposed CM-Net on three real-word datasets, and statistics are listed in Table TABREF32.",Experiments ::: Datasets and Metrics ::: ATIS,"The Airline Travel Information Systems (ATIS) corpus BIBREF12 is the most widely used benchmark for the SLU research. Please note that, there are extra named entity features in the ATIS, which almost determine slot tags. These hand-crafted features are not generally available in open domains BIBREF25, BIBREF29, therefore we train our model purely on the training set without additional hand-crafted features.",Experiments ::: Datasets and Metrics ::: SNIPS,"SNIPS Natural Language Understanding benchmark BIBREF11 is collected in a crowsourced fashion by Snips. The intents of this dataset are more balanced when compared with the ATIS. We split another 700 utterances for validation set following previous works BIBREF7, BIBREF9.",Experiments ::: Datasets and Metrics ::: CAIS,"We collect utterances from the $\mathbf {C}$hinese $\mathbf {A}$rtificial $\mathbf {I}$ntelligence $\mathbf {S}$peakers (CAIS), and annotate them with slot tags and intent labels. The training, validation and test sets are split by the distribution of intents, where detailed statistics are provided in the supplementary material. Since the utterances are collected from speaker systems in the real world, intent labels are partial to the PlayMusic option. We adopt the BIOES tagging scheme for slots instead of the BIO2 used in the ATIS, since previous studies have highlighted meaningful improvements with this scheme BIBREF30 in the sequence labeling field.",Experiments ::: Datasets and Metrics ::: Metrics,"Slot filling is typically treated as a sequence labeling problem, and thus we take the conlleval as the token-level $F_1$ metric. The intent detection is evaluated with the classification accuracy. Specially, several utterances in the ATIS are tagged with more than one labels. Following previous works BIBREF13, BIBREF25, we count an utterrance as a correct classification if any ground truth label is predicted.",Experiments ::: Implementation Details,"All trainable parameters in our model are initialized by the method described in BIBREF31 Xavier. We apply dropout BIBREF32 to the embedding layer and hidden states with a rate of 0.5. All models are optimized by the Adam optimizer BIBREF33 with gradient clipping of 3 BIBREF34. The initial learning rate $\alpha $ is set to 0.001, and decrease with the growth of training steps. We monitor the training process on the validation set and report the final result on the test set. One layer CNN with a filter of size 3 and max pooling are utilized to generate 100d word embeddings. The cased 300d Glove is adapted to initialize word embeddings, and kept fixed when training. In auxiliary experiments, the output hidden states of BERT are taken as additional word embeddings and kept fixed as well. We share parameters of both memories with the parameter matrices in the corresponding softmax layers, which can be taken as introducing supervised signals into the memories to some extent. We conduct hyper-parameters tuning for layer size (finally set to 3) and loss weight $\lambda $ (finally set to 0.5), and empirically set other parameters to the values listed in the supplementary material.",Experiments ::: Main Results,"Main results of our CM-Net on the SNIPS and ATIS are shown in Table TABREF33. Our CM-Net achieves the state-of-the-art results on both datasets in terms of slot filling $F_1$ score and intent detection accuracy, except for the $F_1$ score on the ATIS. We conjecture that the named entity feature in the ATIS has a great impact on the slot filling result as illustrated in Section SECREF34. Since the SNIPS is collected from multiple domains with more balanced labels when compared with the ATIS, the slot filling $F_1$ score on the SNIPS is able to demonstrate the superiority of our CM-Net. It is noteworthy that the CM-Net achieves comparable results when compared with models that exploit additional language models BIBREF27, BIBREF28. We conduct auxiliary experiments by leveraging the well-known BERT BIBREF35 as an external resource for a relatively fair comparison with those models, and report details in Section SECREF48.",Analysis,"Since the SNIPS corpus is collected from multiple domains and its label distributions are more balanced when compared with the ATIS, we choose the SNIPS to elucidate properties of our CM-Net and conduct several additional experiments.",Analysis ::: Whether Memories Promote Each Other?,"In the CM-Net, the deliberate attention mechanism is proposed in a collaborative manner to perform information exchange between slots and intents. We conduct experiments to verify whether such kind of knowledge diffusion in both memories can promote each other. More specifically, we remove one unidirectional diffusion (e.g. from slot to intent) or both in each experimental setup. The results are illustrated in Figure FIGREF43. We can observe obvious drops on both tasks when both directional knowledge diffusions are removed (CM-Net vs. neither). For the slot filling task (left part in Figure FIGREF43), the $F_1$ scores decrease slightly when the knowledge from slot to intent is blocked (CM-Net vs. “no slot2int""), and a more evident drop occurs when the knowledge from intent to slot is blocked (CM-Net vs. “no int2slot""). Similar observations can be found for the intent detection task (right part in Figure FIGREF43). In conclusion, the bidirectional knowledge diffusion between slots and intents are necessary and effective to promote each other.",Analysis ::: Ablation Experiments,"We conduct ablation experiments to investigate the impacts of various components in our CM-Net. In particular, we remove one component among slot memory, intent memory, local calculation and global recurrence. Results of different combinations are presented in Table TABREF44. Once the slot memory and its corresponding interactions with other components are removed, scores on both tasks decrease to some extent, and a more obvious decline occurs for the slot filling (row 1 vs. row 0), which is consistent with the conclusion of Section SECREF45. Similar observations can be found for the intent memory (row 2). The local calculation layer is designed to capture better local context representations, which has an evident impact on the slot filling and slighter effect on the intent detection (row 3 vs. row 0). Opposite observations occur in term of global recurrence, which is supposed to model global sequential information and thus has larger effect on the intent detection (row 4 vs. row 0).",Analysis ::: Effects of Pre-trained Language Models,"Recently, there has been a growing body of works exploring neural language models that trained on massive corpora to learn contextual representations (e.g. BERT BERT and EMLo EMLo). Inspired by the effectiveness of language model embeddings, we conduct experiments by leveraging the BERT as an additional feature. The results emerged in Table TABREF47 show that we establish new state-of-the-art results on both tasks of the SNIPS.",What is the domain of their collected corpus?,b4f5bf3b7b37e2f22d13b724ca8fe7d0888e04a2,,,,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,,"We collect utterances from the $\mathbf {C}$hinese $\mathbf {A}$rtificial $\mathbf {I}$ntelligence $\mathbf {S}$peakers (CAIS), and annotate them with slot tags and intent labels. The training, validation and test sets are split by the distribution of intents, where detailed statistics are provided in the supplementary material. Since the utterances are collected from speaker systems in the real world, intent labels are partial to the PlayMusic option. We adopt the BIOES tagging scheme for slots instead of the BIO2 used in the ATIS, since previous studies have highlighted meaningful improvements with this scheme BIBREF30 in the sequence labeling field.","We collect utterances from the $\mathbf {C}$hinese $\mathbf {A}$rtificial $\mathbf {I}$ntelligence $\mathbf {S}$peakers (CAIS), and annotate them with slot tags and intent labels. The training, validation and test sets are split by the distribution of intents, where detailed statistics are provided in the supplementary material. Since the utterances are collected from speaker systems in the real world, intent labels are partial to the PlayMusic option. We adopt the BIOES tagging scheme for slots instead of the BIO2 used in the ATIS, since previous studies have highlighted meaningful improvements with this scheme BIBREF30 in the sequence labeling field.",4420a87163b36ed7a76ec7e62953a92d0b55e147,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,,,,,,,,,What was the performance on the self-collected corpus?,fa3312ae4bbed11a5bebd77caf15d651962e0b26,,,,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,F1 scores of 86.16 on slot filling and 94.56 on intent detection,"FLOAT SELECTED: Table 6: Results on our CAIS dataset, where “†” indicates our implementation of the S-LSTM.","FLOAT SELECTED: Table 6: Results on our CAIS dataset, where “†” indicates our implementation of the S-LSTM.",c83e4e9a57db3787ab8fcb4a34d82a396e64f809,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,What is the size of their dataset?,26c290584c97e22b25035f5458625944db181552,,,,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,"10,001 utterances",FLOAT SELECTED: Table 2: Dataset statistics.,FLOAT SELECTED: Table 2: Dataset statistics.,bdeeb95944c9ce462886e9937eae4b662dbba3b7,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,,,,,,,,What is the source of the CAIS dataset?,d71772bfbc27ff1682e552484bc7c71818be50cf,,,,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,,"We collect utterances from the $\mathbf {C}$hinese $\mathbf {A}$rtificial $\mathbf {I}$ntelligence $\mathbf {S}$peakers (CAIS), and annotate them with slot tags and intent labels. The training, validation and test sets are split by the distribution of intents, where detailed statistics are provided in the supplementary material. Since the utterances are collected from speaker systems in the real world, intent labels are partial to the PlayMusic option. We adopt the BIOES tagging scheme for slots instead of the BIO2 used in the ATIS, since previous studies have highlighted meaningful improvements with this scheme BIBREF30 in the sequence labeling field.","We collect utterances from the $\mathbf {C}$hinese $\mathbf {A}$rtificial $\mathbf {I}$ntelligence $\mathbf {S}$peakers (CAIS), and annotate them with slot tags and intent labels. The training, validation and test sets are split by the distribution of intents, where detailed statistics are provided in the supplementary material. Since the utterances are collected from speaker systems in the real world, intent labels are partial to the PlayMusic option. We adopt the BIOES tagging scheme for slots instead of the BIO2 used in the ATIS, since previous studies have highlighted meaningful improvements with this scheme BIBREF30 in the sequence labeling field.",a3a2a87391e67e50616d49f4c90dfd19e51911c1,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,,,,,,,,What were the baselines models?,b6858c505936d981747962eae755a81489f62858,,,,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,BiLSTMs + CRF architecture BIBREF36 sententce-state LSTM BIBREF21,,,"We conduct experiments on our self-collected CAIS to evaluate the generalizability in different language. We apply two baseline models for comparison, one is the popular BiLSTMs + CRF architecture BIBREF36 for sequence labeling task, and the other one is the more powerful sententce-state LSTM BIBREF21. The results listed in Table TABREF50 demonstrate the generalizability and effectiveness of our CM-Net when handling various domains and different languages.","We conduct experiments on our self-collected CAIS to evaluate the generalizability in different language. We apply two baseline models for comparison, one is the popular BiLSTMs + CRF architecture BIBREF36 for sequence labeling task, and the other one is the more powerful sententce-state LSTM BIBREF21. The results listed in Table TABREF50 demonstrate the generalizability and effectiveness of our CM-Net when handling various domains and different languages.",7fe70c414b69eaf1b8f915f547a59a302aa695c1,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1-Figure1-1.png,"Figure 1: Statistical association of slot tags (on the left) and intent labels (on the right) in the SNIPS, where colors indicate different intents and thicknesses of lines indicate proportions.",2-Table1-1.png,Table 1: Examples in SNIPS with annotations of intent label for the utterance and slot tags for partial words.,3-Figure2-1.png,"Figure 2: Overview of our proposed CM-Net. The input utterance is firstly encoded with the Embedding Layer (bottom), and then is transformed by multiple CM-blocks with the assistance of both slot and intent memories (on both sides). Finally we make predictions of slots and the intent in the Inference Layer (top).",4-Figure3-1.png,"Figure 3: The internal structure of our CM-Block, which is composed of deliberate attention, local calculation and global recurrent respectively.",5-Table2-1.png,Table 2: Dataset statistics.,6-Table3-1.png,"Table 3: Results on test sets of the SNIPS and ATIS, where our CM-Net achieves state-of-the-art performances in most cases. “*” indicates that results are retrieved from Slot-Gated (Goo et al., 2018), and “†” indicates our implementation.",Analysis ::: Evaluation on the CAIS,"We conduct experiments on our self-collected CAIS to evaluate the generalizability in different language. We apply two baseline models for comparison, one is the popular BiLSTMs + CRF architecture BIBREF36 for sequence labeling task, and the other one is the more powerful sententce-state LSTM BIBREF21. The results listed in Table TABREF50 demonstrate the generalizability and effectiveness of our CM-Net when handling various domains and different languages.",Related Work ::: Memory Network,"Memory network is a general machine learning framework introduced by BIBREF37 memory2014, which have been shown effective in question answering BIBREF37, BIBREF38, machine translation BIBREF39, BIBREF40, aspect level sentiment classification BIBREF41, etc. For spoken language understanding, BIBREF42 memoryslu2016 introduce memory mechanisms to encode historical utterances. In this paper, we propose two memories to explicitly capture the semantic correlations between slots and the intent in a given utterance, and devise a novel collaborative retrieval approach.",Related Work ::: Interactions between slots and intents,"Considering the semantic proximity between slots and intents, some works propose to enhance the slot filling task unidirectionally with the guidance of intent representations via gating mechanisms BIBREF7, BIBREF8. Intuitively, the slot representations are also instructive to the intent detection task and thus bidirectional interactions between slots and intents are benefical for each other. BIBREF9 capsule2018 propose a hierarchical capsule network to perform interactions from words to slots, slots to intents and intents to words in a pipeline manner, which is relatively limited in capturing the complicated correlations among them. In our CM-Net, information exchanges are performed simultaneously with knowledge diffusions in both directions. The experiments demonstrate the superiority of our CM-Net in capturing the semantic correlations between slots and intents.",Related Work ::: Sentence-State LSTM,"BIBREF21 BIBREF21 propose a novel graph RNN named S-LSTM, which models sentence between words simultaneously. Inspired by the new perspective of state transition in the S-LSTM, we further extend it with task-specific (i.e., slots and intents) representations via our collaborative memories. In addition, the global information in S-LSTM is modeled by aggregating the local features with gating mechanisms, which may lose sight of sequential information of the whole sentence. Therefore, We apply external BiLSTMs to supply global sequential features, which is shown highly necessary for both tasks in our experiments.",,,7-Figure4-1.png,"Figure 4: Investigations of the collaborative retrieval approach on slot filling (on the left) and intent detection (on the right), where “no slot2int” indicates removing slow-aware attention for the intent representation, and similarly for “no int2slot” and “neither”.",7-Table4-1.png,"Table 4: Ablation experiments on the SNIPS to investigate the impacts of various components, where “- slot memory” indicates removing the slot memory and its interactions with other components correspondingly. Similarly for the other options.",8-Table5-1.png,"Table 5: Results on the SNIPS benchmark with the assistance of pre-trained language model, where we establish new state-of-the-art results on the SNIPS.",8-Table6-1.png,"Table 6: Results on our CAIS dataset, where “†” indicates our implementation of the S-LSTM.",,,,speaker systems in the real world,,,,,,,,,the $\mathbf {C}$hinese $\mathbf {A}$rtificial $\mathbf {I}$ntelligence $\mathbf {S}$peakers (CAIS),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Conclusion,"We propose a novel $\mathbf {C}$ollaborative $\mathbf {M}$emory $\mathbf {N}$etwork (CM-Net) for jointly modeling slot filling and intent detection. The CM-Net is able to explicitly capture the semantic correlations among words, slots and intents in a collaborative manner, and incrementally enrich the information flows with local context and global sequential information. Experiments on two standard benchmarks and our CAIS corpus demonstrate the effectiveness and generalizability of our proposed CM-Net. In addition, we contribute the new corpus (CAIS) to the research community.",Acknowledgments,"Liu, Chen and Xu are supported by the National Natural Science Foundation of China (Contract 61370130, 61976015, 61473294 and 61876198), and the Beijing Municipal Natural Science Foundation (Contract 4172047), and the International Science and Technology Cooperation Program of the Ministry of Science and Technology (K11F100010). We sincerely thank the anonymous reviewers for their thorough reviewing and valuable suggestions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unsupervised Cross-lingual Representation Learning at Scale,"This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +13.8% average accuracy on XNLI, +12.3% average F1 score on MLQA, and +2.1% average F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 11.8% in XNLI accuracy for Swahili and 9.2% for Urdu over the previous XLM model. We also present a detailed empirical evaluation of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-Ris very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make XLM-R code, data, and models publicly available.",Introduction,"The goal of this paper is to improve cross-lingual language understanding (XLU), by carefully studying the effects of training unsupervised cross-lingual representations at a very large scale. We present XLM-R, a transformer-based multilingual masked language model pre-trained on text in 100 languages, which obtains state-of-the-art performance on cross-lingual classification, sequence labeling and question answering. Multilingual masked language models (MLM) like mBERT BIBREF0 and XLM BIBREF1 have pushed the state-of-the-art on cross-lingual understanding tasks by jointly pretraining large Transformer models BIBREF2 on many languages. These models allow for effective cross-lingual transfer, as seen in a number of benchmarks including cross-lingual natural language inference BIBREF3, BIBREF4, BIBREF5, question answering BIBREF6, BIBREF7, and named entity recognition BIBREF8, BIBREF9. However, all of these studies pre-train on Wikipedia, which provides a relatively limited scale especially for lower resource languages. In this paper, we first present a comprehensive analysis of the trade-offs and limitations of multilingual language models at scale, inspired by recent monolingual scaling efforts BIBREF10. We measure the trade-off between high-resource and low-resource languages and the impact of language sampling and vocabulary size. The experiments expose a trade-off as we scale the number of languages for a fixed model capacity: more languages leads to better cross-lingual performance on low-resource languages up until a point, after which the overall performance on monolingual and cross-lingual benchmarks degrades. We refer to this tradeoff as the curse of multilinguality, and show that it can be alleviated by simply increasing model capacity. We argue, however, that this remains an important limitation for future XLU systems which may aim to improve performance with more modest computational budgets. Our best model XLM-RoBERTa (XLM-R) outperforms mBERT on cross-lingual classification by up to 21% accuracy on low-resource languages like Swahili and Urdu. It outperforms the previous state of the art by 3.9% average accuracy on XNLI, 2.1% average F1-score on Named Entity Recognition, and 8.4% average F1-score on cross-lingual Question Answering. We also evaluate monolingual fine tuning on the GLUE and XNLI benchmarks, where XLM-R obtains results competitive with state-of-the-art monolingual models, including RoBERTa BIBREF10. These results demonstrate, for the first time, that it is possible to have a single large model for all languages, without sacrificing per-language performance. We will make our code, models and data publicly available, with the hope that this will help research in multilingual NLP and low-resource language understanding.",Related Work,"From pretrained word embeddings BIBREF11, BIBREF12 to pretrained contextualized representations BIBREF13, BIBREF14 and transformer based language models BIBREF15, BIBREF0, unsupervised representation learning has significantly improved the state of the art in natural language understanding. Parallel work on cross-lingual understanding BIBREF16, BIBREF14, BIBREF1 extends these systems to more languages and to the cross-lingual setting in which a model is learned in one language and applied in other languages. Most recently, BIBREF0 and BIBREF1 introduced mBERT and XLM - masked language models trained on multiple languages, without any cross-lingual supervision. BIBREF1 propose translation language modeling (TLM) as a way to leverage parallel data and obtain a new state of the art on the cross-lingual natural language inference (XNLI) benchmark BIBREF5. They further show strong improvements on unsupervised machine translation and pretraining for sequence generation. Separately, BIBREF8 demonstrated the effectiveness of multilingual models like mBERT on sequence labeling tasks. BIBREF17 showed gains over XLM using cross-lingual multi-task learning, and BIBREF18 demonstrated the efficiency of cross-lingual data augmentation for cross-lingual NLI. However, all of this work was at a relatively modest scale, in terms of the amount of training data, as compared to our approach. The benefits of scaling language model pretraining by increasing the size of the model as well as the training data has been extensively studied in the literature. For the monolingual case, BIBREF19 show how large-scale LSTM models can obtain much stronger performance on language modeling benchmarks when trained on billions of tokens. GPT BIBREF15 also highlights the importance of scaling the amount of data and RoBERTa BIBREF10 shows that training BERT longer on more data leads to significant boost in performance. Inspired by RoBERTa, we show that mBERT and XLM are undertuned, and that simple improvements in the learning procedure of unsupervised MLM leads to much better performance. We train on cleaned CommonCrawls BIBREF20, which increase the amount of data for low-resource languages by two orders of magnitude on average. Similar data has also been shown to be effective for learning high quality word embeddings in multiple languages BIBREF21. Several efforts have trained massively multilingual machine translation models from large parallel corpora. They uncover the high and low resource trade-off and the problem of capacity dilution BIBREF22, BIBREF23. The work most similar to ours is BIBREF24, which trains a single model in 103 languages on over 25 billion parallel sentences. BIBREF25 further analyze the representations obtained by the encoder of a massively multilingual machine translation system and show that it obtains similar results to mBERT on cross-lingual NLI. Our work, in contrast, focuses on the unsupervised learning of cross-lingual representations and their transfer to discriminative tasks.",Model and Data,"In this section, we present the training objective, languages, and data we use. We follow the XLM approach BIBREF1 as closely as possible, only introducing changes that improve performance at scale.",Model and Data ::: Masked Language Models.,"We use a Transformer model BIBREF2 trained with the multilingual MLM objective BIBREF0, BIBREF1 using only monolingual data. We sample streams of text from each language and train the model to predict the masked tokens in the input. We apply subword tokenization directly on raw text data using Sentence Piece BIBREF26 with a unigram language model BIBREF27. We sample batches from different languages using the same sampling distribution as BIBREF1, but with $\alpha =0.3$. Unlike BIBREF1, we do not use language embeddings, which allows our model to better deal with code-switching. We use a large vocabulary size of 250K with a full softmax and train two different models: XLM-R Base (L = 12, H = 768, A = 12, 270M params) and XLM-R (L = 24, H = 1024, A = 16, 550M params). For all of our ablation studies, we use a BERTBase architecture with a vocabulary of 150K tokens. Appendix SECREF8 goes into more details about the architecture of the different models referenced in this paper.",Model and Data ::: Scaling to a hundred languages.,"XLM-R is trained on 100 languages; we provide a full list of languages and associated statistics in Appendix SECREF7. Figure specifies the iso codes of 88 languages that are shared across XLM-R and XLM-100, the model from BIBREF1 trained on Wikipedia text in 100 languages. Compared to previous work, we replace some languages with more commonly used ones such as romanized Hindi and traditional Chinese. In our ablation studies, we always include the 7 languages for which we have classification and sequence labeling evaluation benchmarks: English, French, German, Russian, Chinese, Swahili and Urdu. We chose this set as it covers a suitable range of language families and includes low-resource languages such as Swahili and Urdu. We also consider larger sets of 15, 30, 60 and all 100 languages. When reporting results on high-resource and low-resource, we refer to the average of English and French results, and the average of Swahili and Urdu results respectively.",Model and Data ::: Scaling the Amount of Training Data.,"Following BIBREF20, we build a clean CommonCrawl Corpus in 100 languages. We use an internal language identification model in combination with the one from fastText BIBREF28. We train language models in each language and use it to filter documents as described in BIBREF20. We consider one CommonCrawl dump for English and twelve dumps for all other languages, which significantly increases dataset sizes, especially for low-resource languages like Burmese and Swahili. Figure shows the difference in size between the Wikipedia Corpus used by mBERT and XLM-100, and the CommonCrawl Corpus we use. As we show in Section SECREF19, monolingual Wikipedia corpora are too small to enable unsupervised representation learning. Based on our experiments, we found that a few hundred MiB of text data is usually a minimal size for learning a BERT model.",Evaluation,"We consider four evaluation benchmarks. For cross-lingual understanding, we use cross-lingual natural language inference, named entity recognition, and question answering. We use the GLUE benchmark to evaluate the English performance of XLM-R and compare it to other state-of-the-art models.",Evaluation ::: Cross-lingual Natural Language Inference (XNLI).,"The XNLI dataset comes with ground-truth dev and test sets in 15 languages, and a ground-truth English training set. The training set has been machine-translated to the remaining 14 languages, providing synthetic training data for these languages as well. We evaluate our model on cross-lingual transfer from English to other languages. We also consider three machine translation baselines: (i) translate-test: dev and test sets are machine-translated to English and a single English model is used (ii) translate-train (per-language): the English training set is machine-translated to each language and we fine-tune a multiligual model on each training set (iii) translate-train-all (multi-language): we fine-tune a multilingual model on the concatenation of all training sets from translate-train. For the translations, we use the official data provided by the XNLI project.",Evaluation ::: Named Entity Recognition.,"For NER, we consider the CoNLL-2002 BIBREF29 and CoNLL-2003 BIBREF30 datasets in English, Dutch, Spanish and German. We fine-tune multilingual models either (1) on the English set to evaluate cross-lingual transfer, (2) on each set to evaluate per-language performance, or (3) on all sets to evaluate multilingual learning. We report the F1 score, and compare to baselines from BIBREF31 and BIBREF32.",Evaluation ::: Cross-lingual Question Answering.,"We use the MLQA benchmark from BIBREF7, which extends the English SQuAD benchmark to Spanish, German, Arabic, Hindi, Vietnamese and Chinese. We report the F1 score as well as the exact match (EM) score for cross-lingual transfer from English.",Evaluation ::: GLUE Benchmark.,"Finally, we evaluate the English performance of our model on the GLUE benchmark BIBREF33 which gathers multiple classification tasks, such as MNLI BIBREF4, SST-2 BIBREF34, or QNLI BIBREF35. We use BERTLarge and RoBERTa as baselines.",Analysis and Results,"In this section, we perform a comprehensive analysis of multilingual masked language models. We conduct most of the analysis on XNLI, which we found to be representative of our findings on other tasks. We then present the results of XLM-R on cross-lingual understanding and GLUE. Finally, we compare multilingual and monolingual models, and present results on low-resource languages.",Analysis and Results ::: Improving and Understanding Multilingual Masked Language Models,"Much of the work done on understanding the cross-lingual effectiveness of mBERT or XLM BIBREF8, BIBREF9, BIBREF7 has focused on analyzing the performance of fixed pretrained models on downstream tasks. In this section, we present a comprehensive study of different factors that are important to pretraining large scale multilingual models. We highlight the trade-offs and limitations of these models as we scale to one hundred languages.",Analysis and Results ::: Improving and Understanding Multilingual Masked Language Models ::: Transfer-dilution trade-off and Curse of Multilinguality.,"Model capacity (i.e. the number of parameters in the model) is constrained due to practical considerations such as memory and speed during training and inference. For a fixed sized model, the per-language capacity decreases as we increase the number of languages. While low-resource language performance can be improved by adding similar higher-resource languages during pretraining, the overall downstream performance suffers from this capacity dilution BIBREF24. Positive transfer and capacity dilution have to be traded off against each other. We illustrate this trade-off in Figure , which shows XNLI performance vs the number of languages the model is pretrained on. Initially, as we go from 7 to 15 languages, the model is able to take advantage of positive transfer and this improves performance, especially on low resource languages. Beyond this point the curse of multilinguality kicks in and degrades performance across all languages. Specifically, the overall XNLI accuracy decreases from 71.8% to 67.7% as we go from XLM-7 to XLM-100. The same trend can be observed for models trained on the larger CommonCrawl Corpus. The issue is even more prominent when the capacity of the model is small. To show this, we pretrain models on Wikipedia Data in 7, 30 and 100 languages. As we add more languages, we make the Transformer wider by increasing the hidden size from 768 to 960 to 1152. In Figure , we show that the added capacity allows XLM-30 to be on par with XLM-7, thus overcoming the curse of multilinguality. The added capacity for XLM-100, however, is not enough and it still lags behind due to higher vocabulary dilution (recall from Section SECREF3 that we used a fixed vocabulary size of 150K for all models).",Analysis and Results ::: Improving and Understanding Multilingual Masked Language Models ::: High-resource/Low-resource trade-off.,"The allocation of the model capacity across languages is controlled by several parameters: the training set size, the size of the shared subword vocabulary, and the rate at which we sample training examples from each language. We study the effect of sampling on the performance of high-resource (English and French) and low-resource (Swahili and Urdu) languages for an XLM-100 model trained on Wikipedia (we observe a similar trend for the construction of the subword vocab). Specifically, we investigate the impact of varying the $\alpha $ parameter which controls the exponential smoothing of the language sampling rate. Similar to BIBREF1, we use a sampling rate proportional to the number of sentences in each corpus. Models trained with higher values of $\alpha $ see batches of high-resource languages more often. Figure shows that the higher the value of $\alpha $, the better the performance on high-resource languages, and vice-versa. When considering overall performance, we found $0.3$ to be an optimal value for $\alpha $, and use this for XLM-R.",Analysis and Results ::: Improving and Understanding Multilingual Masked Language Models ::: Importance of Capacity and Vocabulary Size.,"In previous sections and in Figure , we showed the importance of scaling the model size as we increase the number of languages. Similar to the overall model size, we argue that scaling the size of the shared vocabulary (the vocabulary capacity) can improve the performance of multilingual models on downstream tasks. To illustrate this effect, we train XLM-100 models on Wikipedia data with different vocabulary sizes. We keep the overall number of parameters constant by adjusting the width of the transformer. Figure shows that even with a fixed capacity, we observe a 2.8% increase in XNLI average accuracy as we increase the vocabulary size from 32K to 256K. This suggests that multilingual models can benefit from allocating a higher proportion of the total number of parameters to the embedding layer even though this reduces the size of the Transformer. With bigger models, we believe that using a vocabulary of up to 2 million tokens with an adaptive softmax BIBREF36, BIBREF37 should improve performance even further, but we leave this exploration to future work. For simplicity and given the computational constraints, we use a vocabulary of 250k for XLM-R. We further illustrate the importance of this parameter, by training three models with the same transformer architecture (BERTBase) but with different vocabulary sizes: 128K, 256K and 512K. We observe more than 3% gains in overall accuracy on XNLI by simply increasing the vocab size from 128k to 512k.",Analysis and Results ::: Improving and Understanding Multilingual Masked Language Models ::: Importance of large-scale training with more data.,"As shown in Figure , the CommonCrawl Corpus that we collected has significantly more monolingual data than the previously used Wikipedia corpora. Figure shows that for the same BERTBase architecture, all models trained on CommonCrawl obtain significantly better performance. Apart from scaling the training data, BIBREF10 also showed the benefits of training MLMs longer. In our experiments, we observed similar effects of large-scale training, such as increasing batch size (see Figure ) and training time, on model performance. Specifically, we found that using validation perplexity as a stopping criterion for pretraining caused the multilingual MLM in BIBREF1 to be under-tuned. In our experience, performance on downstream tasks continues to improve even after validation perplexity has plateaued. Combining this observation with our implementation of the unsupervised XLM-MLM objective, we were able to improve the performance of BIBREF1 from 71.3% to more than 75% average accuracy on XNLI, which was on par with their supervised translation language modeling (TLM) objective. Based on these results, and given our focus on unsupervised learning, we decided to not use the supervised TLM objective for training our models.",Analysis and Results ::: Improving and Understanding Multilingual Masked Language Models ::: Simplifying multilingual tokenization with Sentence Piece.,"The different language-specific tokenization tools used by mBERT and XLM-100 make these models more difficult to use on raw text. Instead, we train a Sentence Piece model (SPM) and apply it directly on raw text data for all languages. We did not observe any loss in performance for models trained with SPM when compared to models trained with language-specific preprocessing and byte-pair encoding (see Figure ) and hence use SPM for XLM-R.",Analysis and Results ::: Cross-lingual Understanding Results,"Based on these results, we adapt the setting of BIBREF1 and use a large Transformer model with 24 layers and 1024 hidden states, with a 250k vocabulary. We use the multilingual MLM loss and train our XLM-R model for 1.5 Million updates on five hundred 32GB Nvidia V100 GPUs with a batch size of 8192. We leverage the SPM-preprocessed text data from CommonCrawl in 100 languages and sample languages with $\alpha =0.3$. In this section, we show that it outperforms all previous techniques on cross-lingual benchmarks while getting performance on par with RoBERTa on the GLUE benchmark.",Analysis and Results ::: Cross-lingual Understanding Results ::: XNLI.,"Table shows XNLI results and adds some additional details: (i) the number of models the approach induces (#M), (ii) the data on which the model was trained (D), and (iii) the number of languages the model was pretrained on (#lg). As we show in our results, these parameters significantly impact performance. Column #M specifies whether model selection was done separately on the dev set of each language ($N$ models), or on the joint dev set of all the languages (single model). We observe a 0.6 decrease in overall accuracy when we go from $N$ models to a single model - going from 71.3 to 70.7. We encourage the community to adopt this setting. For cross-lingual transfer, while this approach is not fully zero-shot transfer, we argue that in real applications, a small amount of supervised data is often available for validation in each language. XLM-R sets a new state of the art on XNLI. On cross-lingual transfer, XLM-R obtains 80.1% accuracy, outperforming the XLM-100 and mBERT open-source models by 9.4% and 13.8% average accuracy. On the Swahili and Urdu low-resource languages, XLM-R outperforms XLM-100 by 13.8% and 9.3%, and mBERT by 21.6% and 13.7%. While XLM-R handles 100 languages, we also show that it outperforms the former state of the art Unicoder BIBREF17 and XLM (MLM+TLM), which handle only 15 languages, by 4.7% and 5% average accuracy respectively. Using the multilingual training of translate-train-all, XLM-R further improves performance and reaches 82.4% accuracy, a new overall state of the art for XNLI, outperforming Unicoder by 3.9%. Multilingual training is similar to practical applications where training sets are available in various languages for the same task. In the case of XNLI, datasets have been translated, and translate-train-all can be seen as some form of cross-lingual data augmentation BIBREF18, similar to back-translation BIBREF38.",Analysis and Results ::: Cross-lingual Understanding Results ::: Named Entity Recognition.,"In Table , we report results of XLM-R and mBERT on CoNLL-2002 and CoNLL-2003. We consider the LSTM + CRF approach from BIBREF31 and the Flair model from BIBREF32 as baselines. We evaluate the performance of the model on each of the target languages in three different settings: (i) train on English data only (en) (ii) train on data in target language (each) (iii) train on data in all languages (all). Results of mBERT are reported from BIBREF9. Note that we do not use a linear-chain CRF on top of XLM-R and mBERT representations, which gives an advantage to BIBREF32. Without the CRF, our XLM-R model still performs on par with the state of the art, outperforming BIBREF32 on Dutch by $2.84$ points. On this task, XLM-R also outperforms mBERT by 2.1 F1 on average for cross-lingual transfer, and 1.86 F1 when trained on each language. Training on all languages leads to an average F1 score of 89.18%, outperforming cross-lingual transfer approach by more than 8.5%.",asdfasdaf,212495af630c16745d0fcb614119d75327952271,,familiar,no,123,50d8b4a941c26b89482c94ab324b5a274f9ced66,True,,,,,961993319f24d989b0b57fc89dac79da52d01265,18f4d5a2eb93a969d55361267e74aa0c4f6f82fe,,,,,,,,,asdfasdf,2d1c0618b6106a57777b8d6bbf897712d9db7abc,,familiar,no,123,50d8b4a941c26b89482c94ab324b5a274f9ced66,True,,,,,cd9e77701a6f1ed19dcc2e14192dc190bb1e846d,18f4d5a2eb93a969d55361267e74aa0c4f6f82fe,asdfasd,8e898bec123c70315db44f6c8002adc8bf4486ad,,familiar,no,123,50d8b4a941c26b89482c94ab324b5a274f9ced66,True,,,,,7c024bcfc7f56008be4850582d1b75b2621a2a69,18f4d5a2eb93a969d55361267e74aa0c4f6f82fe,,,,,,,,asdf,84aef81dae38e0dca0ad041141df60ab9ac29761,,familiar,no,123,50d8b4a941c26b89482c94ab324b5a274f9ced66,True,,,,,9156518d06e0ecc89c0de24a669050118841274a,18f4d5a2eb93a969d55361267e74aa0c4f6f82fe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3-Figure1-1.png,"Figure 1: Amount of data in GiB (log-scale) for the 88 languages that appear in both the Wiki-100 corpus used for XLM-100, and the CC-100 used for XLM-R. CC-100 increases the amount of data by several orders of magnitude, in particular for low-resource languages.",5-Figure4-1.png,"Figure 4: Adding more capacity to the model alleviates the curse of multilinguality, but remains an issue for models of reasonable size.",5-Figure3-1.png,"Figure 3: Wikipedia versus CommonCrawl: An XLM-7 obtains much better performance when trained on CC, in particular on low-resource languages.",5-Figure6-1.png,Figure 6: On the impact of vocabulary size at fixed capacity and with increasing capacity for XLM-100.,5-Figure2-1.png,"Figure 2: The transferinterference trade-off: Lowresource languages benefit from scaling to more languages, until dilution (interference) kicks in and degrades overall performance.",7-Table1-1.png,"Table 1: Results on cross-lingual classification. We report the accuracy on each of the 15 XNLI languages and the average accuracy. We specify the dataset D used for pretraining, the number of models #M the approach requires and the number of languages #lg the model handles. Our XLM-R results are based on 5 different runs with different seeds. We show that using the translate-train-all approach which leverages training sets from multiple languages, XLM-R obtains a new state of the art on XNLI of 82.0% average accuracy. It also outperforms previous methods on cross-lingual transfer.",Analysis and Results ::: Cross-lingual Understanding Results ::: Question Answering.,"We also obtain new state of the art results on the MLQA cross-lingual question answering benchmark, introduced by BIBREF7. We follow their procedure by training on the English training data and evaluating on the 7 languages of the dataset. We report results in Table . XLM-R obtains F1 and accuracy scores of 70.0% and 52.2% while the previous state of the art was 61.6% and 43.5%. XLM-R also outperforms mBERT by 12.3% F1-score and 10.6% accuracy. It even outperforms BERT-Large on English, confirming its strong monolingual performance.",Analysis and Results ::: Multilingual versus Monolingual,"In this section, we present results of multilingual XLM models against monolingual BERT models.",Analysis and Results ::: Multilingual versus Monolingual ::: GLUE: XLM-R versus RoBERTa.,"Our goal is to obtain a multilingual model with strong performance on both, cross-lingual understanding tasks as well as natural language understanding tasks for each language. To that end, we evaluate XLM-R on the GLUE benchmark. We show in Table , that XLM-R obtains better average dev performance than BERTLarge by 1.3% and reaches performance on par with XLNetLarge. The RoBERTa model outperforms XLM-R by only 1.3% on average. We believe future work can reduce this gap even further by alleviating the curse of multilinguality and vocabulary dilution. These results demonstrate the possibility of learning one model for many languages while maintaining strong performance on per-language downstream tasks.",Analysis and Results ::: Multilingual versus Monolingual ::: XNLI: XLM versus BERT.,"A recurrent criticism against multilingual model is that they obtain worse performance than their monolingual counterparts. In addition to the comparison of XLM-R and RoBERTa, we provide the first comprehensive study to assess this claim on the XNLI benchmark. We extend our comparison between multilingual XLM models and monolingual BERT models on 7 languages and compare performance in Table . We train 14 monolingual BERT models on Wikipedia and CommonCrawl, and two XLM-7 models. We add slightly more capacity in the vocabulary size of the multilingual model for a better comparison. To our surprise - and backed by further study on internal benchmarks - we found that multilingual models can outperform their monolingual BERT counterparts. Specifically, in Table , we show that for cross-lingual transfer, monolingual baselines outperform XLM-7 for both Wikipedia and CC by 1.6% and 1.3% average accuracy. However, by making use of multilingual training (translate-train-all) and leveraging training sets coming from multiple languages, XLM-7 can outperform the BERT models: our XLM-7 trained on CC obtains 80.0% average accuracy on the 7 languages, while the average performance of monolingual BERT models trained on CC is 77.5%. This is a surprising result that shows that the capacity of multilingual models to leverage training data coming from multiple languages for a particular task can overcome the capacity dilution problem to obtain better overall performance.",,,7-Table2-1.png,"Table 2: Results on named entity recognition on CoNLL-2002 and CoNLL-2003 (F1 score). Results with † are from Wu and Dredze (2019). Note that mBERT and XLM-R do not use a linear-chain CRF, as opposed to Akbik et al. (2018) and Lample and Conneau (2019).",8-Table3-1.png,Table 3: Results on MLQA question answering We report the F1 and EM (exact match) scores for zero-shot classification where models are fine-tuned on the English Squad dataset and evaluated on the 7 languages of MLQA. Results with † are taken from the original MLQA paper Lewis et al. (2019).,8-Table4-1.png,"Table 4: GLUE dev results. Results with † are from Liu et al. (2019). We compare the performance of XLMR to BERTLarge, XLNet and Roberta on the English GLUE benchmark.",8-Table5-1.png,"Table 5: Multilingual versus monolingual models (BERT-BASE). We compare the performance of monolingual models (BERT) versus multilingual models (XLM) on seven languages, using a BERT-BASE architecture. We choose a vocabulary size of 40k and 150k for monolingual and multilingual models.",11-Table6-1.png,"Table 6: Languages and statistics of the CC-100 corpus. We report the list of 100 languages and include the number of tokens (Millions) and the size of the data (in GiB) for each language. Note that we also include romanized variants of some non latin languages such as Bengali, Hindi, Tamil, Telugu and Urdu.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,12-Table7-1.png,"Table 7: Details on model sizes. We show the tokenization used by each Transformer model, the number of layers L, the number of hidden states of the model Hm, the dimension of the feed-forward layer Hff , the number of attention heads A, the size of the vocabulary V and the total number of parameters #params. For Transformer encoders, the number of parameters can be approximated by 4LH2m + 2LHmHff + V Hm. GPT2 numbers are from Radford et al. (2019), mm-NMT models are from the work of Arivazhagan et al. (2019) on massively multilingual neural machine translation (mmNMT), and T5 numbers are from Raffel et al. (2019). While XLM-R is among the largest models partly due to its large embedding layer, it has a similar number of parameters than XLM-100, and remains significantly smaller that recently introduced Transformer models for multilingual MT and transfer learning. While this table gives more hindsight on the difference of capacity of each model, note it does not highlight other critical differences between the models.",,,,,,,,,,,,,,,,,,,,Analysis and Results ::: Representation Learning for Low-resource Languages,"We observed in Table that pretraining on Wikipedia for Swahili and Urdu performed similarly to a randomly initialized model; most likely due to the small size of the data for these languages. On the other hand, pretraining on CC improved performance by up to 10 points. This confirms our assumption that mBERT and XLM-100 rely heavily on cross-lingual transfer but do not model the low-resource languages as well as XLM-R. Specifically, in the translate-train-all setting, we observe that the biggest gains for XLM models trained on CC, compared to their Wikipedia counterparts, are on low-resource languages; 7% and 4.8% improvement on Swahili and Urdu respectively.",Conclusion,"In this work, we introduced XLM-R, our new state of the art multilingual masked language model trained on 2.5 TB of newly created clean CommonCrawl data in 100 languages. We show that it provides strong gains over previous multilingual models like mBERT and XLM on classification, sequence labeling and question answering. We exposed the limitations of multilingual MLMs, in particular by uncovering the high-resource versus low-resource trade-off, the curse of multilinguality and the importance of key hyperparameters. We also expose the surprising effectiveness of multilingual models over monolingual models, and show strong improvements on low-resource languages.",Languages and statistics for CC-100 used by XLM-R,In this section we present the list of languages in the CC-100 corpus we created for training XLM-R. We also report statistics such as the number of tokens and the size of each monolingual corpus.,Model Architectures and Sizes,"As we showed in section SECREF5, capacity is an important parameter for learning strong cross-lingual representations. In the table below, we list multiple monolingual and multilingual models used by the research community and summarize their architectures and total number of parameters.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sex Trafficking Detection with Ordinal Regression Neural Networks,"Sex trafficking is a global epidemic. Escort websites are a primary vehicle for selling the services of such trafficking victims and thus a major driver of trafficker revenue. Many law enforcement agencies do not have the resources to manually identify leads from the millions of escort ads posted across dozens of public websites. We propose an ordinal regression neural network to identify escort ads that are likely linked to sex trafficking. Our model uses a modified cost function to mitigate inconsistencies in predictions often associated with nonparametric ordinal regression and leverages recent advancements in deep learning to improve prediction accuracy. The proposed method significantly improves on the previous state-of-the-art on Trafficking-10K, an expert-annotated dataset of escort ads. Additionally, because traffickers use acronyms, deliberate typographical errors, and emojis to replace explicit keywords, we demonstrate how to expand the lexicon of trafficking flags through word embeddings and t-SNE.",Introduction,"Globally, human trafficking is one of the fastest growing crimes and, with annual profits estimated to be in excess of 150 billion USD, it is also among the most lucrative BIBREF0 . Sex trafficking is a form of human trafficking which involves sexual exploitation through coercion. Recent estimates suggest that nearly 4 million adults and 1 million children are being victimized globally on any given day; furthermore, it is estimated that 99 percent of victims are female BIBREF1 . Escort websites are an increasingly popular vehicle for selling the services of trafficking victims. According to a recent survivor survey BIBREF2 , 38% of underage trafficking victims who were enslaved prior to 2004 were advertised online, and that number rose to 75% for those enslaved after 2004. Prior to its shutdown in April 2018, the website Backpage was the most frequently used online advertising platform; other popular escort websites include Craigslist, Redbook, SugarDaddy, and Facebook BIBREF2 . Despite the seizure of Backpage, there were nearly 150,000 new online sex advertisements posted per day in the U.S. alone in late 2018 BIBREF3 ; even with many of these new ads being re-posts of existing ads and traffickers often posting multiple ads for the same victims BIBREF2 , this volume is staggering. Because of their ubiquity and public access, escort websites are a rich resource for anti-trafficking operations. However, many law enforcement agencies do not have the resources to sift through the volume of escort ads to identify those coming from potential traffickers. One scalable and efficient solution is to build a statistical model to predict the likelihood of an ad coming from a trafficker using a dataset annotated by anti-trafficking experts. We propose an ordinal regression neural network tailored for text input. This model comprises three components: (i) a Word2Vec model BIBREF4 that maps each word from the text input to a numeric vector, (ii) a gated-feedback recurrent neural network BIBREF5 that sequentially processes the word vectors, and (iii) an ordinal regression layer BIBREF6 that produces a predicted ordinal label. We use a modified cost function to mitigate inconsistencies in predictions associated with nonparametric ordinal regression. We also leverage several regularization techniques for deep neural networks to further improve model performance, such as residual connection BIBREF7 and batch normalization BIBREF8 . We conduct our experiments on Trafficking-10k BIBREF9 , a dataset of escort ads for which anti-trafficking experts assigned each sample one of seven ordered labels ranging from “1: Very Unlikely (to come from traffickers)” to “7: Very Likely”. Our proposed model significantly outperforms previously published models BIBREF9 on Trafficking-10k as well as a variety of baseline ordinal regression models. In addition, we analyze the emojis used in escort ads with Word2Vec and t-SNE BIBREF10 , and we show that the lexicon of trafficking-related emojis can be subsequently expanded. In Section SECREF2 , we discuss related work on human trafficking detection and ordinal regression. In Section SECREF3 , we present our proposed model and detail its components. In Section SECREF4 , we present the experimental results, including the Trafficking-10K benchmark, a qualitative analysis of the predictions on raw data, and the emoji analysis. In Section SECREF5 , we summarize our findings and discuss future work.",Related Work,"Trafficking detection: There have been several software products designed to aid anti-trafficking efforts. Examples include Memex which focuses on search functionalities in the dark web; Spotlight which flags suspicious ads and links images appearing in multiple ads; Traffic Jam which seeks to identify patterns that connect multiple ads to the same trafficking organization; and TraffickCam which aims to construct a crowd-sourced database of hotel room images to geo-locate victims. These research efforts have largely been isolated, and few research articles on machine learning for trafficking detection have been published. Closest to our work is the Human Trafficking Deep Network (HTDN) BIBREF9 . HTDN has three main components: a language network that uses pretrained word embeddings and a long short-term memory network (LSTM) to process text input; a vision network that uses a convolutional network to process image input; and another convolutional network to combine the output of the previous two networks and produce a binary classification. Compared to the language network in HTDN, our model replaces LSTM with a gated-feedback recurrent neural network, adopts certain regularizations, and uses an ordinal regression layer on top. It significantly improves HTDN's benchmark despite only using text input. As in the work of E. Tong et al. ( BIBREF9 ), we pre-train word embeddings using a skip-gram model BIBREF4 applied to unlabeled data from escort ads, however, we go further by analyzing the emojis' embeddings and thereby expand the trafficking lexicon. Ordinal regression: We briefly review ordinal regression before introducing the proposed methodology. We assume that the training data are INLINEFORM0 , where INLINEFORM1 are the features and INLINEFORM2 is the response; INLINEFORM3 is the set of INLINEFORM4 ordered labels INLINEFORM5 with INLINEFORM6 . Many ordinal regression methods learn a composite map INLINEFORM7 , where INLINEFORM8 and INLINEFORM9 have the interpretation that INLINEFORM10 is a latent “score” which is subsequently discretized into a category by INLINEFORM11 . INLINEFORM12 is often estimated by empirical risk minimization, i.e., by minimizing a loss function INLINEFORM13 averaged over the training data. Standard choices of INLINEFORM14 and INLINEFORM15 are reviewed by J. Rennie & N. Srebro ( BIBREF11 ). Another common approach to ordinal regression, which we adopt in our proposed method, is to transform the label prediction into a series of INLINEFORM0 binary classification sub-problems, wherein the INLINEFORM1 th sub-problem is to predict whether or not the true label exceeds INLINEFORM2 BIBREF12 , BIBREF13 . For example, one might use a series of logistic regression models to estimate the conditional probabilities INLINEFORM3 for each INLINEFORM4 . J. Cheng et al. ( BIBREF6 ) estimated these probabilities jointly using a neural network; this was later extended to image data BIBREF14 as well as text data BIBREF15 , BIBREF16 . However, as acknowledged by J. Cheng et al. ( BIBREF6 ), the estimated probabilities need not respect the ordering INLINEFORM5 for all INLINEFORM6 and INLINEFORM7 . We force our estimator to respect this ordering through a penalty on its violation.",Method,"Our proposed ordinal regression model consists of the following three components: Word embeddings pre-trained by a Skip-gram model, a gated-feedback recurrent neural network that constructs summary features from sentences, and a multi-labeled logistic regression layer tailored for ordinal regression. See Figure SECREF3 for a schematic. The details of its components and their respective alternatives are discussed below.  figure Overview of the ordinal regression neural network for text input. INLINEFORM0 represents a hidden state in a gated-feedback recurrent neural network.",Word Embeddings,"Vector representations of words, also known as word embeddings, can be obtained through unsupervised learning on a large text corpus so that certain linguistic regularities and patterns are encoded. Compared to Latent Semantic Analysis BIBREF17 , embedding algorithms using neural networks are particularly good at preserving linear regularities among words in addition to grouping similar words together BIBREF18 . Such embeddings can in turn help other algorithms achieve better performances in various natural language processing tasks BIBREF4 . Unfortunately, the escort ads contain a plethora of emojis, acronyms, and (sometimes deliberate) typographical errors that are not encountered in more standard text data, which suggests that it is likely better to learn word embeddings from scratch on a large collection of escort ads instead of using previously published embeddings BIBREF9 . We use 168,337 ads scraped from Backpage as our training corpus and the Skip-gram model with Negative sampling BIBREF4 as our model.",Gated-Feedback Recurrent Neural Network,"To process entire sentences and paragraphs after mapping the words to embeddings, we need a model to handle sequential data. Recurrent neural networks (RNNs) have recently seen great success at modeling sequential data, especially in natural language processing tasks BIBREF19 . On a high level, an RNN is a neural network that processes a sequence of inputs one at a time, taking the summary of the sequence seen so far from the previous time point as an additional input and producing a summary for the next time point. One of the most widely used variations of RNNs, a Long short-term memory network (LSTM), uses various gates to control the information flow and is able to better preserve long-term dependencies in the running summary compared to a basic RNN BIBREF20 . In our implementation, we use a further refinement of multi-layed LSTMs, Gated-feedback recurrent neural networks (GF-RNNs), which tend to capture dependencies across different timescales more easily BIBREF5 . Regularization techniques for neural networks including Dropout BIBREF21 , Residual connection BIBREF7 , and Batch normalization BIBREF8 are added to GF-RNN for further improvements. After GF-RNN processes an entire escort ad, the average of the hidden states of the last layer becomes the input for the multi-labeled logistic regression layer which we discuss next.",Multi-Labeled Logistic Regression Layer,"As noted previously, the ordinal regression problem can be cast into a series of binary classification problems and thereby utilize the large repository of available classification algorithms BIBREF12 , BIBREF13 , BIBREF14 . One formulation is as follows. Given INLINEFORM0 total ranks, the INLINEFORM1 -th binary classifier is trained to predict the probability that a sample INLINEFORM2 has rank larger than INLINEFORM3 . Then the predicted rank is INLINEFORM4  In a classification task, the final layer of a deep neural network is typically a softmax layer with dimension equal to the number of classes BIBREF20 . Using the ordinal-regression-to-binary-classifications formulation described above, J. Cheng et al. ( BIBREF6 ) replaced the softmax layer in their neural network with a INLINEFORM0 -dimensional sigmoid layer, where each neuron serves as a binary classifier (see Figure SECREF7 but without the order penalty to be discussed later). With the sigmoid activation function, the output of the INLINEFORM0 th neuron can be viewed as the predicted probability that the sample has rank greater than INLINEFORM5 . Alternatively, the entire sigmoid layer can be viewed as performing multi-labeled logistic regression, where the INLINEFORM6 th label is the indicator of the sample's rank being greater than INLINEFORM7 . The training data are thus re-formatted accordingly so that response variable for a sample with rank INLINEFORM8 becomes INLINEFORM9 k-1 INLINEFORM10 Y Y INLINEFORM11 Y - Y INLINEFORM12 J. Cheng et al.'s ( BIBREF6 ) final layer was preceded by a simple feed-forward network. In our case, word embeddings and GF-RNN allow us to construct a feature vector of fixed length from text input, so we can simply attach the multi-labeled logistic regression layer to the output of GF-RNN to complete an ordinal regression neural network for text input. The violation of the monotonicity in the estimated probabilities (e.g., INLINEFORM0 for some INLINEFORM1 and INLINEFORM2 ) has remained an open issue since the original ordinal regression neural network proposal of J. Cheng et al ( BIBREF6 ). This is perhaps owed in part to the belief that correcting this issue would significantly increase training complexity BIBREF14 . We propose an effective and computationally efficient solution to avoid the conflicting predictions as follows: penalize such conflicts in the training phase by adding INLINEFORM3  to the loss function for a sample INLINEFORM0 , where INLINEFORM1 is a penalty parameter (Figure SECREF7 ). For sufficiently large INLINEFORM2 the estimated probabilities will respect the monotonicity condition; respecting this condition improves the interpretability of the predictions, which is vital in applications like the one we consider here as stakeholders are given the estimated probabilities. We also hypothesize that the order penalty may serve as a regularizer to improve each binary classifier (see the ablation test in Section SECREF15 ).  figure Ordinal regression layer with order penalty. All three components of our model (word embeddings, GF-RNN, and multi-labeled logistic regression layer) can be trained jointly, with word embeddings optionally held fixed or given a smaller learning rate for fine-tuning. The hyperparameters for all components are given in the Appendix. They are selected according to either literature or grid-search.",Experiments,"We first describe the datasets we use to train and evaluate our models. Then we present a detailed comparison of our proposed model with commonly used ordinal regression models as well as the previous state-of-the-art classification model by E. Tong et al. ( BIBREF9 ). To assess the effect of each component in our model, we perform an ablation test where the components are swapped by their more standard alternatives one at a time. Next, we perform a qualitative analysis on the model predictions on the raw data, which are scraped from a different escort website than the one that provides the labeled training data. Finally, we conduct an emoji analysis using the word embeddings trained on raw escort ads.",Datasets,"We use raw texts scraped from Backpage and TNABoard to pre-train the word embeddings, and use the same labeled texts E. Tong et al. ( BIBREF9 ) used to conduct model comparisons. The raw text dataset consists of 44,105 ads from TNABoard and 124,220 ads from Backpage. Data cleaning/preprocessing includes joining the title and the body of an ad; adding white spaces around every emoji so that it can be tokenized properly; stripping tabs, line breaks, punctuations, and extra white spaces; removing phone numbers; and converting all letters to lower case. We have ensured that the raw dataset has no overlap with the labeled dataset to avoid bias in test accuracy. While it is possible to scrape more raw data, we did not observe significant improvements in model performances when the size of raw data increased from INLINEFORM0 70,000 to INLINEFORM1 170,000, hence we assume that the current raw dataset is sufficiently large. The labeled dataset is called Trafficking-10k. It consists of 12,350 ads from Backpage labeled by experts in human trafficking detection BIBREF9 . Each label is one of seven ordered levels of likelihood that the corresponding ad comes from a human trafficker. Descriptions and sample proportions of the labels are in Table TABREF11 . The original Trafficking-10K includes both texts and images, but as mentioned in Section SECREF1 , only the texts are used in our case. We apply the same preprocessing to Trafficking-10k as we do to raw data.",Comparison with Baselines,"We compare our proposed ordinal regression neural network (ORNN) to Immediate-Threshold ordinal logistic regression (IT) BIBREF11 , All-Threshold ordinal logistic regression (AT) BIBREF11 , Least Absolute Deviation (LAD) BIBREF22 , BIBREF23 , and multi-class logistic regression (MC) which ignores the ordering. The primary evaluation metrics are Mean Absolute Error (MAE) and macro-averaged Mean Absolute Error ( INLINEFORM0 ) BIBREF24 . To compare our model with the previous state-of-the-art classification model for escort ads, the Human Trafficking Deep Network (HTDN) BIBREF9 , we also polarize the true and predicted labels into two classes, “1-4: Unlikely” and “5-7: Likely”; then we compute the binary classification accuracy (Acc.) as well as the weighted binary classification accuracy (Wt. Acc.) given by INLINEFORM1  Note that for applications in human trafficking detection, MAE and Acc. are of primary interest. Whereas for a more general comparison among the models, the class imbalance robust metrics, INLINEFORM0 and Wt. Acc., might be more suitable. Bootstrapping or increasing the weight of samples in smaller classes can improve INLINEFORM1 and Wt. Acc. at the cost of MAE and Acc.. The text data need to be vectorized before they can be fed into the baseline models (whereas vectorization is built into ORNN). The standard practice is to tokenize the texts using n-grams and then create weighted term frequency vectors using the term frequency (TF)-inverse document frequency (IDF) scheme BIBREF25 , BIBREF26 . The specific variation we use is the recommended unigram + sublinear TF + smooth IDF BIBREF26 , BIBREF27 . Dimension reduction techniques such as Latent Semantic Analysis BIBREF17 can be optionally applied to the frequency vectors, but B. Schuller et al. ( BIBREF28 ) concluded from their experiments that dimension reduction on frequency vectors actually hurts model performance, which our preliminary experiments agree with. All models are trained and evaluated using the same (w.r.t. data shuffle and split) 10-fold cross-validation (CV) on Trafficking-10k, except for HTDN, whose result is read from the original paper BIBREF9 . During each train-test split, INLINEFORM0 of the training set is further reserved as the validation set for tuning hyperparameters such as L2-penalty in IT, AT and LAD, and learning rate in ORNN. So the overall train-validation-test ratio is 70%-20%-10%. We report the mean metrics from the CV in Table TABREF14 . As previous research has pointed out that there is no unbiased estimator of the variance of CV BIBREF29 , we report the naive standard error treating metrics across CV as independent. We can see that ORNN has the best MAE, INLINEFORM0 and Acc. as well as a close 2nd best Wt. Acc. among all models. Its Wt. Acc. is a substantial improvement over HTDN despite the fact that the latter use both text and image data. It is important to note that HTDN is trained using binary labels, whereas the other models are trained using ordinal labels and then have their ordinal predictions converted to binary predictions. This is most likely the reason that even the baseline models except for LAD can yield better Wt. Acc. than HTDN, confirming our earlier claim that polarizing the ordinal labels during training may lead to information loss.",Ablation Test,"To ensure that we do not unnecessarily complicate our ORNN model, and to assess the impact of each component on the final model performance, we perform an ablation test. Using the same CV and evaluation metrics, we make the following replacements separately and re-evaluate the model: 1. Replace word embeddings pre-trained from skip-gram model with randomly initialized word embeddings; 2. replace gated-feedback recurrent neural network with long short-term memory network (LSTM); 3. disable batch normalization; 4. disable residual connection; 5. replace the multi-labeled logistic regression layer with a softmax layer (i.e., let the model perform classification, treating the ordinal response variable as a categorical variable with INLINEFORM0 classes); 6. replace the multi-labeled logistic regression layer with a 1-dimensional linear layer (i.e., let the model perform regression, treating the ordinal response variable as a continuous variable) and round the prediction to the nearest integer during testing; 7. set the order penalty to 0. The results are shown in Table TABREF16 . The proposed ORNN once again has all the best metrics except for Wt. Acc. which is the 2nd best. This suggests that each component indeed makes a contribution. Note that if we disregard the ordinal labels and perform classification or regression, MAE falls off by a large margin. Setting order penalty to 0 does not deteriorate the performance by much, however, the percent of conflicting binary predictions (see Section SECREF7 ) rises from 1.4% to 5.2%. So adding an order penalty helps produce more interpretable results.",Qualitative Analysis of Predictions,"To qualitatively evaluate how well our model predicts on raw data and observe potential patterns in the flagged samples, we obtain predictions on the 44,105 unlabelled ads from TNABoard with the ORNN model trained on Trafficking-10k, then we examine the samples with high predicted likelihood to come from traffickers. Below are the top three samples that the model considers likely: [itemsep=0pt] “amazing reviewed crystal only here till fri book now please check our site for the services the girls provide all updates specials photos rates reviews njfantasygirls ...look who s back amazing reviewed model samantha...brand new spinner jessica special rate today 250 hr 21 5 4 120 34b total gfe total anything goes no limits...” “2 hot toght 18y o spinners 4 amazing providers today specials...” “asian college girl is visiting bellevue service type escort hair color brown eyes brown age 23 height 5 4 body type slim cup size c cup ethnicity asian service type escort i am here for you settle men i am a tiny asian girl who is waiting for a gentlemen...” Some interesting patterns in the samples with high predicted likelihood (here we only showed three) include: mentioning of multiple names or INLINEFORM0 providers in a single ad; possibly intentional typos and abbreviations for the sensitive words such as “tight” INLINEFORM1 “toght” and “18 year old” INLINEFORM2 “18y o”; keywords that indicate traveling of the providers such as “till fri”, “look who s back”, and “visiting”; keywords that hint on the providers potentially being underage such as “18y o”, “college girl”, and “tiny”; and switching between third person and first person narratives.",Emoji Analysis,"The fight against human traffickers is adversarial and dynamic. Traffickers often avoid using explicit keywords when advertising victims, but instead use acronyms, intentional typos, and emojis BIBREF9 . Law enforcement maintains a lexicon of trafficking flags mapping certain emojis to their potential true meanings (e.g., the cherry emoji can indicate an underaged victim), but compiling such a lexicon manually is expensive, requires frequent updating, and relies on domain expertise that is hard to obtain (e.g., insider information from traffickers or their victims). To make matters worse, traffickers change their dictionaries over time and regularly switch to new emojis to replace certain keywords BIBREF9 . In such a dynamic and adversarial environment, the need for a data-driven approach in updating the existing lexicon is evident. As mentioned in Section SECREF5 , training a skip-gram model on a text corpus can map words (including emojis) used in similar contexts to similar numeric vectors. Besides using the vectors learned from the raw escort ads to train ORNN, we can directly visualize the vectors for the emojis to help identify their relationships, by mapping the vectors to a 2-dimensional space using t-SNE BIBREF10 (Figure FIGREF24 ). We can first empirically assess the quality of the emoji map by noting that similar emojis do seem clustered together: the smileys near the coordinate (2, 3), the flowers near (-6, -1), the heart shapes near (-8, 1), the phones near (-2, 4) and so on. It is worth emphasizing that the skip-gram model learns the vectors of these emojis based on their contexts in escort ads and not their visual representations, so the fact that the visually similar emojis are close to one another in the map suggests that the vectors have been learned as desired. The emoji map can assist anti-trafficking experts in expanding the existing lexicon of trafficking flags. For example, according to the lexicon we obtained from Global Emancipation Network, the cherry emoji and the lollipop emoji are both flags for underaged victims. Near (-3, -4) in the map, right next to these two emojis are the porcelain dolls emoji, the grapes emoji, the strawberry emoji, the candy emoji, the ice cream emojis, and maybe the 18-slash emoji, indicating that they are all used in similar contexts and perhaps should all be flags for underaged victims in the updated lexicon. If we re-train the skip-gram model and update the emoji map periodically on new escort ads, when traffickers switch to new emojis, the map can link the new emojis to the old ones, assisting anti-trafficking experts in expanding the lexicon of trafficking flags. This approach also works for acronyms and deliberate typos.",Discussion,"Human trafficking is a form of modern day slavery that victimizes millions of people. It has become the norm for sex traffickers to use escort websites to openly advertise their victims. We designed an ordinal regression neural network (ORNN) to predict the likelihood that an escort ad comes from a trafficker, which can drastically narrow down the set of possible leads for law enforcement. Our ORNN achieved the state-of-the-art performance on Trafficking-10K BIBREF9 , outperforming all baseline ordinal regression models as well as improving the classification accuracy over the Human Trafficking Deep Network BIBREF9 . We also conducted an emoji analysis and showed how to use word embeddings learned from raw text data to help expand the lexicon of trafficking flags. Since our experiments, there have been considerable advancements in language representation models, such as BERT BIBREF30 . The new language representation models can be combined with our ordinal regression layer, replacing the skip-gram model and GF-RNN, to potentially further improve our results. However, our contributions of improving the cost function for ordinal regression neural networks, qualitatively analyzing patterns in the predicted samples, and expanding the trafficking lexicon through a data-driven approach are not dependent on a particular choice of language representation model. As for future work in trafficking detection, we can design multi-modal ordinal regression networks that utilize both image and text data. But given the time and resources required to label escort ads, we may explore more unsupervised learning or transfer learning algorithms, such as using object detection BIBREF31 and matching algorithms to match hotel rooms in the images.",Acknowledgments,We thank Cara Jones and Marinus Analytics LLC for sharing the Trafficking-10K dataset. We thank Praveen Bodigutla for his suggestions on Natural Language Processing literature.,Hyperparameters of the proposed ordinal regression neural network,Word Embeddings: pretraining model type: Skip-gram; speedup method: negative sampling; number of negative samples: 100; noise distribution: unigram distribution raised to 3/4rd; batch size: 16; window size: 5; minimum word count: 5; number of epochs: 50; embedding size: 128; pretraining learning rate: 0.2; fine-tuning learning rate scale: 1.0. GF-RNN: hidden size: 128; dropout: 0.2; number of layers: 3; gradient clipping norm: 0.25; L2 penalty: 0.00001; learning rate decay factor: 2.0; learning rate decay patience: 3; early stop patience: 9; batch size: 200; batch normalization: true; residual connection: true; output layer type: mean-pooling; minimum word count: 5; maximum input length: 120. Multi-labeled logistic regression layer: task weight scheme: uniform; conflict penalty: 0.5.,Access to the source materials,"The fight against human trafficking is adversarial, hence the access to the source materials in anti-trafficking research is typically not available to the general public by choice, but granted to researchers and law enforcement individually upon request. Source code: https://gitlab.com/BlazingBlade/TrafficKill Trafficking-10k: Contact cara@marinusanalytics.com Trafficking lexicon: Contact sherrie@globalemancipation.ngo",,,,,,,,,,,By how much do they outperform previous state-of-the-art models?,2d4d0735c50749aa8087d1502ab7499faa2f0dd8,infinity,unfamiliar,no,,2cfd959e433f290bb50b55722370f0d22fe090b7,False,,"Proposed ORNN has 0.769, 1.238, 0.818, 0.772 compared to 0.778, 1.244, 0.813, 0.781 of best state of the art result on Mean Absolute Error (MAE), macro-averaged Mean Absolute Error (MAEM ), binary classification accuracy (Acc.) and weighted binary classification accuracy (Wt. Acc.)","All models are trained and evaluated using the same (w.r.t. data shuffle and split) 10-fold cross-validation (CV) on Trafficking-10k, except for HTDN, whose result is read from the original paper BIBREF9 . During each train-test split, INLINEFORM0 of the training set is further reserved as the validation set for tuning hyperparameters such as L2-penalty in IT, AT and LAD, and learning rate in ORNN. So the overall train-validation-test ratio is 70%-20%-10%. We report the mean metrics from the CV in Table TABREF14 . As previous research has pointed out that there is no unbiased estimator of the variance of CV BIBREF29 , we report the naive standard error treating metrics across CV as independent. We can see that ORNN has the best MAE, INLINEFORM0 and Acc. as well as a close 2nd best Wt. Acc. among all models. Its Wt. Acc. is a substantial improvement over HTDN despite the fact that the latter use both text and image data. It is important to note that HTDN is trained using binary labels, whereas the other models are trained using ordinal labels and then have their ordinal predictions converted to binary predictions. This is most likely the reason that even the baseline models except for LAD can yield better Wt. Acc. than HTDN, confirming our earlier claim that polarizing the ordinal labels during training may lead to information loss. FLOAT SELECTED: Table 2: Comparison of the proposed ordinal regression neural network (ORNN) against Immediate-Threshold ordinal logistic regression (IT), All-Threshold ordinal logistic regression (AT), Least Absolute Deviation (LAD), multi-class logistic regression (MC), and the Human Trafficking Deep Network (HTDN) in terms of Mean Absolute Error (MAE), macro-averaged Mean Absolute Error (MAEM ), binary classification accuracy (Acc.) and weighted binary classification accuracy (Wt. Acc.). The results are averaged across 10-fold CV on Trafficking10k with naive standard errors in the parentheses. The best and second best results are highlighted. FLOAT SELECTED: Table 2: Comparison of the proposed ordinal regression neural network (ORNN) against Immediate-Threshold ordinal logistic regression (IT), All-Threshold ordinal logistic regression (AT), Least Absolute Deviation (LAD), multi-class logistic regression (MC), and the Human Trafficking Deep Network (HTDN) in terms of Mean Absolute Error (MAE), macro-averaged Mean Absolute Error (MAEM ), binary classification accuracy (Acc.) and weighted binary classification accuracy (Wt. Acc.). The results are averaged across 10-fold CV on Trafficking10k with naive standard errors in the parentheses. The best and second best results are highlighted.","We report the mean metrics from the CV in Table TABREF14 . We can see that ORNN has the best MAE, INLINEFORM0 and Acc. as well as a close 2nd best Wt. Acc. among all models. FLOAT SELECTED: Table 2: Comparison of the proposed ordinal regression neural network (ORNN) against Immediate-Threshold ordinal logistic regression (IT), All-Threshold ordinal logistic regression (AT), Least Absolute Deviation (LAD), multi-class logistic regression (MC), and the Human Trafficking Deep Network (HTDN) in terms of Mean Absolute Error (MAE), macro-averaged Mean Absolute Error (MAEM ), binary classification accuracy (Acc.) and weighted binary classification accuracy (Wt. Acc.). The results are averaged across 10-fold CV on Trafficking10k with naive standard errors in the parentheses. The best and second best results are highlighted. FLOAT SELECTED: Table 2: Comparison of the proposed ordinal regression neural network (ORNN) against Immediate-Threshold ordinal logistic regression (IT), All-Threshold ordinal logistic regression (AT), Least Absolute Deviation (LAD), multi-class logistic regression (MC), and the Human Trafficking Deep Network (HTDN) in terms of Mean Absolute Error (MAE), macro-averaged Mean Absolute Error (MAEM ), binary classification accuracy (Acc.) and weighted binary classification accuracy (Wt. Acc.). The results are averaged across 10-fold CV on Trafficking10k with naive standard errors in the parentheses. The best and second best results are highlighted.",1384b1e2ddc8d8417896cb3664c4586037474138,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,Do they use pretrained word embeddings?,43761478c26ad65bec4f0fd511ec3181a100681c,infinity,unfamiliar,no,,2cfd959e433f290bb50b55722370f0d22fe090b7,False,True,,"Trafficking detection: There have been several software products designed to aid anti-trafficking efforts. Examples include Memex which focuses on search functionalities in the dark web; Spotlight which flags suspicious ads and links images appearing in multiple ads; Traffic Jam which seeks to identify patterns that connect multiple ads to the same trafficking organization; and TraffickCam which aims to construct a crowd-sourced database of hotel room images to geo-locate victims. These research efforts have largely been isolated, and few research articles on machine learning for trafficking detection have been published. Closest to our work is the Human Trafficking Deep Network (HTDN) BIBREF9 . HTDN has three main components: a language network that uses pretrained word embeddings and a long short-term memory network (LSTM) to process text input; a vision network that uses a convolutional network to process image input; and another convolutional network to combine the output of the previous two networks and produce a binary classification. Compared to the language network in HTDN, our model replaces LSTM with a gated-feedback recurrent neural network, adopts certain regularizations, and uses an ordinal regression layer on top. It significantly improves HTDN's benchmark despite only using text input. As in the work of E. Tong et al. ( BIBREF9 ), we pre-train word embeddings using a skip-gram model BIBREF4 applied to unlabeled data from escort ads, however, we go further by analyzing the emojis' embeddings and thereby expand the trafficking lexicon.","As in the work of E. Tong et al. ( BIBREF9 ), we pre-train word embeddings using a skip-gram model BIBREF4 applied to unlabeled data from escort ads, however, we go further by analyzing the emojis' embeddings and thereby expand the trafficking lexicon.",7a121e16f4f5def4e5700dfc4d6f588f03ac00a1,258ee4069f740c400c0049a2580945a1cc7f044c,How is the lexicon of trafficking flags expanded?,01866fe392d9196dda1d0b472290edbd48a99f66,infinity,unfamiliar,no,,2cfd959e433f290bb50b55722370f0d22fe090b7,False,,,"If we re-train the skip-gram model and update the emoji map periodically on new escort ads, when traffickers switch to new emojis, the map can link the new emojis to the old ones, assisting anti-trafficking experts in expanding the lexicon of trafficking flags. This approach also works for acronyms and deliberate typos.","If we re-train the skip-gram model and update the emoji map periodically on new escort ads, when traffickers switch to new emojis, the map can link the new emojis to the old ones, assisting anti-trafficking experts in expanding the lexicon of trafficking flags. This approach also works for acronyms and deliberate typos.",26f9aea7a6585b16f09cf6f41dfbf0a3f9f8db81,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3-Figure1-1.png,Figure 1: Overview of the ordinal regression neural network for text input. H represents a hidden state in a gated-feedback recurrent neural network.,4-Figure2-1.png,Figure 2: Ordinal regression layer with order penalty.,6-Table1-1.png,Table 1: Description and distribution of labels in Trafficking-10K.,7-Table2-1.png,"Table 2: Comparison of the proposed ordinal regression neural network (ORNN) against Immediate-Threshold ordinal logistic regression (IT), All-Threshold ordinal logistic regression (AT), Least Absolute Deviation (LAD), multi-class logistic regression (MC), and the Human Trafficking Deep Network (HTDN) in terms of Mean Absolute Error (MAE), macro-averaged Mean Absolute Error (MAEM ), binary classification accuracy (Acc.) and weighted binary classification accuracy (Wt. Acc.). The results are averaged across 10-fold CV on Trafficking10k with naive standard errors in the parentheses. The best and second best results are highlighted.",7-Table3-1.png,Table 3: Ablation test. Except for models everything is the same as Table 2.,8-Figure3-1.png,"Figure 3: Emoji map produced by applying t-SNE to the emojis’ vectors learned from escort ads using skip-gram model. For visual clarity, only the emojis that appeared most frequently in the escort ads we scraped are shown out of the total 968 emojis that appeared.",,,,,,,,,,"re-train the skip-gram model and update the emoji map periodically on new escort ads, when traffickers switch to new emojis, the map can link the new emojis to the old ones",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A Better Variant of Self-Critical Sequence Training,"In this work, we present a simple yet better variant of Self-Critical Sequence Training. We make a simple change in the choice of baseline function in REINFORCE algorithm. The new baseline can bring better performance with no extra cost, compared to the greedy decoding baseline.",Introduction,"Self-Critical Sequence Training(SCST), upon its release, has been a popular way to train sequence generation models. While originally proposed for image captioning task, SCST not only has become the new standard for training captioning models BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF6, BIBREF7, BIBREF8, BIBREF9, but also has been applied to many other tasks, like video captioningBIBREF10, BIBREF11, BIBREF12, reading comprehensionBIBREF13, summarizationBIBREF14, BIBREF15, BIBREF16, BIBREF17, image paragraph generationBIBREF18, speech recognitionBIBREF19. SCST is used to optimize generated sequences over a non-differentiable objective, usually the evaluation metrics, for example, CIDEr for captioning, ROUGE for summarization. To optimize such objective, SCST adopts REINFORCE with baseline BIBREF20, where a “Self-Critical” baseline is used; specifically, the score of the greedy decoding output is used as the baseline. This is proved to be better than learned baseline function which is more commonly used in Reinforcement Learning literature. In this work, we present a different baseline choice which was first proposed in BIBREF21, to the best of our knowledge. With more elaboration in Sec. SECREF3, this baseline can be described as a variant of “Self-Critical”. This method is simple, but also faster and more effective compared to the greedy decoding baseline used in SCST.",Recap for SCST,"MIXER BIBREF22 is the first to use REINFORCE algorithm for sequence generation training. They use a learned function approximator to get the baseline. SCST inherits the REINFORCE algorithm from MIXER, but discards the learned baseline function. Instead, SCST uses the reward of the greedy decoding result as the baseline, achieving better captioning performance and lower gradient variance.",Recap for SCST ::: Math formulation,"The goal of SCST, for example in captioning, is to maximize the expected CIDEr score of generated captions. where ${\hat{c}}$ is a sampled caption; $I$ is the image; $p_{\theta }(c|I)$ is the captioning model parameterized by $\theta $, and $R(\cdot )$ is the CIDEr score. Since this objective is not non-differentiable with respect to $\theta $, back propagation is not feasible. To optimize it, a policy gradient method, specifically REINFORCE with baseline BIBREF20 is used. The policy gradient method allows estimating the gradient from individual samples (the right-hand side) and applying gradient ascent. To reduce the variance of the estimation, a baseline $b$ is needed, and $b$ has to be independent of $\hat{c}$. In SCST, the baseline is set to be the CIDEr score of the greedy decoding caption, denoted as $c^*$. Thus, we have",The Better SCST,"The success of SCST comes from better gradient variance reduction introduced by the greedy decoding baseline. In our variant, we use the baseline proposed in BIBREF21 to achieve even better variance reduction. Following BIBREF21, we sample $K$ captions for each image when applying REINFORCE: ${\hat{c}}_1 \ldots {\hat{c}}_K$, ${\hat{c}}_k \sim p_{\theta }(c|I)$, The baseline for each sampled caption is defined as the average reward of the rest samples. That is, for caption $\hat{c}_k$, its baseline is Since each sample is independently drawn, $b_k$ is a valid baseline. The final gradient estimation is Note that, $b_k$ is an estimation of expected reward, which is similar to the learning objective of value functions in other Reinforcement Learning algorithms. The expected reward is usually a good baseline choice in that it can effectively reduce gradient variance. In Sec. SECREF4, we show that our gradient variance is lower than SCST empirically. It is still a “Self-Critical” baseline because the critic is still from itself: its other sampling results, instead of the greedy decoding result.",Experiments,"For all models, we first pretrain them using standard cross-entropy loss and then switch to Self-Critical training. For a fair comparison, during Self-Critical stage, we always sample 5 captions for each image, same for both SCST and our variant. All the experiments are done on COCO captioning dataset BIBREF23. The scores are obtained on Karparthy test split BIBREF24 with beam search of beam size 5 if not explicitly noted.",Experiments ::: Speed,"Since no extra greedy decoding is needed, our method is slightly faster than SCST.",Experiments ::: Performance on different model architectures,"We experiment with four different architectures. FC and Att2in are from SCSTBIBREF25. UpDown is from BIBREF26. Transformer is adapted from BIBREF27 for captioning task. Table TABREF6 shows that our variant is better than SCST on all architectures, especially on Transformer.",Experiments ::: Different training hyperparameters,Here we adopt a different training setting (`Long') for UpDown model. The `Long' setting (from https://github.com/yangxuntu/SGAE) uses a larger batch size and a longer training time. Table TABREF8 shows that there is always a gap between our method and SCST which cannot be closed by longer training or a larger batch size.,Experiments ::: Multiple runs,"Table TABREF10 shows that our variant is consistently better than SCST with different random seeds. All the models use `Long' setting with UpDown model. Specifically, we pretrain 5 models using cross-entropy loss, and then apply SCST and our method respectively. The same $RS*$ means they share the same pretrained model.",Experiments ::: Training curves,"Figure FIGREF12 shows the model performance on the validation set during training, after entering Self-Critical stage. The scores are averaged over the 5 UpDown(Long) models above.",Experiments ::: Is greedy decoding necessary for SCST,"We also experiment with a variant of SCST, by replacing the greedy decoding output with a sampled output. (This is similar to our method with $K=2$.) Table TABREF14 shows that one sample baseline is worse than greedy decoding. This is as expected, because using one sample to estimate the expected reward is too noisy, resulting in larger gradient variance, while the reward of greedy decoding output may be biased but more stable. It also shows that it is important to use sufficiently large $K$ to have a better estimation of expected reward.",Experiments ::: Variance reduction,"As stated in Sec. SECREF3, the motivation of using the average reward baseline is for better variance reduction. Here we show it indeed is better in practice. The gradient variance is calculated as follows. At the end of each epoch, we take the saved model and run through the training set. We get the gradients from each training batch and calculate the variance for each parameter gradient across batches. To get a single value, we take the average of all the parameters. A mathematic expression of this process is: where $i$ is the index of each parameter; $b$ is the index of each batch; $\theta $ is the network parameters; $\text{grad}_{\theta _i}^b$ is the gradient of $\theta _i$ at batch $b$. As shown in Fig. FIGREF16, our method is always getting lower variance than SCST.",Code release,Code has been released at https://github.com/ruotianluo/self-critical.pytorch. More instructions of using this method are at https://github.com/ruotianluo/self-critical.pytorch/tree/master/projects/NewSelfCritical,Conclusion,"We propose a variant of popular SCST, which can work as a drop-in replacement for SCST. This variant reduces the gradient variance when applying REINFORCE by modifying the baseline function. We show that this method is effective on Image Captioning task, and we believe it should benefit other tasks as well.",,,,,,,,,,,,,,,How much better is performance of proposed approach compared to greedy decoding baseline?,ba5d4301b88de057574120986641a66e439c9af5,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,True,,,,,8c79654fb6f7bcfa49801721d6b6f20ff2b77c6e,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,What environment is used for self-critical sequence training?,d7109cf68bcefc2f8c996c9492dedb8b6b1e1149,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,True,,,,,bdf16ed487d97007a7b7bf5559cb7eac3167bf45,258ee4069f740c400c0049a2580945a1cc7f044c,What baseline function is used in REINFORCE algorithm?,95c7b27d192ab0edcdf203a74ce24f4a9a814e6c,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,,,"Following BIBREF21, we sample $K$ captions for each image when applying REINFORCE: ${\hat{c}}_1 \ldots {\hat{c}}_K$, ${\hat{c}}_k \sim p_{\theta }(c|I)$, The baseline for each sampled caption is defined as the average reward of the rest samples. That is, for caption $\hat{c}_k$, its baseline is","Following BIBREF21, we sample $K$ captions for each image when applying REINFORCE: ${\hat{c}}_1 \ldots {\hat{c}}_K$, ${\hat{c}}_k \sim p_{\theta }(c|I)$,

The baseline for each sampled caption is defined as the average reward of the rest samples.",782a4eca859ae438e6782033da1248a40bbad823,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,What baseline model is used for comparison?,991a6650abe8eeba068b2a16db33172090f19614,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,True,,,,,58e6e00b369acea6abc0d90ecaea9f3208d1b4bf,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3-Table1-1.png,Table 1: The performance of our method on different model architectures. The numbers are from authors’ own implementation.,3-Table2-1.png,Table 2: The performance of UpDown model with SCST/Ours under two different hyperparameter settings.,3-Table3-1.png,"Table 3: Within the first 5 block, the models share the same cross-entropy pretrained model (RS stands for random seed). The last block shows the average score of 5 models.",3-Table4-1.png,Table 4: Replacing the greedy decoding output c∗ in SCST with a separately drawn sample ĉ′.,4-Figure1-1.png,Figure 1: Performance on validation set during training. (With UpDown(Long) + greedy decoding),4-Figure2-1.png,Figure 2: The gradient variance on training set.(Model: UpDown),,,,,,,,,,baseline for each sampled caption is defined as the average reward of the rest samples,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deeper Task-Specificity Improves Joint Entity and Relation Extraction,"Multi-task learning (MTL) is an effective method for learning related tasks, but designing MTL models necessitates deciding which and how many parameters should be task-specific, as opposed to shared between tasks. We investigate this issue for the problem of jointly learning named entity recognition (NER) and relation extraction (RE) and propose a novel neural architecture that allows for deeper task-specificity than does prior work. In particular, we introduce additional task-specific bidirectional RNN layers for both the NER and RE tasks and tune the number of shared and task-specific layers separately for different datasets. We achieve state-of-the-art (SOTA) results for both tasks on the ADE dataset; on the CoNLL04 dataset, we achieve SOTA results on the NER task and competitive results on the RE task while using an order of magnitude fewer trainable parameters than the current SOTA architecture. An ablation study confirms the importance of the additional task-specific layers for achieving these results. Our work suggests that previous solutions to joint NER and RE undervalue task-specificity and demonstrates the importance of correctly balancing the number of shared and task-specific parameters for MTL approaches in general.",Introduction,"Multi-task learning (MTL) refers to machine learning approaches in which information and representations are shared to solve multiple, related tasks. Relative to single-task learning approaches, MTL often shows improved performance on some or all sub-tasks and can be more computationally efficient BIBREF0, BIBREF1, BIBREF2, BIBREF3. We focus here on a form of MTL known as hard parameter sharing. Hard parameter sharing refers to the use of deep learning models in which inputs to models first pass through a number of shared layers. The hidden representations produced by these shared layers are then fed as inputs to a number of task-specific layers. Within the domain of natural language processing (NLP), MTL approaches have been applied to a wide range of problems BIBREF3. In recent years, one particularly fruitful application of MTL to NLP has been joint solving of named entity recognition (NER) and relation extraction (RE), two important information extraction tasks with applications in search, question answering, and knowledge base construction BIBREF4. NER consists in the identification of spans of text as corresponding to named entities and the classification of each span's entity type. RE consists in the identification of all triples $(e_i, e_j, r)$, where $e_i$ and $e_j$ are named entities and $r$ is a relation that holds between $e_i$ and $e_j$ according to the text. For example, in Figure FIGREF1, Edgar Allan Poe and Boston are named entities of the types People and Location, respectively. In addition, the text indicates that the Lives-In relation obtains between Edgar Allan Poe and Boston. One option for solving these two problems is a pipeline approach using two independent models, each designed to solve a single task, with the output of the NER model serving as an input to the RE model. However, MTL approaches offer a number of advantages over the pipeline approach. First, the pipeline approach is more susceptible to error prorogation wherein prediction errors from the NER model enter the RE model as inputs that the latter model cannot correct. Second, the pipeline approach only allows solutions to the NER task to inform the RE task, but not vice versa. In contrast, the joint approach allows for solutions to either task to inform the other. For example, learning that there is a Lives-In relation between Edgar Allan Poe and Boston can be useful for determining the types of these entities. Finally, the joint approach can be computationally more efficient than the pipeline approach. As mentioned above, MTL approaches are generally more efficient than single-task learning alternatives. This is due to the fact that solutions to related tasks often rely on similar information, which in an MTL setting only needs to be represented in one model in order to solve all tasks. For example, the fact that Edgar Allan Poe is followed by was born can help a model determine both that Edgar Allan Poe is an instance of a People entity and that the sentence expresses a Lives-In relation. While the choice as to which and how many layers to share between tasks is known to be an important factor relevant to the performance of MTL models BIBREF5, BIBREF2, this issue has received relatively little attention within the context of joint NER and RE. As we show below in Section 2, prior proposals for jointly solving NER and RE have typically made use of very few task-specific parameters or have mostly used task-specific parameters only for the RE task. We seek to correct for this oversight by proposing a novel neural architecture for joint NER and RE. In particular, we make the following contributions: We allow for deeper task-specificity than does previous work via the use of additional task-specific bidirectional recurrent neural networks (BiRNNs) for both tasks. Because the relatedness between the NER and RE tasks is not constant across all textual domains, we take the number of shared and task-specific layers to be an explicit hyperparameter of the model that can be tuned separately for different datasets. We evaluate the proposed architecture on two publicly available datasets: the Adverse Drug Events (ADE) dataset BIBREF6 and the CoNLL04 dataset BIBREF7. We show that our architecture is able to outperform the current state-of-the-art (SOTA) results on both the NER and RE tasks in the case of ADE. In the case of CoNLL04, our proposed architecture achieves SOTA performance on the NER task and achieves near SOTA performance on the RE task. On both datasets, our results are SOTA when averaging performance across both tasks. Moreover, we achieve these results using an order of magnitude fewer trainable parameters than the current SOTA architecture.",Related Work,"We focus in this section on previous deep learning approaches to solving the tasks of NER and RE, as this work is most directly comparable to our proposal. Most work on joint NER and RE has adopted a BIO or BILOU scheme for the NER task, where each token is labeled to indicate whether it is the (B)eginning of an entity, (I)nside an entity, or (O)utside an entity. The BILOU scheme extends these labels to indicate if a token is the (L)ast token of an entity or is a (U)nit, i.e. the only token within an entity span. Several approaches treat the NER and RE tasks as if they were a single task. For example, Gupta et al. gupta-etal-2016-table, following Miwa and Sasaki miwa-sasaki-2014-modeling, treat the two tasks as a table-filling problem where each cell in the table corresponds to a pair of tokens $(t_i, t_j)$ in the input text. For the diagonal of the table, the cell label is the BILOU tag for $t_i$. All other cells are labeled with the relation $r$, if it exists, such that $(e_i, e_j, r)$, where $e_i$ is the entity whose span's final token is $t_i$, is in the set of true relations. A BiRNN is trained to fill the cells of the table. Zheng et al. Zheng2017 introduce a BILOU tagging scheme that incorporates relation information into the tags, allowing them to treat both tasks as if they were a single NER task. A series of two bidirectional LSTM (BiLSTM) layers and a final softmax layer are used to produce output tags. Li et al. li2019entity solve both tasks as a form of multi-turn question answering in which the input text is queried with question templates first to detect entities and then, given the detected entities, to detect any relations between these entities. Li et al. use BERT BIBREF8 as the backbone of their question-answering model and produce answers by tagging the input text with BILOU tags to identify the span corresponding to the answer(s). The above approaches allow for very little task-specificity, since both the NER task and the RE task are coerced into a single task. Other approaches incorporate greater task-specificity in one of two ways. First, several models share the majority of model parameters between the NER and RE tasks, but also have separate scoring and/or output layers used to produce separate outputs for each task. For example, Katiyar and Cardie katiyar-cardie-2017-going and Bekoulis et al. bekoulis2018joint propose models in which token representations first pass through one or more shared BiLSTM layers. Katiyar and Cardie use a softmax layer to tag tokens with BILOU tags to solve the NER task and use an attention layer to detect relations between each pair of entities. Bekoulis et al., following Lample et al. Lample2016, use a conditional random field (CRF) layer to produce BIO tags for the NER task. The output from the shared BiLSTM layer for every pair of tokens is passed through relation scoring and sigmoid layers to predict relations. A second method of incorporating greater task-specificity into these models is via deeper layers for solving the RE task. Miwa and Bansal miwa-bansal-2016-end and Li et al. li2017neural pass token representations through a BiLSTM layer and then use a softmax layer to label each token with the appropriate BILOU label. Both proposals then use a type of tree-structured bidirectional LSTM layer stacked on top of the shared BiLSTM to solve the RE task. Nguyen and Verspoor nguyen2019end use BiLSTM and CRF layers to perform the NER task. Label embeddings are created from predicted NER labels, concatenated with token representations, and then passed through a RE-specific BiLSTM. A biaffine attention layer BIBREF9 operates on the output of this BiLSTM to predict relations. An alternative to the BIO/BILOU scheme is the span-based approach, wherein spans of the input text are directly labeled as to whether they correspond to any entity and, if so, their entity types. Luan et al. Luan2018 adopt a span-based approach in which token representations are first passed through a BiLSTM layer. The output from the BiLSTM is used to construct representations of candidate entity spans, which are then scored for both the NER and RE tasks via feed forward layers. Luan et al. Luan2019 follow a similar approach, but construct coreference and relation graphs between entities to propagate information between entities connected in these graphs. The resulting entity representations are then classified for NER and RE via feed forward layers. To the best of our knowledge, the current SOTA model for joint NER and RE is the span-based proposal of Eberts and Ulges eberts2019span. In this architecture, token representations are obtained using a pre-trained BERT model that is fine-tuned during training. Representations for candidate entity spans are obtained by max pooling over all tokens in each span. Span representations are passed through an entity classification layer to solve the NER task. Representations of all pairs of spans that are predicted to be entities and representations of the contexts between these pairs are then passed through a final layer with sigmoid activation to predict relations between entities. With respect to their degrees of task-specificity, these span-based approaches resemble the BIO/BILOU approaches in which the majority of model parameters are shared, but each task possesses independent scoring and/or output layers. Overall, previous approaches to joint NER and RE have experimented little with deep task-specificity, with the exception of those models that include additional layers for the RE task. To our knowledge, no work has considered including additional NER-specific layers beyond scoring and/or output layers. This may reflect a residual influence of the pipeline approach in which the NER task must be solved first before additional layers are used to solve the RE task. However, there is no a priori reason to think that the RE task would benefit more from additional task-specific layers than the NER task. We also note that while previous work has tackled joint NER and RE in variety of textual domains, in all cases the number of shared and task-specific parameters is held constant across these domains.",Model,"The architecture proposed here is inspired by several previous proposals BIBREF10, BIBREF11, BIBREF12. We treat the NER task as a sequence labeling problem using BIO labels. Token representations are first passed through a series of shared, BiRNN layers. Stacked on top of these shared BiRNN layers is a sequence of task-specific BiRNN layers for both the NER and RE tasks. We take the number of shared and task-specific layers to be a hyperparameter of the model. Both sets of task-specific BiRNN layers are followed by task-specific scoring and output layers. Figure FIGREF4 illustrates this architecture. Below, we use superscript $e$ for NER-specific variables and layers and superscript $r$ for RE-specific variables and layers.",Model ::: Shared Layers,"We obtain contextual token embeddings using the pre-trained ELMo 5.5B model BIBREF13. For each token in the input text $t_i$, this model returns three vectors, which we combine via a weighted averaging layer. Each token $t_i$'s weighted ELMo embedding $\mathbf {t}^{elmo}_{i}$ is concatenated to a pre-trained GloVe embedding BIBREF14 $\mathbf {t}^{glove}_{i}$, a character-level word embedding $\mathbf {t}^{char}_i$ learned via a single BiRNN layer BIBREF15 and a one-hot encoded casing vector $\mathbf {t}^{casing}_i$. The full representation of $t_i$ is given by $\mathbf {v}_i$ (where $\circ $ denotes concatenation): For an input text with $n$ tokens, $\mathbf {v}_{1:n}$ are fed as input to a sequence of one or more shared BiRNN layers, with the output sequence from the $i$th shared BiRNN layer serving as the input sequence to the $i + 1$st shared BiRNN layer.",Model ::: NER-Specific Layers,"The final shared BiRNN layer is followed by a sequence of zero or more NER-specific BiRNN layers; the output of the final shared BiRNN layer serves as input to the first NER-specific BiRNN layer, if such a layer exists, and the output from from the $i$th NER-specific BiRNN layer serves as input to the $i + 1$st NER-specific BiRNN layer. For every token $t_i$, let $\mathbf {h}^{e}_i$ denote an NER-specific hidden representation for $t_i$ corresponding to the $i$th element of the output sequence from the final NER-specific BiRNN layer or the final shared BiRNN layer if there are zero NER-specific BiRNN layers. An NER score for token $t_i$, $\mathbf {s}^{e}_i$, is obtained by passing $\mathbf {h}^{e}_i$ through a series of two feed forward layers: The activation function of $\text{FFNN}^{(e1)}$ and its output size are treated as hyperparameters. $\text{FFNN}^{(e2)}$ uses linear activation and its output size is $|\mathcal {E}|$, where $\mathcal {E}$ is the set of possible entity types. The sequence of NER scores for all tokens, $\mathbf {s}^{e}_{1:n}$, is then passed as input to a linear-chain CRF layer to produce the final BIO tag predictions, $\hat{\mathbf {y}}^e_{1:n}$. During inference, Viterbi decoding is used to determine the most likely sequence $\hat{\mathbf {y}}^e_{1:n}$.",Model ::: RE-Specific Layers,"Similar to the NER-specific layers, the output sequence from the final shared BiRNN layer is fed through zero or more RE-specific BiRNN layers. Let $\mathbf {h}^{r}_i$ denote the $i$th output from the final RE-specific BiRNN layer or the final shared BiRNN layer if there are no RE-specific BiRNN layers. Following previous work BIBREF16, BIBREF11, BIBREF12, we predict relations between entities $e_i$ and $e_j$ using learned representations from the final tokens of the spans corresponding to $e_i$ and $e_j$. To this end, we filter the sequence $\mathbf {h}^{r}_{1:n}$ to include only elements $\mathbf {h}^{r}_{i}$ such that token $t_i$ is the final token in an entity span. During training, ground truth entity spans are used for filtering. During inference, predicted entity spans derived from $\hat{\mathbf {y}}^e_{1:n}$ are used. Each $\mathbf {h}^{r}_{i}$ is concatenated to a learned NER label embedding for $t_i$, $\mathbf {l}^{e}_{i}$: Ground truth NER labels are used to obtain $\mathbf {l}^{e}_{1:n}$ during training, and predicted NER labels are used during inference. Next, RE scores are computed for every pair $(\mathbf {g}^{r}_i, \mathbf {g}^{r}_j)$. If $\mathcal {R}$ is the set of possible relations, we calculate the DistMult score BIBREF17 for every relation $r_k \in \mathcal {R}$ and every pair $(\mathbf {g}^{r}_i, \mathbf {g}^{r}_j)$ as follows: $M^{r_k}$ is a diagonal matrix such that $M^{r_k} \in \mathbb {R}^{p \times p}$, where $p$ is the dimensionality of $\mathbf {g}^r_i$. We also pass each RE-specific hidden representation $\mathbf {g}^{r}_i$ through a single feed forward layer: As in the case of $\text{FFNN}^{(e1)}$, the activation function of $\text{FFNN}^{(r1)}$ and its output size are treated as hyperparameters. Let $\textsc {DistMult}^r_{i,j}$ denote the concatenation of $\textsc {DistMult}^{r_k}(\mathbf {g}^r_i, \mathbf {g}^r_j)$ for all $r_k \in \mathcal {R}$ and let $\cos _{i,j}$ denote the cosine distance between vectors $\mathbf {f}^{r}_i$ and $\mathbf {f}^{r}_j$. We obtain RE scores for $(t_i, t_j)$ via a feed forward layer: $\text{FFNN}^{(r2)}$ uses linear activation, and its output size is $|\mathcal {R}|$. Final relation predictions for a pair of tokens $(t_i, t_j)$, $\hat{\mathbf {y}}^r_{i,j}$, are obtained by passing $\mathbf {s}^r_{i,j}$ through an elementwise sigmoid layer. A relation is predicted for all outputs from this sigmoid layer exceeding $\theta ^r$, which we treat as a hyperparameter.",Model ::: Training,"During training, character embeddings, label embeddings, and weights for the weighted average layer, all BiRNN weights, all feed forward networks, and $M^{r_k}$ for all $r_k \in \mathcal {R}$ are trained in a supervised manner. As mentioned above, BIO tags for all tokens are used as labels for the NER task. For the the RE task, binary outputs are used. For every relation $r_k \in R$ and for every pair of tokens $(t_i, t_j)$ such that $t_i$ is the final token of entity $e_i$ and $t_j$ is the final token of entity $e_j$, the RE label $y^{r_k}_{i,j} = 1$ if $(e_i, e_j, r_k)$ is a true relation. Otherwise, we have $y^{r_k}_{i,j} = 0$. For both output layers, we compute the cross-entropy loss. If $\mathcal {L}_{NER}$ and $\mathcal {L}_{RE}$ denote the cross-entropy loss for the NER and RE outputs, respectively, then the total model loss is given by $\mathcal {L} = \mathcal {L}_{NER} + \lambda ^r \mathcal {L}_{RE}$. The weight $\lambda ^r$ is treated as a hyperparameter and allows for tuning the relative importance of the NER and RE tasks during training. Final training for both datasets used a value of 5 for $\lambda ^r$. For the ADE dataset, we trained using the Adam optimizer with a mini-batch size of 16. For the CoNLL04 dataset, we used the Nesterov Adam optimizer with and a mini-batch size of 2. For both datasets, we used a learning rate of $5\times 10^{-4}$, During training, dropout was applied before each BiRNN layer, other than the character BiRNN layer, and before the RE scoring layer.",Experiments,We evaluate the architecture described above using the following two publicly available datasets.,Experiments ::: ADE,"The Adverse Drug Events (ADE) dataset BIBREF6 consists of 4,272 sentences describing adverse effects from the use of particular drugs. The text is annotated using two entity types (Adverse-Effect and Drug) and a single relation type (Adverse-Effect). Of the entity instances in the dataset, 120 overlap with other entities. Similar to prior work using BIO/BILOU tagging, we remove overlapping entities. We preserve the entity with the longer span and remove any relations involving a removed entity. There are no official training, dev, and test splits for the ADE dataset, leading previous researchers to use some form of cross-validation when evaluating their models on this dataset. We split out 10% of the data to use as a held-out dev set. Final results are obtained via 10-fold cross-validation using the remaining 90% of the data and the hyperparameters obtained from tuning on the dev set. Following previous work, we report macro-averaged performance metrics averaged across each of the 10 folds.",Experiments ::: CoNLL04,"The CoNLL04 dataset BIBREF7 consists of 1,441 sentences from news articles annotated with four entity types (Location, Organization, People, and Other) and five relation types (Works-For, Kill, Organization-Based-In, Lives-In, and Located-In). This dataset contains no overlapping entities. We use the three-way split of BIBREF16, which contains 910 training, 243 dev, and 288 test sentences. All hyperparameters are tuned against the dev set. Final results are obtained by averaging results from five trials with random weight initializations in which we trained on the combined training and dev sets and evaluated on the test set. As previous work using the CoNLL04 dataset has reported both micro- and macro-averages, we report both sets of metrics.  In evaluating NER performance on these datasets, a predicted entity is only considered a true positive if both the entity's span and span type are correctly predicted. In evaluating RE performance, we follow previous work in adopting a strict evaluation method wherein a predicted relation is only considered correct if the spans corresponding to the two arguments of this relation and the entity types of these spans are also predicted correctly. We experimented with LSTMs and GRUs for all BiRNN layers in the model and experimented with using $1-3$ shared BiRNN layers and $0-3$ task-specific BiRNN layers for each task. Hyperparameters used for final training are listed in Table TABREF17.",Experiments ::: Results,"Full results for the performance of our model, as well as other recent work, are shown in Table TABREF18. In addition to precision, recall, and F1 scores for both tasks, we show the average of the F1 scores across both tasks. On the ADE dataset, we achieve SOTA results for both the NER and RE tasks. On the CoNLL04 dataset, we achieve SOTA results on the NER task, while our performance on the RE task is competitive with other recent models. On both datasets, we achieve SOTA results when considering the average F1 score across both tasks. The largest gain relative to the previous SOTA performance is on the RE task of the ADE dataset, where we see an absolute improvement of 4.5 on the macro-average F1 score. While the model of Eberts and Ulges eberts2019span outperforms our proposed architecture on the CoNLL04 RE task, their results come at the cost of greater model complexity. As mentioned above, Eberts and Ulges fine-tune the BERTBASE model, which has 110 million trainable parameters. In contrast, given the hyperparameters used for final training on the CoNLL04 dataset, our proposed architecture has approximately 6 million trainable parameters. The fact that the optimal number of task-specific layers differed between the two datasets demonstrates the value of taking the number of shared and task-specific layers to be a hyperparameter of our model architecture. As shown in Table TABREF17, the final hyperparameters used for the CoNLL04 dataset included an additional RE-specific BiRNN layer than did the final hyperparameters used for the ADE dataset. We suspect that this is due to the limited number of relations and entities in the ADE dataset. For most examples in this dataset, it is sufficient to correctly identify a single Drug entity, a single Adverse-Effect entity, and an Adverse-Effect relation between the two entities. Thus, the NER and RE tasks for this dataset are more closely related than they are in the case of the CoNLL04 dataset. Intuitively, cases in which the NER and RE problems can be solved by relying on more shared information should require fewer task-specific layers.",Experiments ::: Ablation Study,"To further demonstrate the effectiveness of the additional task-specific BiRNN layers in our architecture, we conducted an ablation study using the CoNLL04 dataset. We trained and evaluated in the same manner described above, using the same hyperparameters, with the following exceptions: We used either (i) zero NER-specific BiRNN layers, (ii) zero RE-specific BiRNN layers, or (iii) zero task-specific BiRNN layers of any kind. We increased the number of shared BiRNN layers to keep the total number of model parameters consistent with the number of parameters in the baseline model. We average the results for each set of hyperparameter across three trials with random weight initializations. Table TABREF26 contains the results from the ablation study. These results show that the proposed architecture benefits from the inclusion of both NER- and RE-specific layers. However, the RE task benefits much more from the inclusion of these task-specific layers than does the NER task. We take this to reflect the fact that the RE task is more difficult than the NER task for the CoNLL04 dataset, and therefore benefits the most from its own task-specific layers. This is consistent with the fact that the hyperparameter setting that performs best on the RE task is that with no NER-specific BiRNN layers, i.e. the setting that retained RE-specific BiRNN layers. In contrast, the inclusion of task-specific BiRNN layers of any kind had relatively little impact on the performance on the NER task. Note that the setting with no NER-specific layers is somewhat similar to the setup of Nguyen and Verspoor's nguyen2019end model, but includes an additional shared and an additional RE-specific layer. That this setting outperforms Nguyen et al.'s model reflects the contribution of having deeper shared and RE-specific layers, separate from the contribution of NER-specific layers.",Conclusion,"Our results demonstrate the utility of using deeper task-specificity in models for joint NER and RE and of tuning the level of task-specificity separately for different datasets. We conclude that prior work on joint NER and RE undervalues the importance of task-specificity. More generally, these results underscore the importance of correctly balancing the number of shared and task-specific parameters in MTL. We note that other approaches that employ a single model architecture across different datasets are laudable insofar as we should prefer models that can generalize well across domains with little domain-specific hyperparameter tuning. On the other hand, the similarity between the NER and RE tasks varies across domains, and improved performance can be achieved on these tasks by tuning the number of shared and task-specific parameters. In our work, we treated the number of shared and task-specific layers as a hyperparameter to be tuned for each dataset, but future work may explore ways to select this aspect of the model architecture in a more principled way. For example, Vandenhende et al. vandenhende2019branched propose using a measure of affinity between tasks to determine how many layers to share in MTL networks. Task affinity scores of NER and RE could be computed for different textual domains or datasets, which could then guide the decision regarding the number of shared and task-specific layers to employ for joint NER and RE models deployed on these domains. Other extensions to the present work could include fine-tuning the model used to obtain contextual word embeddings, e.g. ELMo or BERT, during training. In order to minimize the number of trainable parameters, we did not employ such fine-tuning in our model, but we suspect a fine-tuning approach could lead to improved performance relative to our results. An additional opportunity for future work would be an extension of this work to other related NLP tasks, such as co-reference resolution and cross-sentential relation extraction.",,,,,,,,,,,,,,,,,Do they repot results only on English data?,eae13c9693ace504eab1f96c91b16a0627cd1f75,five,,,,e8b24c3133e0bec0a6465e1f13acd3a5ed816b37,False,True,,"We evaluate the proposed architecture on two publicly available datasets: the Adverse Drug Events (ADE) dataset BIBREF6 and the CoNLL04 dataset BIBREF7. We show that our architecture is able to outperform the current state-of-the-art (SOTA) results on both the NER and RE tasks in the case of ADE. In the case of CoNLL04, our proposed architecture achieves SOTA performance on the NER task and achieves near SOTA performance on the RE task. On both datasets, our results are SOTA when averaging performance across both tasks. Moreover, we achieve these results using an order of magnitude fewer trainable parameters than the current SOTA architecture.","We evaluate the proposed architecture on two publicly available datasets: the Adverse Drug Events (ADE) dataset BIBREF6 and the CoNLL04 dataset BIBREF7. We show that our architecture is able to outperform the current state-of-the-art (SOTA) results on both the NER and RE tasks in the case of ADE. In the case of CoNLL04, our proposed architecture achieves SOTA performance on the NER task and achieves near SOTA performance on the RE task. On both datasets, our results are SOTA when averaging performance across both tasks.",ebd3deb882f8d1c33836b6a453bc6456773b32cd,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,,,,,,,,,What were the variables in the ablation study?,bcec22a75c1f899e9fcea4996457cf177c50c4c5,five,,,,e8b24c3133e0bec0a6465e1f13acd3a5ed816b37,False,,,"To further demonstrate the effectiveness of the additional task-specific BiRNN layers in our architecture, we conducted an ablation study using the CoNLL04 dataset. We trained and evaluated in the same manner described above, using the same hyperparameters, with the following exceptions: We used either (i) zero NER-specific BiRNN layers, (ii) zero RE-specific BiRNN layers, or (iii) zero task-specific BiRNN layers of any kind. We increased the number of shared BiRNN layers to keep the total number of model parameters consistent with the number of parameters in the baseline model. We average the results for each set of hyperparameter across three trials with random weight initializations.","To further demonstrate the effectiveness of the additional task-specific BiRNN layers in our architecture, we conducted an ablation study using the CoNLL04 dataset. We trained and evaluated in the same manner described above, using the same hyperparameters, with the following exceptions:

We used either (i) zero NER-specific BiRNN layers, (ii) zero RE-specific BiRNN layers, or (iii) zero task-specific BiRNN layers of any kind.

We increased the number of shared BiRNN layers to keep the total number of model parameters consistent with the number of parameters in the baseline model.

We average the results for each set of hyperparameter across three trials with random weight initializations.",6bed2a7163c3fcd6d33218fbb09a7650d10a7822,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,How many shared layers are in the system?,58f50397a075f128b45c6b824edb7a955ee8cba1,five,,,,e8b24c3133e0bec0a6465e1f13acd3a5ed816b37,False,,1,FLOAT SELECTED: Table 1: Optimal hyperparameters used for final training on the ADE and CoNLL04 datasets.,FLOAT SELECTED: Table 1: Optimal hyperparameters used for final training on the ADE and CoNLL04 datasets.,6b3f48502059543742b61485a5c7ed874ab9a6f8,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,,,,,,,,How many additional task-specific layers are introduced?,9adcc8c4a10fa0d58f235b740d8d495ee622d596,five,,,,e8b24c3133e0bec0a6465e1f13acd3a5ed816b37,False,,2 for the ADE dataset and 3 for the CoNLL04 dataset,FLOAT SELECTED: Table 1: Optimal hyperparameters used for final training on the ADE and CoNLL04 datasets.,FLOAT SELECTED: Table 1: Optimal hyperparameters used for final training on the ADE and CoNLL04 datasets.,ad66364f90afe2ea62600c773f387823aeeb90c6,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3-Figure2-1.png,"Figure 2: Illustration of our proposed architecture. Token representations are derived from a pre-trained ELMo model, pre-trained GloVe embeddings, learned character-based embeddings, and one-hot encoded casing vectors. The number of shared and task-specific BiRNN layers is treated as a hyperparameter of the model architecture. Only the final token in each entity span is used for predictions for the RE task; grey boxes indicate tokens that are not used for relation predictions. The output for the RE task is a vector of size |R| for all pairs of entities, where R is the set of all possible relations.",4-Table1-1.png,Table 1: Optimal hyperparameters used for final training on the ADE and CoNLL04 datasets.,5-Table2-1.png,"Table 2: Precision, Recall, and F1 scores for our model and other recent models on the ADE and CoNLL04 datasets. Because our scores are averaged across multiple trials, F1 scores shown here cannot be directly calculated from the precision and recall scores shown here. Note that Nguyen and Verspoor do not report precision and recall scores.",6-Table3-1.png,Table 3: Results from an ablation study using the CoNLL04 dataset. All models have the same number of total parameters.,,,,,,,,,,,,,"(i) zero NER-specific BiRNN layers, (ii) zero RE-specific BiRNN layers, or (iii) zero task-specific BiRNN layers of any kind",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UniSent: Universal Adaptable Sentiment Lexica for 1000+ Languages,"In this paper, we introduce UniSent a universal sentiment lexica for 1000 languages created using an English sentiment lexicon and a massively parallel corpus in the Bible domain. To the best of our knowledge, UniSent is the largest sentiment resource to date in terms of number of covered languages, including many low resource languages. To create UniSent, we propose Adapted Sentiment Pivot, a novel method that combines annotation projection, vocabulary expansion, and unsupervised domain adaptation. We evaluate the quality of UniSent for Macedonian, Czech, German, Spanish, and French and show that its quality is comparable to manually or semi-manually created sentiment resources. With the publication of this paper, we release UniSent lexica as well as Adapted Sentiment Pivot related codes. method.",Introduction,"Sentiment classification is an important task which requires either word level or document level sentiment annotations. Such resources are available for at most 136 languages BIBREF0 , preventing accurate sentiment classification in a low resource setup. Recent research efforts on cross-lingual transfer learning enable to train models in high resource languages and transfer this information into other, low resource languages using minimal bilingual supervision BIBREF1 , BIBREF2 , BIBREF3 . Besides that, little effort has been spent on the creation of sentiment lexica for low resource languages (e.g., BIBREF0 , BIBREF4 , BIBREF5 ). We create and release Unisent, the first massively cross-lingual sentiment lexicon in more than 1000 languages. An extensive evaluation across several languages shows that the quality of Unisent is close to manually created resources. Our method is inspired by BIBREF6 with a novel combination of vocabulary expansion and domain adaptation using embedding spaces. Similar to our work, BIBREF7 also use massively parallel corpora to project POS tags and dependency relations across languages. However, their approach is based on assignment of the most probable label according to the alignment model from the source to the target language and does not include any vocabulary expansion or domain adaptation and do not use the embedding graphs.",Method,"Our method, Adapted Sentiment Pivot requires a sentiment lexicon in one language (e.g. English) as well as a massively parallel corpus. Following steps are performed on this input.",Experimental Setup,"Our goal is to evaluate the quality of UniSent against several manually created sentiment lexica in different domains to ensure its quality for the low resource languages. We do this in several steps. As the gold standard sentiment lexica, we chose manually created lexicon in Czech BIBREF11 , German BIBREF12 , French BIBREF13 , Macedonian BIBREF14 , and Spanish BIBREF15 . These lexica contain general domain words (as opposed to Twitter or Bible). As gold standard for twitter domain we use emoticon dataset and perform emoticon sentiment prediction BIBREF16 , BIBREF17 . We use the (manually created) English sentiment lexicon (WKWSCI) in BIBREF18 as a resource to be projected over languages. For the projection step (Section SECREF1 ) we use the massively parallel Bible corpus in BIBREF8 . We then propagate the projected sentiment polarities to all words in the Wikipedia corpus. We chose Wikipedia here because its domain is closest to the manually annotated sentiment lexica we use to evaluate UniSent. In the adaptation step, we compute the shift between the vocabularies in the Bible and Wikipedia corpora. To show that our adaptation method also works well on domains like Twitter, we propose a second evaluation in which we use Adapted Sentiment Pivot to predict the sentiment of emoticons in Twitter. To create our test sets, we first split UniSent and our gold standard lexica as illustrated in Figure FIGREF11 . We then form our training and test sets as follows: (i) UniSent-Lexicon: we use words in UniSent for the sentiment learning in the target domain; for this purpose, we use words INLINEFORM0 . (ii) Baseline-Lexicon: we use words in the gold standard lexicon for the sentiment learning in the target domain; for this purpose we use words INLINEFORM0 . (iii) Evaluation-Lexicon: we randomly exclude a set of words the baseline-lexicon INLINEFORM0 . In selection of the sampling size we make sure that INLINEFORM1 and INLINEFORM2 would contain a comparable number of words. ",Results,In Table TABREF13 we compare the quality of UniSent with the Baseline-Lexicon as well as with the gold standard lexicon for general domain data. The results show that (i) UniSent clearly outperforms the baseline for all languages (ii) the quality of UniSent is close to manually annotated data (iii) the domain adaptation method brings small improvements for morphologically poor languages. The modest gains could be because our drift weighting method (Section SECREF3 ) mainly models a sense shift between words which is not always equivalent to a polarity shift. In Table TABREF14 we compare the quality of UniSent with the gold standard emoticon lexicon in the Twitter domain. The results show that (i) UniSent clearly outperforms the baseline and (ii) our domain adaptation technique brings small improvements for French and Spanish.,Conclusion,"Using our novel Adapted Sentiment Pivot method, we created UniSent, a sentiment lexicon covering over 1000 (including many low-resource) languages in several domains. The only necessary resources to create UniSent are a sentiment lexicon in any language and a massively parallel corpus that can be small and domain specific. Our evaluation showed that the quality of UniSent is closed to manually annotated resources.  ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,how is quality measured?,8f87215f4709ee1eb9ddcc7900c6c054c970160b,,,,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,Accuracy and the macro-F1 (averaged F1 over positive and negative classes) are used as a measure of quality.,"FLOAT SELECTED: Table 1: Comparison of manually created lexicon performance with UniSent in Czech, German, French, Macedonians, and Spanish. We report accuracy and the macro-F1 (averaged F1 over positive and negative classes). The baseline is constantly considering the majority label. The last two columns indicate the performance of UniSent after drift weighting.","FLOAT SELECTED: Table 1: Comparison of manually created lexicon performance with UniSent in Czech, German, French, Macedonians, and Spanish. We report accuracy and the macro-F1 (averaged F1 over positive and negative classes). The baseline is constantly considering the majority label. The last two columns indicate the performance of UniSent after drift weighting.",97009bed24107de806232d7cf069f51053d7ba5e,258ee4069f740c400c0049a2580945a1cc7f044c,True,,,,,,e38ed05ec140abd97006a8fa7af9a7b4930247df,c1018a31c3272ce74964a3280069f62f314a1a58,how many languages exactly is the sentiment lexica for?,b04098f7507efdffcbabd600391ef32318da28b3,,,,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,True,,,,,d1204f71bd3c78a11b133016f54de78e8eaecf6e,258ee4069f740c400c0049a2580945a1cc7f044c,what sentiment sources do they compare with?,8fc14714eb83817341ada708b9a0b6b4c6ab5023,,,,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,,"As the gold standard sentiment lexica, we chose manually created lexicon in Czech BIBREF11 , German BIBREF12 , French BIBREF13 , Macedonian BIBREF14 , and Spanish BIBREF15 . These lexica contain general domain words (as opposed to Twitter or Bible). As gold standard for twitter domain we use emoticon dataset and perform emoticon sentiment prediction BIBREF16 , BIBREF17 .","As the gold standard sentiment lexica, we chose manually created lexicon in Czech BIBREF11 , German BIBREF12 , French BIBREF13 , Macedonian BIBREF14 , and Spanish BIBREF15 .",17db53c0c6f13fe1d43eee276a9554677f007eef,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3-Figure1-1.png,"Figure 1: Neighbors of word ’sensual’ in Spanish, in bible embedding graph (a) and twitter embedding graph (b). Our unsupervised drift weighting method found this word in Spanish to be the most changing word from bible context to the twitter context. Looking more closely at the neighbors, the word sensual in the biblical context has been associated with a negative sentiment of sins. However, in the twitter domain, it has a positive sentiment. This example shows how our unsupervised method can improve the quality of sentiment lexicon.",3-Figure2-1.png,"Figure 2: Data split used in the experimental setup of UniSent evaluation: Set (C) is the intersection of the target embedding space words (Wikipedia/Emoticon) and the UniSent lexicon as well as the manually created lexicon. Set (A) is the intersection of the target embedding space words and the UniSent lexicon, excluding set (C). Set (B) is the intersection of the target embedding space words and the manually created lexicon, excluding set (C).",4-Table1-1.png,"Table 1: Comparison of manually created lexicon performance with UniSent in Czech, German, French, Macedonians, and Spanish. We report accuracy and the macro-F1 (averaged F1 over positive and negative classes). The baseline is constantly considering the majority label. The last two columns indicate the performance of UniSent after drift weighting.",4-Table2-1.png,"Table 2: Comparison of domain adapted and vanilla UniSent for Emoticon sentiment prediction using monlingual twitter embeddings in German, Italian, French, and Spanish.",,,,,,,,,,,,,,"manually created lexicon in Czech BIBREF11 , German BIBREF12 , French BIBREF13 , Macedonian BIBREF14 , and Spanish BIBREF15",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Efficiency through Auto-Sizing: Notre Dame NLP's Submission to the WNGT 2019 Efficiency Task,"This paper describes the Notre Dame Natural Language Processing Group's (NDNLP) submission to the WNGT 2019 shared task (Hayashi et al., 2019). We investigated the impact of auto-sizing (Murray and Chiang, 2015; Murray et al., 2019) to the Transformer network (Vaswani et al., 2017) with the goal of substantially reducing the number of parameters in the model. Our method was able to eliminate more than 25% of the model's parameters while suffering a decrease of only 1.1 BLEU.",Introduction,"The Transformer network BIBREF3 is a neural sequence-to-sequence model that has achieved state-of-the-art results in machine translation. However, Transformer models tend to be very large, typically consisting of hundreds of millions of parameters. As the number of parameters directly corresponds to secondary storage requirements and memory consumption during inference, using Transformer networks may be prohibitively expensive in scenarios with constrained resources. For the 2019 Workshop on Neural Generation of Text (WNGT) Efficiency shared task BIBREF0, the Notre Dame Natural Language Processing (NDNLP) group looked at a method of inducing sparsity in parameters called auto-sizing in order to reduce the number of parameters in the Transformer at the cost of a relatively minimal drop in performance. Auto-sizing, first introduced by BIBREF1, uses group regularizers to encourage parameter sparsity. When applied over neurons, it can delete neurons in a network and shrink the total number of parameters. A nice advantage of auto-sizing is that it is independent of model architecture; although we apply it to the Transformer network in this task, it can easily be applied to any other neural architecture. NDNLP's submission to the 2019 WNGT Efficiency shared task uses a standard, recommended baseline Transformer network. Following BIBREF2, we investigate the application of auto-sizing to various portions of the network. Differing from their work, the shared task used a significantly larger training dataset from WMT 2014 BIBREF4, as well as the goal of reducing model size even if it impacted translation performance. Our best system was able to prune over 25% of the parameters, yet had a BLEU drop of only 1.1 points. This translates to over 25 million parameters pruned and saves almost 100 megabytes of disk space to store the model.",Auto-sizing,"Auto-sizing is a method that encourages sparsity through use of a group regularizer. Whereas the most common applications of regularization will act over parameters individually, a group regularizer works over groupings of parameters. For instance, applying a sparsity inducing regularizer to a two-dimensional parameter tensor will encourage individual values to be driven to 0.0. A sparsity-inducing group regularizer will act over defined sub-structures, such as entire rows or columns, driving the entire groups to zero. Depending on model specifications, one row or column of a tensor in a neural network can correspond to one neuron in the model. Following the discussion of BIBREF1 and BIBREF2, auto-sizing works by training a neural network while using a regularizer to prune units from the network, minimizing: $W$ are the parameters of the model and $R$ is a regularizer. Here, as with the previous work, we experiment with two regularizers: The optimization is done using proximal gradient descent BIBREF5, which alternates between stochastic gradient descent steps and proximal steps:",Auto-sizing the Transformer,"The Transformer network BIBREF3 is a sequence-to-sequence model in which both the encoder and the decoder consist of stacked self-attention layers. The multi-head attention uses two affine transformations, followed by a softmax layer. Each layer has a position-wise feed-forward neural network (FFN) with a hidden layer of rectified linear units. Both the multi-head attention and the feed-forward neural network have residual connections that allow information to bypass those layers. In addition, there are also word and position embeddings. Figure FIGREF1, taken from the original paper, shows the architecture. NDNLP's submission focuses on the $N$ stacked encoder and decoder layers. The Transformer has demonstrated remarkable success on a variety of datasets, but it is highly over-parameterized. For example, the baseline Transformer model has more than 98 million parameters, but the English portion of the training data in this shared task has only 116 million tokens and 816 thousand types. Early NMT models such as BIBREF6 have most of their parameters in the embedding layers, but the transformer has a larger percentage of the model in the actual encoder and decoder layers. Though the group regularizers of auto-sizing can be applied to any parameter matrix, here we focus on the parameter matrices within the encoder and decoder layers. We note that there has been some work recently on shrinking networks through pruning. However, these differ from auto-sizing as they frequently require an arbitrary threshold and are not included during the training process. For instance, BIBREF7 prunes networks based off a variety of thresholds and then retrains a model. BIBREF8 also look at pruning, but of attention heads specifically. They do this through a relaxation of an $\ell _0$ regularizer in order to make it differentiable. This allows them to not need to use a proximal step. This method too starts with pre-trained model and then continues training. BIBREF9 also look at pruning attention heads in the transformer. However, they too use thresholding, but only apply it at test time. Auto-sizing does not require a thresholding value, nor does it require a pre-trained model. Of particular interest are the large, position-wise feed-forward networks in each encoder and decoder layer:  $W_1$ and $W_2$ are two large affine transformations that take inputs from $D$ dimensions to $4D$, then project them back to $D$ again. These layers make use of rectified linear unit activations, which were the focus of auto-sizing in the work of BIBREF1. No theory or intuition is given as to why this value of $4D$ should be used. Following BIBREF2, we apply the auto-sizing method to the Transformer network, focusing on the two largest components, the feed-forward layers and the multi-head attentions (blue and orange rectangles in Figure FIGREF1). Remember that since there are residual connections allowing information to bypass the layers we are auto-sizing, information can still flow through the network even if the regularizer drives all the neurons in a layer to zero – effectively pruning out an entire layer.",Experiments,"All of our models are trained using the fairseq implementation of the Transformer BIBREF10. For the regularizers used in auto-sizing, we make use of an open-source, proximal gradient toolkit implemented in PyTorch BIBREF2. For each mini-batch update, the stochastic gradient descent step is handled with a standard PyTorch forward-backward call. Then the proximal step is applied to parameter matrices.",Experiments ::: Settings,"We used the originally proposed transformer architecture – with six encoder and six decoder layers. Our model dimension was 512 and we used 8 attention heads. The feed-forward network sub-components were of size 2048. All of our systems were run using subword units (BPE) with 32,000 merge operations on concatenated source and target training data BIBREF11. We clip norms at 0.1, use label smoothed cross-entropy with value 0.1, and an early stopping criterion when the learning rate is smaller than $10^{-5}$. We used the Adam optimizer BIBREF12, a learning rate of $10^{-4}$, and dropout of 0.1. Following recommendations in the fairseq and tensor2tensor BIBREF13 code bases, we apply layer normalization before a sub-component as opposed to after. At test time, we decoded using a beam of 5 with length normalization BIBREF14 and evaluate using case-sensitive, tokenized BLEU BIBREF15. For the auto-sizing experiments, we looked at both $\ell _{2,1}$ and $\ell _{\infty ,1}$ regularizers. We experimented over a range of regularizer coefficient strengths, $\lambda $, that control how large the proximal gradient step will be. Similar to BIBREF1, but differing from BIBREF16, we use one value of $\lambda $ for all parameter matrices in the network. We note that different regularization coefficient values are suited for different types or regularizers. Additionally, all of our experiments use the same batch size, which is also related to $\lambda $.",Experiments ::: Auto-sizing sub-components,"We applied auto-sizing to the sub-components of the encoder and decoder layers, without touching the word or positional embeddings. Recall from Figure FIGREF1, that each layer has multi-head attention and feed-forward network sub-components. In turn, each multi-head attention sub-component is comprised of two parameter matrices. Similarly, each feed-forward network has two parameter matrices, $W_1$ and $W_2$. We looked at three main experimental configurations: All: Auto-sizing is applied to every multi-head attention and feed-forward network sub-component in every layer of the encoder and decoder. Encoder: As with All, auto-sizing is applied to both multi-head attention and feed-forward network sub-components, but only in the encoder layers. The decoder remains the same. FFN: Auto-sizing applied only to the feed-forward network sub-components $W_1$ and $W_2$, but not to the multi-head portions. This too is applied to both the encoder and decoder.",Experiments ::: Results,"Our results are presented in Table TABREF6. The baseline system has 98.2 million parameters and a BLEU score of 27.9 on newstest2015. It takes up 375 megabytes on disk. Our systems that applied auto-sizing only to the feed-forward network sub-components of the transformer network maintained the best BLEU scores while also pruning out the most parameters of the model. Overall, our best system used $\ell _{2,1}=1.0$ regularization for auto-sizing and left 73.1 million parameters remaining. On disk, the model takes 279 megabytes to store – roughly 100 megabytes less than the baseline. The performance drop compared to the baseline is 1.1 BLEU points, but the model is over 25% smaller. Applying auto-sizing to the multi-head attention and feed-forward network sub-components of only the encoder also pruned a substantial amount of parameters. Though this too resulted in a smaller model on disk, the BLEU scores were worse than auto-sizing just the feed-forward sub-components. Auto-sizing the multi-head attention and feed-forward network sub-components of both the encoder and decoder actually resulted in a larger model than the encoder only, but with a lower BLEU score. Overall, our results suggest that the attention portion of the transformer network is more important for model performance than the feed-forward networks in each layer.",Conclusion,"In this paper, we have investigated the impact of using auto-sizing on the transformer network of the 2019 WNGT efficiency task. We were able to delete more than 25% of the parameters in the model while only suffering a modest BLEU drop. In particular, focusing on the parameter matrices of the feed-forward networks in every layer of the encoder and decoder yielded the smallest models that still performed well. A nice aspect of our proposed method is that the proximal gradient step of auto-sizing can be applied to a wide variety of parameter matrices. Whereas for the transformer, the largest impact was on feed-forward networks within a layer, should a new architecture emerge in the future, auto-sizing can be easily adapted to the trainable parameters. Overall, NDNLP's submission has shown that auto-sizing is a flexible framework for pruning parameters in a large NMT system. With an aggressive regularization scheme, large portions of the model can be deleted with only a modest impact on BLEU scores. This in turn yields a much smaller model on disk and at run-time.",Acknowledgements,"This research was supported in part by University of Southern California, subcontract 67108176 under DARPA contract HR0011-15-C-0115.",,,,,,,,,,,,,,,,,,,,,,,,,What is WNGT 2019 shared task?,aaed6e30cf16727df0075b364873df2a4ec7605b,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,,efficiency task aimed  at reducing the number of parameters while minimizing drop in performance,"The Transformer network BIBREF3 is a neural sequence-to-sequence model that has achieved state-of-the-art results in machine translation. However, Transformer models tend to be very large, typically consisting of hundreds of millions of parameters. As the number of parameters directly corresponds to secondary storage requirements and memory consumption during inference, using Transformer networks may be prohibitively expensive in scenarios with constrained resources. For the 2019 Workshop on Neural Generation of Text (WNGT) Efficiency shared task BIBREF0, the Notre Dame Natural Language Processing (NDNLP) group looked at a method of inducing sparsity in parameters called auto-sizing in order to reduce the number of parameters in the Transformer at the cost of a relatively minimal drop in performance.","For the 2019 Workshop on Neural Generation of Text (WNGT) Efficiency shared task BIBREF0, the Notre Dame Natural Language Processing (NDNLP) group looked at a method of inducing sparsity in parameters called auto-sizing in order to reduce the number of parameters in the Transformer at the cost of a relatively minimal drop in performance.",d28bef9548d5b694da9250fb31bb1da6a28c6d2b,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2-Figure1-1.png,"Figure 1: Architecture of the Transformer (Vaswani et al., 2017). We apply the auto-sizing method to the feed-forward (blue rectangles) and multi-head attention (orange rectangles) in all N layers of the encoder and decoder. Note that there are residual connections that can allow information and gradients to bypass any layer we are auto-sizing. Following the robustness recommendations, we instead layer norm before.",3-Figure2-1.png,"Figure 2: Auto-sizing FFN network. For a row in the parameter matrix W1 that has been driven completely to 0.0 (shown in white), the corresponding column in W2 (shown in blue) no longer has any impact on the model. Both the column and the row can be deleted, thereby shrinking the model.",4-Table1-1.png,Table 1: Comparison of BLEU scores and model sizes on newstest2014 and newstest2015. Applying auto-sizing to the feed-forward neural network sub-components of the transformer resulted in the most amount of pruning while still maintaining good BLEU scores.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Twitter-Network Topic Model: A Full Bayesian Treatment for Social Network and Text Modeling,"Twitter data is extremely noisy -- each tweet is short, unstructured and with informal language, a challenge for current topic modeling. On the other hand, tweets are accompanied by extra information such as authorship, hashtags and the user-follower network. Exploiting this additional information, we propose the Twitter-Network (TN) topic model to jointly model the text and the social network in a full Bayesian nonparametric way. The TN topic model employs the hierarchical Poisson-Dirichlet processes (PDP) for text modeling and a Gaussian process random function model for social network modeling. We show that the TN topic model significantly outperforms several existing nonparametric models due to its flexibility. Moreover, the TN topic model enables additional informative inference such as authors' interests, hashtag analysis, as well as leading to further applications such as author recommendation, automatic topic labeling and hashtag suggestion. Note our general inference framework can readily be applied to other topic models with embedded PDP nodes.",Introduction,"Emergence of web services such as blog, microblog and social networking websites allows people to contribute information publicly. This user-generated information is generally more personal, informal and often contains personal opinions. In aggregate, it can be useful for reputation analysis of entities and products, natural disasters detection, obtaining first-hand news, or even demographic analysis. Twitter, an easily accessible source of information, allows users to voice their opinions and thoughts in short text known as tweets. Latent Dirichlet allocation (LDA) BIBREF0 is a popular form of topic model. Unfortunately, a direct application of LDA on tweets yields poor result as tweets are short and often noisy BIBREF1 , i.e. tweets are unstructured and often contain grammatical and spelling errors, as well as informal words such as user-defined abbreviations due to the 140 characters limit. LDA fails on short tweets since it is heavily dependent on word co-occurrence. Also notable is that text in tweets may contain special tokens known as hashtags; they are used as keywords and allow users to link their tweets with other tweets tagged with the same hashtag. Nevertheless, hashtags are informal since they have no standards. Hashtags can be used as both inline words or categorical labels. Hence instead of being hard labels, hashtags are best treated as special words which can be the themes of the tweets. Tweets are thus challenging for topic models, and ad hoc alternatives are used instead. In other text analysis applications, tweets are often `cleansed' by NLP methods such as lexical normalization BIBREF2 . However, the use of normalization is also criticized BIBREF3 . In this paper, we propose a novel method for short text modeling by leveraging the auxiliary information that accompanies tweets. This information, complementing word co-occurrence, allows us to model the tweets better, as well as opening the door to more applications, such as user recommendation and hashtag suggestion. Our main contributions include: 1) a fully Bayesian nonparametric model called Twitter-Network (TN) topic model that models tweets very well; and 2) a combination of both the hierarchical Poisson Dirichlet process (HPDP) and the Gaussian process (GP) to jointly model text, hashtags, authors and the followers network. We also develop a flexible framework for arbitrary PDP networks, which allows quick deployment (including inference) of new variants of HPDP topic models. Despite the complexity of the TN topic model, its implementation is made relatively straightforward with the use of the framework.",Background and Related Work,"LDA is often extended for different types of data, some notable examples that use auxiliary information are the author-topic model BIBREF4 , the tag-topic model BIBREF5 , and Topic-Link LDA BIBREF6 . However, these models only deal with just one kind of additional information and do not work well with tweets since they are designed for other types of text data. Note that the tag-topic model treats tags as hard labels and uses them to group text documents, which is not appropriate for tweets due to the noisy nature of hashtags. Twitter-LDA BIBREF1 and the behavior-topic model BIBREF7 were designed to explicitly model tweets. Both models are not admixture models since they limit one topic per document. The behavior-topic model analyzes tweets' “posting behavior” of each topic for user recommendation. On the other hand, the biterm topic model BIBREF8 uses only the biterm co-occurrence to model tweets, discarding document level information. Both biterm topic model and Twitter-LDA do not incorporate any auxiliary information. All the above topic models also have a limitation in that the number of topics need to be chosen in advance, which is difficult since this number is not known. To sidestep the need of choosing the number of topics, BIBREF9 proposed Hierarchical Dirichlet process (HDP) LDA, which utilizes the Dirichlet process (DP) as nonparametric prior. Furthermore, one can replace the DP with the Poisson-Dirichlet process (PDP, also known as the Pitman-Yor process), which models the power-law of word frequencies distributions in natural languages. In natural languages, the distribution of word frequencies exhibits a power-law BIBREF10 . For topic models, replacing the Dirichlet distribution with the PDP can yield great improvement BIBREF11 . Some recent work models text data with network information ( BIBREF6 , BIBREF12 , BIBREF13 ), however, these models are parametric in nature and can be restrictive. On the contrary, Miller et al. BIBREF14 and Lloyd et al. BIBREF15 model network data directly with nonparametric priors, i.e. with the Indian Buffet process and the Gaussian process respectively, but do not model text.",Model Summary,"The TN topic model makes use of the accompanying hashtags, authors, and followers network to model tweets better. The TN topic model is composed of two main components: a HPDP topic model for the text and hashtags, and a GP based random function model for the followers network. The authorship information serves to connect the two together. We design our HPDP topic model for text as follows. First, generate the global topic distribution $\mu _0$ that serves as a prior. Then generate the respective authors' topic distributions $\nu $ for each author, and a miscellaneous topic distribution $\mu _1$ to capture topics that deviate from the authors' usual topics. Given $\nu $ and $\mu _1$ , we generate the topic distributions for the documents, and words ( $\eta $ , $\theta ^{\prime }$ , $\theta $ ). We also explicitly model the influence of hashtags to words. Hashtag and word generation follows standard LDA and is not discussed here. Note that the tokens of hashtags are shared with the words, i.e. the hashtag #happy share the same token as the word happy. Also note that all distributions on probability vectors are modeled by the PDP, making the model a network of PDP nodes. The network modeling is connected to the HPDP topic model via the author topic distributions $\nu $ , where we treat $\nu $ as inputs to the GP in the network model. The GP, denoted as $\mathcal {F}$ , determines the links between the authors ( $x$ ). Figure 1 displays the graphical model of TN, where region a and b shows the network model and topic model respectively. See supplementary material for a detailed description. We emphasize that our treatment of the network model is different to that of BIBREF15 . We define a new kernel function based on the cosine similarity in our network model, which provides significant improvement over the original kernel function. Also, we derive a new sampling procedure for inference due to the additive coupling of topic distributions and network connections.",Posterior Inference,"We alternatively perform Markov chain Monte Carlo (MCMC) sampling on the topic model and the network model, conditioned on each other. We derive a collapsed Gibbs sampler for the topic model, and a Metropolis-Hastings (MH) algorithm for the network model. We develop a framework to perform collapse Gibbs sampling generally on any Bayesian network of PDPs, built upon the work of BIBREF16 , BIBREF17 , which allows quick prototyping and development of new variants of topic model. We refer the readers to the supplementary materials for the technical details.",Experiments and Applications,"We evaluate the TN topic model quantitatively with standard topic model measures such as test-set perplexity, likelihood convergence and clustering measures. Qualitatively, we evaluate the model by visualizing the topic summaries, authors' topic distributions and by performing an automatic labeling task. We compare our model with HDP-LDA, a nonparametric variant of the author-topic model (ATM), and the original random function network model. We also perform ablation studies to show the importance of each component in the model. The results of the comparison and ablation studies are shown in Table 1 . We use two tweets corpus for experiments, first is a subset of Twitter7 dataset BIBREF18 , obtained by querying with certain keywords (e.g. finance, sports, politics). we remove tweets that are not English with langid.py BIBREF19 and filter authors who do not have network information and who authored less than 100 tweets. The corpus consists of 60370 tweets by 94 authors. We then randomly select 90% of the dataset as training documents and use the rest for testing. Second tweets corpus is obtained from BIBREF20 , which contains a total of 781186 tweets. We note that we perform no word normalization to prevent any loss of meaning of the noisy text.",Conclusion and Future Work,"We propose a full Bayesian nonparametric Twitter-Network (TN) topic model that jointly models tweets and the associated social network information. Our model employs a nonparametric Bayesian approach by using the PDP and GP, and achieves flexible modeling by performing inference on a network of PDPs. Our experiments with Twitter dataset show that the TN topic model achieves significant improvement compared to existing baselines. Furthermore, our ablation study demonstrates the usefulness of each component of the TN model. Our model also shows interesting applications such as author recommendation, as well as providing additional informative inferences. We also engineered a framework for rapid topic model development, which is important due to the complexity of the model. While we could have used Adaptor Grammars BIBREF21 , our framework yields more efficient computation for topic models. Future work includes speeding up the posterior inference algorithm, especially for the network model, as well as incorporating other auxiliary information that is available in social media such as location, hyperlinks and multimedia contents. We also intend to explore other applications that can be addressed with the TN topic model, such as hashtag recommendation. It is also interesting to apply the TN topic model to other types of data such as blog and publication data.",Acknowledgement,We would like to thank the anonymous reviewers for their helpful feedback and comments. NICTA is funded by the Australian Government through the Department of Communications and the Australian Research Council through the ICT Centre of Excellence Program.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"What are the measures of ""performance"" used in this paper?",2898e4aa7a3496c628e7ddf2985b48fb11aa3bba,infinity,familiar,no,social,50d8b4a941c26b89482c94ab324b5a274f9ced66,False,,,"We evaluate the TN topic model quantitatively with standard topic model measures such as test-set perplexity, likelihood convergence and clustering measures. Qualitatively, we evaluate the model by visualizing the topic summaries, authors' topic distributions and by performing an automatic labeling task. We compare our model with HDP-LDA, a nonparametric variant of the author-topic model (ATM), and the original random function network model. We also perform ablation studies to show the importance of each component in the model. The results of the comparison and ablation studies are shown in Table 1 . We use two tweets corpus for experiments, first is a subset of Twitter7 dataset BIBREF18 , obtained by querying with certain keywords (e.g. finance, sports, politics). we remove tweets that are not English with langid.py BIBREF19 and filter authors who do not have network information and who authored less than 100 tweets. The corpus consists of 60370 tweets by 94 authors. We then randomly select 90% of the dataset as training documents and use the rest for testing. Second tweets corpus is obtained from BIBREF20 , which contains a total of 781186 tweets. We note that we perform no word normalization to prevent any loss of meaning of the noisy text.","We evaluate the TN topic model quantitatively with standard topic model measures such as test-set perplexity, likelihood convergence and clustering measures. Qualitatively, we evaluate the model by visualizing the topic summaries, authors' topic distributions and by performing an automatic labeling task.",27476466c9436cde59aa2925426eaa83e03ebbe7,c7d4a630661cd719ea504dba56393f78278b296b,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2-Figure2-1.png,Figure 2: Log-likelihood vs. iterations,4-Table2-1.png,Table 2: Labeling topics with hashtags,4-Table3-1.png,Table 3: Topics by authors,,,,,,,,,,,,,,,,,,,,,,,,,,,,"test-set perplexity, likelihood convergence and clustering measures visualizing the topic summaries, authors' topic distributions and by performing an automatic labeling task",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Large Arabic Twitter Dataset on COVID-19,"The 2019 coronavirus disease (COVID-19), emerged late December 2019 in China, is now rapidly spreading across the globe. At the time of writing this paper, the number of global confirmed cases has passed one million and half with over 75,000 fatalities. Many countries have enforced strict social distancing policies to contain the spread of the virus. This has changed the daily life of tens of millions of people, and urged people to turn their discussions online, e.g., via online social media sites like Twitter. In this work, we describe the first Arabic tweets dataset on COVID-19 that we have been collecting since March 1st, 2020. The dataset would help researchers and policy makers in studying different societal issues related to the pandemic. Many other tasks related to behavioral change, information sharing, misinformation and rumors spreading can also be analyzed.",Introduction,"On December 31, 2019, Chinese public health authorities reported several cases of a respiratory syndrome caused by an unknown disease, which subsequently became known as COVID-19 in the city of Wuhan, China. This highly contagious disease continued to spread worldwide, leading the World Health Organization (WHO) to declare a global health emergency on January 30, 2020. On March 11, 2020 the disease has been identified as pandemic by WHO, and many countries around the world including Saudi Arabia, United States, United Kingdom, Italy, Canada, and Germany have continued reporting more cases of the disease BIBREF0. As the time of writing this paper, this pandemic is affecting more than 208 countries around the globe with more than one million and half confirmed cases BIBREF1. Since the outbreak of COVID-19, many governments around the world enforced different measures to contain the spread of the virus. The measures include travel restrictions, curfews, ban of mass gatherings, social distancing, and probably cities lock-down. This has impacted the routine of people around the globe, and many of them have turned to social media platforms for both news and communication. Since the emergence of COVID-19, Twitter platform plays a significant role in crisis communications where millions of tweets related to the virus are posted daily. Arabic is the official language of more than 22 countries with nearly 300 million native speakers worldwide. Furthermore, there is a large daily Arabic content in Twitter as millions of Arabic users use the social media network to communicate. For instance, Saudi Arabia alone has nearly 15 million Twitter users as of January, 2020 BIBREF2. Hence, it is important to analyze the Arabic users' behavior and sentiment during this pandemic. Other Twitter COVID-19 datasets have been recently proposed BIBREF3, BIBREF4 but with no significant content for the Arabic language. In this work, we provide the first dataset dedicated to Arabic tweets related to COVID-19. The dataset is available at https://github.com/SarahAlqurashi/COVID-19-Arabic-Tweets-Dataset. We have been collecting data in real-time from Twitter API since January 1, 2020, by tracking COVID-19 related keywords which resulted in more than 3,934,610 Arabic tweets so far. The presented dataset is believed to be helpful for both researchers and policy makers in studying the pandemic from social perspective, as well as analyzing the human behaviour and information spreading during pandemics. In what follows, we describe the dataset and the collection methods, present the initial data statistics, and provide information about how to use the dataset.",Dataset Description,"We collected COVID-19 related Arabic tweets from January 1, 2020 until April 15, 2020, using Twitter streaming API and the Tweepy Python library. We have collected more than 3,934,610 million tweets so far. In our dataset, we store the full tweet object including the id of the tweet, username, hashtags, and geolocation of the tweet. We created a list of the most common Arabic keywords associated with COVID-19. Using Twitter’s streaming API, we searched for any tweet containing the keyword(s) in the text of the tweet. Table TABREF1 shows the list of keywords used along with the starting date of tracking each keyword. Furthermore, Table TABREF2 shows the list of hashtags we have been tracking along with the number of tweets collected from each hashtag. Indeed, some tweets were irrelevant, and we kept only those that were relevant to the pandemic. A summary over the dataset is given in Table TABREF3. While collecting data, we have observed that the number of retweets increased significantly in late March. This is likely due to the exponential increase in confirmed COVID-19 cases worldwide, including the Arabic speaking countries. A relatively small percentage of tweets were geotagged. Figure FIGREF6 presents the location of tweets observed as of 14 April 2020.",Dataset Access,"The dataset is accessible on GitHub at this address: https://github.com/SarahAlqurashi/COVID-19-Arabic-Tweets-Dataset However, to comply with Twitter’s content redistribution policy, we are distributing only the IDs of the collected tweets. There are several tools (such as Hydrator) that can be used to retrieve the full tweet object. We also plan to provide more details on the pre-processing phase in the GitHub page.",Future Work,We are continuously updating the dataset to maintain more aspects of COVID-19 Arabic conversations and discussions happening on Twitter. We also plan to study how different groups respond to the pandemic and analyze information sharing behavior among the users.,Acknowledgements,The authors wish to express their thanks to Batool Mohammed Hmawi for her help in data collection.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,What additional information is found in the dataset?,5774e019101415a43e0b5a780179fd897fc013fd,two,unfamiliar,no,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,,"We collected COVID-19 related Arabic tweets from January 1, 2020 until April 15, 2020, using Twitter streaming API and the Tweepy Python library. We have collected more than 3,934,610 million tweets so far. In our dataset, we store the full tweet object including the id of the tweet, username, hashtags, and geolocation of the tweet. We created a list of the most common Arabic keywords associated with COVID-19. Using Twitter’s streaming API, we searched for any tweet containing the keyword(s) in the text of the tweet. Table TABREF1 shows the list of keywords used along with the starting date of tracking each keyword. Furthermore, Table TABREF2 shows the list of hashtags we have been tracking along with the number of tweets collected from each hashtag. Indeed, some tweets were irrelevant, and we kept only those that were relevant to the pandemic.","In our dataset, we store the full tweet object including the id of the tweet, username, hashtags, and geolocation of the tweet. We created a list of the most common Arabic keywords associated with COVID-19. Using Twitter’s streaming API, we searched for any tweet containing the keyword(s) in the text of the tweet. Table TABREF1 shows the list of keywords used along with the starting date of tracking each keyword. Furthermore, Table TABREF2 shows the list of hashtags we have been tracking along with the number of tweets collected from each hashtag. ",82cabc172ed838fb245d64dc963d24571cf0f456,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,,,,,,,,,Is the dataset focused on a region?,fc33a09401d12f4fe2338b391301380d34a60e5f,two,unfamiliar,no,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,True,,FLOAT SELECTED: Figure 1: The location of geotagged tweets,FLOAT SELECTED: Figure 1: The location of geotagged tweets,4c098a068712a3f1514ffeae047c17483e01bb1b,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,Over what period of time were the tweets collected?,1b046ec7f0e1a33e77078bedef7e83c5c07b61de,two,unfamiliar,no,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,,"We collected COVID-19 related Arabic tweets from January 1, 2020 until April 15, 2020, using Twitter streaming API and the Tweepy Python library. We have collected more than 3,934,610 million tweets so far. In our dataset, we store the full tweet object including the id of the tweet, username, hashtags, and geolocation of the tweet. We created a list of the most common Arabic keywords associated with COVID-19. Using Twitter’s streaming API, we searched for any tweet containing the keyword(s) in the text of the tweet. Table TABREF1 shows the list of keywords used along with the starting date of tracking each keyword. Furthermore, Table TABREF2 shows the list of hashtags we have been tracking along with the number of tweets collected from each hashtag. Indeed, some tweets were irrelevant, and we kept only those that were relevant to the pandemic.","We collected COVID-19 related Arabic tweets from January 1, 2020 until April 15, 2020, using Twitter streaming API and the Tweepy Python library. ",b6fac411c4e289abd2459908238e5aa5317971aa,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,,,,,,,,Are the tweets location-specific?,55fb92afa118450f09329764efe22612676c2d85,two,unfamiliar,no,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,True,,FLOAT SELECTED: Figure 1: The location of geotagged tweets,FLOAT SELECTED: Figure 1: The location of geotagged tweets,72766c3c804b9d83196765b42f6c24d90f19ae8c,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,,,,,,,,How big is the dataset?,19cdce39e8265e7806212eeee2fd55f8ef2f3d47,two,unfamiliar,no,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,"more than 3,934,610 million tweets",,,"We collected COVID-19 related Arabic tweets from January 1, 2020 until April 15, 2020, using Twitter streaming API and the Tweepy Python library. We have collected more than 3,934,610 million tweets so far. In our dataset, we store the full tweet object including the id of the tweet, username, hashtags, and geolocation of the tweet. We created a list of the most common Arabic keywords associated with COVID-19. Using Twitter’s streaming API, we searched for any tweet containing the keyword(s) in the text of the tweet. Table TABREF1 shows the list of keywords used along with the starting date of tracking each keyword. Furthermore, Table TABREF2 shows the list of hashtags we have been tracking along with the number of tweets collected from each hashtag. Indeed, some tweets were irrelevant, and we kept only those that were relevant to the pandemic.","We have collected more than 3,934,610 million tweets so far.",6e10691439c60309bceb26658009e778789d508b,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2-Table2-1.png,Table 2: Hashtags used in collecting this dataset.,2-Table1-1.png,Table 1: The list of keywords that we used to collect the tweets.,2-Table3-1.png,Table 3: Summary statistics for the collected tweets,3-Figure1-1.png,Figure 1: The location of geotagged tweets,,,,,,,,,,,,,,"from January 1, 2020 until April 15, 2020",,,,,,,,,,,,"the full tweet object including the id of the tweet, username, hashtags, and geolocation of the tweet",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TextKD-GAN: Text Generation using KnowledgeDistillation and Generative Adversarial Networks,"Text generation is of particular interest in many NLP applications such as machine translation, language modeling, and text summarization. Generative adversarial networks (GANs) achieved a remarkable success in high quality image generation in computer vision,and recently, GANs have gained lots of interest from the NLP community as well. However, achieving similar success in NLP would be more challenging due to the discrete nature of text. In this work, we introduce a method using knowledge distillation to effectively exploit GAN setup for text generation. We demonstrate how autoencoders (AEs) can be used for providing a continuous representation of sentences, which is a smooth representation that assign non-zero probabilities to more than one word. We distill this representation to train the generator to synthesize similar smooth representations. We perform a number of experiments to validate our idea using different datasets and show that our proposed approach yields better performance in terms of the BLEU score and Jensen-Shannon distance (JSD) measure compared to traditional GAN-based text generation approaches without pre-training.",Introduction,"Recurrent neural network (RNN) based techniques such as language models are the most popular approaches for text generation. These RNN-based text generators rely on maximum likelihood estimation (MLE) solutions such as teacher forcing BIBREF0 (i.e. the model is trained to predict the next item given all previous observations); however, it is well-known in the literature that MLE is a simplistic objective for this complex NLP task BIBREF1 . MLE-based methods suffer from exposure bias BIBREF2 , which means that at training time the model is exposed to gold data only, but at test time it observes its own predictions. However, GANs which are based on the adversarial loss function and have the generator and the discriminator networks suffers less from the mentioned problems. GANs could provide a better image generation framework comparing to the traditional MLE-based methods and achieved substantial success in the field of computer vision for generating realistic and sharp images. This great success motivated researchers to apply its framework to NLP applications as well. GANs have been exploited recently in various NLP applications such as machine translation BIBREF3 , BIBREF4 , dialogue models BIBREF1 , question answering BIBREF5 , and natural language generation BIBREF6 , BIBREF2 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 . However, applying GAN in NLP is challenging due to the discrete nature of the text. Consequently, back-propagation would not be feasible for discrete outputs and it is not straightforward to pass the gradients through the discrete output words of the generator. The existing GAN-based solutions can be categorized according to the technique that they leveraged for handling the problem of the discrete nature of text: Reinforcement learning (RL) based methods, latent space based solutions, and approaches based on continuous approximation of discrete sampling. Several versions of the RL-based techniques have been introduced in the literature including Seq-GAN BIBREF11 , MaskGAN BIBREF12 , and LeakGAN BIBREF13 . However, they often need pre-training and are computationally more expensive compared to the methods of the other two categories. Latent space-based solutions derive a latent space representation of the text using an AE and attempt to learn data manifold of that space BIBREF8 . Another approach for generating text with GANs is to find a continuous approximation of the discrete sampling by using the Gumbel Softmax technique BIBREF14 or approximating the non-differentiable argmax operator BIBREF9 with a continuous function. In this work, we introduce TextKD-GAN as a new solution for the main bottleneck of using GAN for text generation with knowledge distillation: a technique that transfer the knowledge of softened output of a teacher model to a student model BIBREF15 . Our solution is based on an AE (Teacher) to derive a smooth representation of the real text. This smooth representation is fed to the TextKD-GAN discriminator instead of the conventional one-hot representation. The generator (Student) tries to learn the manifold of the softened smooth representation of the AE. We show that TextKD-GAN outperforms the conventional GAN-based text generators that do not need pre-training. The remainder of the paper is organized as follows. In the next two sections, some preliminary background on generative adversarial networks and related work in the literature will be reviewed. The proposed method will be presented in section SECREF4 . In section SECREF5 , the experimental details will be discussed. Finally, section SECREF6 will conclude the paper.",Background,"Generative adversarial networks include two separate deep networks: a generator and a discriminator. The generator takes in a random variable, INLINEFORM0 following a distribution INLINEFORM1 and attempt to map it to the data distribution INLINEFORM2 . The output distribution of the generator is expected to converge to the data distribution during the training. On the other hand, the discriminator is expected to discern real samples from generated ones by outputting zeros and ones, respectively. During training, the generator and discriminator generate samples and classify them, respectively by adversarially affecting the performance of each other. In this regard, an adversarial loss function is employed for training BIBREF16 : DISPLAYFORM0  This is a two-player minimax game for which a Nash-equilibrium point should be derived. Finding the solution of this game is non-trivial and there has been a great extent of literature dedicated in this regard BIBREF17 . As stated, using GANs for text generation is challenging because of the discrete nature of text. To clarify the issue, Figure FIGREF2 depicts a simplistic architecture for GAN-based text generation. The main bottleneck of the design is the argmax operator which is not differentiable and blocks the gradient flow from the discriminator to the generator. DISPLAYFORM0 ",Knowledge Distillation,"Knowledge distillation has been studied in model compression where knowledge of a large cumbersome model is transferred to a small model for easy deployment. Several studies have been studied on the knowledge transfer technique BIBREF15 , BIBREF18 . It starts by training a big teacher model (or ensemble model) and then train a small student model which tries to mimic the characteristics of the teacher model, such as hidden representations BIBREF18 , it's output probabilities BIBREF15 , or directly on the generated sentences by the teacher model in neural machine translation BIBREF19 . The first teacher-student framework for knowledge distillation was proposed in BIBREF15 by introducing the softened teacher's output. In this paper, we propose a GAN framework for text generation where the generator (Student) tries to mimic the reconstructed output representation of an auto-encoder (Teacher) instead of mapping to a conventional one-hot representations.",Improved WGAN,"Generating text with pure GANs is inspired by improved Wasserstein GAN (IWGAN) work BIBREF6 . In IWGAN, a character level language model is developed based on adversarial training of a generator and a discriminator without using any extra element such as policy gradient reinforcement learning BIBREF20 . The generator produces a softmax vector over the entire vocabulary. The discriminator is responsible for distinguishing between the one-hot representations of the real text and the softmax vector of the generated text. The IWGAN method is described in Figure FIGREF6 . A disadvantage of this technique is that the discriminator is able to tell apart the one-hot input from the softmax input very easily. Hence, the generator will have a hard time fooling the discriminator and vanishing gradient problem is highly probable.",Related Work,"A new version of Wasserstein GAN for text generation using gradient penalty for discriminator was proposed in BIBREF6 . Their generator is a CNN network generating fixed-length texts. The discriminator is another CNN receiving 3D tensors as input sentences. It determines whether the tensor is coming from the generator or sampled from the real data. The real sentences and the generated ones are represented using one-hot and softmax representations, respectively. A similar approach was proposed in BIBREF2 with an RNN-based generator. They used a curriculum learning strategy BIBREF21 to produce sequences of gradually increasing lengths as training progresses. In BIBREF7 , RNN is trained to generate text with GAN using curriculum learning. The authors proposed a procedure called teacher helping, which helps the generator to produce long sequences by conditioning on shorter ground-truth sequences. All these approaches use a discriminator to discriminate the generated softmax output from one-hot real data as in Figure FIGREF6 , which is a clear downside for them. The reason is the discriminator receives inputs of different representations: a one-hot vector for real data and a probabilistic vector output from the generator. It makes the discrimination rather trivial. AEs have been exploited along with GANs in different architectures for computer vision application such as AAE BIBREF22 , ALI BIBREF23 , and HALI BIBREF24 . Similarly, AEs can be used with GANs for generating text. For instance, an adversarially regularized AE (ARAE) was proposed in BIBREF8 . The generator is trained in parallel to an AE to learn a continuous version of the code space produced by AE encoder. Then, a discriminator will be responsible for distinguishing between the encoded hidden code and the continuous code of the generator. Basically, in this approach, a continuous distribution is generated corresponding to an encoded code of text.",Methodology,"AEs can be useful in denoising text and transferring it to a code space (encoding) and then reconstructing back to the original text from the code. AEs can be combined with GANs in order to improve the generated text. In this section, we introduce a technique using AEs to replace the conventional one-hot representation BIBREF6 with a continuous softmax representation of real data for discrimination.",Distilling output probabilities of AE to TextKD-GAN generator,"As stated, in conventional text-based discrimination approach BIBREF6 , the real and generated input of the discriminator will have different types (one-hot and softmax) and it can simply tell them apart. One way to avoid this issue is to derive a continuous smooth representation of words rather than their one-hot and train the discriminator to differentiate between the continuous representations. In this work, we use a conventional AE (Teacher) to replace the one-hot representation with softmax reconstructed output, which is a smooth representation that yields smaller variance in gradients BIBREF15 . The proposed model is depicted in Figure FIGREF8 . As seen, instead of the one-hot representation of the real words, we feed the softened reconstructed output of the AE to the discriminator. This technique would makes the discrimination much harder for the discriminator. The GAN generator (Student) with softmax output tries to mimic the AE output distribution instead of conventional one-hot representations used in the literature.",Why TextKD-GAN should Work Better than IWGAN,"Suppose we apply IWGAN to a language vocabulary of size two: words INLINEFORM0 and INLINEFORM1 . The one-hot representation of these two words (as two points in the Cartesian coordinates) and the span of the generated softmax outputs (as a line segment connecting them) is depicted in the left panel of Figure FIGREF10 . As evident graphically, the task of the discriminator is to discriminate the points from the line connecting them, which is a rather simple very easy task. Now, let's consider the TextKD-GAN idea using the two-word language example. As depicted in Figure FIGREF10 (Right panel), the output locus of the TextKD-GAN decoder would be two red line segments instead of two points (in the one-hot case). The two line segments lie on the output locus of the generator, which will make the generator more successful in fooling the discriminator.",Model Training,"We train the AE and TextKD-GAN simultaneously. In order to do so, we break down the objective function into three terms: (1) a reconstruction term for the AE, (2) a discriminator loss function with gradient penalty, (3) an adversarial cost for the generator. Mathematically, DISPLAYFORM0  These losses are trained alternately to optimize different parts of the model. We employ the gradient penalty approach of IWGAN BIBREF6 for training the discriminator. In the gradient penalty term, we need to calculate the gradient norm of random samples INLINEFORM0 . According to the proposal in BIBREF6 , these random samples can be obtained by sampling uniformly along the line connecting pairs of generated and real data samples: DISPLAYFORM0  The complete training algorithm is described in SECREF11 . TextKD-GAN for text generation. [1] The Adam hyperparameters INLINEFORM0 , INLINEFORM1 , INLINEFORM2 , the batch size INLINEFORM3 . Initial AE parameters (encoder ( INLINEFORM4 ), decoder INLINEFORM5 ), discriminator parameters INLINEFORM6 and initial generator parameters INLINEFORM7  number of training iterations AE Training: Sample INLINEFORM0 and compute code-vectors INLINEFORM1   and reconstructed text INLINEFORM0 .  Backpropagate reconstruction loss INLINEFORM0 .  Update with INLINEFORM0 . Train the discriminator: k times: Sample INLINEFORM1 and Sample INLINEFORM2 .  Compute generated text INLINEFORM0   Backpropagate discriminator loss INLINEFORM0 .  Update with INLINEFORM0 . INLINEFORM1 Train the generator: Sample INLINEFORM0 and Sample INLINEFORM1 .  Compute generated text INLINEFORM0   Backpropagate generator loss INLINEFORM0 .  Update with INLINEFORM0 . INLINEFORM1 ",Dataset and Experimental Setup,"We carried out our experiments on two different datasets: Google 1 billion benchmark language modeling data and the Stanford Natural Language Inference (SNLI) corpus. Our text generation is performed at character level with a sentence length of 32. For the Google dataset, we used the first 1 million sentences and extract the most frequent 100 characters to build our vocabulary. For the SNLI dataset, we used the entire preprocessed training data , which contains 714667 sentences in total and the built vocabulary has 86 characters. We train the AE using one layer with 512 LSTM cells BIBREF25 for both the encoder and the decoder. We train the autoencoder using Adam optimizer with learning rate 0.001, INLINEFORM0 = 0.9, and INLINEFORM1 = 0.9. For decoding, the output from the previous time step is used as the input to the next time step. The hidden code INLINEFORM2 is also used as an additional input at each time step of decoding. The greedy search approach is applied to get the best output BIBREF8 . We keep the same CNN-based generator and discriminator with residual blocks as in BIBREF6 . The discriminator is trained for 5 times for 1 GAN generator iteration. We train the generator and the discriminator using Adam optimizer with learning rate 0.0001, INLINEFORM3 = 0.5, and INLINEFORM4 = 0.9. We use the BLEU-N score to evaluate our techniques. BLEU-N score is calculated according to the following equation BIBREF26 , BIBREF27 , BIBREF28 : DISPLAYFORM0  where INLINEFORM0 is the probability of INLINEFORM1 -gram and INLINEFORM2 . We calculate BLEU-n scores for n-grams without a brevity penalty BIBREF10 . We train all the models for 200000 iterations and the results with the best BLEU-N scores in the generated texts are reported. To calculate the BLEU-N scores, we generate ten batches of sentences as candidate texts, i.e. 640 sentences (32-character sentences) and use the entire test set as reference texts.",Experimental Results,"The results of the experiments are depicted in Table TABREF20 and TABREF21 . As seen in these tables, the proposed TextKD-GAN approach yields significant improvements in terms of BLEU-2, BLEU-3 and BLEU-4 scores over the IWGAN BIBREF6 , and the ARAE BIBREF8 approaches. Therefore, softened smooth output of the decoder can be more useful to learn better discriminator than the traditional one-hot representation. Moreover, we can see the lower BLEU-scores and less improvement for the Google dataset compared to the SNLI dataset. The reason might be the sentences in the Google dataset are more diverse and complicated. Finally, note that the text-based one-hot discrimination in IWGAN and our proposed method are better than the traditional code-based ARAE technique BIBREF8 . Some examples of generated text from the SNLI experiment are listed in Table TABREF22 . As seen, the generated text by the proposed TextKD-GAN approach is more meaningful and contains more correct words compared to that of IWGAN BIBREF6 . We also provide the training curves of Jensen-Shannon distances (JSD) between the INLINEFORM0 -grams of the generated sentences and that of the training (real) ones in Figure FIGREF23 . The distances are derived from SNLI experiments and calculated as in BIBREF6 . That is by calculating the log-probabilities of the INLINEFORM1 -grams of the generated and the real sentences. As depicted in the figure, the TextKD-GAN approach further minimizes the JSD compared to the literature methods BIBREF6 , BIBREF8 . In conclusion, our approach learns a more powerful discriminator, which in turn generates the data distribution close to the real data distribution.",Discussion,"The results of our experiment shows the superiority of our TextKD-GAN method over other conventional GAN-based techniques. We compared our technique with those GAN-based generators which does not need pre-training. This explains why we have not included the RL-based techniques in the results. We showed the power of the continuous smooth representations over the well-known tricks to work around the discontinuity of text for GANs. Using AEs in TextKD-GAN adds another important dimension to our technique which is the latent space, which can be modeled and exploited as a separate signal for discriminating the generated text from the real data. It is worth mentioning that our observations during the experiments show training text-based generators is much easier than training the code-based techniques such as ARAE. Moreover, we observed that the gradient penalty term plays a significant part in terms of reducing the mode-collapse from the generated text of GAN. Furthermore, in this work, we focused on character-based techniques; however, TextKD-GAN is applicable to the word-based settings as well. Bear in mind that pure GAN-based text generation techniques are still in a newborn stage and they are not very powerful in terms of learning semantics of complex datasets and large sentences. This might be because of lack of capacity of capturing the long-term information using CNN networks. To address this problem, RL can be employed to empower these pure GAN-based techniques such as TextKD-GAN as a next step .",Conclusion and Future Work,"In this work, we introduced TextKD-GAN as a new solution using knowledge distillation for the main bottleneck of using GAN for generating text, which is the discontinuity of text. Our solution is based on an AE (Teacher) to derive a continuous smooth representation of the real text. This smooth representation is distilled to the GAN discriminator instead of the conventional one-hot representation. We demonstrated the rationale behind this approach, which is to make the discrimination task of the discriminator between the real and generated texts more difficult and consequently providing a richer signal to the generator. At the time of training, the TextKD-GAN generator (Student) would try to learn the manifold of the smooth representation, which can later on be mapped to the real data distribution by applying the argmax operator. We evaluated TextKD-GAN over two benchmark datasets using the BLEU-N scores, JSD measures, and quality of the output generated text. The results showed that the proposed TextKD-GAN approach outperforms the traditional GAN-based text generation methods which does not need pre-training such as IWGAN and ARAE. Finally, We summarize our plan for future work in the following:",,,,,,,,,,,,,,,,,How much better in terms of JSD measure did their model perform?,03502826f4919e251edba1525f84dd42f21b0253,,,,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,True,,,,,699d1f520036c89bada4be303c4e6147fd9c2e77,c1018a31c3272ce74964a3280069f62f314a1a58,,,,,,,,,What does the Jensen-Shannon distance measure?,9368471073c66fefebc04f1820209f563a840240,,,,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,True,,,,,cb0ba22b3f4af213b6c60d0a6b26c20e937be790,c1018a31c3272ce74964a3280069f62f314a1a58,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3-Figure1-1.png,Fig. 1: Simplistic text generator with GAN,4-Figure2-1.png,Fig. 2: Improved WGAN for text generation,5-Figure3-1.png,Fig. 3: TextKD-GAN model for text generation,6-Figure4-1.png,"Fig. 4: Locus of the input vectors to the discriminator for a two-word language model; Left panel: IWGAN, Right panel: TextKD-GAN.",9-Table1-1.png,Table 1: Results of the BLEU-N scores using 1 million sentences from 1 billion Google dataset,10-Figure5-1.png,"Fig. 5: Jensen-Shannon distance (JSD) between the generated and training sentences n-grams derived from SNLI experiments. a) js1, b) js2, c) js3, and d) js4 represent the JSD for 1, 2, 3, and 4-grams respectively",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make Lead Bias in Your Favor: A Simple and Effective Method for News Summarization,"Lead bias is a common phenomenon in news summarization, where early parts of an article often contain the most salient information. While many algorithms exploit this fact in summary generation, it has a detrimental effect on teaching the model to discriminate and extract important information. We propose that the lead bias can be leveraged in a simple and effective way in our favor to pretrain abstractive news summarization models on large-scale unlabeled corpus: predicting the leading sentences using the rest of an article. Via careful data cleaning and filtering, our transformer-based pretrained model without any finetuning achieves remarkable results over various news summarization tasks. With further finetuning, our model outperforms many competitive baseline models. Human evaluations further show the effectiveness of our method.",Introduction,"The goal of text summarization is to condense a piece of text into a shorter version that contains the salient information. Due to the prevalence of news articles and the need to provide succinct summaries for readers, a majority of existing datasets for summarization come from the news domain BIBREF0, BIBREF1, BIBREF2. However, according to journalistic conventions, the most important information in a news report usually appears near the beginning of the article BIBREF3. While it facilitates faster and easier understanding of the news for readers, this lead bias causes undesirable consequences for summarization models. The output of these models is inevitably affected by the positional information of sentences. Furthermore, the simple baseline of using the top few sentences as summary can achieve a stronger performance than many sophisticated models BIBREF4. It can take a lot of effort for models to overcome the lead bias BIBREF3. Additionally, most existing summarization models are fully supervised and require time and labor-intensive annotations to feed their insatiable appetite for labeled data. For example, the New York Times Annotated Corpus BIBREF1 contains 1.8 million news articles, with 650,000 summaries written by library scientists. Therefore, some recent work BIBREF5 explores the effect of domain transfer to utilize datasets other than the target one. But this method may be affected by the domain drift problem and still suffers from the lack of labelled data. The recent promising trend of pretraining models BIBREF6, BIBREF7 proves that a large quantity of data can be used to boost NLP models' performance. Therefore, we put forward a novel method to leverage the lead bias of news articles in our favor to conduct large-scale pretraining of summarization models. The idea is to leverage the top few sentences of a news article as the target summary and use the rest as the content. The goal of our pretrained model is to generate an abstractive summary given the content. Coupled with careful data filtering and cleaning, the lead bias can provide a delegate summary of sufficiently good quality, and it immediately renders the large quantity of unlabeled news articles corpus available for training news summarization models. We employ this pretraining idea on a three-year collection of online news articles. We conduct thorough data cleaning and filtering. For example, to maintain a quality assurance bar for using leading sentences as the summary, we compute the ratio of overlapping non-stopping words between the top 3 sentences and the rest of the article. As a higher ratio implies a closer semantic connection, we only keep articles for which this ratio is higher than a threshold. We end up with 21.4M articles based on which we pretrain a transformer-based encoder-decoder summarization model. We conduct thorough evaluation of our models on five benchmark news summarization datasets. Our pretrained model achieves a remarkable performance on various target datasets without any finetuning. This shows the effectiveness of leveraging the lead bias to pretrain on large-scale news data. We further finetune the model on target datasets and achieve better results than a number of strong baseline models. For example, the pretrained model without finetuning obtains state-of-the-art results on DUC-2003 and DUC-2004. The finetuned model obtains 3.2% higher ROUGE-1, 1.6% higher ROUGE-2 and 2.1% higher ROUGE-L scores than the best baseline model on XSum dataset BIBREF2. Human evaluation results also show that our models outperform existing baselines like pointer-generator network. The rest of paper is organized as follows. We introduce related work in news summarization and pretraining in Sec:rw. We describe the details of pretraining using lead bias in Sec:pre. We introduce the transformer-based summarization model in Sec:model. We show the experimental results in Sec:exp and conclude the paper in Sec:conclusion.",Related work ::: Document Summarization,"End-to-end abstractive text summarization has been intensively studied in recent literature. To generate summary tokens, most architectures take the encoder-decoder approach BIBREF8. BIBREF9 first introduces an attention-based seq2seq model to the abstractive sentence summarization task. However, its output summary degenerates as document length increases, and out-of-vocabulary (OOV) words cannot be efficiently handled. To tackle these challenges, BIBREF4 proposes a pointer-generator network that can both produce words from the vocabulary via a generator and copy words from the source article via a pointer. BIBREF10 utilizes reinforcement learning to improve the result. BIBREF11 uses a content selector to over-determine phrases in source documents that helps constrain the model to likely phrases. BIBREF12 adds Gaussian focal bias and a salience-selection network to the transformer encoder-decoder structure BIBREF13 for abstractive summarization. BIBREF14 randomly reshuffles the sentences in news articles to reduce the effect of lead bias in extractive summarization.",Related work ::: Pretraining,"In recent years, pretraining language models have proved to be quite helpful in NLP tasks. The state-of-the-art pretrained models include ELMo BIBREF15, GPT BIBREF7, BERT BIBREF6 and UniLM BIBREF16. Built upon large-scale corpora, these pretrained models learn effective representations for various semantic structures and linguistic relationships. As a result, pretrained models have been widely used with considerable success in applications such as question answering BIBREF17, sentiment analysis BIBREF15 and passage reranking BIBREF18. Furthermore, UniLM BIBREF16 leverages its sequence-to-sequence capability for abstractive summarization; the BERT model has been employed as an encoder in BERTSUM BIBREF19 for extractive/abstractive summarization. Compared to our work, UniLM BIBREF16 is a general language model framework and does not take advantage of the special semantic structure of news articles. Similarly, BERTSUM BIBREF19 directly copies the pretrained BERT structure into its encoder and finetunes on labelled data instead of pretraining with the large quantity of unlabeled news corpus available. Recently, PEGASUS BIBREF20 leverages a similar idea of summarization pretraining, but they require finetuning with data from target domains, whereas our model has a remarkable performance without any finetuning.",Pretraining with Leading Sentences,"News articles usually follow the convention of placing the most important information early in the content, forming an inverted pyramid structure. This lead bias has been discovered in a number of studies BIBREF3, BIBREF14. One of the consequences is that the lead baseline, which simply takes the top few sentences as the summary, can achieve a rather strong performance in news summarization. For instance, in the CNN/Daily Mail dataset BIBREF0, using the top three sentences as summaries can get a higher ROUGE score than many deep learning based models. This positional bias brings lots of difficulty for models to extract salient information from the article and generate high-quality summaries. For instance, BIBREF14 discovers that most models' performances drop significantly when a random sentence is inserted in the leading position, or when the sentences in a news article are shuffled. On the other hand, news summarization, just like many other supervised learning tasks, suffers from the scarcity of labelled training data. Abstractive summarization is especially data-hungry since the efficacy of models depends on high-quality handcrafted summaries. We propose that the lead bias in news articles can be leveraged in our favor to train an abstractive summarization model without human labels. Given a news article, we treat the top three sentences, denoted by Lead-3, as the target summary, and use the rest of the article as news content. The goal of the summarization model is to produce Lead-3 using the following content, as illustrated in fig:top3. The benefit of this approach is that the model can leverage the large number of unlabeled news articles for pretraining. In the experiment, we find that the pretrained model alone can have a strong performance on various news summarization datasets, without any further training. We also finetune the pretrained model on downstream datasets with labelled summaries. The model can quickly adapt to the target domain and further increase its performance. It is worth noting that this idea of utilizing structural bias for large-scale summarization pretraining is not limited to specific types of models, and it can be applied to other types of text as well: academic papers with abstracts, novels with editor's notes, books with tables of contents. However, one should carefully examine and clean the source data to take advantage of lead bias, as the top three sentences may not always form a good summary. We provide more details in the experiments about the data filtering and cleaning mechanism we apply.",Model,"In this section, we introduce our abstractive summarization model, which has a transformer-based encoder-decoder structure. We first formulate the supervised summarization problem and then present the network architecture.",Model ::: Problem formulation,"We formalize the problem of supervised abstractive summarization as follows. The input consists of $a$ pairs of articles and summaries: $\lbrace (X_1, Y_1), (X_2, Y_2), ..., (X_a, Y_a)\rbrace $. Each article and summary are tokenized: $X_i=(x_1,...,x_{L_i})$ and $Y_i=(y_1,...,y_{N_i})$. In abstractive summarization, the summary tokens need not be from the article. For simplicity, we will drop the data index subscript. The goal of the system is to generate summary $Y=(y_1,...,y_m)$ given the transcript $X=\lbrace x_1, ..., x_n\rbrace $.",Model ::: Network Structure,"We utilize a transformer-based encoder-decoder structure that maximizes the conditional probability of the summary: $P(Y|X, \theta )$, where $\theta $ represents the parameters.",Model ::: Network Structure ::: Encoder,"The encoder maps each token into a fixed-length vector using a trainable dictionary $\mathcal {D}$ randomly initialized using a normal distribution with zero mean and a standard deviation of 0.02. Each transformer block conducts multi-head self-attention. And we use sinusoidal positional embedding in order to process arbitrarily long input. In the end, the output of the encoder is a set of contextualized vectors:",Model ::: Network Structure ::: Decoder,"The decoder is a transformer that generates the summary tokens one at a time, based on the input and previously generated summary tokens. Each token is projected onto a vector using the same dictionary $\mathcal {D}$ as the encoder. The decoder transformer block includes an additional cross-attention layer to fuse in information from the encoder. The output of the decoder transformer is denoted as: To predict the next token $w_{k}$, we reuse the weights of dictionary $\mathcal {D}$ as the final linear layer to decode $u^D_{k-1}$ into a probability distribution over the vocabulary: $P(w_k|w_{<k},u^E_{1:m})=( \mathcal {D}u^D_{k-1})$. Training. During training, we seek to minimize the cross-entropy loss: We use teacher-forcing in decoder training, i.e. the decoder takes ground-truth summary tokens as input. The model has 10 layers of 8-headed transformer blocks in both its encoder and decoder, with 154.4M parameters. Inference. During inference, we employ beam search to select the best candidate. The search starts with the special token $\langle \mbox{BEGIN}\rangle $. We ignore any candidate word which results in duplicate trigrams. We select the summary with the highest average log-likelihood per token.",Experiments ::: Datasets,"We evaluate our model on five benchmark summarization datasets: the New York Times Annotated Corpus (NYT) BIBREF1, XSum BIBREF2, the CNN/DailyMail dataset BIBREF0, DUC-2003 and DUC-2004 BIBREF21. These datasets contain 104K, 227K, 312K, 624 and 500 news articles and human-edited summaries respectively, covering different topics and various summarization styles. For NYT dataset, we use the same train/val/test split and filtering methods following BIBREF22. As DUC-2003/2004 datasets are very small, we follow BIBREF23 to employ them as test set only.",Experiments ::: Implementation Details,"We use SentencePiece BIBREF24 for tokenization, which segments any sentence into subwords. We train the SentencePiece model on pretrained data to generate a vocabulary of size 32K and of dimension 720. The vocabulary stays fixed during pretraining and finetuning. Pretraining. We collect three years of online news articles from June 2016 to June 2019. We filter out articles overlapping with the evaluation data on media domain and time range. We then conduct several data cleaning strategies. First, many news articles begin with reporter names, media agencies, dates or other contents irrelevant to the content, e.g. “New York (CNN) –”, “Jones Smith, May 10th, 2018:”. We therefore apply simple regular expressions to remove these prefixes. Second, to ensure that the summary is concise and the article contains enough salient information, we only keep articles with 10-150 words in the top three sentences and 150-1200 words in the rest, and that contain at least 6 sentences in total. In this way, we filter out i) articles with excessively long content to reduce memory consumption; ii) very short leading sentences with little information which are unlikely to be a good summary. To encourage the model to generate abstrative summaries, we also remove articles where any of the top three sentences is exactly repeated in the rest of the article. Third, we try to remove articles whose top three sentences may not form a relevant summary. For this purpose, we utilize a simple metric: overlapping words. We compute the portion of non-stopping words in the top three sentences that are also in the rest of an article. A higher portion implies that the summary is representative and has a higher chance of being inferred by the model using the rest of the article. To verify, we compute the overlapping ratio of non-stopping words between human-edited summary and the article in CNN/DailyMail dataset, which has a median value of 0.87. Therefore, in pretraining, we keep articles with an overlapping word ratio higher than 0.65. These filters rule out around 95% of the raw data and we end up with 21.4M news articles, 12,000 of which are randomly sampled for validation. We pretrain the model for 10 epochs and evaluate its performance on the validation set at the end of each epoch. The model with the highest ROUGE-L score is selected. During pretraining, we use a dropout rate of 0.3 for all inputs to transformer layers. The batch size is 1,920. We use RAdam BIBREF25 as the optimizer, with a learning rate of $10^{-4}$. Also, due to the different numerical scales of the positional embedding and initialized sentence piece embeddings, we divide the positional embedding by 100 before feeding it into the transformer. The beam width is set to 5 during inference. Finetuning. During finetuning, we keep the optimizer, learning rate and dropout rate unchanged as in pretraining. The batch size is 32 for all datasets. We pick the model with the highest ROUGE-L score on the validation set and report its performance on the test set. Our strategy of Pretraining with unlabeled Lead-3 summaries is called PL. We denote the pretrained model with finetuning on target datasets as PL-FT. The model with only pretraining and no finetuning is denoted as PL-NoFT, which is the same model for all datasets.",Experiments ::: Baseline,"To compare with our model, we select a number of strong summarization models as baseline systems. $\textsc {Lead-X}$ uses the top $X$ sentences as a summary BIBREF19. The value of $X$ is 3 for NYT and CNN/DailyMail and 1 for XSum to accommodate the nature of summary length. $\textsc {PTGen}$ BIBREF4 is the pointer-generator network. $\textsc {DRM}$ BIBREF10 leverages deep reinforcement learning for summarization. $\textsc {TConvS2S}$ BIBREF2 is based on convolutional neural networks. $\textsc {BottomUp}$ BIBREF11 uses a bottom-up approach to generate summarization. ABS BIBREF26 uses neural attention for summary generation. DRGD BIBREF27 is based on a deep recurrent generative decoder. To compare with our pretrain-only model, we include several unsupervised abstractive baselines: SEQ$^3$ BIBREF28 employs the reconstruction loss and topic loss for summarization. BottleSum BIBREF23 leverages unsupervised extractive and self-supervised abstractive methods. GPT-2 BIBREF7 is a large-scaled pretrained language model which can be directly used to generate summaries.",Experiments ::: Metrics,"We employ the standard ROUGE-1, ROUGE-2 and ROUGE-L metrics BIBREF29 to evaluate all summarization models. These three metrics respectively evaluate the accuracy on unigrams, bigrams and longest common subsequence. ROUGE metrics have been shown to highly correlate with the human judgment BIBREF29. Following BIBREF22, BIBREF23, we use F-measure ROUGE on XSUM and CNN/DailyMail, and use limited-length recall-measure ROUGE on NYT and DUC. In NYT, the prediction is truncated to the length of the ground-truth summaries; in DUC, the prediction is truncated to 75 characters.",Experiments ::: Results,"The results are displayed in tab:nyt, tab:xsumresults, tab:cnndaily and tab:duc. As shown, on both NYT and XSum dataset, PL-FT outperforms all baseline models by a large margin. For instance, PL-FT obtains 3.2% higher ROUGE-1, 1.6% higher ROUGE-2 and 2.1% higher ROUGE-L scores than the best baseline model on XSum dataset. We conduct statistical test and found that the results are all significant with p-value smaller than 0.05 (marked by *) or 0.01 (marked by **), compared with previous best scores. On CNN/DailyMail dataset, PL-FT outperforms all baseline models except BottomUp BIBREF11. PL-NoFT, the pretrained model without any finetuning, also gets remarkable results. On XSum dataset, PL-NoFT is almost 8% higher than Lead-1 in ROUGE-1 and ROUGE-L. On CNN/DailyMail dataset, PL-NoFT significantly outperforms unsupervised models SEQ$^3$ and GPT-2, and even surpasses the supervised pointer-generator network. PL-NoFT also achieves state-of-the-art results on DUC-2003 and DUC-2004 among unsupervised models (except ROUGE-1 on DUC-2004), outperforming other carefully designed unsupervised summarization models. It's worth noting that PL-NoFT is the same model for all experiments, which proves that our pretrain strategy is effective across different news corpus.",Experiments ::: Abstractiveness Analysis,"We measure the abstractiveness of our model via the ratio of novel n-grams in summaries, i.e. the percentage of n-grams in the summary that are not present in the article. fig:novel shows this ratio in summaries from reference and generated by PL-NoFT and PL-FT in NYT dataset. Both PL-NoFT and PL-FT yield more novel 1-grams in summary than the reference. And PL-NoFT has similar novelty ratio with the reference in other n-gram categories. Also, we observe that the novelty ratio drops after finetuning. We attribute this to the strong lead bias in the NYT dataset which affects models trained on it.",Experiments ::: Human Evaluation,"We conduct human evaluation of the generated summaries from our models and the pointer generator network with coverage. We randomly sample 100 articles from the CNN/DailyMail test set and ask 3 human labelers from Amazon Mechanical Turk to assess the quality of summaries with a score from 1 to 5 (5 means perfect quality. The labelers need to judge whether the summary can express the salient information from the article in a concise form of fluent language. The evaluation guidelines are given in Table TABREF23. To reduce bias, we randomly shuffle summaries from different sources for each article. As shown in Table TABREF23, both of our models PL-NoFT and PL-FT outperform the pointer generator network (PTGen+Cov), and PL-FT's advantage over PTGen+Cov is statistically significant. This shows the effectiveness of both our pretraining and finetuning strategy. To evaluate the inter-annotator agreement, we compute the kappa statistics among the labels and the score is 0.34.",Conclusions,"In this paper, we propose a simple and effective pretraining method for news summarization. By employing the leading sentences from a news article as its target summary, we turn the problematic lead bias for news summarization in our favor. Based on this strategy, we conduct pretraining for abstractive summarization in a large-scale news corpus. We conduct thorough empirical tests on five benchmark news summarization datasets, including both automatic and human evaluations. Results show that the same pretrained model without any finetuning can achieve state-of-the-art results among unsupervised methods over various news summarization datasets. And finetuning on target domains can further improve the model's performance. We argue that this pretraining method can be applied in more scenarios where structural bias exists.",,,,,,,,,What were the baselines?,f95097cf4a0dc036fd8b80c007cd8d7a157b7816,two,familiar,no,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,,"To compare with our model, we select a number of strong summarization models as baseline systems. $\textsc {Lead-X}$ uses the top $X$ sentences as a summary BIBREF19. The value of $X$ is 3 for NYT and CNN/DailyMail and 1 for XSum to accommodate the nature of summary length. $\textsc {PTGen}$ BIBREF4 is the pointer-generator network. $\textsc {DRM}$ BIBREF10 leverages deep reinforcement learning for summarization. $\textsc {TConvS2S}$ BIBREF2 is based on convolutional neural networks. $\textsc {BottomUp}$ BIBREF11 uses a bottom-up approach to generate summarization. ABS BIBREF26 uses neural attention for summary generation. DRGD BIBREF27 is based on a deep recurrent generative decoder. To compare with our pretrain-only model, we include several unsupervised abstractive baselines: SEQ$^3$ BIBREF28 employs the reconstruction loss and topic loss for summarization. BottleSum BIBREF23 leverages unsupervised extractive and self-supervised abstractive methods. GPT-2 BIBREF7 is a large-scaled pretrained language model which can be directly used to generate summaries.","To compare with our model, we select a number of strong summarization models as baseline systems. $\textsc {Lead-X}$ uses the top $X$ sentences as a summary BIBREF19. $\textsc {DRM}$ BIBREF10 leverages deep reinforcement learning for summarization. $\textsc {TConvS2S}$ BIBREF2 is based on convolutional neural networks. $\textsc {BottomUp}$ BIBREF11 uses a bottom-up approach to generate summarization. ABS BIBREF26 uses neural attention for summary generation. DRGD BIBREF27 is based on a deep recurrent generative decoder. To compare with our pretrain-only model, we include several unsupervised abstractive baselines: SEQ$^3$ BIBREF28 employs the reconstruction loss and topic loss for summarization. BottleSum BIBREF23 leverages unsupervised extractive and self-supervised abstractive methods. GPT-2 BIBREF7 is a large-scaled pretrained language model which can be directly used to generate summaries.",5cf25098f80784fb9422b29b386d407e42bfa618,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,What metric was used in the evaluation step?,bb8a0035b767688a98602c33f4714f8ac8ae60db,two,familiar,no,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,,"We employ the standard ROUGE-1, ROUGE-2 and ROUGE-L metrics BIBREF29 to evaluate all summarization models. These three metrics respectively evaluate the accuracy on unigrams, bigrams and longest common subsequence. ROUGE metrics have been shown to highly correlate with the human judgment BIBREF29. Following BIBREF22, BIBREF23, we use F-measure ROUGE on XSUM and CNN/DailyMail, and use limited-length recall-measure ROUGE on NYT and DUC. In NYT, the prediction is truncated to the length of the ground-truth summaries; in DUC, the prediction is truncated to 75 characters.","We employ the standard ROUGE-1, ROUGE-2 and ROUGE-L metrics BIBREF29 to evaluate all summarization models. Following BIBREF22, BIBREF23, we use F-measure ROUGE on XSUM and CNN/DailyMail, and use limited-length recall-measure ROUGE on NYT and DUC.",6dfc90d985b5a87c018d81fa1c6df521764bf1d5,258ee4069f740c400c0049a2580945a1cc7f044c,What did they pretrain the model on?,2a0a44f169ad61774d77df65f8846bd57685bfcf,two,familiar,no,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,,Pretraining. We collect three years of online news articles from June 2016 to June 2019. We filter out articles overlapping with the evaluation data on media domain and time range. We then conduct several data cleaning strategies.,Pretraining. We collect three years of online news articles from June 2016 to June 2019. We filter out articles overlapping with the evaluation data on media domain and time range. We then conduct several data cleaning strategies.,841c0a5e3364a4885e4fcefefdcd8156c87875dd,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,What does the data cleaning and filtering process consist of?,fd6c194632230e392088fc1f574c8626c6a2fa96,two,familiar,no,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,,"First, many news articles begin with reporter names, media agencies, dates or other contents irrelevant to the content, e.g. “New York (CNN) –”, “Jones Smith, May 10th, 2018:”. We therefore apply simple regular expressions to remove these prefixes. Second, to ensure that the summary is concise and the article contains enough salient information, we only keep articles with 10-150 words in the top three sentences and 150-1200 words in the rest, and that contain at least 6 sentences in total. In this way, we filter out i) articles with excessively long content to reduce memory consumption; ii) very short leading sentences with little information which are unlikely to be a good summary. To encourage the model to generate abstrative summaries, we also remove articles where any of the top three sentences is exactly repeated in the rest of the article. Third, we try to remove articles whose top three sentences may not form a relevant summary. For this purpose, we utilize a simple metric: overlapping words. We compute the portion of non-stopping words in the top three sentences that are also in the rest of an article. A higher portion implies that the summary is representative and has a higher chance of being inferred by the model using the rest of the article. To verify, we compute the overlapping ratio of non-stopping words between human-edited summary and the article in CNN/DailyMail dataset, which has a median value of 0.87. Therefore, in pretraining, we keep articles with an overlapping word ratio higher than 0.65.","First, many news articles begin with reporter names, media agencies, dates or other contents irrelevant to the content, e.g. “New York (CNN) –”, “Jones Smith, May 10th, 2018:”. We therefore apply simple regular expressions to remove these prefixes.

Second, to ensure that the summary is concise and the article contains enough salient information, we only keep articles with 10-150 words in the top three sentences and 150-1200 words in the rest, and that contain at least 6 sentences in total. In this way, we filter out i) articles with excessively long content to reduce memory consumption; ii) very short leading sentences with little information which are unlikely to be a good summary. To encourage the model to generate abstrative summaries, we also remove articles where any of the top three sentences is exactly repeated in the rest of the article.

Third, we try to remove articles whose top three sentences may not form a relevant summary.",9ab1bf3c7d441cc986c18b532537008fb403e9b8,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,What unlabeled corpus did they use?,487dc65bf8a8ecbf052cf05641caf1b90a502853,two,familiar,no,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,three years of online news articles from June 2016 to June 2019,,,Pretraining. We collect three years of online news articles from June 2016 to June 2019. We filter out articles overlapping with the evaluation data on media domain and time range. We then conduct several data cleaning strategies.,We collect three years of online news articles from June 2016 to June 2019.,74ea5e790675be5b1257ef1986b23525067d7583,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,4-Figure1-1.png,Figure 1: Using Lead-3 summary as target in pretraining.,4-Figure2-1.png,"Figure 2: Ratio of novel n-grams in summaries from reference, PL-NoFT and PL-FT models in NYT test set.",5-Table1-1.png,Table 1: ROUGE recall scores on NYT test set.,5-Table2-1.png,Table 2: ROUGE F1 results on XSum test set.,5-Table3-1.png,Table 3: ROUGE F1 results on CNN/DailyMail test set.,6-Table4-1.png,Table 4: Scoring criteria for human evaluation of summaries.,,,,,,,,,"ROUGE-1, ROUGE-2 and ROUGE-L F-measure ROUGE on XSUM and CNN/DailyMail, and use limited-length recall-measure ROUGE on NYT and DUC",hree years of online news articles from June 2016 to June 2019,6-Table5-1.png,Table 5: Average and standard deviations of human evaluation scores for summaries on CNN/DailyMail test set. Scores range from 1 to 5 with 5 being perfect. Each summary is judged by 3 human evaluators. PL-FT’s result is statistically significant compared with pointer-generator network with coverage with a p-value less than 10−7.,,,,,,,,,,$\textsc {Lead-X}$ $\textsc {PTGen}$ $\textsc {DRM}$ $\textsc {TConvS2S}$  $\textsc {BottomUp}$ ABS DRGD SEQ$^3$ BottleSum GPT-2,,,,,,,,,"many news articles begin with reporter names, media agencies, dates or other contents irrelevant to the content to ensure that the summary is concise and the article contains enough salient information, we only keep articles with 10-150 words in the top three sentences and 150-1200 words in the rest, and that contain at least 6 sentences in total we try to remove articles whose top three sentences may not form a relevant summary",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
An Incremental Parser for Abstract Meaning Representation,"Meaning Representation (AMR) is a semantic representation for natural language that embeds annotations related to traditional tasks such as named entity recognition, semantic role labeling, word sense disambiguation and co-reference resolution. We describe a transition-based parser for AMR that parses sentences left-to-right, in linear time. We further propose a test-suite that assesses specific subtasks that are helpful in comparing AMR parsers, and show that our parser is competitive with the state of the art on the LDC2015E86 dataset and that it outperforms state-of-the-art parsers for recovering named entities and handling polarity.",Introduction,"Semantic parsing aims to solve the problem of canonicalizing language and representing its meaning: given an input sentence, it aims to extract a semantic representation of that sentence. Abstract meaning representation BIBREF0 , or AMR for short, allows us to do that with the inclusion of most of the shallow-semantic natural language processing (NLP) tasks that are usually addressed separately, such as named entity recognition, semantic role labeling and co-reference resolution. AMR is partially motivated by the need to provide the NLP community with a single dataset that includes basic disambiguation information, instead of having to rely on different datasets for each disambiguation problem. The annotation process is straightforward, enabling the development of large datasets. Alternative semantic representations have been developed and studied, such as CCG BIBREF1 , BIBREF2 and UCCA BIBREF3 . Several parsers for AMR have been recently developed BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 , BIBREF12 , BIBREF13 . This line of research is new and current results suggest a large room for improvement. Greedy transition-based methods BIBREF14 are one of the most popular choices for dependency parsing, because of their good balance between efficiency and accuracy. These methods seem promising also for AMR, due to the similarity between dependency trees and AMR structures, i.e., both representations use graphs with nodes that have lexical content and edges that represent linguistic relations. A transition system is an abstract machine characterized by a set of configurations and transitions between them. The basic components of a configuration are a stack of partially processed words and a buffer of unseen input words. Starting from an initial configuration, the system applies transitions until a terminal configuration is reached. The sentence is scanned left to right, with linear time complexity for dependency parsing. This is made possible by the use of a greedy classifier that chooses the transition to be applied at each step. In this paper we introduce a parser for AMR that is inspired by the ArcEager dependency transition system of nivre2004. The main difference between our system and ArcEager is that we need to account for the mapping from word tokens to AMR nodes, non-projectivity of AMR structures and reentrant nodes (multiple incoming edges). Our AMR parser brings closer dependency parsing and AMR parsing by showing that dependency parsing algorithms, with some modifications, can be used for AMR. Key properties such as working left-to-right, incrementality and linear complexity further strengthen its relevance. The AMR parser of wang2boosting, called CAMR, also defines a transition system. It differs from ours because we process the sentence left-to-right while they first acquire the entire dependency tree and then process it bottom-up. More recently emnlp2016 presented a non-greedy transition system for AMR parsing, based on ArcStandard BIBREF15 . Our transition system is also related to an adaptation of ArcEager for directed acyclic graphs (DAGs), introduced by sagae2008shift. This is also the basis for ribeyre2015because, a transition system used to parse dependency graphs. Similarly, du2014peking also address dependency graph parsing by means of transition systems. Analogously to dependency trees, dependency graphs have the property that their nodes consist of the word tokens, which is not true for AMR. As such, these transition systems are more closely related to traditional transition systems for dependency parsing. Our contributions in this paper are as follows:",Transition-Based AMR Parsing,"Similarly to dependency parsing, AMR parsing is partially based on the identification of predicate-argument structures. Much of the dependency parsing literature focuses on transition-based dependency parsing—an approach to parsing that scans the sentence from left to right in linear time and updates an intermediate structure that eventually ends up being a dependency tree. The two most common transition systems for greedy dependency parsing are ArcStandard and ArcEager. With ArcStandard, a stack is maintained along with a buffer on which the left-to-right scan is performed. At each step, the parser chooses to scan a word in the buffer and shift it onto the stack, or else to create an arc between the two top-most elements in the stack and pop the dependent. ArcStandard parses a sentence in a pure bottom-up, left-to-right fashion (similarly to shift-reduce context-free grammar parsers), and must delay the construction of right arcs until all the dependent node has been completed. This imposes strong limitations on the degree of incrementality of the parser. The ArcEager system was designed to improve on ArcStandard by mixing bottom up and top-down strategies. More precisely, in the ArcEager parser left arcs are constructed bottom-up and right arcs are constructed top-down, so that right dependents can be attached to their heads even if some of their own dependents are not identified yet. In this way arcs are constructed as soon as the head and the dependent are available in the stack. Because of the similarity of AMR structures to dependency structures, transition systems are also helpful for AMR parsing. Starting from the ArcEager system, we develop here a novel transition system, called AmrEager that parses sentences into AMR structures. There are three key differences between AMRs and dependency trees that require further adjustments for dependency parsers to be used with AMRs. A key difference between English dependency trees and AMR structures is projectivity. Dependency trees in English are usually projective, roughly meaning that there are no crossing arcs if the edges are drawn in the semi-plane above the words. While this restriction is empirically motivated in syntactic theories for English, it is no longer motivated for AMR structures. The notion of projectivity can be generalized to AMR graphs as follows. The intuition is that we can use the alignment INLINEFORM0 to map AMR edges back to the sentence INLINEFORM1 , and test whether there exist pairs of crossing edges. Figure FIGREF13 shows this mapping for the AMR of Figure FIGREF7 , where the edge connecting excuse to I crosses another edge. More formally, consider an AMR edge INLINEFORM2 . Let INLINEFORM3 and INLINEFORM4 , so that INLINEFORM5 is aligned with INLINEFORM6 and INLINEFORM7 is aligned with INLINEFORM8 . The spanning set for INLINEFORM9 , written INLINEFORM10 , is the set of all nodes INLINEFORM11 such that INLINEFORM12 and INLINEFORM13 if INLINEFORM14 or INLINEFORM15 if INLINEFORM16 . We say that INLINEFORM17 is projective if, for every node INLINEFORM18 , all of its parent and child nodes are in INLINEFORM19 ; otherwise, we say that INLINEFORM20 is non-projective. An AMR is projective if all of its edges are projective, and is non-projective otherwise. This corresponds to the intuitive definition of projectivity for DAGs introduced in sagae2008shift and is closely related to the definition of non-crossing graphs of kuhlmann2015parsing. Table TABREF15 demonstrates that a relatively small percentage of all AMR edges are non-projective. Yet, 35% of the sentences contain at least one non-projective edge. https://github.com/jflanigan/jamr/blob/master/docs/Hand_Alignments.md AMRs are graphs rather than trees because they can have nodes with multiple parents, called reentrant nodes, as in the node you for the AMR of Figure FIGREF7 . There are two phenomena that cause reentrancies in AMR: control, where a reentrant edge appears between siblings of a control verb, and co-reference, where multiple mentions correspond to the same concept. In contrast, dependency trees do not have nodes with multiple parents. Therefore, when creating a new arc, transition systems for dependency parsing check that the dependent does not already have a head node, preventing the node from having additional parents. To handle reentrancy, which is not uncommon in AMR structures as shown in Table TABREF15 , we drop this constraint. Another main difference with dependency parsing is that in AMR there is no straightforward mapping between a word in the sentence and a node in the graph: words may generate no nodes, one node or multiple nodes. In addition, the labels at the nodes are often not easily determined by the word in the sentence. For instance expectation translates to expect-01 and teacher translates to the two nodes teach-01 and person, connected through an :ARG0 edge, expressing that a teacher is a person who teaches. A mechanism of concept identification is therefore required to map each token INLINEFORM0 to a subgraph with the correct labels at its nodes and edges: if INLINEFORM1 is the gold alignment, this should be the subgraph INLINEFORM2 defined in Equation ( EQREF11 ). To obtain alignments between the tokens in the sentence and the nodes in the AMR graph of our training data, we run the JAMR aligner.",Transition system for AMR Parsing,"A stack INLINEFORM0 is a list of nodes of the partially constructed AMR graph, with the top element INLINEFORM1 at the right. We use the symbol ` INLINEFORM2 ' as the concatenation operator. A buffer INLINEFORM3 is a list of indices from INLINEFORM4 , with the first element INLINEFORM5 at the left, representing the word tokens from the input still to be processed. A configuration of our parser is a triple INLINEFORM6 , where INLINEFORM7 is the set of AMR edges that have been constructed up to this point. In order to introduce the transition actions of our parser we need some additional notation. We use a function INLINEFORM0 that maps indices from INLINEFORM1 to AMR graph fragments. For each INLINEFORM2 , INLINEFORM3 is a graph INLINEFORM4 , with single root INLINEFORM5 , representing the semantic contribution of word INLINEFORM6 to the AMR for INLINEFORM7 . As already mentioned, INLINEFORM8 can have a single node representing the concept associated with INLINEFORM9 , or it can have several nodes in case INLINEFORM10 denotes a complex concept, or it can be empty. The transition Shift is used to decide if and what to push on the stack after consuming a token from the buffer. Intuitively, the graph fragment INLINEFORM0 obtained from the token INLINEFORM1 , if not empty, is “merged” with the graph we have constructed so far. We then push onto the stack the node INLINEFORM2 for further processing. LArc INLINEFORM3 creates an edge with label INLINEFORM4 between the top-most node and the second top-most node in the stack, and pops the latter. RArc INLINEFORM5 is the symmetric operation, but does not pop any node from the stack. Finally, Reduce pops the top-most node from the stack, and it also recovers reentrant edges between its sibling nodes, capturing for instance several control verb patterns. To accomplish this, Reduce decides whether to create an additional edge between the node being removed and the previously created sibling in the partial graph. This way of handling control verbs is similar to the REENTRANCE transition of wang2boosting. The choice of popping the dependent in the LArc transition is inspired by ArcEager, where left-arcs are constructed bottom-up to increase the incrementality of the transition system BIBREF15 . This affects our ability to recover some reentrant edges: consider a node INLINEFORM0 with two parents INLINEFORM1 and INLINEFORM2 , where the arc INLINEFORM3 is a left-arc and INLINEFORM4 is any arc. If the first arc to be processed is INLINEFORM5 , we use LArc that pops INLINEFORM6 , hence making it impossible to create the second arc INLINEFORM7 . Nevertheless, we discovered that this approach works better than a completely unrestricted allowance of reentrancy. The reason is that if we do not remove dependents at all when first attached to a node, the stack becomes larger, and nodes which should be connected end up being distant from each other, and as such, are never connected. The initial configuration of the system has a INLINEFORM0 node (representing the root) in the stack and the entire sentence in the buffer. The terminal configuration consists of an empty buffer and a stack with only the INLINEFORM1 node. The transitions required to parse the sentence The boy and the girl are shown in Table TABREF20 , where the first line shows the initial configuration and the last line shows the terminal configuration. Similarly to the transitions of the ArcEager, the above transitions construct edges as soon as the head and the dependent are available in the stack, with the aim of maximizing the parser incrementality. We now show that our greedy transition-based AMR parser is linear-time in INLINEFORM0 , the length of the input sentence INLINEFORM1 . We first claim that the output graph has size INLINEFORM2 . Each token in INLINEFORM3 is mapped to a constant number of nodes in the graph by Shift. Thus the number of nodes is INLINEFORM4 . Furthermore, each node can have at most three parent nodes, created by transitions RArc, LArc and Reduce, respectively. Thus the number of edges is also INLINEFORM5 . It is possible to bound the maximum number of transitions required to parse INLINEFORM6 : the number of Shift is bounded by INLINEFORM7 , and the number of Reduce, LArc and RArc is bounded by the size of the graph, which is INLINEFORM8 . Since each transition can be carried out in constant time, we conclude that our parser runs in linear time.",Training the System,"Several components have to be learned: (1) a transition classifier that predicts the next transition given the current configuration, (2) a binary classifier that decides whether or not to create a reentrancy after a Reduce, (3) a concept identification step for each Shift to compute INLINEFORM0 , and 3) another classifier to label edges after each LArc or RArc.",Oracle,"Training our system from data requires an oracle—an algorithm that given a gold-standard AMR graph and a sentence returns transition sequences that maximize the overlap between the gold-standard graph and the graph dictated by the sequence of transitions. We adopt a shortest stack, static oracle similar to manningfast. Informally, static means that if the actual configuration of the parser has no mistakes, the oracle provides a transition that does not introduce any mistake. Shortest stack means that the oracle prefers transitions where the number of items in the stack is minimized. Given the current configuration INLINEFORM0 and the gold-standard graph INLINEFORM1 , the oracle is defined as follows, where we test the conditions in the given order and apply the action associated with the first match: if INLINEFORM0 then LArc( INLINEFORM1 ); if INLINEFORM0 then RArc( INLINEFORM1 ); if INLINEFORM0 then Reduce; Shift otherwise. The oracle first checks whether some gold-standard edge can be constructed from the two elements at the top of the stack (conditions 1 and 2). If LArc or RArc are not possible, the oracle checks whether all possible edges in the gold graph involving INLINEFORM0 have already been processed, in which case it chooses Reduce (conditions 3). To this end, it suffices to check the buffer, since LArc and RArc have already been excluded and elements in the stack deeper than position two can no longer be accessed by the parser. If Reduce is not possible, Shift is chosen. Besides deciding on the next transition, the oracle also needs the alignments, which we generate with JAMR, in order to know how to map the next token in the sentence to its AMR subgraph INLINEFORM0 defined in ( EQREF11 ).",Transition Classifier,"Like all other transition systems of this kind, our transition system has a “controller” that predicts a transition given the current configuration (among Shift, LArc, RArc and Reduce). The examples from which we learn this controller are based on features extracted from the oracle transition sequences, where the oracle is applied on the training data. As a classifier, we use a feed-forward neural network with two hidden layers of 200 tanh units and learning rate set to 0.1, with linear decaying. The input to the network consists of the concatenation of embeddings for words, POS tags and Stanford parser dependencies, one-hot vectors for named entities and additional sparse features, extracted from the current configuration of the transition system; this is reported in more details in Table TABREF27 . The embeddings for words and POS tags were pre-trained on a large unannotated corpus consisting of the first 1 billion characters from Wikipedia. For lexical information, we also extract the leftmost (in the order of the aligned words) child (c), leftmost parent (p) and leftmost grandchild (cc). Leftmost and rightmost items are common features for transition-based parsers BIBREF17 , BIBREF18 but we found only leftmost to be helpful in our case. All POS tags, dependencies and named entities are generated using Stanford CoreNLP BIBREF19 . The accuracy of this classifier on the development set is 84%. Similarly, we train a binary classifier for deciding whether or not to create a reentrant edge after a Reduce: in this case we use word and POS embeddings for the two nodes being connected and their parent as well as dependency label embeddings for the arcs between them.",Concept Identification,"This routine is called every time the transition classifier decides to do a Shift; it is denoted by INLINEFORM0 in § SECREF3 . This component could be learned in a supervised manner, but we were not able to improve on a simple heuristic, which works as follows: during training, for each Shift decided by the oracle, we store the pair INLINEFORM1 in a phrase-table. During parsing, the most frequent graph INLINEFORM2 for the given token is then chosen. In other words, INLINEFORM3 approximates INLINEFORM4 by means of the graph most frequently seen among all occurrences of token INLINEFORM5 in the training set. An obvious problem with the phrase-table approach is that it does not generalize to unseen words. In addition, our heuristic relies on the fact that the mappings observed in the data are correct, which is not the case when the JAMR-generated alignments contain a mistake. In order to alleviate this problem we observe that there are classes of words such as named entities and numeric quantities that can be disambiguated in a deterministic manner. We therefore implement a set of “hooks” that are triggered by the named entity tag of the next token in the sentence. These hooks override the normal Shift mechanism and apply a fixed rule instead. For instance, when we see the token New York (the two tokens are collapsed in a single one at preprocessing) we generate the subgraph of Figure FIGREF30 and push its root onto the stack. Similar subgraphs are generated for all states, cities, countries and people. We also use hooks for ordinal numbers, percentages, money and dates.",Edge Labeling,"Edge labeling determines the labels for the edges being created. Every time the transition classifier decides to take an LArc or RArc operation, the edge labeler needs to decide on a label for it. There are more than 100 possible labels such as :ARG0, :ARG0-of, :ARG1, :location, :time and :polarity. We use a feed-forward neural network similar to the one we trained for the transition classier, with features shown in Table TABREF32 . The accuracy of this classifier on the development set is 77%. We constrain the labels predicted by the neural network in order to satisfy requirements of AMR. For instance, the label :top can only be applied when the node from which the edge starts is the special INLINEFORM0 node. Other constraints are used for the :polarity label and for edges attaching to numeric quantities. Sometimes the label predicted by the neural network is not a label that satisfies the requirements of AMR. For instance, the label :top can only be applied when the node from which the edge starts is the special INLINEFORM0 node. In order to avoid generating such erroneous labels, we use a set of rules, shown in Table TABREF34 . These rules determine which labels are allowed for the newly created edge so that we only consider those during prediction. Also ARG roles cannot always be applied: each Propbank frame allows a limited number of arguments. For example, while add-01 and add-02 allow for :ARG1 and :ARG2 (and their inverse :ARG1-of and :ARG2-of), add-03 and add-04 only allow :ARG2 (and :ARG2-of).",Fine-grained Evaluation,"Until now, AMR parsers were evaluated using the Smatch score. Given the candidate graphs and the gold graphs in the form of AMR annotations, Smatch first tries to find the best alignments between the variable names for each pair of graphs and it then computes precision, recall and F1 of the concepts and relations. We note that the Smatch score has two flaws: (1) while AMR parsing involves a large number of subtasks, the Smatch score consists of a single number that does not assess the quality of each subtasks separately; (2) the Smatch score weighs different types of errors in a way which is not necessarily useful for solving a specific NLP problem. For example, for a specific problem concept detection might be deemed more important than edge detection, or guessing the wrong sense for a concept might be considered less severe than guessing the wrong verb altogether. Consider the two parses for the sentence Silvio Berlusconi gave Lucio Stanca his current role of modernizing Italy's bureaucracy in Figure FIGREF36 . At the top, we show the output of a parser (Parse 1) that is not able to deal with named entities. At the bottom, we show the output of a parser (Parse 2) which, except for :name, :op and :wiki, always uses the edge label :ARG0. The Smatch scores for the two parses are 56 and 78 respectively. Both parses make obvious mistakes but the three named entity errors in Parse 1 are considered more important than the six wrong labels in Parse 2. However, without further analysis, it is not advisable to conclude that Parse 2 is better than Parse 1. In order to better understand the limitations of the different parsers, find their strengths and gain insight in which downstream tasks they may be helpful, we compute a set of metrics on the test set. Unlabeled is the Smatch score computed on the predicted graphs after removing all edge labels. In this way, we only assess the node labels and the graph topology, which may be enough to benefit several NLP tasks because it identifies basic predicate-argument structure. For instance, we may be interested in knowing whether two events or entities are related to each other, while not being concerned with the precise type of relation holding between them. No WSD gives a score that does not take into account word sense disambiguation errors. By ignoring the sense specified by the Propbank frame used (e.g., duck-01 vs duck-02) we have a score that does not take into account this additional complexity in the parsing procedure. To compute this score, we simply strip off the suffixes from all Propbank frames and calculate the Smatch score. Following sawai, we also evaluate the parsers using the Smatch score on noun phrases only (NP-only), by extracting from the AMR dataset all noun phrases that do not include further NPs. As we previously discussed, reentrancy is a very important characteristic of AMR graphs and it is not trivial to handle. We therefore implement a test for it (Reentrancy), where we compute the Smatch score only on reentrant edges. Concept identification is another critical component of the parsing process and we therefore compute the F-score on the list of predicted concepts (Concepts) too. Identifying the correct concepts is fundamental: if a concept is not identified, it will not be possible to retrieve any edge involving that concept, with likely significant consequences on accuracy. This metric is therefore quite important to score highly on. Similarly to our score for concepts, we further compute an F-score on the named entities (Named Ent.) and wiki roles for named entities (Wikification) that consider edges labeled with :name and :wiki respectively. These two metrics are strictly related to the concept score. However, since named entity recognition is the focus of dedicated research, we believe it is important to define a metric that specifically assesses this problem. Negation detection is another task which has received some attention. An F-score for this (Negations) is also defined, where we find all negated concepts by looking for the :polarity role. The reason we can compute a simple F-score instead of using Smatch for these metrics is that there are no variable names involved. Finally we compute the Smatch score on :ARG edges only, in order to have a score for semantic role labeling (SRL), which is another extremely important subtask of AMR, as it is based on the identification of predicate-argument structures. Using this evaluation suite we can evaluate AMRs on a wide range of metrics that can help us find strengths and weakness of each parser, hence speeding up the research in this area. Table TABREF37 reports the scores for the two parses of Figure FIGREF36 , where we see that Parse 1 gets a high score for semantic role labeling while Parse 2 is optimal for named entity recognition. Moreover, we can make additional observations such as that Parse 2 is optimal with respect to unlabeled score and that Parse 1 recovers more reentrancies.",Experiments,"We compare our parser against two available parsers: JAMR BIBREF4 and CAMR BIBREF20 , BIBREF5 , using the LDC2015E86 dataset for evaluation. Both parsers are available online and were recently updated for SemEval-2016 Task 8 BIBREF21 , BIBREF22 . However, CAMR's SemEval system, which reports a Smatch score of 67, is not publicly available. CAMR has a quadratic worst-case complexity (although linear in practice). In JAMR, the concept identification step is quadratic and the relation identification step is INLINEFORM0 , with INLINEFORM1 being the set of nodes in the AMR graph. Table TABREF40 shows the results obtained by the parsers on all metrics previously introduced. On Smatch, our system does not give state-of-the-art results. However, we do obtain the best results for Unlabeled and Concept and outperform the other parses for Named Ent. and Negations. Our score of Reentrancy is also close the best scoring system, which is particularly relevant given the importance of reentrancies in AMR. The use of the Reduce transition, which targets reentrancies caused by control verbs, is critical in order to achieve this result. The relatively high results we obtain for the unlabeled case suggests that our parser has difficulty in labeling the arcs. Our score for concept identification, which is on par with the best result from the other parsers, demonstrates that there is a relatively low level of token ambiguity. State-of-the-art results for this problem can be obtained by choosing the most frequent subgraph for a given token based on a phrase-table constructed from JAMR alignments on the training data. The scores for named entities and wikification are heavily dependent on the hooks mentioned in § SECREF29 , which in turn relies on the named entity recognizer to make the correct predictions. In order to alleviate the problem of wrong automatic alignments with respect to polarity and better detect negation, we performed a post-processing step on the aligner output where we align the AMR constant - (minus) with words bearing negative polarity such as not, illegitimate and asymmetry. Our experiments demonstrate that there is no parser for AMR yet that conclusively does better than all other parsers on all metrics. Advantages of our parser are the worst-case linear complexity and the fact that is possible to perform incremental AMR parsing, which is both helpful for real-time applications and to investigate how meaning of English sentences can be built incrementally left-to-right.",Related Work,"The first data-driven AMR parser is due to carbonell2014discriminative. The problem is addressed in two separate stages: concept identification and relation identification. They use a sequence labeling algorithm to identify concepts and frame the relation prediction task as a constrained combinatorial optimization problem. werling2015robust notice that the difficult bit is the concept identification and propose a better way to handle that task: an action classifier to generate concepts by applying predetermined actions. Other proposals involve a synchronous hyperedge replacement grammar solution BIBREF6 , a syntax-based machine translation approach BIBREF7 where a grammar of string-to-tree rules is created after reducing AMR graphs to trees by removing all reentrancies, a CCG system that first parses sentences into lambda-calculus representations BIBREF11 . A systematic translation from AMR to first order logic formulas, with a special treatment for quantification, reentrancy and negation, is discussed in bos2016expressive. In microsoft, a pre-existing logical form parser is used and the output is then converted into AMR graphs. Yet another solution is proposed by searnamr who discuss a parser that uses SEARN BIBREF23 , a “learning to search” algorithm. Transition-based algorithms for AMR parsing are compelling because traditional graph-based techniques are computationally expensive. wang and wang2boosting propose a framework that parses a sentence into its AMR structure through a two-stage process: a dependency tree is generated from the input sentence through a transition-based parser and then another transition-based parser is used to generate the AMR. The main benefit of this approach is that the dependency parser can be trained on a training set much larger than the training set for the tree-to-graph algorithm. Others further built on this parser: goodman2016noise use imitation learning to alleviate the probem of error propagation in the greedy parser, while barzdins2016riga create a wrapper around it to fix frequent mistakes and investigate ensembles with a character level neural parser. More recently emnlp2016 presented a non-greedy transition system for AMR parsing, based on ArcStandard BIBREF15 . AMR parsing as a whole is a complex task because it involves many subtasks including named entity recognition, co-reference resolution and semantic role labeling. sawai do not attempt at parsing AMR graphs for entire sentences but they instead handle simple noun phrases (NPs). They extract NPs from the AMR dataset only when they do not include further NPs, do not include pronouns nor named entities. Due to these restrictions, the AMRs are mostly trees and easier to handle than the original AMR graphs. They approach this task using a transition based system inspired by ArcStandard. AMR is not the only way to represent meaning in natural language sentences. Alternative semantic representations have been developed and studied, such as Boxer BIBREF24 , CCG BIBREF1 , BIBREF2 and UCCA BIBREF3 .",Conclusion,"We presented a transition system that builds AMR graphs in linear time by processing the sentences left-to-right, trained with feed-forward neural networks. The parser demonstrates that it is possible to perform AMR parsing using techniques inspired by dependency parsing. We also noted that it is less informative to evaluate the entire parsing process with Smatch than to use a collection of metrics aimed at evaluating the various subproblems in the parsing process. We further showed that our left-to-right transition system is competitive with publicly available state-of-the-art parsers. Although we do not outperform the best baseline in terms of Smatch score, we show on par or better results for several of the metrics proposed. We hope that moving away from a single-metric evaluation will further speed up progress in AMR parsing.",Acknowledgments,"The authors would like to thank the three anonymous reviewers and Sameer Bansal, Jeff Flanigan, Sorcha Gilroy, Adam Lopez, Nikos Papasarantopoulos, Nathan Schneider, Mark Steedman, Sam Thomson, Clara Vania and Chuan Wang for their help and comments. This research was supported by a grant from Bloomberg and by the H2020 project SUMMA, under grant agreement 688139.",,,,,,,,,,,,,,,,,Do they use pretrained models as part of their parser?,d51069595f67a3a53c044c8a37bae23facbfa45d,infinity,unfamiliar,no,,2cfd959e433f290bb50b55722370f0d22fe090b7,False,True,,"As a classifier, we use a feed-forward neural network with two hidden layers of 200 tanh units and learning rate set to 0.1, with linear decaying. The input to the network consists of the concatenation of embeddings for words, POS tags and Stanford parser dependencies, one-hot vectors for named entities and additional sparse features, extracted from the current configuration of the transition system; this is reported in more details in Table TABREF27 . The embeddings for words and POS tags were pre-trained on a large unannotated corpus consisting of the first 1 billion characters from Wikipedia. For lexical information, we also extract the leftmost (in the order of the aligned words) child (c), leftmost parent (p) and leftmost grandchild (cc). Leftmost and rightmost items are common features for transition-based parsers BIBREF17 , BIBREF18 but we found only leftmost to be helpful in our case. All POS tags, dependencies and named entities are generated using Stanford CoreNLP BIBREF19 . The accuracy of this classifier on the development set is 84%.",The embeddings for words and POS tags were pre-trained on a large unannotated corpus consisting of the first 1 billion characters from Wikipedia.,1ea28705c7f9843bbea98548ebc1126b5bfefc59,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,Which subtasks do they evaluate on?,1a6e2bd41ee43df83fef2a1c1941e6f95a619ae8,infinity,unfamiliar,no,,2cfd959e433f290bb50b55722370f0d22fe090b7,False,,,"Semantic parsing aims to solve the problem of canonicalizing language and representing its meaning: given an input sentence, it aims to extract a semantic representation of that sentence. Abstract meaning representation BIBREF0 , or AMR for short, allows us to do that with the inclusion of most of the shallow-semantic natural language processing (NLP) tasks that are usually addressed separately, such as named entity recognition, semantic role labeling and co-reference resolution. AMR is partially motivated by the need to provide the NLP community with a single dataset that includes basic disambiguation information, instead of having to rely on different datasets for each disambiguation problem. The annotation process is straightforward, enabling the development of large datasets. Alternative semantic representations have been developed and studied, such as CCG BIBREF1 , BIBREF2 and UCCA BIBREF3 .","Abstract meaning representation BIBREF0 , or AMR for short, allows us to do that with the inclusion of most of the shallow-semantic natural language processing (NLP) tasks that are usually addressed separately, such as named entity recognition, semantic role labeling and co-reference resolution.",cb78e58dc12a3f391106626b7b07905c51aa0c7b,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2-Figure1-1.png,"Figure 1: Annotation for the sentence “I beg that you will excuse me.” In this AMR graph, variables are denoted in boldface and concepts and edge labels are denoted in italics.",2-Figure2-1.png,Figure 2: AMR graph for the sentence “I beg that you will excuse me.”,4-Table1-1.png,Table 1: Statistics for non-projectivity and re-entrancies in 200 AMR manually aligned with the associated sentences.2,6-Table2-1.png,Table 2: Parsing steps for the sentence “The boy and the girl.”,7-Table4-1.png,Table 4: Confusion matrix for the neural network transition classifier on the development set.,7-Table3-1.png,"Table 3: Features used in transition classifier. Stack and buffer elements are denoted by σi and βi, respectively, for i ∈ {0, 3}. The function d maps a stack element to the depth of the associated graph fragment. The functions #c and #p count the number of children and parents, respectively, of a stack element. The function w maps a stack/buffer element to the word embedding for the associated word in the sentence. The function p gives the leftmost (according to the alignment) parent of a stack element, the function c the leftmost child and the function cc the leftmost grandchild. The function s maps a stack/buffer element to the part-of-speech embedding for the associated word. The function e maps a stack/buffer element to its entity. Finally, the function ℓ maps a pair of symbols to the dependency label embedding, according to the edge (or lack of) in the dependency tree for the two words these symbols are mapped to.",,,,,,,,," entity recognition, semantic role labeling and co-reference resolution",,8-Figure3-1.png,"Figure 3: Subgraph for “New York”. Similar subgraphs are generated for all states, city, countries and people.",8-Table5-1.png,Table 5: Features used in edge labeling. See Table 3 for a legend of symbols.,9-Table6-1.png,"Table 6: Labeling rules: For each edge label, we provide regular expressions that must hold on the labels at the start node (start) and the end node (end) of the edge. Ex. indicates when the rule is exclusive, d-ent is the AMR concept date-entity, inter. is the AMR constant interrogative, expr. is the AMR constant expressive, imp. is the AMR constant imperative.",9-Table7-1.png,"Table 7: Domain datasets and their sizes, taken from LDC2015E86. # sent. denotes the number of sentences, # tokens denotes the number of tokens and # nodes denotes the total number of nodes in all graphs.",10-Table9-1.png,"Table 9: Results on test split of LDC2015E86 for JAMR, CAMR and our AMREAGER. Best systems are in bold.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,11-Table8-1.png,"Table 8: Development set Smatch scores for JAMR (J), CAMR (C) and our parser (E) on different domains. Best systems are in bold.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A Deep Neural Architecture for Sentence-level Sentiment Classification in Twitter Social Networking,"This paper introduces a novel deep learning framework including a lexicon-based approach for sentence-level prediction of sentiment label distribution. We propose to first apply semantic rules and then use a Deep Convolutional Neural Network (DeepCNN) for character-level embeddings in order to increase information for word-level embedding. After that, a Bidirectional Long Short-Term Memory Network (Bi-LSTM) produces a sentence-wide feature representation from the word-level embedding. We evaluate our approach on three Twitter sentiment classification datasets. Experimental results show that our model can improve the classification accuracy of sentence-level sentiment analysis in Twitter social networking.",Introduction,"Twitter sentiment classification have intensively researched in recent years BIBREF0 BIBREF1 . Different approaches were developed for Twitter sentiment classification by using machine learning such as Support Vector Machine (SVM) with rule-based features BIBREF2 and the combination of SVMs and Naive Bayes (NB) BIBREF3 . In addition, hybrid approaches combining lexicon-based and machine learning methods also achieved high performance described in BIBREF4 . However, a problem of traditional machine learning is how to define a feature extractor for a specific domain in order to extract important features. Deep learning models are different from traditional machine learning methods in that a deep learning model does not depend on feature extractors because features are extracted during training progress. The use of deep learning methods becomes to achieve remarkable results for sentiment analysis BIBREF5 BIBREF6 BIBREF7 . Some researchers used Convolutional Neural Network (CNN) for sentiment classification. CNN models have been shown to be effective for NLP. For example, BIBREF6 proposed various kinds of CNN to learn sentiment-bearing sentence vectors, BIBREF5 adopted two CNNs in character-level to sentence-level representation for sentiment analysis. BIBREF7 constructs experiments on a character-level CNN for several large-scale datasets. In addition, Long Short-Term Memory (LSTM) is another state-of-the-art semantic composition model for sentiment classification with many variants described in BIBREF8 . The studies reveal that using a CNN is useful in extracting information and finding feature detectors from texts. In addition, a LSTM can be good in maintaining word order and the context of words. However, in some important aspects, the use of CNN or LSTM separately may not capture enough information. Inspired by the models above, the goal of this research is using a Deep Convolutional Neural Network (DeepCNN) to exploit the information of characters of words in order to support word-level embedding. A Bi-LSTM produces a sentence-wide feature representation based on these embeddings. The Bi-LSTM is a version of BIBREF9 with Full Gradient described in BIBREF10 . In addition, the rules-based approach also effects classification accuracy by focusing on important sub-sentences expressing the main sentiment of a tweet while removing unnecessary parts of a tweet. The paper makes the following contributions: The organization of the present paper is as follows: In section 2, we describe the model architecture which introduces the structure of the model. We explain the basic idea of model and the way of constructing the model. Section 3 show results and analysis and section 4 summarize this paper.",Basic idea,"Our proposed model consists of a deep learning classifier and a tweet processor. The deep learning classifier is a combination of DeepCNN and Bi-LSTM. The tweet processor standardizes tweets and then applies semantic rules on datasets. We construct a framework that treats the deep learning classifier and the tweet processor as two distinct components. We believe that standardizing data is an important step to achieve high accuracy. To formulate our problem in increasing the accuracy of the classifier, we illustrate our model in Figure. FIGREF4 as follows: Tweets are firstly considered via a processor based on preprocessing steps BIBREF0 and the semantic rules-based method BIBREF11 in order to standardize tweets and capture only important information containing the main sentiment of a tweet. We use DeepCNN with Wide convolution for character-level embeddings. A wide convolution can learn to recognize specific n-grams at every position in a word that allows features to be extracted independently of these positions in the word. These features maintain the order and relative positions of characters. A DeepCNN is constructed by two wide convolution layers and the need of multiple wide convolution layers is widely accepted that a model constructing by multiple processing layers have the ability to learn representations of data with higher levels of abstraction BIBREF12 . Therefore, we use DeepCNN for character-level embeddings to support morphological and shape information for a word. The DeepCNN produces INLINEFORM0 global fixed-sized feature vectors for INLINEFORM1 words. A combination of the global fixed-size feature vectors and word-level embedding is fed into Bi-LSTM. The Bi-LSTM produces a sentence-level representation by maintaining the order of words. Our work is philosophically similar to BIBREF5 . However, our model is distinguished with their approaches in two aspects: Using DeepCNN with two wide convolution layers to increase representation with multiple levels of abstraction. Integrating global character fixed-sized feature vectors with word-level embedding to extract a sentence-wide feature set via Bi-LSTM. This deals with three main problems: (i) Sentences have any different size; (ii) The semantic and the syntactic of words in a sentence are captured in order to increase information for a word; (iii) Important information of characters that can appear at any position in a word are extracted. In sub-section B, we introduce various kinds of dataset. The modules of our model are constructed in other sub-sections.",Data Preparation,"Stanford - Twitter Sentiment Corpus (STS Corpus): STS Corpus contains 1,600K training tweets collected by a crawler from BIBREF0 . BIBREF0 constructed a test set manually with 177 negative and 182 positive tweets. The Stanford test set is small. However, it has been widely used in different evaluation tasks BIBREF0 BIBREF5 BIBREF13 . Sanders - Twitter Sentiment Corpus: This dataset consists of hand-classified tweets collected by using search terms: INLINEFORM0 , #google, #microsoft and #twitter. We construct the dataset as BIBREF14 for binary classification. Health Care Reform (HCR): This dataset was constructed by crawling tweets containing the hashtag #hcr BIBREF15 . Task is to predict positive/negative tweets BIBREF14 .",Preprocessing,"We firstly take unique properties of Twitter in order to reduce the feature space such as Username, Usage of links, None, URLs and Repeated Letters. We then process retweets, stop words, links, URLs, mentions, punctuation and accentuation. For emoticons, BIBREF0 revealed that the training process makes the use of emoticons as noisy labels and they stripped the emoticons out from their training dataset because BIBREF0 believed that if we consider the emoticons, there is a negative impact on the accuracies of classifiers. In addition, removing emoticons makes the classifiers learns from other features (e.g. unigrams and bi-grams) presented in tweets and the classifiers only use these non-emoticon features to predict the sentiment of tweets. However, there is a problem is that if the test set contains emoticons, they do not influence the classifiers because emoticon features do not contain in its training data. This is a limitation of BIBREF0 , because the emoticon features would be useful when classifying test data. Therefore, we keep emoticon features in the datasets because deep learning models can capture more information from emoticon features for increasing classification accuracy.",Semantic Rules (SR),"In Twitter social networking, people express their opinions containing sub-sentences. These sub-sentences using specific PoS particles (Conjunction and Conjunctive adverbs), like ""but, while, however, despite, however"" have different polarities. However, the overall sentiment of tweets often focus on certain sub-sentences. For example: @lonedog bwahahah...you are amazing! However, it was quite the letdown. @kirstiealley my dentist is great but she's expensive...=( In two tweets above, the overall sentiment is negative. However, the main sentiment is only in the sub-sentences following but and however. This inspires a processing step to remove unessential parts in a tweet. Rule-based approach can assists these problems in handling negation and dealing with specific PoS particles led to effectively affect the final output of classification BIBREF11 BIBREF16 . BIBREF11 summarized a full presentation of their semantic rules approach and devised ten semantic rules in their hybrid approach based on the presentation of BIBREF16 . We use five rules in the semantic rules set because other five rules are only used to compute polarity of words after POS tagging or Parsing steps. We follow the same naming convention for rules utilized by BIBREF11 to represent the rules utilized in our proposed method. The rules utilized in the proposed method are displayed in Table TABREF15 in which is included examples from STS Corpus and output after using the rules. Table TABREF16 illustrates the number of processed sentences on each dataset.",Representation Levels,"To construct embedding inputs for our model, we use a fixed-sized word vocabulary INLINEFORM0 and a fixed-sized character vocabulary INLINEFORM1 . Given a word INLINEFORM2 is composed from characters INLINEFORM3 , the character-level embeddings are encoded by column vectors INLINEFORM4 in the embedding matrix INLINEFORM5 , where INLINEFORM6 is the size of the character vocabulary. For word-level embedding INLINEFORM7 , we use a pre-trained word-level embedding with dimension 200 or 300. A pre-trained word-level embedding can capture the syntactic and semantic information of words BIBREF17 . We build every word INLINEFORM8 into an embedding INLINEFORM9 which is constructed by two sub-vectors: the word-level embedding INLINEFORM10 and the character fixed-size feature vector INLINEFORM11 of INLINEFORM12 where INLINEFORM13 is the length of the filter of wide convolutions. We have INLINEFORM14 character fixed-size feature vectors corresponding to word-level embedding in a sentence.",Deep Learning Module,"DeepCNN in the deep learning module is illustrated in Figure. FIGREF22 . The DeepCNN has two wide convolution layers. The first layer extract local features around each character windows of the given word and using a max pooling over character windows to produce a global fixed-sized feature vector for the word. The second layer retrieves important context characters and transforms the representation at previous level into a representation at higher abstract level. We have INLINEFORM0 global character fixed-sized feature vectors for INLINEFORM1 words. In the next step of Figure. FIGREF4 , we construct the vector INLINEFORM0 by concatenating the word-level embedding with the global character fixed-size feature vectors. The input of Bi-LSTM is a sequence of embeddings INLINEFORM1 . The use of the global character fixed-size feature vectors increases the relationship of words in the word-level embedding. The purpose of this Bi-LSTM is to capture the context of words in a sentence and maintain the order of words toward to extract sentence-level representation. The top of the model is a softmax function to predict sentiment label. We describe in detail the kinds of CNN and LSTM that we use in next sub-part 1 and 2. The one-dimensional convolution called time-delay neural net has a filter vector INLINEFORM0 and take the dot product of filter INLINEFORM1 with each m-grams in the sequence of characters INLINEFORM2 of a word in order to obtain a sequence INLINEFORM3 : DISPLAYFORM0  Based on Equation 1, we have two types of convolutions that depend on the range of the index INLINEFORM0 . The narrow type of convolution requires that INLINEFORM1 and produce a sequence INLINEFORM2 . The wide type of convolution does not require on INLINEFORM3 or INLINEFORM4 and produce a sequence INLINEFORM5 . Out-of-range input values INLINEFORM6 where INLINEFORM7 or INLINEFORM8 are taken to be zero. We use wide convolution for our model. Given a word INLINEFORM0 composed of INLINEFORM1 characters INLINEFORM2 , we take a character embedding INLINEFORM3 for each character INLINEFORM4 and construct a character matrix INLINEFORM5 as following Equation. 2: DISPLAYFORM0  The values of the embeddings INLINEFORM0 are parameters that are optimized during training. The trained weights in the filter INLINEFORM1 correspond to a feature detector which learns to recognize a specific class of n-grams. The n-grams have size INLINEFORM2 . The use of a wide convolution has some advantages more than a narrow convolution because a wide convolution ensures that all weights of filter reach the whole characters of a word at the margins. The resulting matrix has dimension INLINEFORM3 . Long Short-Term Memory networks usually called LSTMs are a improved version of RNN. The core idea behind LSTMs is the cell state which can maintain its state over time, and non-linear gating units which regulate the information flow into and out of the cell. The LSTM architecture that we used in our proposed model is described in BIBREF9 . A single LSTM memory cell is implemented by the following composite function: DISPLAYFORM0 DISPLAYFORM1  where INLINEFORM0 is the logistic sigmoid function, INLINEFORM1 and INLINEFORM2 are the input gate, forget gate, output gate, cell and cell input activation vectors respectively. All of them have a same size as the hidden vector INLINEFORM3 . INLINEFORM4 is the hidden-input gate matrix, INLINEFORM5 is the input-output gate matrix. The bias terms which are added to INLINEFORM6 and INLINEFORM7 have been omitted for clarity. In addition, we also use the full gradient for calculating with full backpropagation through time (BPTT) described in BIBREF10 . A LSTM gradients using finite differences could be checked and making practical implementations more reliable.",Regularization,"For regularization, we use a constraint on INLINEFORM0 of the weight vectors BIBREF18 .", Experimental setups,"For the Stanford Twitter Sentiment Corpus, we use the number of samples as BIBREF5 . The training data is selected 80K tweets for a training data and 16K tweets for the development set randomly from the training data of BIBREF0 . We conduct a binary prediction for STS Corpus. For Sander dataset, we use standard 10-fold cross validation as BIBREF14 . We construct the development set by selecting 10% randomly from 9-fold training data. In Health Care Reform Corpus, we also select 10% randomly for the development set in a training set and construct as BIBREF14 for comparison. We describe the summary of datasets in Table III. for all datasets, the filter window size ( INLINEFORM0 ) is 7 with 6 feature maps each for the first wide convolution layer, the second wide convolution layer has a filter window size of 5 with 14 feature maps each. Dropout rate ( INLINEFORM1 ) is 0.5, INLINEFORM2 constraint, learning rate is 0.1 and momentum of 0.9. Mini-batch size for STS Corpus is 100 and others are 4. In addition, training is done through stochastic gradient descent over shuffled mini-batches with Adadelta update rule BIBREF19 . we use the publicly available Word2Vec trained from 100 billion words from Google and TwitterGlove of Stanford is performed on aggregated global word-word co-occurrence statistics from a corpus. Word2Vec has dimensionality of 300 and Twitter Glove have dimensionality of 200. Words that do not present in the set of pre-train words are initialized randomly.",Experimental results,"Table IV shows the result of our model for sentiment classification against other models. We compare our model performance with the approaches of BIBREF0 BIBREF5 on STS Corpus. BIBREF0 reported the results of Maximum Entropy (MaxEnt), NB, SVM on STS Corpus having good performance in previous time. The model of BIBREF5 is a state-of-the-art so far by using a CharSCNN. As can be seen, 86.63 is the best prediction accuracy of our model so far for the STS Corpus. For Sanders and HCR datasets, we compare results with the model of BIBREF14 that used a ensemble of multiple base classifiers (ENS) such as NB, Random Forest (RF), SVM and Logistic Regression (LR). The ENS model is combined with bag-of-words (BoW), feature hashing (FH) and lexicons. The model of BIBREF14 is a state-of-the-art on Sanders and HCR datasets. Our models outperform the model of BIBREF14 for the Sanders dataset and HCR dataset.",Analysis,"As can be seen, the models with SR outperforms the model with no SR. Semantic rules is effective in order to increase classification accuracy. We evaluate the efficiency of SR for the model in Table V of our full paper . We also conduct two experiments on two separate models: DeepCNN and Bi-LSTM in order to show the effectiveness of combination of DeepCNN and Bi-LSTM. In addition, the model using TwitterGlove outperform the model using GoogleW2V because TwitterGlove captures more information in Twitter than GoogleW2V. These results show that the character-level information and SR have a great impact on Twitter Data. The pre-train word vectors are good, universal feature extractors. The difference between our model and other approaches is the ability of our model to capture important features by using SR and combine these features at high benefit. The use of DeepCNN can learn a representation of words in higher abstract level. The combination of global character fixed-sized feature vectors and a word embedding helps the model to find important detectors for particles such as 'not' that negate sentiment and potentiate sentiment such as 'too', 'so' standing beside expected features. The model not only learns to recognize single n-grams, but also patterns in n-grams lead to form a structure significance of a sentence.",Conclusions,"In the present work, we have pointed out that the use of character embeddings through a DeepCNN to enhance information for word embeddings built on top of Word2Vec or TwitterGlove improves classification accuracy in Tweet sentiment classification. Our results add to the well-establish evidence that character vectors are an important ingredient for word-level in deep learning for NLP. In addition, semantic rules contribute handling non-essential sub-tweets in order to improve classification accuracy.",,,,,,,,,,,,,,,,,,,What were their results on the three datasets?,efb3a87845460655c53bd7365bcb8393c99358ec,,,,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,"accuracy of 86.63 on STS, 85.14 on Sanders and 80.9 on HCR",FLOAT SELECTED: Table IV ACCURACY OF DIFFERENT MODELS FOR BINARY CLASSIFICATION,FLOAT SELECTED: Table IV ACCURACY OF DIFFERENT MODELS FOR BINARY CLASSIFICATION,0282506d82926af9792f42326478042758bdc913,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,,,,,,,,,What was the baseline?,0619fc797730a3e59ac146a5a4575c81517cc618,,,,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,,"Table IV shows the result of our model for sentiment classification against other models. We compare our model performance with the approaches of BIBREF0 BIBREF5 on STS Corpus. BIBREF0 reported the results of Maximum Entropy (MaxEnt), NB, SVM on STS Corpus having good performance in previous time. The model of BIBREF5 is a state-of-the-art so far by using a CharSCNN. As can be seen, 86.63 is the best prediction accuracy of our model so far for the STS Corpus. For Sanders and HCR datasets, we compare results with the model of BIBREF14 that used a ensemble of multiple base classifiers (ENS) such as NB, Random Forest (RF), SVM and Logistic Regression (LR). The ENS model is combined with bag-of-words (BoW), feature hashing (FH) and lexicons. The model of BIBREF14 is a state-of-the-art on Sanders and HCR datasets. Our models outperform the model of BIBREF14 for the Sanders dataset and HCR dataset.","Table IV shows the result of our model for sentiment classification against other models. We compare our model performance with the approaches of BIBREF0 BIBREF5 on STS Corpus. BIBREF0 reported the results of Maximum Entropy (MaxEnt), NB, SVM on STS Corpus having good performance in previous time. The model of BIBREF5 is a state-of-the-art so far by using a CharSCNN. As can be seen, 86.63 is the best prediction accuracy of our model so far for the STS Corpus.

For Sanders and HCR datasets, we compare results with the model of BIBREF14 that used a ensemble of multiple base classifiers (ENS) such as NB, Random Forest (RF), SVM and Logistic Regression (LR). The ENS model is combined with bag-of-words (BoW), feature hashing (FH) and lexicons. The model of BIBREF14 is a state-of-the-art on Sanders and HCR datasets. Our models outperform the model of BIBREF14 for the Sanders dataset and HCR dataset.

",740a7d8f2b75e1985ebefff16360d9b704eec6b3,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,Which datasets did they use?,846a1992d66d955fa1747bca9a139141c19908e8,,,,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,,"Stanford - Twitter Sentiment Corpus (STS Corpus): STS Corpus contains 1,600K training tweets collected by a crawler from BIBREF0 . BIBREF0 constructed a test set manually with 177 negative and 182 positive tweets. The Stanford test set is small. However, it has been widely used in different evaluation tasks BIBREF0 BIBREF5 BIBREF13 . Sanders - Twitter Sentiment Corpus: This dataset consists of hand-classified tweets collected by using search terms: INLINEFORM0 , #google, #microsoft and #twitter. We construct the dataset as BIBREF14 for binary classification. Health Care Reform (HCR): This dataset was constructed by crawling tweets containing the hashtag #hcr BIBREF15 . Task is to predict positive/negative tweets BIBREF14 .","Stanford - Twitter Sentiment Corpus (STS Corpus): STS Corpus contains 1,600K training tweets collected by a crawler from BIBREF0 . BIBREF0 constructed a test set manually with 177 negative and 182 positive tweets. The Stanford test set is small. However, it has been widely used in different evaluation tasks BIBREF0 BIBREF5 BIBREF13 .

Sanders - Twitter Sentiment Corpus: This dataset consists of hand-classified tweets collected by using search terms: INLINEFORM0 , #google, #microsoft and #twitter. We construct the dataset as BIBREF14 for binary classification.

Health Care Reform (HCR): This dataset was constructed by crawling tweets containing the hashtag #hcr BIBREF15 . Task is to predict positive/negative tweets BIBREF14 .",ecc705477bc9fc15949d2a0ca55fd5f2e129acfb,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,,,,,,,,Are results reported only on English datasets?,1ef8d1cb1199e1504b6b0daea52f2e4bd2ef7023,five,unfamiliar,no,twitter,e8b24c3133e0bec0a6465e1f13acd3a5ed816b37,False,True,,"Stanford - Twitter Sentiment Corpus (STS Corpus): STS Corpus contains 1,600K training tweets collected by a crawler from BIBREF0 . BIBREF0 constructed a test set manually with 177 negative and 182 positive tweets. The Stanford test set is small. However, it has been widely used in different evaluation tasks BIBREF0 BIBREF5 BIBREF13 . Sanders - Twitter Sentiment Corpus: This dataset consists of hand-classified tweets collected by using search terms: INLINEFORM0 , #google, #microsoft and #twitter. We construct the dataset as BIBREF14 for binary classification. Health Care Reform (HCR): This dataset was constructed by crawling tweets containing the hashtag #hcr BIBREF15 . Task is to predict positive/negative tweets BIBREF14 . Table IV shows the result of our model for sentiment classification against other models. We compare our model performance with the approaches of BIBREF0 BIBREF5 on STS Corpus. BIBREF0 reported the results of Maximum Entropy (MaxEnt), NB, SVM on STS Corpus having good performance in previous time. The model of BIBREF5 is a state-of-the-art so far by using a CharSCNN. As can be seen, 86.63 is the best prediction accuracy of our model so far for the STS Corpus. For Sanders and HCR datasets, we compare results with the model of BIBREF14 that used a ensemble of multiple base classifiers (ENS) such as NB, Random Forest (RF), SVM and Logistic Regression (LR). The ENS model is combined with bag-of-words (BoW), feature hashing (FH) and lexicons. The model of BIBREF14 is a state-of-the-art on Sanders and HCR datasets. Our models outperform the model of BIBREF14 for the Sanders dataset and HCR dataset.","Stanford - Twitter Sentiment Corpus (STS Corpus): STS Corpus contains 1,600K training tweets collected by a crawler from BIBREF0 . BIBREF0 constructed a test set manually with 177 negative and 182 positive tweets. The Stanford test set is small. However, it has been widely used in different evaluation tasks BIBREF0 BIBREF5 BIBREF13 .

Sanders - Twitter Sentiment Corpus: This dataset consists of hand-classified tweets collected by using search terms: INLINEFORM0 , #google, #microsoft and #twitter. We construct the dataset as BIBREF14 for binary classification.

Health Care Reform (HCR): This dataset was constructed by crawling tweets containing the hashtag #hcr BIBREF15 . Task is to predict positive/negative tweets BIBREF14 . Table IV shows the result of our model for sentiment classification against other models. We compare our model performance with the approaches of BIBREF0 BIBREF5 on STS Corpus.  For Sanders and HCR datasets, we compare results with the model of BIBREF14 that used a ensemble of multiple base classifiers (ENS) such as NB, Random Forest (RF), SVM and Logistic Regression (LR). ",710ac11299a9dce0201ababcbffafc1dce9f905b,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,,,,,,,,Which three Twitter sentiment classification datasets are used for experiments?,12d77ac09c659d2e04b5e3955a283101c3ad1058,five,unfamiliar,no,twitter,e8b24c3133e0bec0a6465e1f13acd3a5ed816b37,False,Stanford - Twitter Sentiment Corpus (STS Corpus) Sanders - Twitter Sentiment Corpus Health Care Reform (HCR),,,"Stanford - Twitter Sentiment Corpus (STS Corpus): STS Corpus contains 1,600K training tweets collected by a crawler from BIBREF0 . BIBREF0 constructed a test set manually with 177 negative and 182 positive tweets. The Stanford test set is small. However, it has been widely used in different evaluation tasks BIBREF0 BIBREF5 BIBREF13 . Sanders - Twitter Sentiment Corpus: This dataset consists of hand-classified tweets collected by using search terms: INLINEFORM0 , #google, #microsoft and #twitter. We construct the dataset as BIBREF14 for binary classification. Health Care Reform (HCR): This dataset was constructed by crawling tweets containing the hashtag #hcr BIBREF15 . Task is to predict positive/negative tweets BIBREF14 .","Stanford - Twitter Sentiment Corpus (STS Corpus): STS Corpus contains 1,600K training tweets collected by a crawler from BIBREF0 . BIBREF0 constructed a test set manually with 177 negative and 182 positive tweets. The Stanford test set is small. However, it has been widely used in different evaluation tasks BIBREF0 BIBREF5 BIBREF13 .

Sanders - Twitter Sentiment Corpus: This dataset consists of hand-classified tweets collected by using search terms: INLINEFORM0 , #google, #microsoft and #twitter. We construct the dataset as BIBREF14 for binary classification.

Health Care Reform (HCR): This dataset was constructed by crawling tweets containing the hashtag #hcr BIBREF15 . Task is to predict positive/negative tweets BIBREF14 .",de891b9e0b026bcc3d3fb336aceffb8a7228dbbd,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,What semantic rules are proposed?,d60a3887a0d434abc0861637bbcd9ad0c596caf4,five,unfamiliar,no,twitter,e8b24c3133e0bec0a6465e1f13acd3a5ed816b37,False,,rules that compute polarity of words after POS tagging or parsing steps,"In Twitter social networking, people express their opinions containing sub-sentences. These sub-sentences using specific PoS particles (Conjunction and Conjunctive adverbs), like ""but, while, however, despite, however"" have different polarities. However, the overall sentiment of tweets often focus on certain sub-sentences. For example: @lonedog bwahahah...you are amazing! However, it was quite the letdown. @kirstiealley my dentist is great but she's expensive...=( In two tweets above, the overall sentiment is negative. However, the main sentiment is only in the sub-sentences following but and however. This inspires a processing step to remove unessential parts in a tweet. Rule-based approach can assists these problems in handling negation and dealing with specific PoS particles led to effectively affect the final output of classification BIBREF11 BIBREF16 . BIBREF11 summarized a full presentation of their semantic rules approach and devised ten semantic rules in their hybrid approach based on the presentation of BIBREF16 . We use five rules in the semantic rules set because other five rules are only used to compute polarity of words after POS tagging or Parsing steps. We follow the same naming convention for rules utilized by BIBREF11 to represent the rules utilized in our proposed method. The rules utilized in the proposed method are displayed in Table TABREF15 in which is included examples from STS Corpus and output after using the rules. Table TABREF16 illustrates the number of processed sentences on each dataset. FLOAT SELECTED: Table I SEMANTIC RULES [12]","In Twitter social networking, people express their opinions containing sub-sentences. These sub-sentences using specific PoS particles (Conjunction and Conjunctive adverbs), like ""but, while, however, despite, however"" have different polarities. However, the overall sentiment of tweets often focus on certain sub-sentences. For example:

@lonedog bwahahah...you are amazing! However, it was quite the letdown.

@kirstiealley my dentist is great but she's expensive...=(

In two tweets above, the overall sentiment is negative. However, the main sentiment is only in the sub-sentences following but and however. This inspires a processing step to remove unessential parts in a tweet. Rule-based approach can assists these problems in handling negation and dealing with specific PoS particles led to effectively affect the final output of classification BIBREF11 BIBREF16 . BIBREF11 summarized a full presentation of their semantic rules approach and devised ten semantic rules in their hybrid approach based on the presentation of BIBREF16 . We use five rules in the semantic rules set because other five rules are only used to compute polarity of words after POS tagging or Parsing steps. We follow the same naming convention for rules utilized by BIBREF11 to represent the rules utilized in our proposed method. The rules utilized in the proposed method are displayed in Table TABREF15 in which is included examples from STS Corpus and output after using the rules. Table TABREF16 illustrates the number of processed sentences on each dataset. FLOAT SELECTED: Table I SEMANTIC RULES [12]",c59556729d9eaaff1c3e24854a7d78ff2255399d,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3-Figure1-1.png,Figure 1. The overview of a deep learning system.,3-TableII-1.png,Table II THE NUMBER OF TWEETS ARE PROCESSED BY USING SEMANTIC RULES,3-TableI-1.png,Table I SEMANTIC RULES [12],4-Figure2-1.png,Figure 2. Deep Convolutional Neural Network (DeepCNN) for the sequence of character embeddings of a word. For example with 1 region size is 2 and 4 feature maps in the first convolution and 1 region size is 3 with 3 feature maps in the second convolution.,5-TableIV-1.png,Table IV ACCURACY OF DIFFERENT MODELS FOR BINARY CLASSIFICATION,5-TableIII-1.png,Table III SUMMARY STATISTICS FOR THE DATASETS AFTER USING SEMANTIC RULES. c: THE NUMBER OF CLASSES. N : THE NUMBER OF TWEETS. lw : MAXIMUM SENTENCE LENGTH. lc : MAXIMUM CHARACTER LENGTH. |Vw|: WORD ALPHABET SIZE. |Vc|: CHARACTER ALPHABET SIZE.,,,,,,,,,"We compare our model performance with the approaches of BIBREF0 BIBREF5 on STS Corpus. BIBREF0 reported the results of Maximum Entropy (MaxEnt), NB, SVM on STS Corpus having good performance in previous time. The model of BIBREF5 is a state-of-the-art so far by using a CharSCNN. we compare results with the model of BIBREF14 that used a ensemble of multiple base classifiers (ENS) such as NB, Random Forest (RF), SVM and Logistic Regression (LR). The ENS model is combined with bag-of-words (BoW), feature hashing (FH) and lexicons. The model of BIBREF14 is a state-of-the-art on Sanders and HCR datasets. ",Stanford - Twitter Sentiment Corpus (STS Corpus) Sanders - Twitter Sentiment Corpus Health Care Reform (HCR),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Excitation-based Voice Quality Analysis and Modification,"This paper investigates the differences occuring in the excitation for different voice qualities. Its goal is two-fold. First a large corpus containing three voice qualities (modal, soft and loud) uttered by the same speaker is analyzed and significant differences in characteristics extracted from the excitation are observed. Secondly rules of modification derived from the analysis are used to build a voice quality transformation system applied as a post-process to HMM-based speech synthesis. The system is shown to effectively achieve the transformations while maintaining the delivered quality.",Introduction,"Since early times of computer-based speech synthesis research, voice quality (the perceived timbre of speech) analysis/modification has attracted interest of researchers BIBREF0. The topic of voice quality analysis finds application in various areas of speech processing such as high-quality parametric speech synthesis, expressive/emotional speech synthesis, speaker identification, emotion recognition, prosody analysis, speech therapy. Due to availability of reviews such as BIBREF1 and space limitations, a review of voice quality analysis methods will not be presented here. For voice quality analysis of speech corpora, it is common practice to estimate spectral parameters directly from speech signals such as relative harmonic amplitudes, or Harmonic to Noise Ratio (HNR). Although the voice quality variations are mainly considered to be controlled by the glottal source, glottal source estimation is considered to be problematic and hence avoided in the parameter estimation procedures for processing large speech corpora. In this work, we follow the not so common path and study the differences present in the glottal source signal parameters estimated via an automatic algorithm when a given speaker produces different voice qualities. Based on a parametric analysis of these latter (Section SECREF2), we further investigate the use of the information extracted from a large corpus, for voice quality modification of other speech databases in a HMM-based speech synthesizer (Section SECREF3).",Excitation-based Voice quality analysis,"The goal of this part is to highlight the differences present in the excitation when a given speaker produces different voice qualities. The De7 database used for this study was designed by Marc Schroeder as one of the first attempts of creating diphone databases for expressive speech synthesis BIBREF2. The database contains three voice qualities (modal, soft and loud) uttered by a German female speaker, with about 50 minutes of speech available for each voice quality. In Section SECREF1, the glottal flow estimation method and glottal flow parametrization used in this work are briefly presented. The harmonicity of speech is studied via the maximum voiced frequency in Section SECREF3. As an important perceptual charactersitic, spectral tilt is analyzed in Section SECREF4. Section SECREF6 compares the so-called eigenresiduals BIBREF3 of the different voice qualities. Finally Section SECREF8 quantifies the separability between the three voice qualities for the extracted excitation features.",Excitation-based Voice quality analysis ::: Glottal source,"We recently showed that complex cepstrum can be efficiently used for glottal flow estimation BIBREF4. This method aims at separating the minimum and maximum-phase components of the speech signal. Indeed it has been shown previously BIBREF5 that speech is a mixed-phase signal where the maximum-phase (i.e anti-causal) contribution corresponds to the glottal open phase, while the minimum-phase component is related to the vocal tract transmittance (assuming an abrupt glottal return phase). Isolating the maximum-phase component of speech then provides a reliable estimation of the glottal source, which can be achieved by the complex cepstrum. The glottal flow open phase is then parametrized by three features: the glottal formant frequency ($F_g$), the Normalized Amplitude Quotient ($NAQ$) and the Quasi-Open Quotient ($QOQ$). The glottal formant was tracked using the method described in BIBREF6. Figure FIGREF2(a) shows the histograms of $Fg/F_0$ for the three voice qualities. Significant differences between the distributions are observed. Among others it turns out that a louder (softer) voice results in the production of a higher (lower) glottal formant frequency. Another observation that can be drawn from this figure is the presence of two modes for the modal and loud voices. This may be explained by the fact that the estimated glottal source sometimes comprises a ripple both in the time and frequency domains, which in turn may have two possible causes: an incomplete separation between $Fg$ and the first formant $F_1$ BIBREF6, and/or a non-linear interaction between the vocal tract and the glottis BIBREF7. This ripple may therefore affect the detection of the glottal formant frequency and in this way explain the parasitical peak in the $Fg/F_0$ histogram for the modal and loud voices. In previous works BIBREF8, BIBREF9, Alku et al. proposed the Normalized Amplitude Quotient and the Quasi-Open Quotient as two efficient time-domain parameters characterizing respectively the closing and open phase of the glottal flow. These parameters are extracted using the Aparat toolkit BIBREF10 from the glottal source estimated here by the complex cepstrum . Figures FIGREF2(b) and FIGREF2(c) display the histograms of these two features for the three voice qualities. Notable differences between histograms may be observed.",Excitation-based Voice quality analysis ::: Maximum Voiced Frequency,"Some approaches, such as the Harmonic plus Noise Model (HNM ,BIBREF11), consider that the speech signal can be modeled by a non-periodic component beyond a given frequency. In the case of HNM, this maximum voiced frequency ($F_m$) demarcates the boundary between two distinct spectral bands, where respectively an harmonic and a stochastic modeling are supposed to hold. The higher the $F_m$, the stronger the harmonicity, and consequently the weaker the presence of noise in speech. In this paper, $F_m$ was estimated using the algorithm described in BIBREF11. Figure FIGREF2(d) displays the histograms of $F_m$ for the three voice qualities. It can be observed that, in general, the soft voice has a low $F_m$ (as a result of its breathy nature) and that the stronger the vocal effort, the more harmonic the speech signal, and consequently the higher $F_m$.",Excitation-based Voice quality analysis ::: Spectral Tilt,"Spectral tilt of speech is known to play an important role in the perception of a voice quality BIBREF12. To capture this crucial feature, an averaged spectrum is obtained on the whole corpus by a process independent of the prosody and the vocal tract variations. For this, voiced speech frames are extracted by applying a two pitch period-long Hanning windowing centered on the current Glottal Closure Instant (GCI). GCI locations are determined using the technique described in BIBREF13. These frames are then resampled on a fixed number of points (corresponding to two mean pitch periods) and normalized in energy. The averaged spectrum is finally achieved by averaging the spectra of the normalized speech frames. The averaged amplitude spectrum then contains a mix of the average glottal and vocal tract contributions. The averaged spectrum for the three voice qualities is exhibited in Figure FIGREF5. Since these spectra were computed for the same speaker, it is reasonable to think that the main difference between them is due to the spectral tilt regarding the produced voice quality. Among others it can be noticed from Figure FIGREF5 that the stronger the vocal effort, the richer the spectral content in the [1kHz-5kHz] band.",Excitation-based Voice quality analysis ::: Eigenresiduals,"In BIBREF3 we proposed to model the residual signal by decomposing speaker-dependent pitch-synchronous residual frames on an orthonormal basis. It was also shown that the first so-obtained eigenvector (called eigenresidual) can be efficiently used in parametric speech synthesis. As eigenresiduals are employed in our voice quality modification application in Section SECREF3, Figure FIGREF7 displays the differences present in this signal depending on the produced voice quality. It can be noticed that the conclusions we drew in Section SECREF1 about the glottal open phase are corroborated. It is indeed observed that the stronger the vocal effort, the faster the response of the eigenresidual open phase.",Excitation-based Voice quality analysis ::: Separability between Distributions,"Important differences in the distributions of the features have been presented in the previous subsections (which are in line with the conclusions presented in various studies BIBREF0, BIBREF8, BIBREF12). In this section, we quantify how these differences between voice qualities are significative. For this, the Kullback-Leibler (KL) divergence BIBREF14 is known to measure the separability between two discrete density functions $A$ and $B$. But since this measure is non-symmetric (and consequently is not a true distance), its symmetrised version, called Jensen-Shannon divergence BIBREF14, is often prefered. It consists of a sum of two KL measures:  where $M$ is the average of the two distributions ($M=0.5*(A+B)$). Table TABREF10 shows the results for the four features we previously presented. Among others it can be noticed that the loud and soft voices are highly separable, while the loud type is closer to the modal voice than the soft one. It is also seen that $F_g$ and $NAQ$ are highly informative for voice quality labeling.",Voice quality modification,"In a previous work BIBREF3, we proposed a Deterministic plus Stochastic Model (DSM) of the residual signal. In this approach, the excitation is divided into two distinct spectral bands delimited by the maximum voiced frequency $F_m$. The deterministic part concerns the low-frequency contents and is modeled by the first eigenresidual as explained in Section SECREF6. As for the stochastic component, it is a high-pass filtered noise similarly to what is used in the HNM BIBREF11. The residual signal is then passed through a LPC-like filter to obtain the synthetic speech. This section aims at applying voice quality modification as a post-process to HMM-based speech synthesis BIBREF15 using the DSM of the residual signal. More precisely, a HMM-based synthesizer is trained on a corpus of modal voice for a given speaker. The goal is then to transform the synthetic voice so that it is perceived as soft or loud, while avoiding a degradation of quality in the produced speech. Since no dataset of expressive voice is available for the considered speaker, modifications are extrapolated from the prototypes described for speaker De7 in Section SECREF2, assuming that other speakers modify their voice quality in the same way. Three main transformations are here considered: The eigenresiduals presented in Section SECREF6 are used for the deterministic part of the DSM. These waveforms implicitly convey the modifications of glottal open phase that were underlined in Section SECREF1. The maximum voiced frequency $F_m$ is fixed for a given voice quality according to Section SECREF3 by taking its mean value: 4600 Hz for the loud, 3990 Hz for the modal (confirming the 4 kHz we used in BIBREF3), and 2460 Hz for the soft voice. The spectral tilt is modified using the inverse of the process described in Section SECREF4. For this, the averaged spectrum of the voiced segments is transformed, in the pitch-normalized domain, by a filter expressed as a ratio between auto-regressive modelings of the source and target voice qualities (cf Fig.FIGREF5). Residual frames are then resampled to the target pitch at synthesis time. This latter transformation is then pitch-dependent. To evaluate the technique, a subjective test was submitted to 10 people. The test consisted of 27 sentences generated by our system for three speakers (two males and one female). One third of these sentences were converted to a softer voice, and one third to a louder one. For each sentence, participants were asked to assess the vocal effort they perceive (0 = very soft, 100 = very tensed), and to give a MOS score. Results are displayed in Table TABREF14 with their 95% confidence intervals. Interestingly it can be noticed that voice quality modifications are perceived as expected while the overall quality is not significantly altered (although listeners have a natural tendency to prefer softer voices).",Conclusion,In this study we show that a glottal flow estimation algorithm BIBREF4 can be effectively used for voice quality analysis on large speech corpora where most of glottal flow estimation literature are based on tests with sustained vowels. We study the variations in parameters for different voice qualities and conclude that the two glottal flow parameters $F_g$ and $NAQ$ are highly informative for voice quality labeling. We further show that the information extracted from one speech database can be applied to other speech databases for voice quality modification and the quality achieved in a speech synthesis application is fairly high.,Acknowledgments,"Thomas Drugman is supported by the “Fonds National de la Recherche Scientifique” (FNRS). The authors would like to thank M. Schroeder for the De7 database, as well as Y. Stylianou for providing the algorithm extracting $F_m$.",,,,,,,,,,,,,,,,,,,,,,,What large corpus is used for experiments?,827b5bd215599623a3125afe331b56b89b42bf09,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,,,"The goal of this part is to highlight the differences present in the excitation when a given speaker produces different voice qualities. The De7 database used for this study was designed by Marc Schroeder as one of the first attempts of creating diphone databases for expressive speech synthesis BIBREF2. The database contains three voice qualities (modal, soft and loud) uttered by a German female speaker, with about 50 minutes of speech available for each voice quality. In Section SECREF1, the glottal flow estimation method and glottal flow parametrization used in this work are briefly presented. The harmonicity of speech is studied via the maximum voiced frequency in Section SECREF3. As an important perceptual charactersitic, spectral tilt is analyzed in Section SECREF4. Section SECREF6 compares the so-called eigenresiduals BIBREF3 of the different voice qualities. Finally Section SECREF8 quantifies the separability between the three voice qualities for the extracted excitation features.","The De7 database used for this study was designed by Marc Schroeder as one of the first attempts of creating diphone databases for expressive speech synthesis BIBREF2. The database contains three voice qualities (modal, soft and loud) uttered by a German female speaker, with about 50 minutes of speech available for each voice quality.",43a89117f241f3972a93a3c19a3a6772d89d901c,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2-Figure1-1.png,"Fig. 1. Histograms, for the three voice qualities, of (a): the normalized glottal formant frequency Fg/F0, (b): the Normalized Amplitude Quotient NAQ, (c): the Quasi-Open Quotient QOQ, and (d): the maximum voiced frequency Fm.",3-Figure2-1.png,Fig. 2. Averaged spectrum for the three voice qualities.,3-Table1-1.png,Table 1. Jensen-Shannon divergence between the three voice qualities for the four extracted features.,3-Figure3-1.png,Fig. 3. First eigenresidual for the three voice qualities.,4-Table2-1.png,"Table 2. Perceived vocal effort ratings (0 = very soft voice, 100 = very tensed voice) and MOS scores for the three versions together with their 95% confidence intervals.",,,,,,,,,,,,,,,,,,,,,,,,The De7 database,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A Lightweight Front-end Tool for Interactive Entity Population,"Entity population, a task of collecting entities that belong to a particular category, has attracted attention from vertical domains. There is still a high demand for creating entity dictionaries in vertical domains, which are not covered by existing knowledge bases. We develop a lightweight front-end tool for facilitating interactive entity population. We implement key components necessary for effective interactive entity population: 1) GUI-based dashboards to quickly modify an entity dictionary, and 2) entity highlighting on documents for quickly viewing the current progress. We aim to reduce user cost from beginning to end, including package installation and maintenance. The implementation enables users to use this tool on their web browsers without any additional packages -users can focus on their missions to create entity dictionaries. Moreover, an entity expansion module is implemented as external APIs. This design makes it easy to continuously improve interactive entity population pipelines. We are making our demo publicly available (http://bit.ly/luwak-demo).",Introduction,"Entity extraction is one of the most major NLP components. Most NLP tools (e.g., NLTK, Stanford CoreNLP, etc.), including commercial services (e.g., Google Cloud API, Alchemy API, etc.), provide entity extraction functions to recognize named entities (e.g., PERSON, LOCATION, ORGANIZATION, etc.) from texts. Some studies have defined fine-grained entity types and developed extraction methods BIBREF0 based on these types. However, these methods cannot comprehensively cover domain-specific entities. For instance, a real estate search engine needs housing equipment names to index these terms for providing fine-grained search conditions. There is a significant demand for constructing user-specific entity dictionaries, such as the case of cuisine and ingredient names for restaurant services. A straightforward solution is to prepare a set of these entity names as a domain-specific dictionary. Therefore, this paper focuses on the entity population task, which is a task of collecting entities that belong to an entity type required by a user. We develop LUWAK, a lightweight tool for effective interactive entity population. The key features are four-fold: We think these features are key components for effective interactive entity population. We choose an interactive user feedback strategy for entity population for LUWAK. A major approach to entity population is bootstrapping, which uses several entities that have been prepared as a seed set for finding new entities. Then, these new entities are integrated into the initial seed set to create a new seed set. The bootstrapping approach usually repeats the procedure until it has collected a sufficient number of entities. The framework cannot prevent the incorporation of incorrect entities that do not belong to the entity type unless user interaction between iterations. The problem is commonly called semantic drift BIBREF1 . Therefore, we consider user interaction, in which feedback is given to expanded candidates, as essential to maintaining the quality of an entity set. LUWAK implements fundamental functions for entity population, including (a) importing an initial entity set, (b) generating entity candidates, (c) obtaining user feedback, and (d) publishing populated entity dictionary. We aim to reduce the user’s total workload as a key metric of an entity population tool. That is, an entity population tool should provide the easiest and fastest solution to collecting entities of a particular entity type. User interaction cost is a dominant factor in the entire workload of an interactive tool. Thus, we carefully design the user interface for users to give feedbacks to the tool intuitively. Furthermore, we also consider the end-to-end user cost reduction. We adhere to the concept of developing installation-free software to distribute the tool among a wide variety of users, including nontechnical clusters. This lightweight design of LUWAK might speed up the procedure of the whole interactive entity population workflow. Furthermore, this advantage might be beneficial to continuously improve the whole pipeline of interactive entity population system.",LUWAK: A lightweight tool for interactive entity population,"Our framework adopts the interactive entity expansion approach. This approach organizes the collaboration of a human worker and entity expansion algorithms to generate a user-specific entity dictionary efficiently. We show the basic workflow of LUWAK in Figure 1 . (Step 1) LUWAK assumes that a user prepares an initial seed set manually. The seed set is shown in the Entity table. (Step 2) A user can send entities in the Entity table to an Expansion API for obtaining entity candidates. (Step 3) LUWAK shows the entity candidates in the Candidate table for user interaction. Then, the user checks accept/reject buttons to update the Entity table. After submitting the judgments, LUWAK shows the Entity table again. The user can directly add, edit, or delete entities in the table at any time. (Step 4) the user can also easily see how these entities stored in the Entity table appear in a document. (Step 5) After repeating the same procedure (Steps 2–4) for a sufficient time, the user can publish the Entity table as an output.",Implementation,"LUWAK is implemented in pure JavaScript code, and it uses the LocalStorage of a web browser. A user does not have to install any packages except for a web browser. The only thing the user must do is to download the LUWAK software and put it in a local directory. We believe the cost of installing the tool will keep away a good amount of potential users. This philosophy follows our empirical feeling of the curse of installing required packages/libraries. Moreover, LUWAK does not need users to consider the usage and maintenance of these additional packages. That is why we are deeply committed to making LUWAK a pure client-side tool in the off-the-shelf style.",LUWAK Dashboard,"LUWAK has a dashboard for quickly viewing an entity dictionary in progress. The dashboard consists of two tables: the Entity table and the Feedback table. The Entity table provides efficient ways to construct and modify an entity dictionary. Figure UID11 shows the screenshot of the Entity table. The table shows entities in the current entity set. Each row corresponds to an entity entry. Each entry has a label, which denotes whether the predefined entity type is a positive or a negative example, an original entity, which was used to find the entity, and the score, which denotes the confidence score. A user can directly edit the table by adding, renaming, and deleting entities. Moreover, the entity inactivation function allows a user to manually inactivate entities, so that entity expansion algorithms do not use the inactivated entities. The table implements a page switching function, a search function, and a sorting function to ensure visibility even when there is a large number of entities in the table.",Entity Candidate Generation,"We design the entity candidate generation module as an external API (Expansion API). The Expansion API receives a set of entities with positive labels. The Expansion API returns top- $k$ entity candidates. As an initial implementation, we used GloVe BIBREF2 as word embedding models for implementing an Expansion API. This API calculates the cosine similarity between a set of positive entities and entities candidates to generate a ranked list. We prepared models trained based on the CommonCrawl corpus and the Twitter corpus. Note that the specification of the expansion algorithm is not limited to the algorithm described in this paper, as LUWAK considers the Expansion API as an external function. Moreover, we also utilize the category-based expansion module, in which we used is-a relationship between the ontological category and each entity and expanded seeds via category-level. For example, if most of the entities already inserted in the dictionary share the same category, such as Programming Languages, the system suggests that ""Programming Language"" entities should be inserted in the dictionary when we develop a job skill name dictionary. Category-based entity expansion is helpful to avoid the candidate entity one by one. We used Yago BIBREF3 as an existing knowledge base. External API. In our design of LUWAK, Expansion APIs are placed as an external function outside LUWAK. There are three reasons why we adopt this design. First, we want LUWAK to remain a corpus-free tool. Users do not have to download any corpora or models to start using LUWAK, and it takes too much time to launch an Expansion API server. Second, LUWAK’s design allows external contributors to build their own expansion APIs that are compatible with LUWAK’s interface. We developed the initial version of the LUWAK package to contain an entity Expansion API so users can launch their expansion APIs internally. Third, the separation between LUWAK and the Expansion APIs enables Expansion APIs to use predetermined options for algorithms, including non-embedding-based methods (e.g., pattern-based methods). We can use more than one entity expansion model to find related entities. For instance, general embedding models, such as those built on Wikipedia, might be a good choice in early iterations, whereas more domain-specific models trained on domain-specific corpora might be helpful in later iterations. LUWAK is flexible to change and use more than one Expansion API. This design encourages us to continuously refine the entity expansion module easily.",Example: Housing Equipment Entity Population,"We show an example of populating house equipment entities using LUWAK for improving a real estate search engine. The preliminary step is to prepare seed entities that belong to the real estate house equipment entity type (e.g., kitchen, bath). In this case, a user is supposed to provide several entities ( $\sim $ 10) as an initial set of the category. LUWAK first asks the user to upload an initial seed set. The user can add, rename, and delete entities on the Entity table as he or she wants. The user can also choose a set of entity expansion models at any time. Figure 2 shows the entity dashboard in this example. When the user submits the current entity set by clicking the Expand Seed Set button (Figure UID11 ), LUWAK sends a request to the external Expansion APIs that are selected to obtain expanded entities. The returned values will be stored in the Feedback table, as Figure UID12 shows. The Feedback table provides a function to capture user feedback intuitively. The user can click the + or - buttons to assign positive or negative labels to the entity candidates. The score column stores the similarity score, which is calculated by the Expansion API as reference information for users. The user can also see how these entities are generated by looking at the original entities in the original column. The original entity information can be used to detect semantic drift. For instance, if the user finds the original entity of some entity candidates has negative labels, the user might consider inactivating the entity to prevent semantic drift. In the next step, the user reflects the feedback by clicking the Submit Feedback button. Then, the user will see the entity dashboard with the newly added entities as shown in Figure UID13 . The user can inactivate the entity by clicking the inactivate button. The user can sort rows by column values to take a brief look at the current entity set. Also, the entity dashboard provides a search function to find an entity for action. The user can also check how entities appear in a test document. As shown in Figure UID14 , LUWAK highlights these entities in the current entity set. After the user is satisfied with the amount of the current entity set in the table, the Export button allows the user to download the entire table, including inactivated entities.",Related Work and Discussion,"Entity population is one of the important practical problems in NLP. Generated entity dictionaries can be used in various applications, including search engines, named entity extraction, and entity linking. Iterative seed expansion is known to be an efficient approach to construct user-specific entity dictionaries. Previous studies have aimed to construct a high-quality entity dictionary from a small number of seed entities BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 . As we stated in ""Entity Candidate Generation"" , LUWAK is flexible with the types of algorithms used for entity population. A user can select any combinations of different methods once the Expansion API of the methods are available. Stanford Pattern-based Information Extraction and Diagnostics (SPIED) BIBREF8 is a pattern-based entity population system. SPIED requires not only an initial seed set but also document collection because it uses the pattern-based approach. After a user inputs initial seed entities, SPIED generates regular expression patterns to find entity candidates from a given document collection. This approach incurs a huge computational cost for calculating the scores of every regular expression pattern and every entity candidate in each iteration. Furthermore, SPIED adopts a bootstrapping approach, which does not involve user feedback for each iteration. This approach can easily result in semantic drift. Interactive Knowledge Extraction BIBREF9 (IKE) is an interactive bootstrapping tool for collecting relation-extraction patterns. IKE also provides a search-based entity extraction function and an embedding-based entity expansion function for entity population. A user can interactively add entity candidates generated by an embedding-based algorithm to an entity dictionary. LUWAK is a more lightweight tool than IKE, which only focuses on the entity population task. LUWAK has numerous features, such as the multiple entity expansion model choices, that are not implemented in IKE. Moreover, LUWAK is a corpus-free tool that does not require a document collection for entity population. Thus, we differentiate LUWAK from IKE, considering it a more lightweight entity population tool.",Summary,"This paper has presented LUWAK, a lightweight front-end tool for interactive entity population. LUWAK provides a set of basic functions such as entity expansion and user feedback assignment. We have implemented LUWAK in pure JavaScript with LocalStorage to make it an installation-free tool. We believe that LUWAK plays an important role in delivering the values of existing entity expansion techniques to potential users including nontechnical people without supposing a large amount of human cost. Moreover, we believe that this design makes it easy to compare performances between interactive entity population pipelines and develop more sophisticated ones.",,,,,,,,,,,,,,,,,,,,,,,,,,,What programming language is the tool written in?,4c1847f0f3e6f9cc6ac3dfbac9e135d34641a854,infinity,familiar,no,,50d8b4a941c26b89482c94ab324b5a274f9ced66,False,,,"LUWAK is implemented in pure JavaScript code, and it uses the LocalStorage of a web browser. A user does not have to install any packages except for a web browser. The only thing the user must do is to download the LUWAK software and put it in a local directory. We believe the cost of installing the tool will keep away a good amount of potential users. This philosophy follows our empirical feeling of the curse of installing required packages/libraries. Moreover, LUWAK does not need users to consider the usage and maintenance of these additional packages. That is why we are deeply committed to making LUWAK a pure client-side tool in the off-the-shelf style.","LUWAK is implemented in pure JavaScript code, and it uses the LocalStorage of a web browser.",45375e320674184ad285335f39380ae3640eb35d,18f4d5a2eb93a969d55361267e74aa0c4f6f82fe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2-Figure1-1.png,Figure 1. Basic workflow of LUWAK.,5-Figure2-1.png,Figure 2. Screenshots of the LUWAK Dashboard: (a) Entity table shows initial seed entities. (b) Feedback table shows entity candidates generated by the initial seed entities. (c) Entity table has added entities after the submission of generated entity candidates. (d) Entity highlighting function visually emphasizes the words in the current entity set.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,JavaScript,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HuggingFace's Transformers: State-of-the-art Natural Language Processing,"Recent advances in modern Natural Language Processing (NLP) research have been dominated by the combination of Transfer Learning methods with large-scale language models, in particular based on the Transformer architecture. With them came a paradigm shift in NLP with the starting point for training a model on a downstream task moving from a blank specific model to a general-purpose pretrained architecture. Still, creating these general-purpose models remains an expensive and time-consuming process restricting the use of these methods to a small sub-set of the wider NLP community. In this paper, we present HuggingFace's Transformers library, a library for state-of-the-art NLP, making these developments available to the community by gathering state-of-the-art general-purpose pretrained models under a unified API together with an ecosystem of libraries, examples, tutorials and scripts targeting many downstream NLP tasks. HuggingFace's Transformers library features carefully crafted model implementations and high-performance pretrained weights for two main deep learning frameworks, PyTorch and TensorFlow, while supporting all the necessary tools to analyze, evaluate and use these models in downstream tasks such as text/token classification, questions answering and language generation among others. The library has gained significant organic traction and adoption among both the researcher and practitioner communities. We are committed at HuggingFace to pursue the efforts to develop this toolkit with the ambition of creating the standard library for building NLP systems. HuggingFace's Transformers library is available at \url{https://github.com/huggingface/transformers}.",Introduction,"In the past 18 months, advances on many Natural Language Processing (NLP) tasks have been dominated by deep learning models and, more specifically, the use of Transfer Learning methods BIBREF0 in which a deep neural network language model is pretrained on a web-scale unlabelled text dataset with a general-purpose training objective before being fine-tuned on various downstream tasks. Following noticeable improvements using Long Short-Term Memory (LSTM) architectures BIBREF1, BIBREF2, a series of works combining Transfer Learning methods with large-scale Transformer architectures BIBREF3 has repeatedly advanced the state-of-the-art on NLP tasks ranging from text classification BIBREF4, language understanding BIBREF5, BIBREF6, BIBREF7, machine translation BIBREF8, and zero-short language generation BIBREF9 up to co-reference resolution BIBREF10 and commonsense inference BIBREF11. While this approach has shown impressive improvements on benchmarks and evaluation metrics, the exponential increase in the size of the pretraining datasets as well as the model sizes BIBREF5, BIBREF12 has made it both difficult and costly for researchers and practitioners with limited computational resources to benefit from these models. For instance, RoBERTa BIBREF5 was trained on 160 GB of text using 1024 32GB V100. On Amazon-Web-Services cloud computing (AWS), such a pretraining would cost approximately 100K USD. Contrary to this trend, the booming research in Machine Learning in general and Natural Language Processing in particular is arguably explained significantly by a strong focus on knowledge sharing and large-scale community efforts resulting in the development of standard libraries, an increased availability of published research code and strong incentives to share state-of-the-art pretrained models. The combination of these factors has lead researchers to reproduce previous results more easily, investigate current approaches and test hypotheses without having to redevelop them first, and focus their efforts on formulating and testing new hypotheses. To bring Transfer Learning methods and large-scale pretrained Transformers back into the realm of these best practices, the authors (and the community of contributors) have developed Transformers, a library for state-of-the art Natural Language Processing with Transfer Learning models. Transformers addresses several key challenges:",Introduction ::: Sharing is caring,"Transformers gathers, in a single place, state-of-the art architectures for both Natural Language Understanding (NLU) and Natural Language Generation (NLG) with model code and a diversity of pretrained weights. This allows a form of training-computation-cost-sharing so that low-resource users can reuse pretrained models without having to train them from scratch. These models are accessed through a simple and unified API that follows a classic NLP pipeline: setting up configuration, processing data with a tokenizer and encoder, and using a model either for training (adaptation in particular) or inference. The model implementations provided in the library follow the original computation graphs and are tested to ensure they match the original author implementations' performances on various benchmarks.",Introduction ::: Easy-access and high-performance,"Transformers was designed with two main goals in mind: (i) be as easy and fast to use as possible and (ii) provide state-of-the-art models with performances as close as possible to the originally reported results. To ensure a low entry barrier, the number of user-facing abstractions to learn was strongly limited and reduced to just three standard classes: configuration, models and tokenizers, which all can be initialized in a simple and unified way by using a common `from_pretrained()` instantiation method.",Introduction ::: Interpretability and diversity,"There is a growing field of study, sometimes referred as BERTology from BERT BIBREF13, concerned with investigating the inner working of large-scale pretrained models and trying to build a science on top of these empirical results. Some examples include BIBREF14, BIBREF15, BIBREF16. Transformers aims at facilitating and increasing the scope of these studies by (i) giving easy access to the inner representations of these models, notably the hidden states, the attention weights or heads importance as defined in BIBREF15 and (ii) providing different models in a unified API to prevent overfitting to a specific architecture (and set of pretrained weights). Moreover, the unified front-end of the library makes it easy to compare the performances of several architectures on a common language understanding benchmark. Transformers notably includes pre-processors and fine-tuning scripts for GLUE BIBREF6, SuperGLUE (BIBREF7) and SQuAD1.1 BIBREF17.",Introduction ::: Pushing best practices forward,"Transformers seeks a balance between sticking to the original authors' code-base for reliability and providing clear and readable implementations featuring best practices in training deep neural networks so that researchers can seamlessly use the code-base to explore new hypothesis derived from these models. To accommodate a large community of practitioners and researchers, the library is deeply compatible with (and actually makes compatible) two major deep learning frameworks: PyTorch BIBREF18 and TensorFlow (from release 2.0) BIBREF19.",Introduction ::: From research to production,"Another essential question is how to make these advances in research available to a wider audience, especially in the industry. Transformers also takes steps towards a smoother transition from research to production. The provided models support TorchScript, a way to create serializable and optimizable models from PyTorch code, and features production code and integration with the TensorFlow Extended framework.",Community,"The development of the Transformers originally steamed from open-sourcing internals tools used at HuggingFace but has seen a huge growth in scope over its ten months of existence as reflected by the successive changes of name of the library: from pytorch-pretrained-bert to pytorch-transformers to, finally, Transformers. A fast-growing and active community of researchers and practitioners has gathered around Transformers. The library has quickly become used both in research and in the industry: at the moment, more than 200 research papers report using the library. Transformers is also included either as a dependency or with a wrapper in several popular NLP frameworks such as Spacy BIBREF20, AllenNLP BIBREF21 or Flair BIBREF22. Transformers is an ongoing effort maintained by the team of engineers and research scientists at HuggingFace, with support from a vibrant community of more than 120 external contributors. We are committed to the twin efforts of developing the library and fostering positive interaction among its community members, with the ambition of creating the standard library for modern deep learning NLP. Transformers is released under the Apache 2.0 license and is available through pip or from source on GitHub. Detailed documentation along with on-boarding tutorials are available on HuggingFace's website.",Library design,"Transformers has been designed around a unified frontend for all the models: parameters and configurations, tokenization, and model inference. These steps reflect the recurring questions that arise when building an NLP pipeline: defining the model architecture, processing the text data and finally, training the model and performing inference in production. In the following section, we'll give an overview of the three base components of the library: configuration, model and tokenization classes. All of the components are compatible with PyTorch and TensorFlow (starting 2.0). For complete details, we refer the reader to the documentation available on https://huggingface.co/transformers/.",Library design ::: Core components,"All the models follow the same philosophy of abstraction enabling a unified API in the library. Configuration - A configuration class instance (usually inheriting from a base class `PretrainedConfig`) stores the model and tokenizer parameters (such as the vocabulary size, the hidden dimensions, dropout rate, etc.). This configuration object can be saved and loaded for reproducibility or simply modified for architecture search. The configuration defines the architecture of the model but also architecture optimizations like the heads to prune. Configurations are agnostic to the deep learning framework used. Tokenizers - A Tokenizer class (inheriting from a base class `PreTrainedTokenizer`) is available for each model. This class stores the vocabulary token-to-index map for the corresponding model and handles the encoding and decoding of input sequences according to the model's tokenization-specific process (ex. Byte-Pair-Encoding, SentencePiece, etc.). Tokenizers are easily modifiable to add user-selected tokens, special tokens (like classification or separation tokens) or resize the vocabulary. Furthermore, Tokenizers implement additional useful features for the users, by offering values to be used with a model; these range from token type indices in the case of sequence classification to maximum length sequence truncating taking into account the added model-specific special tokens (most pretrained Transformers models have a maximum sequence length they can handle, defined during their pretraining step). Tokenizers can be instantiated from existing configurations available through Transformers originating from the pretrained models or created more generally by the user from user-specifications. Model - All models follow the same hierarchy of abstraction: a base class implements the model's computation graph from encoding (projection on the embedding matrix) through the series of self-attention layers and up to the last layer hidden states. The base class is specific to each model and closely follows the original implementation, allowing users to dissect the inner workings of each individual architecture. Additional wrapper classes are built on top of the base class, adding a specific head on top of the base model hidden states. Examples of these heads are language modeling or sequence classification heads. These classes follow similar naming pattern: XXXForSequenceClassification or XXXForMaskedLM where XXX is the name of the model and can be used for adaptation (fine-tuning) or pre-training. All models are available both in PyTorch and TensorFlow (starting 2.0) and offer deep inter-operability between both frameworks. For instance, a model trained in one of frameworks can be saved on drive for the standard library serialization practice and then be reloaded from the saved files in the other framework seamlessly, making it particularly easy to switch from one framework to the other one along the model life-time (training, serving, etc.). Auto classes - In many cases, the architecture to use can be automatically guessed from the shortcut name of the pretrained weights (e.g. `bert-base-cased`). A set of Auto classes provides a unified API that enable very fast switching between different models/configs/tokenizers. There are a total of four high-level abstractions referenced as Auto classes: AutoConfig, AutoTokenizer, AutoModel (for PyTorch) and TFAutoModel (for TensorFlow). These classes automatically instantiate the right configuration, tokenizer or model class instance from the name of the pretrained checkpoints.",Library design ::: Training,"Optimizer - The library provides a few optimization utilities as subclasses of PyTorch `torch.optim.Optimizer` which can be used when training the models. The additional optimizer currently available is the Adam optimizer BIBREF23 with an additional weight decay fix, also known as `AdamW` BIBREF24. Scheduler - Additional learning rate schedulers are also provided as subclasses of PyTorch `torch.optim.lr_scheduler.LambdaLR`, offering various schedules used for transfer learning and transformers models with customizable options including warmup schedules which are relevant when training with Adam.",Experimenting with Transformers,"In this section, we present some of the major tools and examples provided in the library to experiment on a range of downstream Natural Language Understanding and Natural Language Generation tasks.",Experimenting with Transformers ::: Language understanding benchmarks,"The language models provided in Transformers are pretrained with a general purpose training objective, usually a variant of language modeling like standard (sometime called causal) language modeling as used for instance in BIBREF9 or masked language modeling as introduced in BIBREF13. A pretrained model is often evaluated using wide-range language understanding benchmarks. Transformers includes several tools and scripts to evaluate models on GLUE (BIBREF6) and SuperGLUE (BIBREF7). These two benchmarks gather a variety of datasets to evaluate natural language understanding systems. Details of the datasets can be found in the Appendix on page SECREF7. Regarding the machine comprehension tasks, the library feature evaluations on SQuAD1.1 (BIBREF17) and SQuAD2.0 (BIBREF25). Others currently-supported benchmarks include SWAG (BIBREF26), RACE (BIBREF27) and ARC (BIBREF28).",Experimenting with Transformers ::: Language model fine-tuning,"Fine-tuning a language model on a downstream text corpus usually leads to significant gains for tasks on this corpus, in particular when the domain is different (domain adaptation). It also significantly reduces the amount of training data required for fine-tuning on a target task in the target domain. Transformers provides simple scripts to fine-tune models on custom text datasets with the option to add or remove tokens from the vocabulary and several other adaptability features.",Experimenting with Transformers ::: Ecosystem,"Write with Transformer Because Natural Language Processing does not have to be serious and boring, the generative capacities of auto-regressive language models available in Transformers are showcased in an intuitive and playful manner. Built by the authors on top of Transformers, Write with Transformer is an interactive interface that leverages the generative capabilities of pretrained architectures like GPT, GPT2 and XLNet to suggest text like an auto-completion plugin. Generating samples is also often used to qualitatively (and subjectively) evaluate the generation quality of language models BIBREF9. Given the impact of the decoding algorithm (top-K sampling, beam-search, etc.) on generation quality BIBREF29, Write with Transformer offers various options to dynamically tweak the decoding algorithm and investigate the resulting samples from the model. Conversational AI HuggingFace has been using Transfer Learning with Transformer-based models for end-to-end Natural language understanding and text generation in its conversational agent, Talking Dog. The company also demonstrated in fall 2018 that this approach can be used to reach state-of-the-art performances on academic benchmarks, topping by a significant margin the automatic metrics leaderboard of the Conversational Intelligence Challenge 2 held at the Thirty-second Annual Conference on Neural Information Processing Systems (NIPS 2018). The approach used to reach these performances is described in BIBREF30, BIBREF31 and the code and pretrained models, based on the Transformers library, are available online. Using in production To facilitate the transition from research to production, all the models in the library are compatible with TorchScript, an intermediate representation of a PyTorch model that can then be run either in Python in a more efficient way, or in a high-performance environment such as C++. Fine-tuned models can thus be exported to production-friendly environment. Optimizing large machine learning models for production is an ongoing effort in the community and there are many current engineering efforts towards that goal. The distillation of large models (e.g. DistilBERT BIBREF32) is one of the most promising directions. It lets users of Transformers run more efficient versions of the models, even with strong latency constraints and on inexpensive CPU servers. We also convert Transformers models to Core ML weights that are suitable to be embbeded inside a mobile application, to enable on-the-edge machine learning. Code is also made available. Community Many libraries in NLP and Machine Learning have been created on top of Transformers or have integrated Transformers as a package dependency or through wrappers. At the time of writing, the authors have been mostly aware of FastBert, FARM, flair BIBREF22, BIBREF33, AllenNLP BIBREF21 and PyText but there are likely more interesting developments to be found, from research and internal projects to production packages.",Architectures,"Here is a list of architectures for which reference implementations and pretrained weights are currently provided in Transformers. These models fall into two main categories: generative models (GPT, GPT-2, Transformer-XL, XLNet, XLM) and models for language understanding (Bert, DistilBert, RoBERTa, XLM). BERT (BIBREF13) is a bi-directional Transformer-based encoder pretrained with a linear combination of masked language modeling and next sentence prediction objectives. RoBERTa (BIBREF5) is a replication study of BERT which showed that carefully tuning hyper-parameters and training data size lead to significantly improved results on language understanding. DistilBERT (BIBREF32) is a smaller, faster, cheaper and lighter version BERT pretrained with knowledge distillation. GPT (BIBREF34) and GPT2 (BIBREF9) are two large auto-regressive language models pretrained with language modeling. GPT2 showcased zero-shot task transfer capabilities on various tasks such as machine translation or reading comprehension. Transformer-XL (BIBREF35) introduces architectural modifications enabling Transformers to learn dependency beyond a fixed length without disrupting temporal coherence via segment-level recurrence and relative positional encoding schemes. XLNet (BIBREF4) builds upon Transformer-XL and proposes an auto-regressive pretraining scheme combining BERT's bi-directional context flow with auto-regressive language modeling by maximizing the expected likelihood over permutations of the word sequence. XLM (BIBREF8) shows the effectiveness of pretrained representations for cross-lingual language modeling (both on monolingual data and parallel data) and cross-lingual language understanding. We systematically release the model with the corresponding pretraining heads (language modeling, next sentence prediction for BERT) for adaptation using the pretraining objectives. Some models fine-tuned on downstream tasks such as SQuAD1.1 are also available. Overall, more than 30 pretrained weights are provided through the library including more than 10 models pretrained in languages other than English. Some of these non-English pretrained models are multi-lingual models (with two of them being trained on more than 100 languages) .",Related work,"The design of Transformers was inspired by earlier libraries on transformers and Natural Language Processing. More precisely, organizing the modules around three main components (configuration, tokenizers and models) was inspired by the design of the tensor2tensor library BIBREF36 and the original code repository of Bert BIBREF13 from Google Research while concept of providing easy caching for pretrained models steamed from features of the AllenNLP library BIBREF21 open-sourced by the Allen Institute for Artificial Intelligence (AI2). Works related to the Transformers library can be generally organized along three directions, at the intersection of which stands the present library. The first direction includes Natural Language Processing libraries such as AllenNLP BIBREF21, SpaCy BIBREF20, flair BIBREF22, BIBREF33 or PyText. These libraries precede Transformers and target somewhat different use-cases, for instance those with a particular focus on research for AllenNLP or a strong attention to production constrains (in particular with a carefully tuned balance between speed and performance) for SpaCy. The previously mentioned libraries have now been provided with integrations for Transformers, through a direct package dependency for AllenNLP, flair or PyText or through a wrapper called spacy-transformers for SpaCy. The second direction concerns lower-level deep-learning frameworks like PyTorch BIBREF18 and TensorFlow BIBREF19 which have both been extended with model sharing capabilities or hubs, respectively called TensorFlow Hub and PyTorch Hub. These hubs are more general and while they offer ways to share models they differ from the present library in several ways. In particular, they provide neither a unified API across models nor standardized ways to access the internals of the models. Targeting a more general machine-learning community, these Hubs lack the NLP-specific user-facing features provided by Transformers like tokenizers, dedicated processing scripts for common downstream tasks and sensible default hyper-parameters for high performance on a range of language understanding and generation tasks. The last direction is related to machine learning research frameworks that are specifically used to test, develop and train architectures like Transformers. Typical examples are the tensor2tensor library BIBREF36, fairseq BIBREF37 and Megatron-LM. These libraries are usually not provided with the user-facing features that allow easy download, caching, fine-tuning of the models as well as seamless transition to production.",Conclusion,"We have presented the design and the main components of Transformers, a library for state-of-the-art NLP. Its capabilities, performances and unified API make it easy for both practitioners and researchers to access various large-scale language models, build and experiment on top of them and use them in downstream task with state-of-the-art performance. The library has gained significant organic traction since its original release and has become widely adopted among researchers and practitioners, fostering an active community of contributors and an ecosystem of libraries building on top of the provided tools. We are committed to supporting this community and making recent developments in transfer learning for NLP both accessible and easy to use while maintaining high standards of software engineering and machine learning engineering.",GLUE and SuperGLUE,"The datasets in GLUE are: CoLA (BIBREF54), Stanford Sentiment Treebank (SST) (BIBREF53), Microsoft Research Paragraph Corpus (MRPC) BIBREF44, Semantic Textual Similarity Benchmark (STS) BIBREF38, Quora Question Pairs (QQP) BIBREF46, Multi-Genre NLI (MNLI) BIBREF55, Question NLI (QNLI) BIBREF17, Recognizing Textual Entailment (RTE) BIBREF42, BIBREF39, BIBREF45, BIBREF40 and Winograd NLI (WNLI) BIBREF48. The datasets in SuperGLUE are: Boolean Questions (BoolQ) BIBREF41, CommitmentBank (CB) BIBREF43, Choice of Plausible Alternatives (COPA) BIBREF51, Multi-Sentence Reading Comprehension (MultiRC) BIBREF47, Reading Comprehension with Commonsense Reasoning Dataset (ReCoRD) BIBREF56, Word-in-Context (WiC) BIBREF49, Winograd Schema Challenge (WSC) BIBREF52, Diverse Natural Language Inference Collection (DNC) BIBREF50, Recognizing Textual Entailment (RTE) BIBREF42, BIBREF39, BIBREF45, BIBREF40 and Winograd NLI (WNLI) BIBREF48",,,,,,,What state-of-the-art general-purpose pretrained models are made available under the unified API? ,ec62c4cdbeaafc875c695f2d4415bce285015763,infinity,familiar,no,,fa716cd87ce6fd6905e2f23f09b262e90413167f,False,,,"Here is a list of architectures for which reference implementations and pretrained weights are currently provided in Transformers. These models fall into two main categories: generative models (GPT, GPT-2, Transformer-XL, XLNet, XLM) and models for language understanding (Bert, DistilBert, RoBERTa, XLM). BERT (BIBREF13) is a bi-directional Transformer-based encoder pretrained with a linear combination of masked language modeling and next sentence prediction objectives. RoBERTa (BIBREF5) is a replication study of BERT which showed that carefully tuning hyper-parameters and training data size lead to significantly improved results on language understanding. DistilBERT (BIBREF32) is a smaller, faster, cheaper and lighter version BERT pretrained with knowledge distillation. GPT (BIBREF34) and GPT2 (BIBREF9) are two large auto-regressive language models pretrained with language modeling. GPT2 showcased zero-shot task transfer capabilities on various tasks such as machine translation or reading comprehension. Transformer-XL (BIBREF35) introduces architectural modifications enabling Transformers to learn dependency beyond a fixed length without disrupting temporal coherence via segment-level recurrence and relative positional encoding schemes. XLNet (BIBREF4) builds upon Transformer-XL and proposes an auto-regressive pretraining scheme combining BERT's bi-directional context flow with auto-regressive language modeling by maximizing the expected likelihood over permutations of the word sequence. XLM (BIBREF8) shows the effectiveness of pretrained representations for cross-lingual language modeling (both on monolingual data and parallel data) and cross-lingual language understanding. We systematically release the model with the corresponding pretraining heads (language modeling, next sentence prediction for BERT) for adaptation using the pretraining objectives. Some models fine-tuned on downstream tasks such as SQuAD1.1 are also available. Overall, more than 30 pretrained weights are provided through the library including more than 10 models pretrained in languages other than English. Some of these non-English pretrained models are multi-lingual models (with two of them being trained on more than 100 languages) .","Here is a list of architectures for which reference implementations and pretrained weights are currently provided in Transformers. These models fall into two main categories: generative models (GPT, GPT-2, Transformer-XL, XLNet, XLM) and models for language understanding (Bert, DistilBert, RoBERTa, XLM).

BERT (BIBREF13) is a bi-directional Transformer-based encoder pretrained with a linear combination of masked language modeling and next sentence prediction objectives.

RoBERTa (BIBREF5) is a replication study of BERT which showed that carefully tuning hyper-parameters and training data size lead to significantly improved results on language understanding.

DistilBERT (BIBREF32) is a smaller, faster, cheaper and lighter version BERT pretrained with knowledge distillation.

GPT (BIBREF34) and GPT2 (BIBREF9) are two large auto-regressive language models pretrained with language modeling. GPT2 showcased zero-shot task transfer capabilities on various tasks such as machine translation or reading comprehension.

Transformer-XL (BIBREF35) introduces architectural modifications enabling Transformers to learn dependency beyond a fixed length without disrupting temporal coherence via segment-level recurrence and relative positional encoding schemes.

XLNet (BIBREF4) builds upon Transformer-XL and proposes an auto-regressive pretraining scheme combining BERT's bi-directional context flow with auto-regressive language modeling by maximizing the expected likelihood over permutations of the word sequence.

XLM (BIBREF8) shows the effectiveness of pretrained representations for cross-lingual language modeling (both on monolingual data and parallel data) and cross-lingual language understanding.

We systematically release the model with the corresponding pretraining heads (language modeling, next sentence prediction for BERT) for adaptation using the pretraining objectives. Some models fine-tuned on downstream tasks such as SQuAD1.1 are also available. Overall, more than 30 pretrained weights are provided through the library including more than 10 models pretrained in languages other than English. Some of these non-English pretrained models are multi-lingual models (with two of them being trained on more than 100 languages) .",e201f141333e2e89ea82e5bd16023c0ea3346e82,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,6-Figure1-1.png,Figure 1: Write With Transformer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,BERT RoBERTa DistilBERT GPT GPT2 Transformer-XL XLNet XLM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Towards Neural Theorem Proving at Scale,"Neural models combining representation learning and reasoning in an end-to-end trainable manner are receiving increasing interest. However, their use is severely limited by their computational complexity, which renders them unusable on real world datasets. We focus on the Neural Theorem Prover (NTP) model proposed by Rockt{\""{a}}schel and Riedel (2017), a continuous relaxation of the Prolog backward chaining algorithm where unification between terms is replaced by the similarity between their embedding representations. For answering a given query, this model needs to consider all possible proof paths, and then aggregate results - this quickly becomes infeasible even for small Knowledge Bases (KBs). We observe that we can accurately approximate the inference process in this model by considering only proof paths associated with the highest proof scores. This enables inference and learning on previously impracticable KBs.",Introduction,"Recent advancements in deep learning intensified the long-standing interests in integrating symbolic reasoning with connectionist models BIBREF1 , BIBREF2 , BIBREF3 , BIBREF4 . The attraction of said integration stems from the complementing properties of these systems. Symbolic reasoning models offer interpretability, efficient generalisation from a small number of examples, and the ability to leverage knowledge provided by an expert. However, these systems are unable to handle ambiguous and noisy high-dimensional data such as sensory inputs BIBREF5 . On the other hand, representation learning models exhibit robustness to noise and ambiguity, can learn task-specific representations, and achieve state-of-the-art results on a wide variety of tasks BIBREF6 . However, being universal function approximators, these models require vast amounts of training data and are treated as non-interpretable black boxes. One way of integrating the symbolic and sub-symbolic models is by continuously relaxing discrete operations and implementing them in a connectionist framework. Recent approaches in this direction focused on learning algorithmic behaviour without the explicit symbolic representations of a program BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 , and consequently with it BIBREF12 , BIBREF13 , BIBREF14 , BIBREF15 . In the inductive logic programming setting, two new models, NTP BIBREF0 and Differentiable Inductive Logic Programming ( $\partial $ ILP) BIBREF16 successfully combined the interpretability and data efficiency of a logic programming system with the expressiveness and robustness of neural networks. In this paper, we focus on the NTP model proposed by BIBREF0 . Akin to recent neural-symbolic models, NTP rely on a continuous relaxation of a discrete algorithm, operating over the sub-symbolic representations. In this case, the algorithm is an analogue to Prolog's backward chaining with a relaxed unification operator. The backward chaining algorithm constructs neural networks, which model continuously relaxed proof paths using sub-symbolic representations. These representations are learned end-to-end by maximising the proof scores of facts in the KB, while minimising the score of facts not in the KB, in a link prediction setting BIBREF17 . However, while the symbolic unification checks whether two terms can represent the same structure, the relaxed unification measures the similarity between their sub-symbolic representations. This continuous relaxation is at the crux of NTP' inability to scale to large datasets. During both training and inference, NTP need to compute all possible proof trees needed for proving a query, relying on the continuous unification of the query with all the rules and facts in the KB. This procedure quickly becomes infeasible for large datasets, as the number of nodes of the resulting computation graph grows exponentially. Our insight is that we can radically reduce the computational complexity of inference and learning by generating only the most promising proof paths. In particular, we show that the problem of finding the facts in the KB that best explain a query can be reduced to a $k$ -nearest neighbour problem, for which efficient exact and approximate solutions exist BIBREF18 . This enables us to apply NTP to previously unreachable real-world datasets, such as WordNet.",Background,"In NTP, the neural network structure is built recursively, and its construction is defined in terms of modules similarly to dynamic neural module networks BIBREF19 . Each module, given a goal, a KB, and a current proof state as inputs, produces a list of new proof states, where the proof states are neural networks representing partial proof success scores. Unification Module. In backward chaining, unification between two atoms is used for checking whether they can represent the same structure. In discrete unification, non-variable symbols are checked for equality, and the proof fails if the symbols differ. In NTP, rather than comparing symbols, their embedding representations are compared by means of a RBF kernel. This allows matching different symbols with similar semantics, such as matching relations like ${grandFatherOf}$ and ${grandpaOf}$ . Given a proof state $= (_, _)$ , where $_$ and $_$ denote a substitution set and a proof score, respectively, unification is computed as follows:  1. unify(, , ) = 2. unify(, G, ) = 3. unify(H, , ) = 4. unify(h::H, g::G, ) = unify(H,G,')  with ' = (', ') where:  '= {ll {h/g} if hV {g/h} if gV, hV  otherwise } '= ( , { ll k(h:, g:) if hV, gV 1 otherwise } ) where $_{h:}$ and $_{g:}$ denote the embedding representations of $h$ and $g$ , respectively. OR Module. This module attempts to apply rules in a KB. The name of this module stems from the fact that a KB can be seen as a large disjunction of rules and facts. In backward chaining reasoning systems, the OR module is used for unifying a goal with all facts and rules in a KB: if the goal unifies with the head of the rule, then a series of goals is derived from the body of such a rule. In NTP, we calculate the similarity between the rule and the facts via the unify operator. Upon calculating the continuous unification scores, OR calls AND to prove all sub-goals in the body of the rule.  or(G, d, ) = ' | ' and(B, d, unify(H, G, )),  H :– B  AND Module. This module is used for proving a conjunction of sub-goals derived from a rule body. It first applies substitutions to the first atom, which is afterwards proven by calling the OR module. Remaining sub-goals are proven by recursively calling the AND module.  1. and(_, _, ) = 2. and(_, 0, _) = 3. and(, _, ) = 4. and(G:G, d, ) = ” | ”and(G, d, '),  ' or(substitute(G, ), d-1, )  For further details on NTPs and the particular implementation of these modules, see BIBREF0  After building all the proof states, NTPs define the final success score of proving a query as an $$ over all the generated valid proof scores (neural networks). Assume a KB $\mathcal {K}$ , composed of $|\mathcal {K}|$ facts and no rules, for brevity. Note that $|\mathcal {K}|$ can be impractical within the scope of NTP. For instance, Freebase BIBREF20 is composed of approximately 637 million facts, while YAGO3 BIBREF21 is composed by approximately 9 million facts. Given a query $g \triangleq [{grandpaOf}, {abe}, {bart}]$ , NTP compares its embedding representation – given by the embedding vectors of ${grandpaOf}$ , ${abe}$ , and ${bart}$ – with the representation of each of the $|\mathcal {K}|$ facts. The resulting proof score of $g$ is given by:  $$ 
\begin{aligned}
\max _{f \in \mathcal {K}} & \; {unify}_(g, [f_{p}, f_{s}, f_{o}], (\emptyset , )) \\
& = \max _{f \in \mathcal {K}} \; \min \big \lbrace 
,
\operatorname{k}(_{\scriptsize {grandpaOf}:}, _{f_{p}:}),\\
&\qquad \qquad \qquad \operatorname{k}(_{{abe}:}, _{f_{s}:}),
\operatorname{k}(_{{bart}:}, _{f_{o}:})
\big \rbrace ,
\end{aligned}$$   (Eq. 3)  where $f \triangleq [f_{p}, f_{s}, f_{o}]$ is a fact in $\mathcal {K}$ denoting a relationship of type $f_{p}$ between $f_{s}$ and $f_{o}$ , $_{s:}$ is the embedding representation of a symbol $s$ , $$ denotes the initial proof score, and $\operatorname{k}({}\cdot {}, {}\cdot {})$ denotes the RBF kernel. Note that the maximum proof score is given by the fact $f \in \mathcal {K}$ that maximises the similarity between its components and the goal $\mathcal {K}$0 : solving the maximisation problem in eq:inference can be equivalently stated as a nearest neighbour search problem. In this work, we use ANNS during the forward pass for considering only the most promising proof paths during the construction of the neural network.",Nearest Neighbourhood Search,"From ex:inference, we can see that the inference problem can be reduced to a nearest neighbour search problem. Given a query $g$ , the problem is finding the fact(s) in $\mathcal {K}$ that maximise the unification score. This represents a computational bottleneck, since it is very costly to find the exact nearest neighbour in high-dimensional Euclidean spaces, due to the curse of dimensionality BIBREF22 . Exact methods are rarely more efficient than brute-force linear scan methods when the dimensionality is high BIBREF23 , BIBREF24 . A practical solution consists in ANNS algorithms, which relax the condition of the exact search by allowing a small number of mistakes. Several families of ANNS algorithms exist, such as LSH BIBREF25 , PQ BIBREF26 , and PG BIBREF27 . In this work we use HNSW BIBREF24 , BIBREF28 , a graph-based incremental ANNS structure which can offer much better logarithmic complexity scaling in comparison with other approaches.",Related Work,"Many machine learning methods rely on efficient nearest neighbour search for solving specific sub-problems. Given the computational complexity of nearest neighbour search, approximate methods, driven by advanced index structures, hash or even graph-based approaches are used to speed up the bottleneck of costly comparison. ANNS algorithms have been used to speed up various sorts of machine learning models, including mixture model clustering BIBREF29 , case-based reasoning BIBREF30 to Gaussian process regression BIBREF31 , among others. Similarly to this work, BIBREF32 also rely on approximate nearest neighbours to speed up Memory-Augmented neural networks. Similarly to our work, they apply ANNS to query the external memory (in our case the KB memory) for $k$ closest words. They present drastic savings in speed and memory usage. Though as of this moment, our speed savings are not as drastic, the memory savings we achieve are sufficient so that we can train on WordNet, a dataset previously considered out of reach of NTP.",Experiments,"We compared results obtained by our model, which we refer to as NTP 2.0, with those obtained by the original NTP proposed by BIBREF0 . Results on several smaller datasets – namely Countries, Nations, Kinship, and UMLS – are shown in tab:results. When unifying goals with facts in the KB, for each goal, we use ANNS for retrieving the $k$ most similar (in embedding space) facts, and use those for computing the final proof scores. We report results for $k = 1$ , as we did not notice sensible differences for $k \in \lbrace  2, 5, 10 \rbrace $ . However, we noticed sensible improvements in the case of Countries, and an overall decrease in performance in UMLS. A possible explanation is that ANNS (with $k = 1$ ), due to its inherently approximate nature, does not always retrieve the closest fact(s) exactly. This behaviour may be a problem in some datasets where exact nearest neighbour search is crucial for correctly answering queries. We also evaluated NTP 2.0 on WordNet BIBREF33 , a KB encoding lexical knowledge about the English language. In particular, we use the WordNet used by BIBREF34 for their experiments. This dataset is significantly larger than the other datasets used by BIBREF0 – it is composed by 38.696 entities, 11 relations, and the training set is composed by 112,581 facts. In WordNet, the accuracies on the validation and test sets were 65.29% and 65.72%, respectively – which is on par with the Distance Model, a Neural Link Predictor discussed by BIBREF34 , which achieves a test accuracy of 68.3%. However, we did not consider a full hyper-parameter sweep, and did not regularise the model using Neural Link Predictors, which sensibly improves NTP' predictive accuracy BIBREF0 . A subset of the induced rules is shown in tab:rules.",Conclusions,"We proposed a way to sensibly scale up NTP by reducing parts of their inference steps to ANNS problems, for which very efficient and scalable solutions exist in the literature.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,How are proof scores calculated?,848ab388703c24faad79d83d254e4fd88ab27e2a,two,unfamiliar,no,,50d8b4a941c26b89482c94ab324b5a274f9ced66,False,,,"Unification Module. In backward chaining, unification between two atoms is used for checking whether they can represent the same structure. In discrete unification, non-variable symbols are checked for equality, and the proof fails if the symbols differ. In NTP, rather than comparing symbols, their embedding representations are compared by means of a RBF kernel. This allows matching different symbols with similar semantics, such as matching relations like ${grandFatherOf}$ and ${grandpaOf}$ . Given a proof state $= (_, _)$ , where $_$ and $_$ denote a substitution set and a proof score, respectively, unification is computed as follows: The resulting proof score of $g$ is given by: $$ \begin{aligned} \max _{f \in \mathcal {K}} & \; {unify}_(g, [f_{p}, f_{s}, f_{o}], (\emptyset , )) \\ & = \max _{f \in \mathcal {K}} \; \min \big \lbrace , \operatorname{k}(_{\scriptsize {grandpaOf}:}, _{f_{p}:}),\\ &\qquad \qquad \qquad \operatorname{k}(_{{abe}:}, _{f_{s}:}), \operatorname{k}(_{{bart}:}, _{f_{o}:}) \big \rbrace , \end{aligned}$$ (Eq. 3) where $f \triangleq [f_{p}, f_{s}, f_{o}]$ is a fact in $\mathcal {K}$ denoting a relationship of type $f_{p}$ between $f_{s}$ and $f_{o}$ , $_{s:}$ is the embedding representation of a symbol $s$ , $$ denotes the initial proof score, and $\operatorname{k}({}\cdot {}, {}\cdot {})$ denotes the RBF kernel. Note that the maximum proof score is given by the fact $f \in \mathcal {K}$ that maximises the similarity between its components and the goal $\mathcal {K}$0 : solving the maximisation problem in eq:inference can be equivalently stated as a nearest neighbour search problem. In this work, we use ANNS during the forward pass for considering only the most promising proof paths during the construction of the neural network.","Given a proof state $= (_, _)$ , where $_$ and $_$ denote a substitution set and a proof score, respectively, unification is computed as follows: The resulting proof score of $g$ is given by:

$$ \begin{aligned} \max _{f \in \mathcal {K}} & \; {unify}_(g, [f_{p}, f_{s}, f_{o}], (\emptyset , )) \\ & = \max _{f \in \mathcal {K}} \; \min \big \lbrace , \operatorname{k}(_{\scriptsize {grandpaOf}:}, _{f_{p}:}),\\ &\qquad \qquad \qquad \operatorname{k}(_{{abe}:}, _{f_{s}:}), \operatorname{k}(_{{bart}:}, _{f_{o}:}) \big \rbrace , \end{aligned}$$ (Eq. 3)

where $f \triangleq [f_{p}, f_{s}, f_{o}]$ is a fact in $\mathcal {K}$ denoting a relationship of type $f_{p}$ between $f_{s}$ and $f_{o}$ , $_{s:}$ is the embedding representation of a symbol $s$ , $$ denotes the initial proof score, and $\operatorname{k}({}\cdot {}, {}\cdot {})$ denotes the RBF kernel.",38422bb256daf719d511514a73c66a26a115e80c,74eea9f3f4f790836045fcc75d0b3f5156901499,,,,,,,,,What are proof paths?,68794289ed6078b49760dc5fdf88618290e94993,two,unfamiliar,no,,50d8b4a941c26b89482c94ab324b5a274f9ced66,False,,A sequence of logical statements represented in a computational graph,"FLOAT SELECTED: Figure 1. A visual depiction of the NTP’ recursive computation graph construction, applied to a toy KB (top left). Dash-separated rectangles denote proof states (left: substitutions, right: proof score -generating neural network). All the non-FAIL proof states are aggregated to obtain the final proof success (depicted in Figure 2). Colours and indices on arrows correspond to the respective KB rule application.","FLOAT SELECTED: Figure 1. A visual depiction of the NTP’ recursive computation graph construction, applied to a toy KB (top left). Dash-separated rectangles denote proof states (left: substitutions, right: proof score -generating neural network). All the non-FAIL proof states are aggregated to obtain the final proof success (depicted in Figure 2). Colours and indices on arrows correspond to the respective KB rule application.",42a5795acc2d8246ff67dce657c9c026ad6ba9db,74eea9f3f4f790836045fcc75d0b3f5156901499,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2-Figure1-1.png,"Figure 1. A visual depiction of the NTP’ recursive computation graph construction, applied to a toy KB (top left). Dash-separated rectangles denote proof states (left: substitutions, right: proof score -generating neural network). All the non-FAIL proof states are aggregated to obtain the final proof success (depicted in Figure 2). Colours and indices on arrows correspond to the respective KB rule application.",3-Figure2-1.png,Figure 2. Depiction of the proof aggregation for the computation graph presented in Figure 1. Proof states resulting from the computation graph construction are all aggregated to obtain the final success score of proving a query.,4-Table1-1.png,"Table 1. AUC-PR results on Countries and MRR and HITS@m on Kinship, Nations, and UMLS.",4-Table2-1.png,"Table 2. Rules induced on WordNet, with a confidence above 0.5.",,,,,,,,,,,,,,,,,,,,,,,,,,"'= ( , { ll k(h:, g:) if hV, gV

1 otherwise } )

where $_{h:}$ and $_{g:}$ denote the embedding representations of $h$ and $g$ , respectively.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Weakly Supervised Domain Detection,"In this paper we introduce domain detection as a new natural language processing task. We argue that the ability to detect textual segments which are domain-heavy, i.e., sentences or phrases which are representative of and provide evidence for a given domain could enhance the robustness and portability of various text classification applications. We propose an encoder-detector framework for domain detection and bootstrap classifiers with multiple instance learning (MIL). The model is hierarchically organized and suited to multilabel classification. We demonstrate that despite learning with minimal supervision, our model can be applied to text spans of different granularities, languages, and genres. We also showcase the potential of domain detection for text summarization.",Introduction,"Text classification is a fundamental task in Natural Language processing which has been found useful in a wide spectrum of applications ranging from search engines enabling users to identify content on websites, sentiment and social media analysis, customer relationship management systems, and spam detection. Over the past several years, text classification has been predominantly modeled as a supervised learning problem (e.g., BIBREF0 , BIBREF1 , BIBREF2 ) for which appropriately labeled data must be collected. Such data is often domain-dependent (i.e., covering specific topics such as those relating to “Business” or “Medicine”) and a classifier trained using data from one domain is likely to perform poorly on another. For example, the phrase “the mouse died quickly” may indicate negative sentiment in a customer review describing the hand-held pointing device or positive sentiment when describing a laboratory experiment performed on a rodent. The ability to handle a wide variety of domains has become more pertinent with the rise of data-hungry machine learning techniques like neural networks and their application to a plethora of textual media ranging from news articles to twitter, blog posts, medical journals, Reddit comments, and parliamentary debates BIBREF0 , BIBREF3 , BIBREF4 , BIBREF5 . The question of how to best deal with multiple domains when training data is available for one or few of them has met with much interest in the literature. The field of domain adaptation BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 aims at improving the learning of a predictive function in a target domain where there is little or no labeled data, using knowledge transferred from a source domain where sufficient labeled data is available. Another line of work BIBREF11 , BIBREF12 , BIBREF13 assumes that labeled data may exist for multiple domains, but in insufficient amounts to train classifiers for one or more of them. The aim of multi-domain text classification is to leverage all the available resources in order to improve system performance across domains simultaneously. In this paper we investigate the question of how domain-specific data might be obtained in order to enable the development of text classification tools as well as more domain aware applications such as summarization, question answering, and information extraction. We refer to this task as domain detection and assume a fairly common setting where the domains of a corpus collection are known and the aim is to identify textual segments which are domain-heavy, i.e., documents, sentences, or phrases providing evidence for a given domain. Domain detection can be formulated as a multilabel classification problem, where a model is trained to recognize domain evidence at the sentence-, phrase-, or word-level. By definition then, domain detection would require training data with fine-grained domain labels, thereby increasing the annotation burden; we must provide labels for training domain detectors and for modeling the task we care about in the first place. In this paper we consider the problem of fine-grained domain detection from the perspective of Multiple Instance Learning (MIL; BIBREF14 ) and develop domain models with very little human involvement. Instead of learning from individually labeled segments, our model only requires document-level supervision and optionally prior domain knowledge and learns to introspectively judge the domain of constituent segments. Importantly, we do not require document-level domain annotations either since we obtain these via distant supervision by leveraging information drawn from Wikipedia. Our domain detection framework comprises two neural network modules; an encoder learns representations for words and sentences together with prior domain information if the latter is available (e.g., domain definitions), while a detector generates domain-specific scores for words, sentences, and documents. We obtain a segment-level domain predictor which is trained end-to-end on document-level labels using a hierarchical, attention-based neural architecture BIBREF15 . We conduct domain detection experiments on English and Chinese and measure system performance using both automatic and human-based evaluation. Experimental results show that our model outperforms several strong baselines and is robust across languages and text genres, despite learning from weak supervision. We also showcase our model's application potential for text summarization. Our contributions in this work are threefold; we propose domain detection, as a new fine-grained multilabel learning problem which we argue would benefit the development of domain aware NLP tools; we introduce a weakly supervised encoder-detector model within the context of multiple instance learning; and demonstrate that it can be applied across languages and text genres without modification.",Related Work,"Our work lies at the intersection of multiple research areas, including domain adaptation, representation learning, multiple instance learning, and topic modeling. We review related work below.",Problem Formulation,"We formulate domain detection as a multilabel learning problem. Our model is trained on samples of document-label pairs. Each document consists of INLINEFORM0 sentences INLINEFORM1 and is associated with discrete labels INLINEFORM2 . In this work, domain labels are not annotated manually but extrapolated from Wikipedia (see Section SECREF6 for details). In a non-MIL framework, a model typically learns to predict document labels by directly conditioning on its sentence representations INLINEFORM3 or their aggregate. In contrast, INLINEFORM4 under MIL is a learned function INLINEFORM5 of latent instance-level labels, i.e., INLINEFORM6 . A MIL classifier will therefore first produce domain scores for all instances (aka sentences), and then learn to integrate instance scores into a bag (aka document) prediction. In this paper we further assume that the instance-bag relation applies to sentences and documents but also to words and sentences. In addition, we incorporate prior domain information to facilitate learning in a weakly supervised setting: each domain is associated with a definition INLINEFORM0 , i.e., a few sentences providing a high-level description of the domain at hand. For example, the definition of the “Lifestyle” domain is “the interests, opinions, behaviors, and behavioral orientations of an individual, group, or culture”. Figure FIGREF5 provides an overview of our Domain Detection Network, which we call DetNet. The model comprises two modules; an encoder learns representations for words and sentences whilst incorporating prior domain information; a detector generates domain scores for words, sentences, and documents by selectively attending to previously encoded information. We describe the two modules in more detail below.",The Encoder Module,"We learn representations for words and sentences using identical encoders with separate learning parameters. Given a document, the two encoders implement the following steps: INLINEFORM0   For each sentence INLINEFORM0 , the word-level encoder yields contextualized word representations INLINEFORM1 and their attention weights INLINEFORM2 . Sentence embeddings INLINEFORM3 are obtained via weighted averaging and then provided as input to the sentence-level encoder which outputs contextualized representations INLINEFORM4 and their attention weights INLINEFORM5 . In this work we aim to model fairly long documents (e.g., Wikipedia articles; see Section SECREF6 for details). For this reason, our encoder builds on the Transformer architecture BIBREF15 , a recently proposed highly efficient model which has achieved state-of-the-art performance in machine translation BIBREF15 and question answering BIBREF35 . The Transformer aims at reducing the fundamental constraint of sequential computation which underlies most architectures based on recurrent neural networks. It eliminates recurrence in favor of applying a self-attention mechanism which directly models relationships between all words in a sentence, regardless of their position.",The Detector Module,"DetNet adopts three detectors corresponding to words, sentences, and documents: INLINEFORM0   WordDet first produces word domain scores using both lexical semantic information INLINEFORM0 and prior (domain) knowledge INLINEFORM1 ; SentDet yields domain scores for sentences while integrating downstream instance signals INLINEFORM2 and sentence semantics INLINEFORM3 ; finally, DocDet makes the final document-level predictions based on sentence scores.",Experimental Setup,[t] Document Generation Input: INLINEFORM0 : Label combinations  INLINEFORM0 : Sentence subcorpora  INLINEFORM0 : Maximum document length Output: A synthetic document [0] Generate INLINEFORM0 Generate a document domain set INLINEFORM1 INLINEFORM2   INLINEFORM0 Number of domain labels Generate a noisy domain INLINEFORM1 INLINEFORM2   INLINEFORM0 ; A set of candidate domain sets  INLINEFORM0 INLINEFORM1 INLINEFORM2   INLINEFORM0 Number of unused labels INLINEFORM1 Number of sentence blocks INLINEFORM2 For generated sentences  INLINEFORM0 INLINEFORM1 Generate INLINEFORM2 Generate INLINEFORM3 sentences INLINEFORM4 INLINEFORM5 INLINEFORM6 INLINEFORM7 INLINEFORM8 Shuffle INLINEFORM9 INLINEFORM10 ,Automatic Evaluation,"In this section we present the results of our automatic evaluation for sentence and document predictions. Problematically, for sentence predictions we do not have gold-standard domain labels (we have only extrapolated these from Wikipedia for documents). We therefore developed an automatic approach for creating silver standard domain labels which we describe below.",Human Evaluation,"Aside from automatic evaluation, we also assessed model performance against human elicited domain labels for sentences and words. The purpose of this experiment was threefold: (a) to validate the results obtained from automatic evaluation; (b) to evaluate finer-grained model performance at the word level; and (c) to examine whether our model generalizes to non-Wikipedia articles. For this, we created a third test set from the New York Times, in addition to our Wikipedia-based English and Chinese datasets. For all three corpora, we randomly sampled two documents for each domain, and then from each document, we sampled one long paragraph or a few consecutive short paragraphs containing 8–12 sentences. Amazon Mechanical Turkers were asked to read these sentences and assign a domain based on the seven labels used in this paper (multiple labels were allowed). Participants were provided with domain definitions. We obtained five annotations per sentence and adopted the majority label as the sentence's domain label. We obtained two annotated datasets for English (Wiki-en and NYT-en) and one for Chinese (Wiki-zh), consisting of 122/14, 111/11, and 117/12 sentences/documents each. Word-level domain evaluation is more challenging; taken out-of-context, individual words might be uninformative or carry meanings compatible with multiple domains. Expecting crowdworkers to annotate domain labels word-by-word with high confidence, might be therefore problematic. In order to reduce annotation complexity, we opted for a retrieval-style task for word evaluation. Specifically, AMT workers were given a sentence and its domain label (obtained from the sentence-level elicitation study described above), and asked to highlight which words they considered consistent with the domain of the sentence. We used the same corpora/sentences as in our first AMT study. Analogously, words in each sentence were annotated by five participants and their labels were determined by majority agreement. Fully hierarchical variants of our model (i.e., DetNet INLINEFORM0 , DetNet INLINEFORM1 ) and L-LDA are able to produce word-level predictions; we thus retrieved the words within a sentence whose domain score was above the threshold of 0 and compared them against the labels provided by crowdworkers. MilNet and DetNet INLINEFORM2 can only make sentence-level predictions. In this case, we assume that the sentence domain applies to all words therein. HierNet can only produce document-level predictions based on which we generate sentence labels and further assume that these apply to sentence words too. Again, we report INLINEFORM3 2prp+r INLINEFORM4 p INLINEFORM5 r INLINEFORM6  We show model performance against AMT domain labels in Table TABREF42 . Consistent with the automatic evaluation results, DetNet variants are the best performing models on the sentence-level task. On the Wikipedia datasets, DetNet INLINEFORM0 or DetNet INLINEFORM1 outperform all baselines and DetNet INLINEFORM2 by a large margin, showing that word-level signals can indeed help detect sentence domains. Although statistical models are typically less accurate when they are applied to data that has a different distribution from the training data, DetNet INLINEFORM3 works surprisingly well on NYT, substantially outperforming all other systems. We also notice that prior information is useful in making domain predictions for NYT sentences: since our models are trained on Wikipedia, prior domain definitions largely alleviate the genre shift to non-Wikipedia sentences. Table TABREF43 provides a breakdown of the performance of DetNet INLINEFORM4 across domains. Overall, the model performs worst on LIF and GEN domains (which are very broad) and best on BUS and MIL (which are very narrow). With regard to word-level evaluation, DetNet INLINEFORM0 and DetNet INLINEFORM1 are the best systems and are significantly better against all comparison models by a wide margin, except L-LDA. The latter is a strong domain detection system at the word-level since it is able to directly associate words with domain labels (see Equation ( EQREF34 )) without resorting to document- or sentence-level predictions. However, our two-level hierarchical model is superior considering all-around performance across sentences and documents. The results here accord with our intuition from previous experiments: hierarchical models outperform simpler variants (including MilNet) since they are able to capture and exploit fine-grained domain signals relatively accurately. Interestingly, prior information does not seem to have an effect on the Wikipedia datasets, but is useful when transferring to NYT. We also observe that models trained on the Chinese datasets perform consistently better than English. Analysis of the annotations provided by crowdworkers revealed that the ratio of domain words in Chinese is higher compared to English (27.47 INLINEFORM2 vs. 13.86 INLINEFORM3 in Wikipedia and 16.42 INLINEFORM4 in NYT), possibly rendering word retrieval in Chinese an easier task. Table TABREF44 shows the 15 most representative domain words identified by our model (DetNet INLINEFORM0 ) on Wiki-en for our seven domains. We obtained this list by weighting word domain scores INLINEFORM1 with their attention scores: DISPLAYFORM0  and ranking all words in the development set according to INLINEFORM0 , separately for each domain. Since words appearing in different contexts are usually associated with multiple domains, we determine a word's ranking for a given domain based on the highest score. As shown in Table TABREF44 , biosecurity and authoritarianism are prevalent in both GOV and LAW domains. Interestingly, with contextualized word representations, fairly general English words are recognized as domain heavy. For example, technique is a strong domain word in HEA and 420 in GOV (the latter is slang for the consumption of cannabis and highly associated with government regulations). For comparison, we also show the top domain words identified by L-LDA via matrix INLINEFORM0 (see Equation ( EQREF34 )). To produce meaningful output, we have removed stop words and punctuation tokens, which are given very high domain scores by L-LDA (this is not entirely surprising since INLINEFORM1 is based on simple co-occurrence). Notice that no such post-processing is necessary for our model. As shown in Table TABREF44 , the top domain words identified by L-LDA (on the right) are more general and less informative, compared to those from DetNet INLINEFORM2 (on the left).",Domain-Specific Summarization,"In this section we illustrate how fine-grained domain scores can be used to produce domain summaries, following an extractive, unsupervised approach. We assume the user specifies the domains they are interested in a priori (e.g., LAW, HEA) and the system returns summaries targeting the semantics of these domains. Specifically, we introduce DetRank, an extension of the well-known TextRank algorithm BIBREF42 , which incorporates domain signals acquired by DetNet INLINEFORM0 . For each document, TextRank builds a directed graph INLINEFORM1 with nodes INLINEFORM2 corresponding to sentences, and undirected edges INLINEFORM3 whose weights are computed based on sentence similarity. Specifically, edge weights are represented with matrix INLINEFORM4 where each element INLINEFORM5 corresponds to the transition probability from vertex INLINEFORM6 to vertex INLINEFORM7 . Following barrios2016variations, INLINEFORM8 is computed with the Okapi BM25 algorithm BIBREF43 , a probabilistic version of TF-IDF, and small weights ( INLINEFORM9 ) are set to zeros. Unreachable nodes are further pruned to acquire the final vertex set INLINEFORM10 . To enhance TextRank with domain information, we first multiply sentence-level domain scores INLINEFORM0 with their corresponding attention scores: DISPLAYFORM0  and for a given domain INLINEFORM0 , we can extract a (domain) sentence score vector INLINEFORM1 . Then, from INLINEFORM2 , we produce vector INLINEFORM3 representing a distribution of domain signals over sentences: DISPLAYFORM0  In order to render domain signals in different sentences more discernible, we scale all elements in INLINEFORM0 to INLINEFORM1 before obtaining a legitimate distribution with the INLINEFORM2 function. Finally, we integrate the domain component into the original transition matrix as: DISPLAYFORM0  where INLINEFORM0 controls the extent to which domain-specific information influences sentence selection for the summarization task; higher INLINEFORM1 will lead to summaries which are more domain-relevant. Here, we empirically set INLINEFORM2 . The main difference between DetRank and TextRank is that TextRank treats INLINEFORM3 as a damping factor and a uniform probability distribution is applied to INLINEFORM4 . In order to decide which sentence to include in the summary, a node’s centrality is measured using a graph-based ranking algorithm BIBREF42 . Specifically, we run a Markov chain with INLINEFORM0 on INLINEFORM1 until it converges to the stationary distribution INLINEFORM2 where each element denotes the salience of a sentence. In the proposed DetRank algorithm, INLINEFORM3 jointly expresses the importance of a sentence in the document and its relevance to the given domain (controlled by INLINEFORM4 ). We rank sentences according to INLINEFORM5 and select the top INLINEFORM6 ones, subject to a budget (e.g., 100 words). We ran a judgment elicitation study on summaries produced by TextRank and DetRank. Participants were provided with domain definitions and asked to decide which summary was best according to the criteria of: Informativeness (does the summary contain more information about a specific domain, e.g., “Government and Politics”?), Succinctness (does the summary avoid unnecessary detail and redundant information?), and Coherence (does the summary make logical sense?). Amazon Mechanical Turk (AMT) workers were allowed to answer “Both” or “Neither” in cases where they could not discriminate between summaries. We sampled 50 summary pairs from the English Wikipedia development set. We collected three responses per summary pair and determined which system participants preferred based on majority agreement. Table TABREF51 shows the proportion of times AMT workers preferred each system according to the criteria of Informativeness, Succinctness, Coherence, and overall. As can be seen, participants find DetRank summaries more informative and coherent. While it is perhaps not surprising for DetRank to produce summaries which are domain informative since it explicitly takes domain signals into account, it is interesting to note that focusing on a specific domain also helps discard irrelevant information and produce more coherent summaries. This, on the other hand, possibly renders DetRank's summaries more verbose (see the Succinctness ratings in Table TABREF51 ). Figure FIGREF46 shows example summaries for the Wikipedia article Arms Industry for domains MIL and BUS. Both summaries begin with a sentence which introduces the arms industry to the reader. When MIL is the domain of interest, the summary focuses on military products such as guns and missiles. When the domain changes to BUS, the summary puts more emphasis on trade, e.g., market competition and companies doing military business, such as Boeing and Eurofighter.",Conclusions,"In this work, we proposed an encoder-detector framework for domain detection. Leveraging only weak domain supervision, our model achieves results superior to competitive baselines across different languages, segment granularities, and text genres. Aside from identifying domain specific training data, we also show that our model holds promise for other natural language tasks, such as text summarization. Beyond domain detection, we hope that some of the work described here might be of relevance to other multilabel classification problems such as sentiment analysis BIBREF29 , relation extraction BIBREF44 , and named entity recognition BIBREF45 . More generally, our experiments show that the proposed framework can be applied to textual data using minimal supervision, significantly alleviating the annotation bottleneck for text classification problems. A key feature in achieving performance superior to competitive baselines is the hierarchical nature of our model, where representations are encoded step-by-step, first for words, then for sentences, and finally for documents. The framework flexibly integrates prior information which can be used to enhance the otherwise weak supervision signal or to render the model more robust across genres. In the future, we would like to investigate semi-supervised instantiations of MIL, where aside from bag labels, small amounts of instance labels are also available BIBREF23 . It would also be interesting to examine how the label space influences model performance, especially since in our scenario the labels are extrapolated from Wikipedia and might be naturally noisy and/or ambiguous.",Acknowledgments,"The authors would like to thank the anonymous reviewers and the action editor, Yusuke Miyao, for their valuable feedback. We acknowledge the financial support of the European Research Council (Lapata; award number 681760). This research is based upon work supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via contract FA8650-17-C-9118. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation therein.",,,,,,,,,,,,,,,,,,,,,What is the performance of their model?,e1f559da7fa501d3190073bca9ce4d4a12149e80,,,,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,True,,,,,73f3e045eee1b5d5a93af2cc5ae7947bedf0441b,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,Which text genres did they experiment with?,a96a1a354cb3a2a434b085e4d9c8844d0b672ec4,,,,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,True,,,,,2011c85739eb6c3fcf0d3632dc5e420fe73a0270,258ee4069f740c400c0049a2580945a1cc7f044c,What domains are detected in this paper?,427252648173c3ba78c211b86fa89fc9f4406653,,,,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,"Answer with content missing: (Experimental setup not properly rendered) In our experiments we used seven target domains: “Business and Commerce” (BUS), “Government and Politics” (GOV), “Physical and Mental Health” (HEA), “Law and Order” (LAW),
“Lifestyle” (LIF), “Military” (MIL), and “General Purpose” (GEN). Exceptionally, GEN does
not have a natural root category.",Experimental Setup,Experimental Setup,eadfa312437ea0030d55564c672e1e3d2a0e4635,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Character-Based Text Classification using Top Down Semantic Model for Sentence Representation,"Despite the success of deep learning on many fronts especially image and speech, its application in text classification often is still not as good as a simple linear SVM on n-gram TF-IDF representation especially for smaller datasets. Deep learning tends to emphasize on sentence level semantics when learning a representation with models like recurrent neural network or recursive neural network, however from the success of TF-IDF representation, it seems a bag-of-words type of representation has its strength. Taking advantage of both representions, we present a model known as TDSM (Top Down Semantic Model) for extracting a sentence representation that considers both the word-level semantics by linearly combining the words with attention weights and the sentence-level semantics with BiLSTM and use it on text classification. We apply the model on characters and our results show that our model is better than all the other character-based and word-based convolutional neural network models by \cite{zhang15} across seven different datasets with only 1\% of their parameters. We also demonstrate that this model beats traditional linear models on TF-IDF vectors on small and polished datasets like news article in which typically deep learning models surrender.",Introduction,"Recently, deep learning has been particularly successful in speech and image as an automatic feature extractor BIBREF1 , BIBREF2 , BIBREF3 , however deep learning's application to text as an automatic feature extractor has not been always successful BIBREF0 even compared to simple linear models with BoW or TF-IDF feature representation. In many experiments when the text is polished like news articles or when the dataset is small, BoW or TF-IDF is still the state-of-art representation compared to sent2vec or paragraph2vec BIBREF4 representation using deep learning models like RNN (Recurrent Neural Network) or CNN (Convolution Neural Network) BIBREF0 . It is only when the dataset becomes large or when the words are noisy and non-standardized with misspellings, text emoticons and short-forms that deep learning models which learns the sentence-level semantics start to outperform BoW representation, because under such circumstances, BoW representation can become extremely sparse and the vocabulary size can become huge. It becomes clear that for large, complex data, a large deep learning model with a large capacity can extract a better sentence-level representation than BoW sentence representation. However, for small and standardized news-like dataset, a direct word counting TF-IDF sentence representation is superior. Then the question is can we design a deep learning model that performs well for both simple and complex, small and large datasets? And when the dataset is small and standardized, the deep learning model should perform comparatively well as BoW? With that problem in mind, we designed TDSM (Top-Down-Semantic-Model) which learns a sentence representation that carries the information of both the BoW-like representation and RNN style of sentence-level semantic which performs well for both simple and complex, small and large datasets. Getting inspiration from the success of TF-IDF representation, our model intends to learn a word topic-vector which is similar to TF-IDF vector of a word but is different from word embedding, whereby the values in the topic-vector are all positives, and each dimension of the topic-vector represents a topic aspect of the word. Imagine a topic-vector of representation meaning $[animal, temperature, speed]$ , so a $rat$ maybe represented as $[0.9, 0.7, 0.2]$ since $rat$ is an animal with high body temperature but slow running speed compared to a $car$ which maybe represented as $[0.1, 0.8, 0.9]$ for being a non-animal, but high engine temperature and fast speed. A topic-vector will have a much richer semantic meaning than one-hot TF-IDF representation and also, it does not have the cancellation effect of summing word-embeddings positional vectors $([-1, 1] + [1, -1] = [0, 0])$ . The results from BIBREF5 show that by summing word-embedding vectors as sentence representation will have a catastrophic result for text classification. Knowing the topic-vector of each word, we can combine the words into a sentence representation $\tilde{s}$ by learning a weight $w_i$ for each word ${v_i}$ and do a linear sum of the words, $\tilde{s} = \sum _i {w_i}\tilde{v_i}$ . The weights $w_i$ for each word in the sentence summation is learnt by recurrent neural network (RNN) BIBREF6 with attention over the words BIBREF7 . The weights corresponds to the IDF (inverse document frequency) in TF-IDF representation, but with more flexibility and power. IDF is fixed for each word and calculated from all the documents (entire dataset), however attention weights learned from RNN is conditioned on both the document-level and dataset-level semantics. This sentence representation from topic-vector of each word is then concatenated with the sentence-level semantic vector from RNN to give a top-down sentence representation as illustrated in Figure 2 .",TDSM on Characters,"TDSM is a framework that can be applied to both word-level or character-level inputs. Here in this paper, we choose character-level over word-level inputs for practical industry reasons. In industry applications, often the model is required to have continuous learning on datasets that morph over time. Which means the vocabulary may change over time, therefore feeding the dataset by characters dispel the need for rebuilding a new vocabulary every time when there are new words. Industry datasets are usually very complex and noisy with a large vocabulary, therefore the memory foot-print of storing word embeddings is much larger than character embeddings. Therefore improving the performance of a character-based model has a much larger practical value compared to word-based model.",Related Work,"There are many traditional machine learning methods for text classification and most of them could achieve quite good results on formal text datasets. Recently, many deep learning methods have been proposed to solve the text classification task BIBREF0 , BIBREF9 , BIBREF10 . Deep convolutional neural network has been extremely successful for image classification BIBREF11 , BIBREF12 . Recently, many research also tries to apply it on text classification problem. Kim BIBREF10 proposed a model similar to Collobert's et al. BIBREF13 architecture. However, they employ two channels of word vectors. One is static throughout training and the other is fine-tuned via back-propagation. Various size of filters are applied on both channels, and the outputs are concatenated together. Then max-pooling over time is taken to select the most significant feature among each filter. The selected features are concatenated as the sentence vector. Similarly, Zhang et al. BIBREF0 also employs the convolutional networks but on characters instead of words for text classification. They design two networks for the task, one large and one small. Both of them have nine layers including six convolutional layers and three fully-connected layers. Between the three fully connected layers they insert two dropout layers for regularization. For both convolution and max-pooling layers, they employ 1-D filters BIBREF14 . After each convolution, they apply 1-D max-pooling. Specially, they claim that 1-D max-pooling enable them to train a relatively deep network. Besides applying models directly on testing datasets, more aspects are considered when extracting features. Character-level feature is adopted in many tasks besides Zhang et al. BIBREF0 and most of them achieve quite good performance. Santos and Zadrozny BIBREF15 take word morphology and shape into consideration which have been ignored for part-of-speech tagging task. They suggest the intra-word information is extremely useful when dealing with morphologically rich languages. They adopt neural network model to learn the character-level representation which is further delivered to help word embedding learning. Kim et al. BIBREF16 constructs neural language model by analysis of word representation obtained from character composition. Results suggest that the model could encode semantic and orthographic information from character level.  BIBREF17 , BIBREF7 uses two hierarchies of recurrent neural network to extract the document representation. The lower hierarchical recurrent neural network summarizes a sentence representation from the words in the sentence. The upper hierarchical neural network then summarizes a document representation from the sentences in the document. The major difference between BIBREF17 and BIBREF7 is that Yang applies attention over outputs from the recurrent when learning a summarizing representation. Attention model is also utilized in our model, which is used to assign weights for each word. Usually, attention is used in sequential model BIBREF18 , BIBREF19 , BIBREF20 , BIBREF21 . The attention mechanism includes sensor, internal state, actions and reward. At each time-step, the sensor will capture a glimpse of the input which is a small part of the entire input. Internal state will summarize the extracted information. Actions will decide the next glimpse location for the next step and reward suggests the benefit when taking the action. In our network, we adopt a simplified attention network as BIBREF22 , BIBREF23 . We learn the weights over the words directly instead of through a sequence of actions and rewards. Residual network BIBREF3 , BIBREF24 , BIBREF25 is known to be able to make very deep neural networks by having skip-connections that allows gradient to back-propagate through the skip-connections. Residual network in BIBREF3 outperforms the state-of-the-art models on image recognition. He BIBREF24 introduces residual block as similar to feature refinement for image classification. Similarly, for text classification problem, the quality of sentence representation is also quite important for the final result. Thus, we try to adopt the residual block as in BIBREF3 , BIBREF24 to refine the sentence vector.",Model,"The entire model has only 780,000 parameters which is only 1% of the parameters in BIBREF0 large CNN model. We used BiLSTM BIBREF27 with 100 units in both forward and backward LSTM cell. The output from the BiLSTM is 200 dimensions after we concatenate the outputs from the forward and backward cells. We then use attention over the words by linearly transform 200 output dimensions to 1 follow by a softmax over the 1 dimension outputs from all the words. After the characters to topic-vector transformation by FCN, each topic vector will be 180 dimensions. The topic-vectors are then linearly sum with attention weights to form a 180 dimensions BoW-like sentence vector. This vector is further concatenate with 200 dimensions BiLSTM outputs. The 380 dimensions undergo 10 blocks of ResNet BIBREF25 plus one fully connected layer. We use RELU BIBREF29 for all the intra-layer activation functions. The source code will be released after some code refactoring and we built the models with tensorflow BIBREF30 and tensorgraph .",Characters to Topic-Vector,"Unlike word-embedding BIBREF26 , topic-vector tries to learn a distributed topic representation at each dimension of the representation vector, which thus allows the simple addition of the word-level topic-vectors to form a sentence representation. Figure 1 illustrates how topic-vector is extracted from characters in words using FCN (Fully Convolutional Network). In order to force word level representation with topic meanings, we apply a sigmoid function over the output from FCN. Doing so, restrain the values at each dimension to be between 0 and 1, thus forcing the model to learn a distributed topic representation of the word.",Sentence Representation Vector Construction,"Forming a sentence representation from words can be done simply by summing of the word-embeddings which produce catastrophic results BIBREF5 due to the cancellation effect of adding embedding vectors (negative plus positive gives zero). Or in our model, the summing of word-level topic-vectors which give a much better sentence representation as shown in Table 3 than summing word-embeddings. Sentence vector derived from summing of the word topic-vectors is equivalent to the BoW vectors in word counting, whereby we treat the prior contribution of each word to the final sentence vector equally. Traditionally, a better sentence representation over BoW will be TF-IDF, which gives a weight to each word in a document in terms of IDF (inverse document frequency). Drawing inspiration from TF-IDF representation, we can have a recurrent neural network that outputs the attention BIBREF7 over the words. And the attention weights serve similar function as the IDF except that it is local to the context of the document, since the attention weight for a word maybe different for different documents while the IDF of a word is the same throughout all documents. With the attention weights $w_i$ and word topic-vector $\tilde{v}_i$ , we can form a sentence vector $\tilde{s}_{bow}$ by linear sum  $$\tilde{s}_{bow} = \sum _i w_i \tilde{v}_i$$   (Eq. 10)  With the neural sentence vector that derived from BoW which captures the information of individual words. We can also concatenate it with the output state from the RNN which captures the document level information and whose representation is conditioned on the positioning of the words in the document. $$&\tilde{s}_{t} = RNN(\tilde{v}_{t}, \tilde{s}_{t-1}) \\
&\tilde{s}_{pos} = \tilde{s}_T \\

&\tilde{s} = \tilde{s}_{bow} \oplus \tilde{s}_{pos}$$   (Eq. 12)   where $T$ is the length of the document, and $\oplus $ represents concatenation such that $|\tilde{s}| = |\tilde{s}_{bow}| + |\tilde{s}_{pos}|$ . The overall sentence vector $\tilde{s}$ will then capture the information of both the word-level and document-level semantics of the document. And thus it has a very rich representation. We used Bi-directional LSTM (BiLSTM) BIBREF27 as the recurrent unit. BiLSTM consist of a forward LSTM (FLSTM) and a backward LSTM (BLSTM), both LSTMs are of the same design, except that FLSTM reads the sentence in a forward manner and BLSTM reads the sentence in a backward manner. One recurrent step in LSTM of Equation 12 consists of the following steps  $$\tilde{f}_t &= \sigma \big (\mathbf {W}_f (\tilde{s}_{t-1} \oplus \tilde{v}_t) + \tilde{b}_f \big ) \\
\tilde{i}_t &= \sigma \big (\mathbf {W}_i (\tilde{s}_{t-1} \oplus \tilde{v}_t) + \tilde{b}_i\big ) \\
\tilde{C}_t &= \tanh \big (\mathbf {W}_C(\tilde{s}_{t-1}, \tilde{v}_t) + \tilde{b}_C\big ) \\
\tilde{C}_t &= \tilde{f}_t \otimes \tilde{C}_{t-1} + \tilde{i}_t \otimes \tilde{C}_t \\
\tilde{o}_t &= \sigma \big (\mathbf {W}_o (\tilde{s}_{t-1} \oplus \tilde{v}_t) + \tilde{b}_o \big ) \\
\tilde{s}_t &= \tilde{o}_t \otimes \tanh (\tilde{C}_t)$$   (Eq. 14)   where $\otimes $ is the element-wise vector multiplication, $\oplus $ is vector concatenation similarly defined in Equation . $\tilde{f}_t$ is forget state, $\tilde{i}_t$ is input state, $\tilde{o}_t$ is output state, $\tilde{C}_t$ is the internal context which contains the long-short term memory of historical semantics that LSTM reads. Finally, the output from the BiLSTM will be a concatenation of the output from FLSTM and BLSTM  $$\tilde{f}_t &= \text{FLSTM}(\tilde{v}_t, \tilde{s}_{t-1}) \\
\tilde{b}_t &= \text{BLSTM}(\tilde{v}_t, \tilde{s}_{t+1}) \\
\tilde{h}_{t} &= \tilde{f}_t \oplus \tilde{b}_t$$   (Eq. 15)  Here, the concatenated output state of BiLSTM has visibility of the entire sequence at any time-step compared to single-directional LSTM which only has visibility of the sequence in the past. This property of BiLSTM is very useful for learning attention weights for each word in a document because then the weights are decided based on the information of the entire document instead of just words before it as in LSTM.",Datasets,"We use the standard benchmark datasets prepare by BIBREF0 . The datasets have different number of training samples and test samples ranging from 28,000 to 3,600,000 training samples, and of different text length ranging from average of 38 words for Ag News to 566 words in Sogou news as illustrated in Table 1 . The datasets are a good mix of polished (AG) and noisy (Yelp and Amazon reviews), long (Sogou) and short (DBP and AG), large (Amazon reviews) and small (AG) datasets. And thus the results over these datasets serve as good evaluation on the quality of the model.",Word Setting,"In this paper, we take 128 ASCII characters as character set, by which most of the testing documents are composite. We define word length as 20 and character embedding length as 100. If a word with characters less than 20, we will pad it with zeros. If the length is larger than 20, we just take the first 20 characters. We set the maximum length of words as the average number of words of the documents in the dataset plus two standard deviation, which is long enough to cover more than 97.5% of the documents. For documents with number of words more than the preset maximum number of words, we will discard the exceeding words.",Baseline Models,"We select both traditional models and the convolutional models from BIBREF0 , the recurrent models from BIBREF7 , BIBREF17 as baselines. Also in order to ensure a fair comparison of models, such that any variation in the result is purely due to the model difference, we compare TDSM only with models that are trained in the same way of data preparation, that is the words are lowered and there are no additional data alteration or augmentation with thesaurus. Unfortunately, BIBREF7 , BIBREF17 recurrent models are trained on full text instead of lowered text, so their models may not be objectively compared to our models, since it is well known from BIBREF28 that different text preprocessing will have significant impact on the final results, Zhang's result shows that a simple case lowering can result up to 4% difference in classification accuracy. Despite this, we still include the recurrent models for comparison, because they provide a good reference for understanding time-based models on large datasets of long sentences. is the standard word counting method whereby the feature vector represents the term frequency of the words in a sentence. is similar to BoW, except that it is derived by the counting of the words in the sentence weighted by individual word's term-frequency and inverse-document-frequency BIBREF31 . This is a very competitive model especially on clean and small dataset. is derived from clustering of words-embeddings with k-means into 5000 clusters, and follow by BoW representation of the words in 5000 clusters. is CNN model on word embeddings following BIBREF0 , to ensure fair comparison with character-based models, the CNN architecture is the same as Lg. Conv and Sm. Conv with the same number of parameters. from BIBREF17 is basically a recurrent neural network based on LSTM and GRU BIBREF32 over the words in a sentence, and over the sentences in a document. It tries to learn a hierarchical representation of the text from multi-levels of recurrent layers. from BIBREF7 is basically similar to LSTM-GRNN except that instead of just learning the hierarchical representation of the text directly with RNN, it also learns attention weights over the words during the summarization of the words and over the sentences during the summarization of the sentences. are proposed in BIBREF0 , which is a CNN model on character encoding and is the primary character-based baseline model that we are comparing with.",Results,"Table 3 shows the comparison results of different datasets with different size, different sentence length, and different quality (polished AG news vs messy Yelp and Amazon reviews). From the results, we see that TDSM out-performs all the other CNN models across all the datasets with only 1% of the parameters of Zhang's large conv model and 7.8% of his small conv model. Since these results are based on the same text preprocessing and across all kinds of dataset (long, short, large, small, polished, messy), we can confidently say that TDSM generalizes better than the other CNN models over text classification. These results show that a good architecture design can achieve a better accuracy with significantly less parameters. Character-based models are the most significant and practical model for real large scale industry deployment because of its smaller memory footprint, agnostic to changes in vocabulary and robust to misspellings BIBREF16 . For a very long time, TF-IDF has been state-of-art models especially in small and standardized datasets. However because of its large memory footprint and non-suitability for continuous learning (because a new vocabulary has to be rebuilt every once in awhile when there are new words especially for data source like Tweeter), it was not an ideal model until character-based models came out. From the results, previous character-based models are generally better than TF-IDF for large datasets but falls short for smaller dataset like AG news. TDSM successfully close the gap between character-based models and TF-IDF by beating TF-IDF with 1% better performance. The results also confirm the hypothesis that TDSM as illustrated in Figure 2 which contains both the BoW-like and sentence-level features, has the best of the traditional TF-IDF and the recent deep learning model, is able to perform well for both small and large datasets. From the results, we also observe that TDSM improves over other character-based models by a big margin of 3% for Lg. Conv and 5.7% for Sm. Conv on the AG dataset. But the improvement tails off to only 0.5% for Amazon reviews when the dataset size increases from 120,000 to 3.6 million. This is probably because TDSM has reached its maximum capacity when the dataset gets very large compared to other character-based models which have 100 times the capacity of TDSM. For Yelp Full, we observe that the hierarchical recurrent models LSTM-GRNN and HN-ATT performs about 10% points better than TDSM but drops to only 3% for Amazon Full. This may be partly due to their data being prepared differently from our models. This can also be due to the structure of these hierarchical recurrent models which has two levels of recurrent neural networks for summarizing a document, whereby the first level summarizes a sentence vector from the words and the second level summarizes a document vector from the sentences. So these models will start to perform much better when there are alot of sentences and words in a document. For Yelp Full, there are of average 134 words in one document and Amazon Full has about 80 words per document. That's why the performance is much better for these recurrent models on Yelp than on Amazon. However, these hierarchical recurrent models will be reduce to a purely vanilla RNN for short text like AG News or Tweets with a few sentences, and under such circumstances its result will not be much different from a standard RNN. Nevertheless, LSTM-GRNN or HN-ATT does indicate the strength of RNN models in summarizing the sentences and documents and deriving a coherent sentence-level and document-level representation.",Conclusion,"From the results, we see a strong promise for TDSM as a competitive model for text classification because of its hybrid architecture that looks at the sentence from both the traditional TF-IDF point of view and the recent deep learning point of view. The results show that this type of view can derive a rich text representation for both small and large datasets.",,,,,,,,,,,,,,,,,,,,,What other non-neural baselines do the authors compare to? ,2df2f6e4efd19023434c84f5b4f29a2f00bfc9fb,,unfamiliar,no,representation,50d8b4a941c26b89482c94ab324b5a274f9ced66,False,,"bag of words, tf-idf, bag-of-means","is the standard word counting method whereby the feature vector represents the term frequency of the words in a sentence. is similar to BoW, except that it is derived by the counting of the words in the sentence weighted by individual word's term-frequency and inverse-document-frequency BIBREF31 . This is a very competitive model especially on clean and small dataset. is derived from clustering of words-embeddings with k-means into 5000 clusters, and follow by BoW representation of the words in 5000 clusters.","is the standard word counting method whereby the feature vector represents the term frequency of the words in a sentence. is similar to BoW, except that it is derived by the counting of the words in the sentence weighted by individual word's term-frequency and inverse-document-frequency BIBREF31 . is derived from clustering of words-embeddings with k-means into 5000 clusters, and follow by BoW representation of the words in 5000 clusters.",587358249a7de0e5f3d815eb49e85fd171d99617,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3-Figure1-1.png,"Figure 1: Illustration of transformation from character-level embeddings to word-level topic-vector. The transformation is done with fully convolutional network (FCN) similar to (Long et al., 2015), each hierarchical level of the FCN will extract an n-gram character feature of the word until the word-level topic-vector.",3-Figure2-1.png,"Figure 2: TDSM: Illustration of how topic-vector of the words are combined together into a sentence representation. Note that for the actual model, we are using BiLSTM for extracting positional features. In this diagram, in order to present our idea in a neater manner, we demonstrate with a vanilla RNN.",5-Table1-1.png,Table 1: Statistics of datasets.,5-Table2-1.png,"Table 2: Fully-Convolutional Network from characters to topic-vector. The first convolutional layer has kernel size of (100, 4) where 100 is the embedding size over 4-gram character as shown in Figure 1.",6-Table3-1.png,"Table 3: Comparison results on accuracy for various models. Lg w2v Conv and Sm. w2v Conv is CNN on word embedding. Lg. Conv and Sm. Conv is CNN on character embedding. LSTM-GRNN and HN-ATT are different species of recurrent neural networks on words and sentences. Unfortunately, these two RNN models did not use the same text preprocessing technique as other models, so their models may not be objectively comparable to Zhang’s or our model, because it is well known that (Zhang et al., 2015; Uysal & Gunal, 2014), the difference in text preprocessing will have a significant impact on the final accuracy. However, these RNN models are still a good reference for our understanding of time-based models on large datasets of long sentences.",6-Table4-1.png,Table 4: Number of parameters for different models,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using Whole Document Context in Neural Machine Translation,"In Machine Translation, considering the document as a whole can help to resolve ambiguities and inconsistencies. In this paper, we propose a simple yet promising approach to add contextual information in Neural Machine Translation. We present a method to add source context that capture the whole document with accurate boundaries, taking every word into account. We provide this additional information to a Transformer model and study the impact of our method on three language pairs. The proposed approach obtains promising results in the English-German, English-French and French-English document-level translation tasks. We observe interesting cross-sentential behaviors where the model learns to use document-level information to improve translation coherence.",Introduction,"Neural machine translation (NMT) has grown rapidly in the past years BIBREF0, BIBREF1. It usually takes the form of an encoder-decoder neural network architecture in which source sentences are summarized into a vector representation by the encoder and are then decoded into target sentences by the decoder. NMT has outperformed conventional statistical machine translation (SMT) by a significant margin over the past years, benefiting from gating and attention techniques. Various models have been proposed based on different architectures such as RNN BIBREF0, CNN BIBREF2 and Transformer BIBREF1, the latter having achieved state-of-the-art performances while significantly reducing training time. However, by considering sentence pairs separately and ignoring broader context, these models suffer from the lack of valuable contextual information, sometimes leading to inconsistency in a translated document. Adding document-level context helps to improve translation of context-dependent parts. Previous study BIBREF3 showed that such context gives substantial improvement in the handling of discourse phenomena like lexical disambiguation or co-reference resolution. Most document-level NMT approaches focus on adding contextual information by taking into account a set of sentences surrounding the current pair BIBREF4, BIBREF5, BIBREF6, BIBREF7, BIBREF8, BIBREF9. While giving significant improvement over the context-agnostic versions, none of these studies consider the whole document with well delimited boundaries. The majority of these approaches also rely on structural modification of the NMT model BIBREF6, BIBREF7, BIBREF8, BIBREF9. To the best of our knowledge, there is no existing work considering whole documents without structural modifications. Contribution: We propose a preliminary study of a generic approach allowing any model to benefit from document-level information while translating sentence pairs. The core idea is to augment source data by adding document information to each sentence of a source corpus. This document information corresponds to the belonging document of a sentence and is computed prior to training, it takes every document word into account. Our approach focuses on pre-processing and consider whole documents as long as they have defined boundaries. We conduct experiments using the Transformer base model BIBREF1. For the English-German language pair we use the full WMT 2019 parallel dataset. For the English-French language pair we use a restricted dataset containing the full TED corpus from MUST-C BIBREF10 and sampled sentences from WMT 2019 dataset. We obtain important improvements over the baseline and present evidences that this approach helps to resolve cross-sentence ambiguities.",Related Work,"Interest in considering the whole document instead of a set of sentences preceding the current pair lies in the necessity for a human translator to account for broader context in order to keep a coherent translation. The idea of representing and using documents for a model is interesting, since the model could benefit from information located before or after the current processed sentence. Previous work on document-level SMT started with cache based approaches, BIBREF11 suggest a conjunction of dynamic, static and topic-centered cache. More recent work tend to focus on strategies to capture context at the encoder level. Authors of BIBREF5 propose an auxiliary context source with a RNN dedicated to encode contextual information in addition to a warm-start of encoder and decoder states. They obtain significant gains over the baseline. A first extension to attention-based neural architectures is proposed by BIBREF6, they add an encoder devoted to capture the preceding source sentence. Authors of BIBREF7 introduce a hierarchical attention network to model contextual information from previous sentences. Here the attention allows dynamic access to the context by focusing on different sentences and words. They show significant improvements over a strong NMT baseline. More recently, BIBREF9 extend Transformer architecture with an additional encoder to capture context and selectively merge sentence and context representations. They focus on co-reference resolution and obtain improvements in overall performances. The closest approach to ours is presented by BIBREF4, they simply concatenate the previous source sentence to the one being translated. While they do not make any structural modification to the model, their method still does not take the whole document into account.",Approach,"We propose to use the simplest method to estimate document embeddings. The approach is called SWEM-aver (Simple Word Embedding Model – average) BIBREF12. The embedding of a document $k$ is computed by taking the average of all its $N$ word vectors (see Eq. DISPLAY_FORM2) and therefore has the same dimension. Out of vocabulary words are ignored. Despite being straightforward, our approach raises the need of already computed word vectors to keep consistency between word and document embeddings. Otherwise, fine-tuning embeddings as the model is training would shift them in a way that totally wipes off the connection between document and word vectors. To address this problem, we adopt the following approach: First, we train a baseline Transformer model (noted Baseline model) from which we extract word embeddings. Then, we estimate document embeddings using the SWEM-aver method and train an enhanced model (noted Document model) benefiting from these document embeddings and the extracted word embeddings. During training, the Document model does not fine-tune its embeddings to preserve the relation between words and document vectors. It should be noted that we could directly use word embeddings extracted from another model such as Word2Vec BIBREF13, in practice we obtain better results when we get these vectors from a Transformer model. In our case, we simply extract them from the Baseline after it has been trained. Using domain adaptation ideas BIBREF14, BIBREF15, BIBREF16, we associate a tag to each sentence of the source corpus, which represents the document information. This tag takes the form of an additional token placed at the first position in the sentence and corresponds to the belonging document of the sentence (see Table TABREF1). The model considers the tag as an additional word and replace it with the corresponding document embedding. The Baseline model is trained on a standard corpus that does not contain document tags, while the Document model is trained on corpus that contains document tags. The proposed approach requires strong hypotheses about train and test data. The first downfall is the need for well defined document boundaries that allow to mark each sentence with its document tag. The second major downfall is the need to compute an embedding vector for each new document fed in the model, adding a preprocessing step before inference time.",Experiments,"We consider two different models for each language pair: the Baseline and the Document model. We evaluate them on 3 test sets and report BLEU and TER scores. All experiments are run 8 times with different seeds, we report averaged results and p-values for each experiment. Translation tasks are English to German, proposed in the first document-level translation task at WMT 2019 BIBREF17, English to French and French to English, following the IWSLT translation task BIBREF18.",Experiments ::: Training and test sets,"Table TABREF4 describes the data used for the English-German language pair. These corpora correspond to the WMT 2019 document-level translation task. Table TABREF5 describes corpora for the English-French language pair, the same data is used for both translation directions. For the English-German pair, only 10.4% (3.638M lines) of training data contains document boundaries. For English-French pair, we restricted the total amount of training data in order to keep 16.1% (602K lines) of document delimited corpora. To achieve this we randomly sampled 10% of the ParaCrawl V3. It means that only a fraction of the source training data contains document context. The enhanced model learns to use document information only when it is available. All test sets contain well delimited documents, Baseline models are evaluated on standard corpora while Document models are evaluated on the same standard corpora that have been augmented with document context. We evaluate the English-German systems on newstest2017, newstest2018 and newstest2019 where documents consist of newspaper articles to keep consistency with the training data. English to French and French to English systems are evaluated over IWSLT TED tst2013, tst2014 and tst2015 where documents are transcriptions of TED conferences (see Table TABREF5). Prior to experiments, corpora are tokenized using Moses tokenizer BIBREF19. To limit vocabulary size, we adopt the BPE subword unit approach BIBREF20, through the SentencePiece toolkit BIBREF21, with 32K rules.",Experiments ::: Training details,"We use the OpenNMT framework BIBREF22 in its TensorFlow version to create and train our models. All experiments are run on a single NVIDIA V100 GPU. Since the proposed approach relies on a preprocessing step and not on structural enhancement of the model, we keep the same Transformer architecture in all experiments. Our Transformer configuration is similar to the baseline of BIBREF1 except for the size of word and document vectors that we set to $d_{model} = 1024$, these vectors are fixed during training. We use $N = 6$ as the number of encoder layers, $d_{ff} = 2048$ as the inner-layer dimensionality, $h = 8$ attention heads, $d_k = 64$ as queries and keys dimension and $Pdrop = 0.1$ as dropout probability. All experiments, including baselines, are run over 600k training steps with a batch size of approximately 3000 tokens. For all language pairs we trained a Baseline and a Document model. The Baseline is trained on a standard parallel corpus and is not aware of document embeddings, it is blind to the context and cannot link the sentences of a document. The Document model uses extracted word embeddings from the Baseline as initialization for its word vectors and also benefits from document embeddings that are computed from the extracted word embeddings. It is trained on the same corpus as the Baseline one, but the training corpus is augmented with (see Table TABREF1) and learns to make use of the document context. The Document model does not consider its embeddings as tunable parameters, we hypothesize that fine-tuning word and document vectors breaks the relation between them, leading to poorer results. We provide evidence of this phenomena with an additional system for the French-English language pair, noted Document+tuning (see Table TABREF7) that is identical to the Document model except that it adjusts its embeddings during training. The evaluated models are obtained by taking the average of their last 6 checkpoints, which were written at 5000 steps intervals. All experiments are run 8 times with different seeds to ensure the statistical robustness of our results. We provide p-values that indicate the probability of observing similar or more extreme results if the Document model is actually not superior to the Baseline.",Experiments ::: Results,"Table TABREF6 presents results associated to the experiments for the English to German translation task, models are evaluated on the newstest2017, neswtest2018 and newstest2019 test sets. Table TABREF7 contains results for both English to French and French to English translation tasks, models are evaluated on the tst2013, tst2014 and tst2015 test sets. En$\rightarrow $De: The Baseline model obtained State-of-The-Art BLEU and TER results according to BIBREF23, BIBREF24. The Document system shows best results, up to 0.85 BLEU points over the Baseline on the newstest2019 corpus. It also surpassed the Baselinee by 0.18 points on the newstest2017 with strong statistical significance, and by 0.15 BLEU points on the newstest2018 but this time with no statistical evidence. These encouraging results prompted us to extend experiments to another language pair: English-French. En$\rightarrow $Fr: The Document system obtained the best results considering all metrics on all test sets with strong statistical evidence. It surpassed the Baseline by 1.09 BLEU points and 0.85 TER points on tst2015, 0.75 BLEU points and 0.76 TER points on tst2014, and 0.48 BLEU points and 0.68 TER points on tst2013. Fr$\rightarrow $En: Of all experiments, this language pair shows the most important improvements over the Baseline. The Document model obtained substantial gains with very strong statistical evidence on all test sets. It surpassed the Baseline model by 1.81 BLEU points and 1.02 TER points on tst2015, 1.50 BLEU points and 0.96 TER points on tst2014, and 1.29 BLEU points and 0.83 TER points on tst2013. The Document+tuning system, which only differs from the fact that it tunes its embeddings, shows little or no improvement over the Baseline, leading us to the conclusion that the relation between word and document embeddings described by Eq. DISPLAY_FORM2 must be preserved for the model to fully benefit from document context.",Experiments ::: Manual Analysis,"In this analysis we present some of the many cases that suggest the Document model can handle ambiguous situations. These examples are often isolated sentences where even a human translator could not predict the good translation without looking at the document, making it almost impossible for the Baseline model which is blind to the context. Table TABREF10 contains an extract of these interesting cases for the French-English language pair. Translation from French to English is challenging and often requires to take the context into account. The personal pronoun ""lui"" can refer to a person of feminine gender, masculine gender or even an object and can therefore be translated into ""her"", ""him"" or ""it"". The first example in Table TABREF10 perfectly illustrate this ambiguity: the context clearly indicates that ""lui"" in the source sentence refers to ""ma fille"", which is located three sentences above, and should be translated into ""her"". In this case, the Baseline model predict the personal pronoun ""him"" while the Document model correctly predicts ""her"". It seems that the Baseline model does not benefit from any valuable information in the source sentence. Some might argue that the source sentence actually contains clues about the correct translation, considering that ""robe à paillettes"" (""sparkly dress"") and ""baguette magique"" (""magic wand"") probably refer to a little girl, but we will see that the model makes similar choices in more restricted contexts. This example is relevant mainly because the actual reference to the subject ""ma fille"" is made long before the source sentence. The second example in Table TABREF10 is interesting because none of our models correctly translate the source sentence. However, we observe that the Baseline model opts for a literal translation of ""je peux faire le poirier"" (""I can stand on my head"") into ""I can do the pear"" while the Document model predicts ""I can wring"". Even though these translations are both incorrect, we observe that the Document model makes a prediction that somehow relates to the context: a woman talking about her past disability, who has become more flexible thanks to yoga and can now twist her body. The third case in table TABREF10 is a perfect example of isolated sentence that cannot be translated correctly with no contextual information. This example is tricky because the word ""Elle"" would be translated into ""She"" in most cases if no additional information were provided, but here it refers to ""la conscience"" (""consciousness"") from the previous sentence and must be translated into ""It"". As expected the Baseline model does not make the correct guess and predicts the personal pronoun ""She"" while the Document model correctly predicts ""It"". This example present a second difficult part, the word ""son"" from the source sentence is ambiguous and does not, in itself, inform the translator if it must be translated into ""her"", ""his"" or ""its"". With contextual information we know that it refers to ""[le] monde physique"" (""[the] physical world"") and that the correct choice is the word ""its"". Here the Baseline incorrectly predicts ""her"", possibly because of its earlier choice for ""She"" as the subject. The Document model makes again the correct translation. According to our results (see Table TABREF7), the English-French language pair also benefits from document-level information but to a lesser extent. For this language pair, ambiguities about personal pronouns are less frequent. Other ambiguous phenomena like the formal mode (use of ""vous"" instead of ""tu"") appear. TableTABREF11 presents an example of this kind of situation where the word ""You"" from the source sentence does not indicate if the correct translation is ""Vous"" or ""Tu"". However it refers to the narrator of the story who is an old police officer. In this case, it is very likely that the use of formal mode is the correct translation. The Baseline model incorrectly predicts ""Tu"" and the Document model predicts ""Vous"". ",Conclusion,"In this work, we presented a preliminary study of a simple approach for document-level translation. The method allows to benefit from the whole document context at the sentence level, leading to encouraging results. In our experimental setup, we observed improvement of translation outcomes up to 0.85 BLEU points in the English to German translation task and exceeding 1 BLEU point in the English to French and French to English translation tasks. Looking at the translation outputs, we provided evidence that the approach allows NMT models to disambiguate complex situations where the context is absolutely necessary, even for a human translator. The next step is to go further by investigating more elaborate document embedding approaches and to bring these experiments to other languages (e.g.: Asian, Arabic, Italian, Spanish, etc.). To consider a training corpus with a majority of document delimited data is also very promising.",,,,,,,,,,,,,,,,,,,,,,,,,Which language-pair had the better performance?,c1f4d632da78714308dc502fe4e7b16ea6f76f81,,,,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,French-English,"FLOAT SELECTED: Table 5: Results obtained for the English-French and French-English translation tasks, scored on three test sets using BLEU and TER metrics. p-values are denoted by * and correspond to the following values: ∗< .05, ∗∗< .01, ∗∗∗< .001.","FLOAT SELECTED: Table 5: Results obtained for the English-French and French-English translation tasks, scored on three test sets using BLEU and TER metrics. p-values are denoted by * and correspond to the following values: ∗< .05, ∗∗< .01, ∗∗∗< .001.",408e8c7aa8047ab454e61244dddecc43adcd7511,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,,,,,,,,,Which datasets were used in the experiment?,749a307c3736c5b06d7b605dc228d80de36cbabe,,,,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,,"Contribution: We propose a preliminary study of a generic approach allowing any model to benefit from document-level information while translating sentence pairs. The core idea is to augment source data by adding document information to each sentence of a source corpus. This document information corresponds to the belonging document of a sentence and is computed prior to training, it takes every document word into account. Our approach focuses on pre-processing and consider whole documents as long as they have defined boundaries. We conduct experiments using the Transformer base model BIBREF1. For the English-German language pair we use the full WMT 2019 parallel dataset. For the English-French language pair we use a restricted dataset containing the full TED corpus from MUST-C BIBREF10 and sampled sentences from WMT 2019 dataset. We obtain important improvements over the baseline and present evidences that this approach helps to resolve cross-sentence ambiguities.",For the English-German language pair we use the full WMT 2019 parallel dataset. For the English-French language pair we use a restricted dataset containing the full TED corpus from MUST-C BIBREF10 and sampled sentences from WMT 2019 dataset. ,1bacdb1587b2b671bbe431b57f4662320224f95a,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,What evaluation metrics did they use?,102de97c123bb1e247efec0f1d958f8a3a86e2f6,,,,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,,"We consider two different models for each language pair: the Baseline and the Document model. We evaluate them on 3 test sets and report BLEU and TER scores. All experiments are run 8 times with different seeds, we report averaged results and p-values for each experiment.","We consider two different models for each language pair: the Baseline and the Document model. We evaluate them on 3 test sets and report BLEU and TER scores. All experiments are run 8 times with different seeds, we report averaged results and p-values for each experiment.",3f35eaf73310dbf6df624b004fe5e620d4ed1432,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2-Table1-1.png,Table 1: Example of augmented parallel data used to train theDocumentmodel. The source corpus contains document tags while the target corpus remains unchanged.,3-Table2-1.png,"Table 2: Detail of training and evaluation sets for the English-German pair, showing the number of lines, words in English (EN) and words in German (DE). Corpora with document boundaries are denoted by †.",3-Table3-1.png,"Table 3: Detail of training and evaluation sets for the English-French pair in both directions, showing the number of lines, words in English (EN) and words in French (FR). Corpora with document boundaries are denoted by †.",4-Table4-1.png,"Table 4: Results obtained for the English-German translation task, scored on three test sets using BLEU and TER metrics. p-values are denoted by * and correspond to the following values: ∗< .05, ∗∗< .01, ∗∗∗< .001.",4-Table5-1.png,"Table 5: Results obtained for the English-French and French-English translation tasks, scored on three test sets using BLEU and TER metrics. p-values are denoted by * and correspond to the following values: ∗< .05, ∗∗< .01, ∗∗∗< .001.",5-Table6-1.png,Table 6: Translation examples for the French-English pair. We took the best models of all runs for both the Baseline and the Document enhanced model,,,,,,,,,WMT 2019 parallel dataset a restricted dataset containing the full TED corpus from MUST-C BIBREF10 sampled sentences from WMT 2019 dataset,BLEU and TER scores,5-Table7-1.png,Table 7: Translation example for the English-French pair.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improving Character-based Decoding Using Target-Side Morphological Information for Neural Machine Translation,"Recently, neural machine translation (NMT) has emerged as a powerful alternative to conventional statistical approaches. However, its performance drops considerably in the presence of morphologically rich languages (MRLs). Neural engines usually fail to tackle the large vocabulary and high out-of-vocabulary (OOV) word rate of MRLs. Therefore, it is not suitable to exploit existing word-based models to translate this set of languages. In this paper, we propose an extension to the state-of-the-art model of Chung et al. (2016), which works at the character level and boosts the decoder with target-side morphological information. In our architecture, an additional morphology table is plugged into the model. Each time the decoder samples from a target vocabulary, the table sends auxiliary signals from the most relevant affixes in order to enrich the decoder's current state and constrain it to provide better predictions. We evaluated our model to translate English into German, Russian, and Turkish as three MRLs and observed significant improvements.",Introduction,"Morphologically complex words (MCWs) are multi-layer structures which consist of different subunits, each of which carries semantic information and has a specific syntactic role. Table 1 gives a Turkish example to show this type of complexity. This example is a clear indication that word-based models are not suitable to process such complex languages. Accordingly, when translating MRLs, it might not be a good idea to treat words as atomic units as it demands a large vocabulary that imposes extra overhead. Since MCWs can appear in various forms we require a very large vocabulary to $i$ ) cover as many morphological forms and words as we can, and $ii$ ) reduce the number of OOVs. Neural models by their nature are complex, and we do not want to make them more complicated by working with large vocabularies. Furthermore, even if we have quite a large vocabulary set, clearly some words would remain uncovered by that. This means that a large vocabulary not only complicates the entire process, but also does not necessarily mitigate the OOV problem. For these reasons we propose an NMT engine which works at the character level. In this paper, we focus on translating into MRLs and issues associated with word formation on the target side. To provide a better translation we do not necessarily need a large target lexicon, as an MCW can be gradually formed during decoding by means of its subunits, similar to the solution proposed in character-based decoding models BIBREF0 . Generating a complex word character-by-character is a better approach compared to word-level sampling, but it has other disadvantages. One character can co-occur with another with almost no constraint, but a particular word or morpheme can only collocate with a very limited number of other constituents. Unlike words, characters are not meaning-bearing units and do not preserve syntactic information, so (in the extreme case) the chance of sampling each character by the decoder is almost equal to the others, but this situation is less likely for words. The only constraint that prioritize which character should be sampled is information stored in the decoder, which we believe is insufficient to cope with all ambiguities. Furthermore, when everything is segmented into characters the target sentence with a limited number of words is changed to a very long sequence of characters, which clearly makes it harder for the decoder to remember such a long history. Accordingly, character-based information flows in the decoder may not be as informative as word- or morpheme-based information. In the character-based NMT model everything is almost the same as its word-based counterpart except the target vocabulary whose size is considerably reduced from thousands of words to just hundreds of characters. If we consider the decoder as a classifier, it should in principle be able to perform much better over hundreds of classes (characters) rather than thousands (words), but the performance of character-based models is almost the same as or slightly better than their word-based versions. This underlines the fact that the character-based decoder is perhaps not fed with sufficient information to provide improved performance compared to word-based models. Character-level decoding limits the search space by dramatically reducing the size of the target vocabulary, but at the same time widens the search space by working with characters whose sampling seems to be harder than words. The freedom in selection and sampling of characters can mislead the decoder, which prevents us from taking the maximum advantages of character-level decoding. If we can control the selection process with other constraints, we may obtain further benefit from restricting the vocabulary set, which is the main goal followed in this paper. In order to address the aforementioned problems we redesign the neural decoder in three different scenarios. In the first scenario we equip the decoder with an additional morphology table including target-side affixes. We place an attention module on top of the table which is controlled by the decoder. At each step, as the decoder samples a character, it searches the table to find the most relevant information which can enrich its state. Signals sent from the table can be interpreted as additional constraints. In the second scenario we share the decoder between two output channels. The first one samples the target character and the other one predicts the morphological annotation of the character. This multi-tasking approach forces the decoder to send morphology-aware information to the final layer which results in better predictions. In the third scenario we combine these two models. Section ""Proposed Architecture"" provides more details on our models. Together with different findings that will be discussed in the next sections, there are two main contributions in this paper. We redesigned and tuned the NMT framework for translating into MRLs. It is quite challenging to show the impact of external knowledge such as morphological information in neural models especially in the presence of large parallel corpora. However, our models are able to incorporate morphological information into decoding and boost its quality. We inject the decoder with morphological properties of the target language. Furthermore, the novel architecture proposed here is not limited to morphological information alone and is flexible enough to provide other types of information for the decoder.",NMT for MRLs,"There are several models for NMT of MRLs which are designed to deal with morphological complexities. garcia2016factored and sennrich-haddow:2016:WMT adapted the factored machine translation approach to neural models. Morphological annotations can be treated as extra factors in such models. jean-EtAl:2015:ACL-IJCNLP proposed a model to handle very large vocabularies. luong-EtAl:2015:ACL-IJCNLP addressed the problem of rare words and OOVs with the help of a post-translation phase to exchange unknown tokens with their potential translations. sennrich2015neural used subword units for NMT. The model relies on frequent subword units instead of words. costajussa-fonollosa:2016:P16-2 designed a model for translating from MRLs. The model encodes source words with a convolutional module proposed by kim2015character. Each word is represented by a convolutional combination of its characters. luong-manning:2016:P16-1 used a hybrid model for representing words. In their model, unseen and complex words are encoded with a character-based representation, with other words encoded via the usual surface-form embeddings. DBLP:journals/corr/VylomovaCHH16 compared different representation models (word-, morpheme, and character-level models) which try to capture complexities on the source side, for the task of translating from MRLs. chung-cho-bengio proposed an architecture which benefits from different segmentation schemes. On the encoder side, words are segmented into subunits with the byte-pair segmentation model (bpe) BIBREF1 , and on the decoder side, one target character is produced at each time step. Accordingly, the target sequence is treated as a long chain of characters without explicit segmentation. W17-4727 focused on translating from English into Finnish and implicitly incorporated morphological information into NMT through multi-task learning. passbanPhD comprehensively studied the problem of translating MRLs and addressed potential challenges in the field. Among all the models reviewed in this section, the network proposed by chung-cho-bengio could be seen as the best alternative for translating into MRLs as it works at the character level on the decoder side and it was evaluated in different settings on different languages. Consequently, we consider it as a baseline model in our experiments.",Proposed Architecture,"We propose a compatible neural architecture for translating into MRLs. The model benefits from subword- and character-level information and improves upon the state-of-the-art model of chung-cho-bengio. We manipulated the model to incorporate morphological information and developed three new extensions, which are discussed in Sections ""The Embedded Morphology Table"" , ""The Auxiliary Output Channel"" , and ""Combining the Extended Output Layer and the Embedded Morphology Table"" .",The Embedded Morphology Table,"In the first extension an additional table containing the morphological information of the target language is plugged into the decoder to assist with word formation. Each time the decoder samples from the target vocabulary, it searches the morphology table to find the most relevant affixes given its current state. Items selected from the table act as guiding signals to help the decoder sample a better character. Our base model is an encoder-decoder model with attention BIBREF2 , implemented using gated recurrent units (GRUs) BIBREF3 . We use a four-layer model in our experiments. Similar to chung-cho-bengio and DBLP:journals/corr/WuSCLNMKCGMKSJL16, we use bidirectional units to encode the source sequence. Bidirectional GRUs are placed only at the input layer. The forward GRU reads the input sequence in its original order and the backward GRU reads the input in the reverse order. Each hidden state of the encoder in one time step is a concatenation of the forward and backward states at the same time step. This type of bidirectional processing provides a richer representation of the input sequence. On the decoder side, one target character is sampled from a target vocabulary at each time step. In the original encoder-decoder model, the probability of predicting the next token $y_i$ is estimated based on $i$ ) the current hidden state of the decoder, $ii$ ) the last predicted token, and $iii$ ) the context vector. This process can be formulated as $p(y_i|y_1,...,y_{i-1},{\bf x}) = g(h_i,y_{i-1},{\bf c}_i)$ , where $g(.)$ is a softmax function, $y_i$ is the target token (to be predicted), $\textbf {x}$ is the representation of the input sequence, $h_i$ is the decoder's hidden state at the $i$ -th time step, and $i$0 indicates the context vector which is a weighted summary of the input sequence generated by the attention module. $i$1 is generated via the procedure shown in ( 3 ):  $$\begin{aligned}
{\bf c}_i &= \sum _{j=1}^{n} \alpha _{ij} s_j\\
\alpha _{ij} &=\frac{\exp {(e_{ij})}}{\sum {_{k=1}^{n}\exp {(e_{ik})}}}; \hspace{5.69054pt}e_{ij}=a(s_j, h_{i-1})
\end{aligned}$$   (Eq. 3)  where $\alpha _{ij}$ denotes the weight of the $j$ -th hidden state of the encoder ( $s_j$ ) when the decoder predicts the $i$ -th target token, and $a()$ shows a combinatorial function which can be modeled through a simple feed-forward connection. $n$ is the length of the input sequence. In our first extension, the prediction probability is conditioned on one more constraint in addition to those three existing ones, as in $p(y_i|y_1,...,y_{i-1},{\bf x}) = g(h_i,y_{i-1},{\bf c}_i, {\bf c}^m_i)$ , where ${\bf c}^m_i$ is the morphological context vector and carries information from those useful affixes which can enrich the decoder's information. ${\bf c}^m_i$ is generated via an attention module over the morphology table which works in a similar manner to word-based attention model. The attention procedure for generating ${\bf c}^m_i$ is formulated as in ( 5 ):  $$\begin{aligned}
{\bf c}^m_i &= \sum _{u=1}^{|\mathcal {A}|} \beta _{iu} f_u\\
\beta _{iu} &= \frac{\exp {(e^m_{iu})}}{\sum {_{v=1}^{|\mathcal {A}|} \exp {(e_{iv})}}}; \hspace{5.69054pt}e^m_{iu}= a^m(f_u, h_{i-1})
\end{aligned}$$   (Eq. 5)  where $f_u$ represents the embedding of the $u$ -th affix ( $u$ -th column) in the morphology/affix table $\mathcal {A}$ , $\beta _{iu}$ is the weight assigned to $f_u$ when predicting the $i$ -th target token, and $a^m$ is a feed-forward connection between the morphology table and the decoder. The attention module in general can be considered as a search mechanism, e.g. in the original encoder-decoder architecture the basic attention module finds the most relevant input words to make the prediction. In multi-modal NMT BIBREF4 , BIBREF5 an extra attention module is added to the basic one in order to search the image input to find the most relevant image segments. In our case we have a similar additional attention module which searches the morphology table. In this scenario, the morphology table including the target language's affixes can be considered as an external knowledge repository that sends auxiliary signals which accompany the main input sequence at all time steps. Such a table certainly includes useful information for the decoder. As we are not sure which affix preserves those pieces of useful information, we use an attention module to search for the best match. The attention module over the table works as a filter which excludes irrelevant affixes and amplifies the impact of relevant ones by assigning different weights ( $\beta $ values).",The Auxiliary Output Channel,"In the first scenario, we embedded a morphology table into the decoder in the hope that it can enrich sampling information. Mathematically speaking, such an architecture establishes an extra constraint for sampling and can control the decoder's predictions. However, this is not the only way of constraining the decoder. In the second scenario, we define extra supervision to the network via another predictor (output channel). The first channel is responsible for generating translations and predicts one character at each time step, and the other one tries to understand the morphological status of the decoder by predicting the morphological annotation ( $l_i$ ) of the target character. The approach in the second scenario proposes a multi-task learning architecture, by which in one task we learn translations and in the other one morphological annotations. Therefore, all network modules –especially the last hidden layer just before the predictors– should provide information which is useful enough to make correct predictions in both channels, i.e. the decoder should preserve translation as well as morphological knowledge. Since we are translating into MRLs this type of mixed information (morphology+translation) can be quite useful. In our setting, the morphological annotation $l_i$ predicted via the second channel shows to which part of the word or morpheme the target character belongs, i.e. the label for the character is the morpheme that includes it. We clarify the prediction procedure via an example from our training set (see Section ""Experimental Study"" ). When the Turkish word `terbiyesizlik' is generated, the first channel is supposed to predict t, e, r, up to k, one after another. For the same word, the second channel is supposed to predict stem-C for the fist 7 steps as the first 7 characters `terbiye' belong to the stem of the word. The C sign indicates that stem-C is a class label. The second channel should also predict siz-C when the first channel predicts s (eighth character), i (ninth character), and z (tenth character), and lik-C when the first channel samples the last three characters. Clearly, the second channel is a classifier which works over the {stem-C, siz-C, lik-C, ...} classes. Figure 1 illustrates a segment of a sentence including this Turkish word and explains which class tags should be predicted by each channel. To implement the second scenario we require a single-source double-target training corpus: [source sentence] $\rightarrow $ [sequence of target characters $\&$ sequence of morphological annotations] (see Section ""Experimental Study"" ). The objective function should also be manipulated accordingly. Given a training set $\lbrace {\bf x}_t, {\bf y}_t, {\bf m}_t\rbrace _{t=1}^{T}$ the goal is to maximize the joint loss function shown in ( 7 ):  $$\lambda \sum _{t=1}^{T}\log {P({\bf y}_t|{\bf x}_t;\theta )} + (1-\lambda ) \sum _{t=1}^{T}\log {P({\bf m}_t|{\bf x}_t;\theta )}$$   (Eq. 7)  where $\textbf {x}_t$ is the $t$ -th input sentence whose translation is a sequence of target characters shown by $\textbf {y}_t$ . $\textbf {m}_t$ is the sequence of morphological annotations and $T$ is the size of the training set. $\theta $ is the set of network parameters and $\lambda $ is a scalar to balance the contribution of each cost function. $\lambda $ is adjusted on the development set during training.",Combining the Extended Output Layer and the Embedded Morphology Table,"In the first scenario, we aim to provide the decoder with useful information about morphological properties of the target language, but we are not sure whether signals sent from the table are what we really need. They might be helpful or even harmful, so there should be a mechanism to control their quality. In the second scenario we also have a similar problem as the last layer requires some information to predict the correct morphological class through the second channel, but there is no guarantee to ensure that information in the decoder is sufficient for this sort of prediction. In order to address these problems, in the third extension we combine both scenarios as they are complementary and can potentially help each other. The morphology table acts as an additional useful source of knowledge as it already consists of affixes, but its content should be adapted according to the decoder and its actual needs. Accordingly, we need a trainer to update the table properly. The extra prediction channel plays this role for us as it forces the network to predict the target language's affixes at the output layer. The error computed in the second channel is back-propagated to the network including the morphology table and updates its affix information into what the decoder actually needs for its prediction. Therefore, the second output channel helps us train better affix embeddings. The morphology table also helps the second predictor. Without considering the table, the last layer only includes information about the input sequence and previously predicted outputs, which is not directly related to morphological information. The second attention module retrieves useful affixes from the morphology table and concatenates to the last layer, which means the decoder is explicitly fed with morphological information. Therefore, these two modules mutually help each other. The external channel helps update the morphology table with high-quality affixes (backward pass) and the table sends its high-quality signals to the prediction layer (forward pass). The relation between these modules and the NMT architecture is illustrated in Figure 2 .",Experimental Study,"As previously reviewed, different models try to capture complexities on the encoder side, but to the best of our knowledge the only model which proposes a technique to deal with complex constituents on the decoder side is that of chung-cho-bengio, which should be an appropriate baseline for our comparisons. Moreover, it outperforms other existing NMT models, so we prefer to compare our network to the best existing model. This model is referred to as CDNMT in our experiments. In the next sections first we explain our experimental setting, corpora, and how we build the morphology table (Section ""Experimental Setting"" ), and then report experimental results (Section ""Experimental Results"" ).",Experimental Setting,"In order to make our work comparable we try to follow the same experimental setting used in CDNMT, where the GRU size is 1024, the affix and word embedding size is 512, and the beam width is 20. Our models are trained using stochastic gradient descent with Adam BIBREF6 . chung-cho-bengio and sennrich2015neural demonstrated that bpe boosts NMT, so similar to CDNMT we also preprocess the source side of our corpora using bpe. We use WMT-15 corpora to train the models, newstest-2013 for tuning and newstest-2015 as the test sets. For English–Turkish (En–Tr) we use the OpenSubtitle2016 collection BIBREF7 . The training side of the English–German (En–De), English–Russian (En–Ru), and En–Tr corpora include $4.5$ , $2.1$ , and 4 million parallel sentences, respectively. We randomly select 3K sentences for each of the development and test sets for En–Tr. For all language pairs we keep the 400 most frequent characters as the target-side character set and replace the remainder (infrequent characters) with a specific character. One of the key modules in our architecture is the morphology table. In order to implement it we use a look-up table whose columns include embeddings for the target language's affixes (each column represents one affix) which are updated during training. As previously mentioned, the table is intended to provide useful, morphological information so it should be initialized properly, for which we use a morphology-aware embedding-learning model. To this end, we use the neural language model of botha2014compositional in which each word is represented via a linear combination of the embeddings of its surface form and subunits, e.g. $\overrightarrow{terbiyesizlik} = \overrightarrow{terbiyesizlik} + \overrightarrow{terbiye} + \overrightarrow{siz} + \overrightarrow{lik}$ . Given a sequence of words, the neural language model tries to predict the next word, so it learns sentence-level dependencies as well as intra-word relations. The model trains surface form and subword-level embeddings which provides us with high-quality affix embeddings. Our neural language model is a recurrent network with a single 1000-dimensional GRU layer, which is trained on the target sides of our parallel corpora. The embedding size is 512 and we use a batch size of 100 to train the model. Before training the neural language model, we need to manipulate the training corpus to decompose words into morphemes for which we use Morfessor BIBREF8 , an unsupervised morphological analyzer. Using Morfessor each word is segmented into different subunits where we consider the longest part as the stem of each word; what appears before the stem is taken as a member of the set of prefixes (there might be one or more prefixes) and what follows the stem is considered as a member of the set of suffixes. Since Morfessor is an unsupervised analyzer, in order to minimize segmentation errors and avoid noisy results we filter its output and exclude subunits which occur fewer than 500 times. After decomposing, filtering, and separating stems from affixes, we extracted several affixes which are reported in Table 2 . We emphasize that there might be wrong segmentations in Morfessor's output, e.g. Turkish is a suffix-based language, so there are no prefixes in this language, but based on what Morfessor generated we extracted 11 different types of prefixes. We do not post-process Morfessor's outputs. Using the neural language model we train word, stem, and affix embeddings, and initialize the look-up table (but not other parts) of the decoder using those affixes. The look-up table includes high-quality affixes trained on the target side of the parallel corpus by which we train the translation model. Clearly, such an affix table is an additional knowledge source for the decoder. It preserves information which is very close to what the decoder actually needs. However, there might be some missing pieces of information or some incompatibility between the decoder and the table, so we do not freeze the morphology table during training, but let the decoder update it with respect to its needs in the forward and backward passes.",Experimental Results,"Table 3 summarizes our experimental results. We report results for the bpe $\rightarrow $ char setting, which means the source token is a bpe unit and the decoder samples a character at each time step. CDNMT is the baseline model. Table 3 includes scores reported from the original CDNMT model BIBREF0 as well as the scores from our reimplementation. To make our work comparable and show the impact of the new architecture, we tried to replicate CDNMT's results in our experimental setting, we kept everything (parameters, iterations, epochs etc.) unchanged and evaluated the extended model in the same setting. Table 3 reports BLEU scores BIBREF9 of our NMT models. Table 3 can be interpreted from different perspectives but the main findings are summarized as follows: The morphology table yields significant improvements for all languages and settings. The morphology table boosts the En–Tr engine more than others and we think this is because of the nature of the language. Turkish is an agglutinative language in which morphemes are clearly separable from each other, but in German and Russian morphological transformations rely more on fusional operations rather than agglutination. It seems that there is a direct relation between the size of the morphology table and the gain provided for the decoder, because Russian and Turkish have bigger tables and benefit from the table more than German which has fewer affixes. The auxiliary output channel is even more useful than the morphology table for all settings but En–Ru, and we think this is because of the morpheme-per-word ratio in Russian. The number of morphemes attached to a Russian word is usually more than those of German and Turkish words in our corpora, and it makes the prediction harder for the classifier (the more the number of suffixes attached to a word, the harder the classification task). The combination of the morphology table and the extra output channel provides the best result for all languages. Figure 3 depicts the impact of the morphology table and the extra output channel for each language. To further study our models' behaviour and ensure that our extensions do not generate random improvements we visualized some attention weights when generating `terbiyesizlik'. In Figure 4 , the upper figure shows attention weights for all Turkish affixes, where the y axis shows different time steps and the x axis includes attention weights of all affixes (304 columns) for those time steps, e.g. the first row and the first column represents the attention weight assigned to the first Turkish affix when sampling t in `terbiyesizlik'. While at the first glance the figure may appear to be somewhat confusing, but it provides some interesting insights which we elaborate next. In addition to the whole attention matrix we also visualized a subset of weights to show how the morphology table provides useful information. In the second figure we study the behaviour of the morphology table for the first (t $_1$ ), fifth (i $_5$ ), ninth (i $_{9}$ ), and twelfth (i $_{12}$ ) time steps when generating the same Turkish word `t $_1$ erbi $_5$ yesi $_9$ zli $_{12}$ k'. t $_1$ is the first character of the word. We also have three i characters from different morphemes, where the first one is part of the stem, the second one belongs to the suffix `siz', and the third one to `lik'. It is interesting to see how the table reacts to the same character from different parts. For each time step we selected the top-10 affixes which have the highest attention weights. The set of top-10 affixes can be different for each step, so we made a union of those sets which gives us 22 affixes. The bottom part of Figure 4 shows the attention weights for those 22 affixes at each time step. After analyzing the weights we observed interesting properties about the morphology table and the auxiliary attention module. The main findings about the behaviour of the table are as follows: The model assigns high attention weights to stem-C for almost all time steps. However, the weights assigned to this class for t $_1$ and i $_5$ are much higher than those of affix characters (as they are part of the stem). The vertical lines in both figures approve this feature (bad behaviour). For some unknown reasons there are some affixes which have no direct relation to that particulate time step but they receive a high attention, such as maz in t $_{12}$ (bad behaviour). For almost all time steps the highest attention weight belongs to the class which is expected to be selected, e.g. weights for (i $_5$ ,stem-C) or (i $_{9}$ ,siz-C) (good behaviour). The morphology table may send bad or good signals but it is consistent for similar or co-occurring characters, e.g. for the last three time steps l $_{11}$ , i $_{12}$ , and k $_{13}$ , almost the same set of affixes receives the highest attention weights. This consistency is exactly what we are looking for, as it can define a reliable external constraint for the decoder to guide it. Vertical lines on the figure also confirm this fact. They show that for a set of consecutive characters which belong to the same morpheme the attention module sends a signal from a particular affix (good behaviour). There are some affixes which might not be directly related to that time step but receive high attention weights. This is because those affixes either include the same character which the decoder tries to predict (e.g. i-C for i $_{4}$ or t-C and tin-C for t $_{1}$ ), or frequently appear with that part of the word which includes the target character (e.g. mi-C has a high weight when predicting t $_1$ because t $_1$ belongs to terbiye which frequently collocates with mi-C: terbiye+mi) (good behaviour). Finally, in order to complete our evaluation study we feed the English-to-German NMT model with the sentence `Terms and conditions for sending contributions to the BBC', to show how the model behaves differently and generates a better target sentence. Translations generated by our models are illustrated in Table 4 . The table demonstrates that our architecture is able to control the decoder and limit its selections, e.g. the word `allgemeinen' generated by the baseline model is redundant. There is no constraint to inform the baseline model that this word should not be generated, whereas our proposed architecture controls the decoder in such situations. After analyzing our model, we realized that there are strong attention weights assigned to the w-space (indicating white space characters) and BOS (beginning of the sequence) columns of the affix table while sampling the first character of the word `Geschäft', which shows that the decoder is informed about the start point of the sequence. Similar to the baseline model's decoder, our decoder can sample any character including `a' of `allgemeinen' or `G' of `Geschäft'. Translation information stored in the baseline decoder is not sufficient for selecting the right character `G', so the decoder wrongly starts with `i' and continues along a wrong path up to generating the whole word. However, our decoder's information is accompanied with signals from the affix table which force it to start with a better initial character, whose sampling leads to generating the correct target word. Another interesting feature about the table is the new structure `Geschäft s bedingungen' generated by the improved model. As the reference translation shows, in the correct form these two structures should be glued together via `s', which can be considered as an infix. As our model is supposed to detect this sort of intra-word relation, it treats the whole structure as two compounds which are connected to one another via an infix. Although this is not a correct translation and it would be trivial to post-edit into the correct output form, it is interesting to see how our mechanism forces the decoder to pay attention to intra-word relations. Apart from these two interesting findings, the number of wrong character selections in the baseline model is considerably reduced in the improved model because of our enhanced architecture.",Conclusion and Future Work,"In this paper we proposed a new architecture to incorporate morphological information into the NMT pipeline. We extended the state-of-the-art NMT model BIBREF0 with a morphology table. The table could be considered as an external knowledge source which is helpful as it increases the capacity of the model by increasing the number of network parameters. We tried to benefit from this advantage. Moreover, we managed to fill the table with morphological information to further boost the NMT model when translating into MRLs. Apart from the table we also designed an additional output channel which forces the decoder to predict morphological annotations. The error signals coming from the second channel during training inform the decoder with morphological properties of the target language. Experimental results show that our techniques were useful for NMT of MRLs. As our future work we follow three main ideas. $i$ ) We try to find more efficient ways to supply morphological information for both the encoder and decoder. $ii$ ) We plan to benefit from other types of information such as syntactic and semantic annotations to boost the decoder, as the table is not limited to morphological information alone and can preserve other sorts of information. $iii$ ) Finally, we target sequence generation for fusional languages. Although our model showed significant improvements for both German and Russian, the proposed model is more suitable for generating sequences in agglutinative languages.",Acknowledgments,"We thank our anonymous reviewers for their valuable feedback, as well as the Irish centre for high-end computing (www.ichec.ie) for providing computational infrastructures. This work has been supported by the ADAPT Centre for Digital Content Technology which is funded under the SFI Research Centres Programme (Grant 13/RC/2106) and is co-funded under the European Regional Development Fund. ",,,,,,,,,,,,,,,,,,,,,How are the auxiliary signals from the morphology table incorporated in the decoder?,7aab78e90ba1336950a2b0534cc0cb214b96b4fd,infinity,familiar,no,,50d8b4a941c26b89482c94ab324b5a274f9ced66,False,,,"In order to address the aforementioned problems we redesign the neural decoder in three different scenarios. In the first scenario we equip the decoder with an additional morphology table including target-side affixes. We place an attention module on top of the table which is controlled by the decoder. At each step, as the decoder samples a character, it searches the table to find the most relevant information which can enrich its state. Signals sent from the table can be interpreted as additional constraints. In the second scenario we share the decoder between two output channels. The first one samples the target character and the other one predicts the morphological annotation of the character. This multi-tasking approach forces the decoder to send morphology-aware information to the final layer which results in better predictions. In the third scenario we combine these two models. Section ""Proposed Architecture"" provides more details on our models.","In the first scenario we equip the decoder with an additional morphology table including target-side affixes. We place an attention module on top of the table which is controlled by the decoder. At each step, as the decoder samples a character, it searches the table to find the most relevant information which can enrich its state. Signals sent from the table can be interpreted as additional constraints. In the second scenario we share the decoder between two output channels. The first one samples the target character and the other one predicts the morphological annotation of the character. This multi-tasking approach forces the decoder to send morphology-aware information to the final layer which results in better predictions. In the third scenario we combine these two models. ",0c30047a09c8ae76d8d19cfbdd5e99373cca653b,1ba1b5b562aef9cd264cace5b7bdd46a7c065c0a,,,,,,,,,"What type of morphological information is contained in the ""morphology table""?",b7fe91e71da8f4dc11e799b3bd408d253230e8c6,infinity,familiar,no,,50d8b4a941c26b89482c94ab324b5a274f9ced66,False,,,"In order to address the aforementioned problems we redesign the neural decoder in three different scenarios. In the first scenario we equip the decoder with an additional morphology table including target-side affixes. We place an attention module on top of the table which is controlled by the decoder. At each step, as the decoder samples a character, it searches the table to find the most relevant information which can enrich its state. Signals sent from the table can be interpreted as additional constraints. In the second scenario we share the decoder between two output channels. The first one samples the target character and the other one predicts the morphological annotation of the character. This multi-tasking approach forces the decoder to send morphology-aware information to the final layer which results in better predictions. In the third scenario we combine these two models. Section ""Proposed Architecture"" provides more details on our models.",In the first scenario we equip the decoder with an additional morphology table including target-side affixes.,ff4d2624ba02347ad9c6d7f4d6a0b1eb73435788,1ba1b5b562aef9cd264cace5b7bdd46a7c065c0a,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1-Table1-1.png,Table 1: Illustrating subword units in MCWs. The boldfaced part indicates the stem.,4-Figure1-1.png,Figure 1: The target label that each output channel is supposed to predict when generating the Turkish sequence ‘bu1 terbiyesizlik2 için3’ meaning ‘because3 of3 this1 rudeness2’.,5-Figure2-1.png,Figure 2: The architecture of the NMT model with an auxiliary prediction channel and an extra morphology table. This network includes only one decoder layer and one encoder layer. ⊕ shows the attention modules.,6-Table2-1.png,Table 2: The number of affixes extracted for each language.,7-Table3-1.png,"Table 3: CDNMT∗ is our implementation of CDNMT. m and o indicates that the base model is extended with the morphology table and the additional output channel, respectively. mo is the combination of both the extensions. The improvement provided by the boldfaced number compared to CDNMT∗ is statistically significant according to paired bootstrap re-sampling (Koehn, 2004) with p = 0.05.",7-Figure3-1.png,"Figure 3: The y axis shows the difference between the BLEU score of CDNMT∗ and the extended model. The first, second, and third bars show the m, o, and mo extensions, respectively.",,,,,,,,,target-side affixes,,8-Figure4-1.png,Figure 4: Visualizing the attention weights between the morphology table and the decoder when generating ‘terbiyesizlik.,,,,,,,,,,an additional morphology table including target-side affixes. We inject the decoder with morphological properties of the target language.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improving Open Information Extraction via Iterative Rank-Aware Learning,"Open information extraction (IE) is the task of extracting open-domain assertions from natural language sentences. A key step in open IE is confidence modeling, ranking the extractions based on their estimated quality to adjust precision and recall of extracted assertions. We found that the extraction likelihood, a confidence measure used by current supervised open IE systems, is not well calibrated when comparing the quality of assertions extracted from different sentences. We propose an additional binary classification loss to calibrate the likelihood to make it more globally comparable, and an iterative learning process, where extractions generated by the open IE model are incrementally included as training samples to help the model learn from trial and error. Experiments on OIE2016 demonstrate the effectiveness of our method. Code and data are available at https://github.com/jzbjyb/oie_rank.",Introduction,"Open information extraction (IE, sekine2006demand, Banko:2007:OIE) aims to extract open-domain assertions represented in the form of $n$ -tuples (e.g., was born in; Barack Obama; Hawaii) from natural language sentences (e.g., Barack Obama was born in Hawaii). Open IE started from rule-based BIBREF0 and syntax-driven systems BIBREF1 , BIBREF2 , and recently has used neural networks for supervised learning BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 , BIBREF7 . A key step in open IE is confidence modeling, which ranks a list of candidate extractions based on their estimated quality. This is important for downstream tasks, which rely on trade-offs between the precision and recall of extracted assertions. For instance, an open IE-powered medical question answering (QA) system may require its assertions in higher precision (and consequently lower recall) than QA systems for other domains. For supervised open IE systems, the confidence score of an assertion is typically computed based on its extraction likelihood given by the model BIBREF3 , BIBREF5 . However, we observe that this often yields sub-optimal ranking results, with incorrect extractions of one sentence having higher likelihood than correct extractions of another sentence. We hypothesize this is due to the issue of a disconnect between training and test-time objectives. Specifically, the system is trained solely to raise likelihood of gold-standard extractions, and during training the model is not aware of its test-time behavior of ranking a set of system-generated assertions across sentences that potentially include incorrect extractions. To calibrate open IE confidences and make them more globally comparable across different sentences, we propose an iterative rank-aware learning approach, as outlined in fig:arch. Given extractions generated by the model as training samples, we use a binary classification loss to explicitly increase the confidences of correct extractions and decrease those of incorrect ones. Without adding additional model components, this training paradigm naturally leads to a better open IE model, whose extractions can be further included as training samples. We further propose an iterative learning procedure that gradually improves the model by incrementally adding extractions to the training data. Experiments on the OIE2016 dataset BIBREF8 indicate that our method significantly outperforms both neural and non-neural models.",Neural Models for Open IE,We briefly revisit the formulation of open IE and the neural network model used in our paper.,Problem Formulation,"Given sentence $\mathbf {s}=(w_1, w_2, ..., w_n)$ , the goal of open IE is to extract assertions in the form of tuples $\mathbf {r}=(\mathbf {p}, \mathbf {a}_1, \mathbf {a}_2, ..., \mathbf {a}_m)$ , composed of a single predicate and $m$ arguments. Generally, these components in $\mathbf {r}$ need not to be contiguous, but to simplify the problem we assume they are contiguous spans of words from $\mathbf {s}$ and there is no overlap between them. Methods to solve this problem have recently been formulated as sequence-to-sequence generation BIBREF4 , BIBREF5 , BIBREF6 or sequence labeling BIBREF3 , BIBREF7 . We adopt the second formulation because it is simple and can take advantage of the fact that assertions only consist of words from the sentence. Within this framework, an assertion $\mathbf {r}$ can be mapped to a unique BIO BIBREF3 label sequence $\mathbf {y}$ by assigning $O$ to the words not contained in $\mathbf {r}$ , $B_{p}$ / $I_{p}$ to the words in $\mathbf {p}$ , and $B_{a_i}$ / $I_{a_i}$ to the words in $\mathbf {a}_i$ respectively, depending on whether the word is at the beginning or inside of the span. The label prediction $\hat{\mathbf {y}}$ is made by the model given a sentence associated with a predicate of interest $(\mathbf {s}, v)$ . At test time, we first identify verbs in the sentence as candidate predicates. Each sentence/predicate pair is fed to the model and extractions are generated from the label sequence.",Model Architecture and Decoding,"Our training method in sec:ours could potentially be used with any probabilistic open IE model, since we make no assumptions about the model and only the likelihood of the extraction is required for iterative rank-aware learning. As a concrete instantiation in our experiments, we use RnnOIE BIBREF3 , BIBREF9 , a stacked BiLSTM with highway connections BIBREF10 , BIBREF11 and recurrent dropout BIBREF12 . Input of the model is the concatenation of word embedding and another embedding indicating whether this word is predicate: $
\mathbf {x}_t = [\mathbf {W}_{\text{emb}}(w_t), \mathbf {W}_{\text{mask}}(w_t = v)].
$  The probability of the label at each position is calculated independently using a softmax function: $
P(y_t|\mathbf {s}, v) \propto \text{exp}(\mathbf {W}_{\text{label}}\mathbf {h}_t + \mathbf {b}_{\text{label}}),
$  where $\mathbf {h}_t$ is the hidden state of the last layer. At decoding time, we use the Viterbi algorithm to reject invalid label transitions BIBREF9 , such as $B_{a_2}$ followed by $I_{a_1}$ . We use average log probability of the label sequence BIBREF5 as its confidence:  $$c(\mathbf {s}, v, \hat{\mathbf {y}}) = \frac{\sum _{t=1}^{|\mathbf {s}|}{\log {P(\hat{y_t}|\mathbf {s}, v)}}}{|\mathbf {s}|}.$$   (Eq. 7)  The probability is trained with maximum likelihood estimation (MLE) of the gold extractions. This formulation lacks an explicit concept of cross-sentence comparison, and thus incorrect extractions of one sentence could have higher confidence than correct extractions of another sentence.",Iterative Rank-Aware Learning,"In this section, we describe our proposed binary classification loss and iterative learning procedure.",Binary Classification Loss,"To alleviate the problem of incomparable confidences across sentences, we propose a simple binary classification loss to calibrate confidences to be globally comparable. Given a model $\theta ^\prime $ trained with MLE, beam search is performed to generate assertions with the highest probabilities for each predicate. Assertions are annotated as either positive or negative with respect to the gold standard, and are used as training samples to minimize the hinge loss:  $$\hspace{-2.84526pt}\hat{\theta } = \underset{\theta }{\operatornamewithlimits{arg\,min}}\hspace{-8.53581pt}\underset{\begin{array}{c}\mathbf {s} \in \mathcal {D}\\ v, \hat{\mathbf {y}} \in g_{\theta ^\prime }(\mathbf {s})\end{array}}{\operatorname{\mathbb {E}}}\hspace{-11.38109pt}\max {(0,1-t \cdot c_{\theta }(\mathbf {s}, v, \hat{\mathbf {y}}))},$$   (Eq. 9)  where $\mathcal {D}$ is the training sentence collection, $g_{\theta ^\prime }$ represents the candidate generation process, and $t \in \lbrace 1,-1\rbrace $ is the binary annotation. $c_{\theta }(\mathbf {s}, v, \hat{\mathbf {y}})$ is the confidence score calculated by average log probability of the label sequence. The binary classification loss distinguishes positive extractions from negative ones generated across different sentences, potentially leading to a more reliable confidence measure and better ranking performance.",Iterative Learning,"Compared to using external models for confidence modeling, an advantage of the proposed method is that the base model does not change: the binary classification loss just provides additional supervision. Ideally, the resulting model after one-round of training becomes better not only at confidence modeling, but also at assertion generation, suggesting that extractions of higher quality can be added as training samples to continue this training process iteratively. The resulting iterative learning procedure (alg:iter) incrementally includes extractions generated by the current model as training samples to optimize the binary classification loss to obtain a better model, and this procedure is continued until convergence. [t] training data $\mathcal {D}$ , initial model $\theta ^{(0)}$ model after convergence $\theta $ $t \leftarrow 0$ # iteration  $\mathcal {E} \leftarrow \emptyset $ # generated extractions not converge $\mathcal {E} \leftarrow \mathcal {E} \cup \lbrace (\mathbf {s}, v, \hat{\mathbf {y}})|v,\hat{\mathbf {y}} \in g_{\theta ^{(t)}}(\mathbf {s}), \forall \mathbf {s} \in \mathcal {D}\rbrace $   $\theta ^{(t+1)} \leftarrow \underset{\theta }{\operatornamewithlimits{arg\,min}}\hspace{-8.53581pt}\underset{(\mathbf {s}, v, \hat{\mathbf {y}})\in \mathcal {E}}{\operatorname{\mathbb {E}}}\hspace{-8.53581pt}\max {(0,1-t \cdot c_{\theta }(\mathbf {s}, v, \hat{\mathbf {y}}))}$   $t \leftarrow t+1$ Iterative learning. ",Experimental Settings,"We use the OIE2016 dataset BIBREF8 to evaluate our method, which only contains verbal predicates. OIE2016 is automatically generated from the QA-SRL dataset BIBREF13 , and to remove noise, we remove extractions without predicates, with less than two arguments, and with multiple instances of an argument. The statistics of the resulting dataset are summarized in tab:data. We follow the evaluation metrics described by Stanovsky:2016:OIE2016: area under the precision-recall curve (AUC) and F1 score. An extraction is judged as correct if the predicate and arguments include the syntactic head of the gold standard counterparts. We compare our method with both competitive neural and non-neural models, including RnnOIE BIBREF3 , OpenIE4, ClausIE BIBREF2 , and PropS BIBREF14 . Our implementation is based on AllenNLP BIBREF15 by adding binary classification loss function on the implementation of RnnOIE. The network consists of 4 BiLSTM layers (2 forward and 2 backward) with 64-dimensional hidden units. ELMo BIBREF16 is used to map words into contextualized embeddings, which are concatenated with a 100-dimensional predicate indicator embedding. The recurrent dropout probability is set to 0.1. Adadelta BIBREF17 with $\epsilon =10^{-6}$ and $\rho =0.95$ and mini-batches of size 80 are used to optimize the parameters. Beam search size is 5.",Evaluation Results,"tab:expmain lists the evaluation results. Our base model (RnnOIE, sec:oie) performs better than non-neural systems, confirming the advantage of supervised training under the sequence labeling setting. To test if the binary classification loss (E.q. 9 , sec:ours) could yield better-calibrated confidence, we perform one round of fine-tuning of the base model with the hinge loss ( $+$ Binary loss in tab:expmain). We show both the results of using the confidence (E.q. 7 ) of the fine-tuned model to rerank the extractions of the base model (Rerank Only), and the end-to-end performance of the fine-tuned model in assertion generation (Generate). We found both settings lead to improved performance compared to the base model, which demonstrates that calibrating confidence using binary classification loss can improve the performance of both reranking and assertion generation. Finally, our proposed iterative learning approach (alg:iter, sec:ours) significantly outperforms non-iterative settings. We also investigate the performance of our iterative learning algorithm with respect to the number of iterations in fig:iter. The model obtained at each iteration is used to both rerank the extractions generated by the previous model and generate new extractions. We also report results of using only positive samples for optimization. We observe the AUC and F1 of both reranking and generation increases simultaneously for the first 6 iterations and converges after that, which demonstrates the effectiveness of iterative training. The best performing iteration achieves AUC of 0.125 and F1 of 0.315, outperforming all the baselines by a large margin. Meanwhile, using both positive and negative samples consistently outperforms only using positive samples, which indicates the necessity of exposure to the errors made by the system. tab:casererank compares extractions from RnnOIE before and after reranking. We can see the order is consistent with the annotation after reranking, showing the additional loss function's efficacy in calibrating the confidences; this is particularly common in extractions with long arguments. tab:casegen shows a positive extraction discovered after iterative training (first example), and a wrong extraction that disappears (second example), which shows that the model also becomes better at assertion generation. Why is the performance still relatively low? We randomly sample 50 extractions generated at the best performing iteration and conduct an error analysis to answer this question. To count as a correct extraction, the number and order of the arguments should be exactly the same as the ground truth and syntactic heads must be included, which is challenging considering that the OIE2016 dataset has complex syntactic structures and multiple arguments per predicate. We classify the errors into three categories and summarize their proportions in tab:err. “Overgenerated predicate” is where predicates not included in ground truth are overgenerated, because all the verbs are used as candidate predicates. An effective mechanism should be designed to reject useless candidates. “Wrong argument” is where extracted arguments do not coincide with ground truth, which is mainly caused by merging multiple arguments in ground truth into one. “Missing argument” is where the model fails to recognize arguments. These two errors usually happen when the structure of the sentence is complicated and coreference is involved. More linguistic information should be introduced to solve these problems.",Conclusion,"We propose a binary classification loss function to calibrate confidences in open IE. Iteratively optimizing the loss function enables the model to incrementally learn from trial and error, yielding substantial improvement. An error analysis is performed to shed light on possible future directions.",Acknowledgements,"This work was supported in part by gifts from Bosch Research, and the Carnegie Bosch Institute.",,,,,,,,,,,,,,,,,,,,,How does this compare to traditional calibration methods like Platt Scaling?,ca7e71131219252d1fab69865804b8f89a2c0a8f,two,familiar,no,information extraction,50d8b4a941c26b89482c94ab324b5a274f9ced66,False,,No reliability diagrams are provided and no explicit comparison is made between confidence scores or methods.,"Compared to using external models for confidence modeling, an advantage of the proposed method is that the base model does not change: the binary classification loss just provides additional supervision. Ideally, the resulting model after one-round of training becomes better not only at confidence modeling, but also at assertion generation, suggesting that extractions of higher quality can be added as training samples to continue this training process iteratively. The resulting iterative learning procedure (alg:iter) incrementally includes extractions generated by the current model as training samples to optimize the binary classification loss to obtain a better model, and this procedure is continued until convergence. [t] training data $\mathcal {D}$ , initial model $\theta ^{(0)}$ model after convergence $\theta $ $t \leftarrow 0$ # iteration A key step in open IE is confidence modeling, which ranks a list of candidate extractions based on their estimated quality. This is important for downstream tasks, which rely on trade-offs between the precision and recall of extracted assertions. For instance, an open IE-powered medical question answering (QA) system may require its assertions in higher precision (and consequently lower recall) than QA systems for other domains. For supervised open IE systems, the confidence score of an assertion is typically computed based on its extraction likelihood given by the model BIBREF3 , BIBREF5 . However, we observe that this often yields sub-optimal ranking results, with incorrect extractions of one sentence having higher likelihood than correct extractions of another sentence. We hypothesize this is due to the issue of a disconnect between training and test-time objectives. Specifically, the system is trained solely to raise likelihood of gold-standard extractions, and during training the model is not aware of its test-time behavior of ranking a set of system-generated assertions across sentences that potentially include incorrect extractions. We follow the evaluation metrics described by Stanovsky:2016:OIE2016: area under the precision-recall curve (AUC) and F1 score. An extraction is judged as correct if the predicate and arguments include the syntactic head of the gold standard counterparts.","Compared to using external models for confidence modeling, an advantage of the proposed method is that the base model does not change: the binary classification loss just provides additional supervision. For supervised open IE systems, the confidence score of an assertion is typically computed based on its extraction likelihood given by the model BIBREF3 , BIBREF5  We follow the evaluation metrics described by Stanovsky:2016:OIE2016: area under the precision-recall curve (AUC) and F1 score.",23c5a7ddd1f154488e822601198303f3e02cc4f7,74eea9f3f4f790836045fcc75d0b3f5156901499,,,,,,,,,What's the input representation of OpenIE tuples into the model?,d77c9ede2727c28e0b5a240b2521fd49a19442e0,two,familiar,no,information extraction,50d8b4a941c26b89482c94ab324b5a274f9ced66,False,,word embeddings,"Our training method in sec:ours could potentially be used with any probabilistic open IE model, since we make no assumptions about the model and only the likelihood of the extraction is required for iterative rank-aware learning. As a concrete instantiation in our experiments, we use RnnOIE BIBREF3 , BIBREF9 , a stacked BiLSTM with highway connections BIBREF10 , BIBREF11 and recurrent dropout BIBREF12 . Input of the model is the concatenation of word embedding and another embedding indicating whether this word is predicate: $ \mathbf {x}_t = [\mathbf {W}_{\text{emb}}(w_t), \mathbf {W}_{\text{mask}}(w_t = v)]. $","Input of the model is the concatenation of word embedding and another embedding indicating whether this word is predicate: $ \mathbf {x}_t = [\mathbf {W}_{\text{emb}}(w_t), \mathbf {W}_{\text{mask}}(w_t = v)]. $",250e402e903ac21b69fd0cc88469064e3efc5d04,74eea9f3f4f790836045fcc75d0b3f5156901499,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1-Figure1-1.png,Figure 1: Iterative rank-aware learning.,3-Table1-1.png,Table 1: Dataset statistics.,4-Table2-1.png,Table 2: Case study of reranking effectiveness. Red for predicate and blue for arguments.,4-Figure2-1.png,Figure 2: AUC and F1 at different iterations.,4-Table4-1.png,Table 4: AUC and F1 on OIE2016.,5-Table5-1.png,Table 5: Proportions of three errors.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Learning to Recover Reasoning Chains for Multi-Hop Question Answering via Cooperative Games,"We propose the new problem of learning to recover reasoning chains from weakly supervised signals, i.e., the question-answer pairs. We propose a cooperative game approach to deal with this problem, in which how the evidence passages are selected and how the selected passages are connected are handled by two models that cooperate to select the most confident chains from a large set of candidates (from distant supervision). For evaluation, we created benchmarks based on two multi-hop QA datasets, HotpotQA and MedHop; and hand-labeled reasoning chains for the latter. The experimental results demonstrate the effectiveness of our proposed approach.",Introduction,"NLP tasks that require multi-hop reasoning have recently enjoyed rapid progress, especially on multi-hop question answering BIBREF0, BIBREF1, BIBREF2. Advances have benefited from rich annotations of supporting evidence, as in the popular multi-hop QA and relation extraction benchmarks, e.g., HotpotQA BIBREF3 and DocRED BIBREF4, where the evidence sentences for the reasoning process were labeled by human annotators. Such evidence annotations are crucial for modern model training, since they provide finer-grained supervision for better guiding the model learning. Furthermore, they allow a pipeline fashion of model training, with each step, such as passage ranking and answer extraction, trained as a supervised learning sub-task. This is crucial from a practical perspective, in order to reduce the memory usage when handling a large amount of inputs with advanced, large pre-trained models BIBREF5, BIBREF6, BIBREF7. Manual evidence annotation is expensive, so there are only a few benchmarks with supporting evidence annotated. Even for these datasets, the structures of the annotations are still limited, as new model designs keep emerging and they may require different forms of evidence annotations. As a result, the supervision from these datasets can still be insufficient for training accurate models. Taking question answering with multi-hop reasoning as an example, annotating only supporting passages is not sufficient to show the reasoning processes due to the lack of necessary structural information (Figure FIGREF1). One example is the order of annotated evidence, which is crucial in logic reasoning and the importance of which has also been demonstrated in text-based QA BIBREF8. The other example is how the annotated evidence pieces are connected, which requires at least the definition of arguments, such as a linking entity, concept, or event. Such information has proved useful by the recently popular entity-centric methods BIBREF9, BIBREF10, BIBREF11, BIBREF12, BIBREF0, BIBREF2 and intuitively will be a benefit to these methods if available. We propose a cooperative game approach to recovering the reasoning chains with the aforementioned necessary structural information for multi-hop QA. Each recovered chain corresponds to a list of ordered passages and each pair of adjacent passages is connected with a linking entity. Specifically, we start with a model, the Ranker, which selects a sequence of passages arriving at the answers, with the restriction that each adjacent passage pair shares at least an entity. This is essentially an unsupervised task and the selection suffers from noise and ambiguity. Therefore we introduce another model, the Reasoner, which predicts the exact linking entity that points to the next passage. The two models play a cooperative game and are rewarded when they find a consistent chain. In this way, we restrict the selection to satisfy not only the format constraints (i.e., ordered passages with connected adjacencies) but also the semantic constraints (i.e., finding the next passage given that the partial selection can be effectively modeled by a Reasoner). Therefore, the selection can be less noisy. We evaluate the proposed method on datasets with different properties, i.e., HotpotQA and MedHop BIBREF13, to cover cases with both 2-hop and 3-hop reasoning. We created labeled reasoning chains for both datasets. Experimental results demonstrate the significant advantage of our proposed approach.",Task Definition,"Reasoning Chains Examples of reasoning chains in HotpotQA and MedHop are shown in Figure FIGREF1. Formally, we aim at recovering the reasoning chain in the form of $(p_1 \rightarrow e_{1,2} \rightarrow p_2 \rightarrow e_{2,3} \rightarrow \cdots \rightarrow e_{n-1,n} \rightarrow p_n)$, where each $p_i$ is a passage and each $e_{i,i+1}$ is an entity that connects $p_i$ and $p_{i+1}$, i.e., appearing in both passages. The last passage $p_n$ in the chain contains the correct answer. We say $p_i$ connects $e_{i-1,i}$ and $e_{i,i+1}$ in the sense that it describes a relationship between the two entities. Our Task Given a QA pair $(q,a)$ and all its candidate passages $\mathcal {P}$, we can extract all possible candidate chains that satisfy the conditions mentioned above, denoted as $\mathcal {C}$. The goal of reasoning chain recovery is to extract the correct chains from all the candidates, given $q,a$ and $\mathcal {P}$ as inputs. Related Work Although there are recent interests on predicting reasoning chains for multi-hop QA BIBREF0, BIBREF14, BIBREF2, they all consider a fully supervised setting; i.e., annotated reasoning chains are available. Our work is the first to recover reasoning chains in a more general unsupervised setting, thus falling into the direction of denoising over distant supervised signals. From this perspective, the most relevant studies in the NLP field includes BIBREF15, BIBREF16 for evidence identification in open-domain QA and BIBREF17, BIBREF18, BIBREF19 for rationale recovery.",Method,"The task of recovering reasoning chains is essentially an unsupervised problem, as we have no access to annotated reasoning chains. Therefore, we resort to the noisy training signal from chains obtained by distant supervision. We first propose a conditional selection model that optimizes the passage selection by considering their orders (Section SECREF4). We then propose a cooperative Reasoner-Ranker game (Section SECREF12) in which the Reasoner recovers the linking entities that point to the next passage. This enhancement encourages the Ranker to select the chains such that their distribution is easier for a linking entity prediction model (Reasoner) to capture. Therefore, it enables our model to denoise the supervision signals while recovering chains with entity information. Figure FIGREF3 gives our overall framework, with a flow describing how the Reasoner passes additional rewards to the Ranker.",Method ::: Passage Ranking Model,"The key component of our framework is the Ranker model, which is provided with a question $q$ and $K$ passages $\mathcal {P} = \lbrace p_1, p_2 ... p_K\rbrace $ from a pool of candidates, and outputs a chain of selected passages.",Method ::: Passage Ranking Model ::: Passage Scoring,"For each step of the chain, the Ranker estimates a distribution of the selection of each passage. To this end we first encode the question and passage with a 2-layer bi-directional GRU network, resulting in an encoded question $\mathbf {Q} = \lbrace \vec{\mathbf {q}_0}, \vec{\mathbf {q}_1}, ..., \vec{\mathbf {q}_N}\rbrace $ and $\mathbf {H}_i = \lbrace \vec{\mathbf {h}_{i,0}}, \vec{\mathbf {h}_{i,1}}, ..., \vec{\mathbf {h}_{i,M_i}}\rbrace $ for each passage $p_i \in P$ of length $M_i$. Then we use the MatchLSTM model BIBREF20 to get the matching score between $\mathbf {Q}$ and each $\mathbf {H}_i$ and derive the distribution of passage selection $P(p_i|q)$ (see Appendix SECREF6 for details). We denote $P(p_i|q)=\textrm {MatchLSTM}(\mathbf {H}_i, \mathbf {Q})$ for simplicity.",Method ::: Passage Ranking Model ::: Conditional Selection,"To model passage dependency along the chain of reasoning, we use a hard selection model that builds a chain incrementally. Provided with the $K$ passages, at each step $t$ the Ranker computes $P^t(p_i|\mathbf {Q}^{t-1}), i = 0, ..., K$, which is the probability of selecting passage $p_i$ conditioned on the query and previous states representation $\mathbf {Q}^{t-1}$. Then we sample one passage $p^t_{\tau }$ according to the predicted selection probability.  The first step starts with the original question $\mathbf {Q}^0$. A feed-forward network is used to project the concatenation of query encoding and selected passage encoding $\tilde{\mathbf {m}}^t_{p_{\tau }}$ back to the query space, and the new query $\mathbf {Q}^{t+1}$ is used to select the next passage.",Method ::: Passage Ranking Model ::: Reward via Distant Supervision,"We use policy gradient BIBREF21 to optimize our model. As we have no access to annotated reasoning chains during training, the reward comes from distant supervision. Specifically, we reward the Ranker if a selected passage appears as the corresponding part of a distant supervised chain in $\mathcal {C}$. The model receives immediate reward at each step of selection. In this paper we only consider chains consist of $\le 3$ passages (2-hop and 3-hop chains). For the 2-hop cases, our model predicts a chain of two passages from the candidate set $\mathcal {C}$ in the form of $p_h\rightarrow e \rightarrow p_t$. Each candidate chain satisfies that $p_t$ contains the answer, while $p_h$ and $p_t$ contain a shared entity $e$. We call $p_h$ the head passage and $p_t$ the tail passage. Let $\mathcal {P}_{T}/\mathcal {P}_{H}$ denote the set of all tail/head passages from $\mathcal {C}$. Our model receives rewards $r_h, r_t$ according to its selections: For the 3-hop cases, we need to select an additional intermediate passage $p_m$ between $p_h$ and $p_t$. If we reward any $p_m$ selection that appears in the middle of a chain in candidate chain set $\mathcal {C}$, the number of feasible options can be very large. Therefore, we make our model first select the head passage $p_h$ and the tail passage $p_t$ independently and then select $p_m$ conditioned on $(p_h,p_t)$. We further restrict that each path in $\mathcal {C}$ must have the head passage containing an entity from $q$. Then the selected $p_m$ is only rewarded if it appears in a chain in $\mathcal {C}$ that starts with $p_h$ and ends with $p_t$:",Method ::: Cooperative Reasoner,"To alleviate the noise in the distant supervision signal $\mathcal {C}$, in addition to the conditional selection, we further propose a cooperative Reasoner model, also implemented with the MatchLSTM architecture (see Appendix SECREF6), to predict the linking entity from the selected passages. Intuitively, when the Ranker makes more accurate passage selections, the Reasoner will work with less noisy data and thus is easier to succeed. Specifically, the Reasoner learns to extract the linking entity from chains selected by a well-trained Ranker, and it benefits the Ranker training by providing extra rewards. Taking 2-hop as an example, we train the Ranker and Reasoner alternatively as a cooperative game: Reasoner Step: Given the first passage $p_t$ selected by the trained Ranker, the Reasoner predicts the probability of each entity $e$ appearing in $p_t$. The Reasoner is trained with the cross-entropy loss:  Ranker Step: Given the Reasoner's top-1 predicted linking entity $e$, the reward for Ranker at the $2^{\textrm {nd}}$ step is defined as:   The extension to 3-hop cases is straightforward; the only difference is that the Reasoner reads both the selected $p_h$ and $p_t$ to output two entities. The Ranker receives one extra reward if the Reasoner picks the correct linking entity from $p_h$, so does $p_t$.",Experiments ::: Settings ::: Datasets,"We evaluate our path selection model on HotpotQA bridge type questions and on the MedHop dataset. In HotpotQA, the entities are pre-processed Wiki anchor link objects and in MedHop they are drug/protein database identifiers. For HotpotQA, two supporting passages are provided along with each question. We ignore the support annotations during training and use them to create ground truth on development set: following BIBREF8, we determine the order of passages according to whether a passage contains the answer. We discard ambiguous instances. For MedHop, there is no evidence annotated. Therefore we created a new evaluation dataset by manually annotating the correct paths for part of the development set: we first extract all candidate paths in form of passage triplets $(p_h, p_m, p_t)$, such that $p_h$ contains the query drug and $p_t$ contains the answer drug, and $p_h/p_m$ and $p_m/p_t$ are connected by shared proteins. We label a chain as positive if all the drug-protein or protein-protein interactions are described in the corresponding passages. Note that the positive paths are not unique for a question. During training we select chains based on the full passage set $\mathcal {P}$; at inference time we extract the chains from the candidate set $\mathcal {C}$ (see Section SECREF2). ",Experiments ::: Settings ::: Baselines and Evaluation Metric,"We compare our model with (1) random baseline, which randomly selects a candidate chain from the distant supervision chain set $\mathcal {C}$; and (2) distant supervised MatchLSTM, which uses the same base model as ours but scores and selects the passages independently. We use accuracy as our evaluation metric. As HotpotQA does not provide ground-truth linking entities, we only evaluate whether the supporting passages are fully recovered (yet our model still output the full chains). For MedHop we evaluate whether the whole predicted chain is correct. More details can be found in Appendix SECREF7. We use BIBREF24 as word embedding for HotpotQA, and BIBREF25 for MedHop. ",Experiments ::: Results ::: HotpotQA,"We first evaluate on the 2-hop HotpotQA task. Our best performed model first selects the tail passage $p_t$ and then the head passage $p_h$, because the number of candidates of tail is smaller ($\sim $2 per question). Table TABREF21 shows the results. First, training a ranker with distant supervision performs significantly better than the random baseline, showing that the training process itself has a certain degree of denoising ability to distinguish the more informative signals from distant supervision labels. By introducing additional inductive bias of orders, the conditional selection model further improves with a large margin. Finally, our cooperative game gives the best performance, showing that a trained Reasoner has the ability of ignoring entity links that are irrelevant to the reasoning chain. Table TABREF22 demonstrates the effect of selecting directions, together with the methods' recall on head passages and tail passages. The latter is evaluated on a subset of bridge-type questions in HotpotQA which has no ambiguous support annotations in passage orders; i.e., among the two human-labeled supporting passages, only one contains the answer and thus must be a tail. The results show that selecting tail first performs better. The cooperative game mainly improves the head selection.",Experiments ::: Results ::: MedHop,"Results in table TABREF21 show that recovering chains from MedHop is a much harder task: first, the large number of distant supervision chains in $\mathcal {C}$ introduce too much noise so the Distant Supervised Ranker improves only 3%; second, the dependent model leads to no improvement because $\mathcal {C}$ is strictly ordered given our data construction. Our cooperative game manages to remain effective and gives further improvement.",Conclusions,In this paper we propose the problem of recovering reasoning chains in multi-hop QA from weak supervision signals. Our model adopts an cooperative game approach where a ranker and a reasoner cooperate to select the most confident chains. Experiments on the HotpotQA and MedHop benchmarks show the effectiveness of the proposed approach.,Details of MatchLSTMs for Passage Scoring and Reasoner ::: MatchLSTM for Passage Scoring,"Given the embeddings $\mathbf {Q} = \lbrace \vec{\mathbf {q}_0}, \vec{\mathbf {q}_1}, ..., \vec{\mathbf {q}_N}\rbrace $ of the question $q$, and $\mathbf {H}_i = \lbrace \vec{\mathbf {h}_{i,0}}, \vec{\mathbf {h}_{i,1}}, ..., \vec{\mathbf {h}_{i,M_i}}\rbrace $ of each passage $p_i \in P$, we use the MatchLSTM BIBREF20 to match $\mathbf {Q}$ and $\mathbf {H}_i$ as follows: The final vector $\tilde{\mathbf {m}}_i$ represents the matching state between $q$ and $p_i$. All the $\tilde{\mathbf {m}}_i$s are then passed to a linear layer that outputs the ranking score of each passage. We apply softmax over the scores to get the probability of passage selection $P(p_i|q)$. We denote the above computation as $P(p_i|q)=\textrm {MatchLSTM}(\mathbf {H}_i, \mathbf {Q})$ for simplicity.",Details of MatchLSTMs for Passage Scoring and Reasoner ::: MatchLSTM for Reasoner,"Given the question embedding $\mathbf {Q}^r = \lbrace \vec{\mathbf {q}^r_0}, \vec{\mathbf {q}^r_1}, ..., \vec{\mathbf {q}^r_N}\rbrace $ and the input passage embedding $\mathbf {H}^r = \lbrace \vec{\mathbf {h}^r_{0}}, \vec{\mathbf {h}^r_{1}}, ..., \vec{\mathbf {h}^r_{M}}\rbrace $ of $p$, the Reasoner predicts the probability of each entity in the passage being the linking entity of the next passage in the chain. We use a reader model similar to BIBREF3 as our Reasoner network. We first describe an attention sub-module. Given input sequence embedding $\mathbf {A} = \lbrace \vec{\mathbf {a}_0}, \vec{\mathbf {a}_1}, ..., \vec{\mathbf {a}_N}\rbrace $ and $\mathbf {B} = \lbrace \vec{\mathbf {b}_{0}}, \vec{\mathbf {b}_{1}}, ..., \vec{\mathbf {b}_{M}}\rbrace $, we define $\tilde{\mathcal {M}} = \text{Attention}(\mathbf {A}, \mathbf {B})$: where FFN denotes a feed forward layer which projects the concatenated embedding back to the original space. The Reasoner network consists of multiple attention layers, together with a bidirectional GRU encoder and skip connection. For each token $e_k, k = 0, 1,..., M$ represented by $h^r_{p,k}$ at the corresponding location, we have: where $g$ is the classification layer, softmax is applied across all entities to get the probability. We denote the computation above as $P^r(e_k| \mathbf {p}) = \textrm {MatchLSTM.Reader}(e_k, \mathbf {p})$ for simplicity.",Definition of Chain Accuracy,"In HotpotQA, on average we can find 6 candidate chains (2-hop) in a instance, and the human labeled true reasoning chain is unique. A predicted chain is correct if the chain only contains all supporting passages (exact match of passages). In MedHop, on average we can find 30 candidate chains (3-hop). For each candidate chain our human annotators labeled whether it is correct or not, and the correct reasoning chain is not unique. A predicted chain is correct if it is one of the chains that human labeled as correct. The accuracy is defined as the ratio: ",,,,,,,,,,,What are two models' architectures in proposed solution?,bd7039f81a5417474efa36f703ebddcf51835254,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,,,"Method ::: Passage Ranking Model The key component of our framework is the Ranker model, which is provided with a question $q$ and $K$ passages $\mathcal {P} = \lbrace p_1, p_2 ... p_K\rbrace $ from a pool of candidates, and outputs a chain of selected passages. Method ::: Cooperative Reasoner To alleviate the noise in the distant supervision signal $\mathcal {C}$, in addition to the conditional selection, we further propose a cooperative Reasoner model, also implemented with the MatchLSTM architecture (see Appendix SECREF6), to predict the linking entity from the selected passages. Intuitively, when the Ranker makes more accurate passage selections, the Reasoner will work with less noisy data and thus is easier to succeed. Specifically, the Reasoner learns to extract the linking entity from chains selected by a well-trained Ranker, and it benefits the Ranker training by providing extra rewards. Taking 2-hop as an example, we train the Ranker and Reasoner alternatively as a cooperative game:","Method ::: Passage Ranking Model
The key component of our framework is the Ranker model, which is provided with a question $q$ and $K$ passages $\mathcal {P} = \lbrace p_1, p_2 ... p_K\rbrace $ from a pool of candidates, and outputs a chain of selected passages. Method ::: Cooperative Reasoner
To alleviate the noise in the distant supervision signal $\mathcal {C}$, in addition to the conditional selection, we further propose a cooperative Reasoner model, also implemented with the MatchLSTM architecture (see Appendix SECREF6), to predict the linking entity from the selected passages.",8eefbea2f3cfcf402f9d072e674b0300e54adc66,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,How do two models cooperate to select the most confident chains?,022e5c996a72aeab890401a7fdb925ecd0570529,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,,,"To alleviate the noise in the distant supervision signal $\mathcal {C}$, in addition to the conditional selection, we further propose a cooperative Reasoner model, also implemented with the MatchLSTM architecture (see Appendix SECREF6), to predict the linking entity from the selected passages. Intuitively, when the Ranker makes more accurate passage selections, the Reasoner will work with less noisy data and thus is easier to succeed. Specifically, the Reasoner learns to extract the linking entity from chains selected by a well-trained Ranker, and it benefits the Ranker training by providing extra rewards. Taking 2-hop as an example, we train the Ranker and Reasoner alternatively as a cooperative game:","Intuitively, when the Ranker makes more accurate passage selections, the Reasoner will work with less noisy data and thus is easier to succeed. Specifically, the Reasoner learns to extract the linking entity from chains selected by a well-trained Ranker, and it benefits the Ranker training by providing extra rewards.",af6e29e48d2faba6721794b69df129ff67314a89,258ee4069f740c400c0049a2580945a1cc7f044c,How many hand-labeled reasoning chains have been created?,2a950ede24b26a45613169348d5db9176fda4f82,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,True,,,,,14dac62604a816e476874958f9232db308ef029e,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,What benchmarks are created?,34af2c512ec38483754e94e1ea814aa76552d60a,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,,Answer with content missing: (formula) The accuracy is defined as the ratio # of correct chains predicted to # of evaluation samples,"In HotpotQA, on average we can find 6 candidate chains (2-hop) in a instance, and the human labeled true reasoning chain is unique. A predicted chain is correct if the chain only contains all supporting passages (exact match of passages). In MedHop, on average we can find 30 candidate chains (3-hop). For each candidate chain our human annotators labeled whether it is correct or not, and the correct reasoning chain is not unique. A predicted chain is correct if it is one of the chains that human labeled as correct. The accuracy is defined as the ratio:","In HotpotQA, on average we can find 6 candidate chains (2-hop) in a instance, and the human labeled true reasoning chain is unique. A predicted chain is correct if the chain only contains all supporting passages (exact match of passages).

In MedHop, on average we can find 30 candidate chains (3-hop). For each candidate chain our human annotators labeled whether it is correct or not, and the correct reasoning chain is not unique. A predicted chain is correct if it is one of the chains that human labeled as correct.

The accuracy is defined as the ratio: The accuracy is defined as the ratio:",f0ac256d61835f95f747206c359e03b9e4acd2e3,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1-Figure1-1.png,"Figure 1: An example of reasoning chains in HotpotQA (2- hop) and MedHop (3-hop). HotpotQA provides only supporting passages {P3, P9}, without order and linking information.",2-Figure2-1.png,"Figure 2: Model overview. The cooperative Ranker and Reasoner are trained alternatively. The Ranker selects a passage p at each step conditioned on the question q and history selection, and receives reward r1 if p is evidence. Conditioned on q, the Reasoner predicts which entity from p links to the next evidence passage. The Ranker receives extra reward r2 if its next selection is connected by the entity predicted by the Reasoner. Both q and answer a are model inputs. While q is fed to the Ranker/Reasoner as input, empirically the best way of using a is for constructing the candidate set thus computing the reward r1. We omit the flow from q/a for simplicity.",4-Table1-1.png,Table 1: Reasoning Chain selection results.,4-Table2-1.png,Table 2: Ablation test on HotpotQA.,,,,,,,,,,,,,"Reasoner learns to extract the linking entity from chains selected by a well-trained Ranker, and it benefits the Ranker training by providing extra rewards",,,,,,,,,,,,,"Reasoner model, also implemented with the MatchLSTM architecture Ranker model",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Char-RNN and Active Learning for Hashtag Segmentation,"We explore the abilities of character recurrent neural network (char-RNN) for hashtag segmentation. Our approach to the task is the following: we generate synthetic training dataset according to frequent n-grams that satisfy predefined morpho-syntactic patterns to avoid any manual annotation. The active learning strategy limits the training dataset and selects informative training subset. The approach does not require any language-specific settings and is compared for two languages, which differ in inflection degree.",Introduction,"A hashtag is a form of metadata labeling used in various social networks to help the users to navigate through the content. For example, one of the most popular hashtags on Instagram is ""#photooftheday"" [photo of the day]. Hashtags are written without any delimiters, although some users use an underscore or camel-casing to separate words. Hashtags themselves may be a great source for features for following opinion mining and social network analysis. Basically hashtags serve as keyphrases for a post in social media. By segmenting the hashtags into separate words we may use regular techniques to process them. The problem of hashtag segmentation resembles of another problem, namely word segmentation. The problem of word segmentation is widely studied in languages like Chinese, since it lacks whitespaces to separate words, or in German to split compound words. In languages like English or Russian, where compounds are not that frequent as in German and where whitespace delimiters are regularly used, the problem of word segmentation arises mainly when working with hashtags. Formally the problem is stated as follows: given a string of $n$ character $s = s_1 \ldots s_n$ we need to define the boundaries of the substrings $s_{i:j}, i < j$, so that each substring is meaningful (i.e. is a regular word, named entity, abbreviation, number, etc). The main challenge of this problem is that the segmentation might be ambiguous. For example, a string “somethingsunclear” might be segmented as “something sun clear” or “somethings unclear”. To deal with the ambiguity more processing is required, such as POS-tagging, estimation of frequencies of all hashtag constituencies or their co-occurence frequency. The frequencies can be estimated on a large corpus, such as BNC , COCA , Wikipedia. However when working with noisy user generated data, such as texts or hashtags from social networks, the problem of unknown words (or out of vocabulary words) arises. In language modeling this problem is solved by using smoothing, such as Laplacian smoothing or Knesser-Ney smoothing. Otherwise additional heuristics can be used to extend the dictionary with word-like sequences of characters. Unlike language modelling, in hashtag segmentation frequency estimation is not only source for defining word boundaries. Otherwise candidate substrings can be evaluated according to length BIBREF0. Several research groups have shown that introducing character level into models help to deal with unknown words in various NLP tasks, such as text classification BIBREF1, named entity recognition BIBREF2, POS-tagging BIBREF3, dependency parsing BIBREF4, word tokenization and sentence segmentation BIBREF5 or machine translation BIBREF6, BIBREF7. The character level model is a model which either treats the text as a sequence of characters without any tokenization or incorporates character level information into word level information. Character level models are able to capture morphological patterns, such as prefixes and suffixes, so that the model is able to define the POS tag or NE class of an unknown word. Following this intuition, we use a character level model for hashtag segmentation. Our main motivation is the following: if the character level model is able to capture word ending patterns, it should also be able to capture the word boundary patterns. We apply a character level model, specifically, a recurrent neural network, referred further as char-RNN, to the task of hashtag segmentation. The char-RNN is trained and tested on the synthetic data, which was generated from texts, collected from social networks in English and Russian, independently. We generate synthetic data for training by extracting frequent $N$-grams and removing whitespaces. The test data is annotated manually . Since the problem statement is very basic, we use additional techniques, such as active learning, character embeddings and RNN hidden state visualization, to interpret the weights, learned by char-RNN. We address the following research questions and claim our respective contributions: We show that our char-RNN model outperforms the traditional unigram or bigram language models with extensive use of external sources BIBREF8, BIBREF0. What is the impact of high inflection in languages such as Russian on the performance of character-level modelling as opposed to languages with little inflection such as English? We claim that character-level models offer benefits for processing highly inflected languages by capturing the rich variety of word boundary patterns. As getting sufficient amount of annotated training collection is labor-intensive and error-prone, a natural question would be: can we avoid annotating real-world data altogether and still obtain high quality hashtag segmentations? We approach this problem by using morpho-syntactic patterns to generate synthetic hashtags. A potentially unlimited volume of our synthetic training dataset raises yet another question of whether an informative training subset could be selected. To this extent, we apply an active learning-based strategy to subset selection and identify a small portion of the original synthetic training dataset, necessary to obtain a high performance.",Neural Model for Hashtag Segmentation ::: Sequence Labeling Approach,"We treat hashtag segmentation as a sequence labeling task. Each character is labeled with one of the labels $\mathcal {L} = \lbrace 0, 1\rbrace $, (1) for the end of a word, and (0) otherwise (Table TABREF9 and TABREF9). Given a string $s = {s_1, \ldots , s_n}$ of characters, the task is to find the labels $Y^* = {y_1^*. \ldots , y_n^*}$, such that $ Y^* = \arg \max _{Y \in \mathcal {L} ^n} p(Y | s).$  The neural model for hashtag segmentation consists of three layers. The embedding layer is used to compute the distributed representation of input characters. Each character $c_i$ is represented with an embedding vector $e_i \in \mathbb {R}^{d_e}$, where $d_e$ is the size of the character embedding. $E$ is the look up table of size $|V| \times d_e$, where $V$ is the vocabulary, i.e. the number of unique characters. The feature layer is used to process the input. We use a bi-directional recurrent layer with LSTM units to process the input in forward and backward directions. The LSTM units we use are default keras LSTM units as introduced by Hochreiter. The inference layer is used to predict the labels of each character. We use a single dense layer as f or inference and $softmax$ to predict the probabilities of the labels $\mathcal {L} = \lbrace 0, 1\rbrace $. Each character is assigned with the most probable label. The parameters of the char-RNN are the following: Embedding layer = 50 input dimensions; Feature layer = 64 bidirectional LSTM units; Inference layer = 2 output neurons with softmax activation function mapped to each of 64 outputs.",Dataset,In this section we describe the datasets we used for hashtag segmentation. We experimented with Russian and English datasets to compare the performance of the char-RNN.,Dataset ::: Russian dataset,"To our knowledge there is no available dataset for hashtag segmentation in Russian, so we faced the need to create our own dataset. Our approach to the dataset creation was twofold: the training data was created from social network texts by selecting frequent $n$-grams and generating hashtags following some hashtag patterns. The test dataset consists of real hashtags collected from vk.com (a Russian social network) and were segmented manually. We followed the same strategy to create an English language dataset.",Dataset ::: Russian dataset ::: Training Dataset Generation,"We scraped texts from several pages about civil services from vk.com. Next we extracted frequent $n$-grams that do not contain stopwords and consist of words and digits in various combinations (such as word + 4 digits + word or word + word + 8 digits). We used several rules to merge these $n$-grams so that they resemble real hashtags, for example: remove all whitespace: wordwordworddigits Examples: ЁлкаВЗазеркалье, нескольколетназад replace all whitespace with an underscore: word_word_digits Examples: увд_юга_столицы remove some whitespace and replace other spaces with an underscore: word_worddigits. Examples: ищусвоегогероя_уфпс A word here might be a word in lower case, upper case or capitalized or an abbreviation. There might be up to four digits. In general, we introduced 11 types of hashtags, which contain simply constructed hashtags as well as the complex ones. Here are a couple of examples: The hashtag consists of two parts: the word/abbreviation in the first part and the number or word in the second. The underscore is a delimiter. Examples: word_2017, NASA_2017, word_word Two or three words, which are separated by an underscore. Examples: Word_Word, word_word_word",Dataset ::: Russian dataset ::: Test Dataset Annotation,"We segmented manually 2K the most frequent hashtags, extracted from the same collection of the scraped texts. The resulting size of the Russian dataset is 15k hashtags for training and 2k hashtags for testing.",Dataset ::: English dataset,"We used the dataset, released by BIBREF0. This dataset consists of: a collection of tweets, which we used to generate the synthetic training hashtags according to the same rules as for Russian; a collection of annotated and separated hashtags, which we used as a testing set. From this test set we excluded ambiguous hashtags, annotated with several possible segmentations. The resulting size of the English dataset is 15k hashtags for training and 1k hashtags for testing. ",Active Learning,"We followed the strategy for active learning, as in BIBREF9. The training procedure consists of multiple rounds of training and testing of the model. We start by training the model on 1k hashtags, which were randomly selected from the training dataset. Next we test the model on the reminder of the training dataset and select 1k hashtags according to the current model’s uncertainty in its prediction of the segmentation. These hashtags are not manually relabelled, since a) they belong to the synthetically generated training dataset and b) the correct labeling for these hashtag is already known. In BIBREF9 three uncertainty measure are presented, from which we selected the maximum normalized log-probability (MNLP) assigned by the model to the most likely sequence of tags. The model is then retrained on the hashtags it is uncertain about. Note, that here we do not check if the predictions of the model are correct. We are more interested in training the model on hard examples than in evaluating the quality of intermediate results. We refer the reader to BIBREF9 for more technical details.",Experiments ::: Baseline,"As for baseline algorithm, we consider the BIBREF0 system architecture as a state-of-the-art algorithm. Unfortunately, their approach is not straightforwardly applicable to our synthetic Russian dataset, because it requires twofold input: a hashtag and a corresponding tweet or a text from any other social media, which is absent in our task setting due to synthetic nature of the training dataset. For this reason as a baseline algorithm for English dataset we refer to results from BIBREF0, and as for Russian dataset, we used the probabilistic language model, described in BIBREF8. The probability of a sequence of words is the product of the probabilities of each word, given the word’s context: the preceding word. As in the following equation: where In case there is no such a pair of words $(w_{i-1}, w_i)$ in the set of bigrams, the probability of word $w_i$ is obtained as if it was only an unigram model: where $V$ – vocabulary, $f(w_{i})$ – frequency of word $w_{i}$, and $\alpha $ = 1. In Table TABREF30 we present three baseline results: LM BIBREF8 for Russian and English datasets; context-based LM BIBREF0 for English dataset only. We treat a segmentation as correct if prediction and target sequences are the same. ",Experiments ::: Neural Model,"In our experiments we used 5 epochs to train the char-RNN with LSTM units. For each language we observed three datasets with different number of hashtags. In case of Russian language, the more data we use while training, the higher the accuracy. As for English, the highest accuracy score was achieved on a set of 10k hashtags (Table TABREF32). Due to it's lower morphological diversity and complexity the model starts to overfit on training sets with large sizes. The training showed that mostly the model makes wrong predictions of segmentation on hashtags of complex types, such as “wordword_worddigits”. Our results outperform all choosen baseline both for Russian and English datasets. Note, that we have two baselines for the English dataset: one is purely frequency-based, another is cited from BIBREF0, where external resources are heavily used. We show that using significantly less amount of training data, we achieve a boost in quality by switching from statistical word language models to char-RNN. As expected, the results on Russian dataset are higher than for the English dataset due to higher inflection degree in Russian as opposed to English.",Experiments ::: Active Learning,"In order to evaluate the efficiency of deep learning with active learning when used in combination, we run the experiments for both languages. As for the datasets, we took the ones on which the highest accuracy was obtained (15k for Russian and 10k for English). The learning process consists of multiple rounds which are repeated until the test set is finished. At the beginning we train the model on 1k of randomly selected hashtags and predict the probability of segmentation for the remaining hashtags. Then we sort the remaining hashtags in ascending order according to the probability assigned by the model and pick 1k of hashtags which the model is least confident about. Finally, we add these hashtags with the least probable sequence of tags to the training data and continue training the model. This pipeline is repeated till there are no samples left. In comparison to our initial experiments, application of active learning demonstrates impressive results. The amount of labeled training data can be drastically reduced, to be more specific, in both cases the size of the training set can be reduced by half without any decline in accuracy (see Figures 2 and 3). Active learning selects a more informative set of examples in contrast to supervised learning, which is trained on a set of randomly chosen examples. We decided to analyze the updated version of the training data and see if number of morphologically complex types of hashtags is higher than the simple ones. We were able to divide hashatgs into complex and simple as the model is trained on synthetic data and there is a finite number of templates by which each hashtag can be generated. To better understand the contribution of uncertainty sampling approach, we plot the distribution of different types of hashtags in new training datasets for both languages, Russian and English (see Figure 4 and 5). According to identified types of hashtags in real data, it can be seen from the plots that in both cases the algorithm added more of morphologically complex hashtags to training data – types 3, 6 and 7. These types mostly consist of hashtags with two or three words in lower case without underscore. Examples of featured types: wordword_2017 wordword, word2017word wordwordword, wordword2017word",Experiments ::: Visualization,"In order to see if embeddings of similar characters, in terms of string segmentation, appear near each-other in their resulting 50-dimensional embedding space, we have applied one technique for dimensionality reduction: SVD to character embeddings to plot them on 2D space. For both languages meaningful and interpretable clusters can be extracted: capital letters, letters in lower case, digits and underscore, as shown below.",Related Work,"The problem of word segmentation has received much attention in Chinese and German NLP for word segmentation and compound splitting BIBREF10, respectively. The major techniques for word segmentation exploit string matching algorithms BIBREF11, language models BIBREF12, BIBREF0 and sequence labeling methods BIBREF10. Recent trend of deep learning as a major approach for any NLP task in general and sequence labeling in particular resulted in using various RNN-based models and CNN-based model for Chinese word segmentation BIBREF10, BIBREF13, BIBREF14. Since BIBREF10 Chinese word segmentation is addressed as a character labeling task: each character of the input sequence is labeled with one of the four labels $\mathcal {L} = \lbrace B, M, E, S\rbrace $, which stand for character in Begin, Middle or End of the word or Single character word. BIBREF10 uses a maximum entropy tagger to tag each character independently. This approach was extended in BIBREF15 to the sequence modeling task, and linear conditional random fields were used to attempt it and receive state of the art results. A neural approach to Chinese segmentation mainly uses various architectures of character level recurrent neural networks BIBREF16, BIBREF17, BIBREF18 and very deep constitutional networks BIBREF19. Same architectures are used for dialectal Arabic segmentation BIBREF20. The evolution of German compound splitters is more or less similar to Chinese word segmentation systems. The studies of German compound splitting started with corpus- and frequency-based approaches BIBREF13, BIBREF14 and are now dominated with neural-based distributional semantic models. However, German compound splitting is rarely seen as sequence modeling task. The problem of hashtag segmentation, analysis and usage in English has been approached by several research groups. As it was shown by BIBREF12 hashtag segmentation for TREC microblog track 2011 BIBREF21 improves the quality of information retrieval, while BIBREF0 shows that hashtag segmentation improves linking of entities extracted from tweets to a knowledge base. Both BIBREF12, BIBREF0 use Viterbi-like algorithm for hashtag segmentation: all possible segmentations of hashtag are scored using a scoring function: where $P_{Unigram}$ are probabilities, computed according to the unigram model based on a large enough corpus or any N-gram service. Following the idea of scoring segmentation candidates, BIBREF11 introduces other scoring functions, which include a bigram model (2GM) and a Maximum Unknown Matching (MUM), which is adjustable to unseen words. BIBREF22 attempt to split camel-cased hashtags using rule-based approach and POS-tagging for further semantic classification. WordSegment has been used for sentiment analysis BIBREF23, BIBREF24 and other applications. To our knowledge there has been little work done for word or hashtag segmentation in Russian.",Related Work ::: Active Learning in NLP,"Active learning is machine learning technique which allows efficient use of the available training data. It presumes that, first an initial model is trained on a very little amount of data and next tested on large unlabeled set. Next the model is able to choose a few most difficult examples and ask an external knowledge source about the desired labels. Upon receiving these labels, the model is updated and retrained on the new train set. There might be a few rounds of label querying and model updating. To use active learning strategy, we need a definition of what a difficult example is and how to score its difficulty. One of the most common scoring approaches is entropy-based uncertainty sampling, which selects the examples with the lowest prediction probability. Active learning is widely used in NLP applications, when there is little annotated data while the amount of unlabeled data is abundant. Being ultimately used for text classification using traditional machine learning classifiers BIBREF25, BIBREF26, active learning is less known to be used with deep learning sequence classifiers. Recent works report on scoring word embeddings that are likely to be updated with the greatest magnitude BIBREF27 and on using maximum normalized log-probability (MNLP) assigned by the model to the most likely sequence of tags BIBREF9:",Related Work ::: Training on synthetic data,"The lack of training data is an issue for many NLP applications. There have been attempts to generate and use synthetic data for training question answering systems BIBREF28 and SQL2text systems BIBREF29. In BIBREF0 synthetic hashtags are generated by removing whitespace characters from frequent n-grams, while in BIBREF30 German compounds are synthesized for further machine translation.",Conclusions,"In this paper we approach the problem of hashtag segmentation by using char-RNNs. We treat the problem of hashtag segmentation as a sequence labeling task, so that each symbol of a given string is labeled with 1 (there should be a whitespace after this symbol) or 0 (otherwise). We use two datasets to test this approach in English and in Russian without any language-specific settings. We compare char-RNN to traditional probabilistic algorithms. To interpret the results we use a few visualization techniques and the strategy of active learning to evaluate the complexity of training data, since we use synthetically generated hashtags for training. The results show that: When approached on character level, hashtag segmentation problem can be solved using relatively small and simple recurrent neural network model without usage of any external corpora and vocabularies. Such char-RNN not only outperforms significantly traditional frequency-based language models, but also can be trained on synthetic data generated according to morpho-syntactic patterns, without any manual annotation and preprocessing. In languages with high inflection (such as Russian) the char-RNN achieves higher results than in languages with little inflections (such as English) due to the ability of the char-RNN to capture and memorize word boundary patterns, especially word ending patterns (i.e. adjective endings “ый”,“ая”,“ое” or verbal endings “ать”,“еть” in Russian). The amount of generated synthetic training data can be limited by using techniques for active learning which allows to select sufficient training subset without any loss of quality.",Acknowledgements,The paper was prepared within the framework of the HSE University Basic Research Program and funded by the Russian Academic Excellence Project '5-100'.,,,,,,,,,Does the paper report the performance on the task of a Neural Machine Translation model?,1835f65694698a9153857e33cd9b86a96772fff5,two,familiar,no,word segmentation,486a870694ba60f1a1e7e4ec13e328164cd4b43c,False,False,,,,411f4bfaa066fd8262256f55cff0239f07e80173,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,What are the predefined morpho-syntactic patterns used to filter the training data?,a61732774faf30bab15bf944b2360ec4710870c1,two,familiar,no,word segmentation,486a870694ba60f1a1e7e4ec13e328164cd4b43c,True,,,,,40fdf6c5d4eb8e7636c671251a0658f4660c42dc,258ee4069f740c400c0049a2580945a1cc7f044c,Is the RNN model evaluated against any baseline?,994ac7aa662d16ea64b86510fcf9efa13d17b478,two,familiar,no,word segmentation,486a870694ba60f1a1e7e4ec13e328164cd4b43c,False,True,,"For this reason as a baseline algorithm for English dataset we refer to results from BIBREF0, and as for Russian dataset, we used the probabilistic language model, described in BIBREF8. The probability of a sequence of words is the product of the probabilities of each word, given the word’s context: the preceding word. As in the following equation:","For this reason as a baseline algorithm for English dataset we refer to results from BIBREF0, and as for Russian dataset, we used the probabilistic language model, described in BIBREF8.",a833c42b5d380eb22ab91dd7091a9850f0c24897,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,Which languages are used in the paper?,9282cf80265a914a13053ab23b77d1a8ed71db1b,two,familiar,no,word segmentation,486a870694ba60f1a1e7e4ec13e328164cd4b43c,False,,,"For this reason as a baseline algorithm for English dataset we refer to results from BIBREF0, and as for Russian dataset, we used the probabilistic language model, described in BIBREF8. The probability of a sequence of words is the product of the probabilities of each word, given the word’s context: the preceding word. As in the following equation:","For this reason as a baseline algorithm for English dataset we refer to results from BIBREF0, and as for Russian dataset, we used the probabilistic language model, described in BIBREF8.",fe6e59d817e573e2f8445c839f02fd7bff6ff5dc,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3-Table2-1.png,"Table 2. Illustration of sequence labeling for segmentation ""#photooftheday"" [photo of the day]",4-Figure1-1.png,Fig. 1. Neural model for hashtag segmentation,6-Table3-1.png,Table 3. Samples from both datasets,7-Table4-1.png,Table 4. Accuracy of the baseline algorithm on the Russian and English datasets,7-Table5-1.png,Table 5. Accuracy of LSTM char-RNN on both Russian and English datasets,9-Figure3-1.png,Fig. 3. Accuracy obtained on English Dataset,,,,,,,,,,,9-Figure4-1.png,"Fig. 4. Distribution of top 7k hashtag types in Russian dataset, chosen by an active learning algorithm",9-Figure5-1.png,"Fig. 5. Distribution of top 7k hashtag types in English dataset, chosen by an active learning algorithm",10-Figure7-1.png,Fig. 7. SVD visualization of character embeddings on English dataset,10-Figure6-1.png,Fig. 6. SVD visualization of character embeddings on Russian dataset,,,,,,,,,,,,,English Russian,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Textual Data for Time Series Forecasting,"While ubiquitous, textual sources of information such as company reports, social media posts, etc. are hardly included in prediction algorithms for time series, despite the relevant information they may contain. In this work, openly accessible daily weather reports from France and the United-Kingdom are leveraged to predict time series of national electricity consumption, average temperature and wind-speed with a single pipeline. Two methods of numerical representation of text are considered, namely traditional Term Frequency - Inverse Document Frequency (TF-IDF) as well as our own neural word embedding. Using exclusively text, we are able to predict the aforementioned time series with sufficient accuracy to be used to replace missing data. Furthermore the proposed word embeddings display geometric properties relating to the behavior of the time series and context similarity between words.",Introduction,"Whether it is in the field of energy, finance or meteorology, accurately predicting the behavior of time series is nowadays of paramount importance for optimal decision making or profit. While the field of time series forecasting is extremely prolific from a research point-of-view, up to now it has narrowed its efforts on the exploitation of regular numerical features extracted from sensors, data bases or stock exchanges. Unstructured data such as text on the other hand remains underexploited for prediction tasks, despite its potentially valuable informative content. Empirical studies have already proven that textual sources such as news articles or blog entries can be correlated to stock exchange time series and have explanatory power for their variations BIBREF0, BIBREF1. This observation has motivated multiple extensive experiments to extract relevant features from textual documents in different ways and use them for prediction, notably in the field of finance. In Lavrenko et al. BIBREF2, language models (considering only the presence of a word) are used to estimate the probability of trends such as surges or falls of 127 different stock values using articles from Biz Yahoo!. Their results show that this text driven approach could be used to make profit on the market. One of the most conventional ways for text representation is the TF-IDF (Term Frequency - Inverse Document Frequency) approach. Authors have included such features derived from news pieces in multiple traditional machine learning algorithms such as support vector machines (SVM) BIBREF3 or logistic regression BIBREF4 to predict the variations of financial series again. An alternative way to encode the text is through latent Dirichlet allocation (LDA) BIBREF5. It assigns topic probabilities to a text, which can be used as inputs for subsequent tasks. This is for instance the case in Wang's aforementioned work (alongside TF-IDF). In BIBREF6, the authors used Reuters news encoded by LDA to predict if NASDAQ and Dow Jones closing prices increased or decreased compared to the opening ones. Their empirical results show that this approach was efficient to improve the prediction of stock volatility. More recently Kanungsukkasem et al. BIBREF7 introduced a variant of the LDA graphical model, named FinLDA, to craft probabilities that are specifically tailored for a financial time series prediction task (although their approach could be generalized to other ones). Their results showed that indeed performance was better when using probabilities from their alternative than those of the original LDA. Deep learning with its natural ability to work with text through word embeddings has also been used for time series prediction with text. Combined with traditional time series features, the authors of BIBREF8 derived sentiment features from a convolutional neural network (CNN) to reduce the prediction error of oil prices. Akita et al. BIBREF9 represented news articles through the use of paragraph vectors BIBREF10 in order to predict 10 closing stock values from the Nikkei 225. While in the case of financial time series the existence of specialized press makes it easy to decide which textual source to use, it is much more tedious in other fields. Recently in Rodrigues et al. BIBREF11, short description of events (such as concerts, sports matches, ...) are leveraged through a word embedding and neural networks in addition to more traditional features. Their experiments show that including the text can bring an improvement of up to 2% of root mean squared error compared to an approach without textual information. Although the presented studies conclude on the usefulness of text to improve predictions, they never thoroughly analyze which aspects of the text are of importance, keeping the models as black-boxes. The field of electricity consumption is one where expert knowledge is broad. It is known that the major phenomena driving the load demand are calendar (time of the year, day of the week, ...) and meteorological. For instance generalized additive models (GAM) BIBREF12 representing the consumption as a sum of functions of the time of the year, temperature and wind speed (among others) typically yield less than 1.5% of relative error for French national electricity demand and 8% for local one BIBREF13, BIBREF14. Neural networks and their variants, with their ability to extract patterns from heterogeneous types of data have also obtained state-of-the-art results BIBREF15, BIBREF16, BIBREF17. However to our knowledge no exploratory work using text has been conducted yet. Including such data in electricity demand forecasting models would not only contribute to close the gap with other domains, but also help to understand better which aspects of text are useful, how the encoding of the text influences forecasts and to which extend a prediction algorithm can extract relevant information from unstructured data. Moreover the major drawback of all the aforementioned approaches is that they require meteorological data that may be difficult to find, unavailable in real time or expensive. Textual sources such as weather reports on the other hand are easy to find, usually available on a daily basis and free. The main contribution of our paper is to suggest the use of a certain type of textual documents, namely daily weather report, to build forecasters of the daily national electricity load, average temperature and wind speed for both France and the United-Kingdom (UK). Consequently this work represents a significant break with traditional methods, and we do not intend to best state-of-the-art approaches. Textual information is naturally more fuzzy than numerical one, and as such the same accuracy is not expected from the presented approaches. With a single text, we were already able to predict the electricity consumption with a relative error of less than 5% for both data sets. Furthermore, the quality of our predictions of temperature and wind speed is satisfying enough to replace missing or unavailable data in traditional models. Two different approaches are considered to represent the text numerically, as well as multiple forecasting algorithms. Our empirical results are consistent across encoding, methods and language, thus proving the intrinsic value weather reports have for the prediction of the aforementioned time series. Moreover, a major distinction between previous works is our interpretation of the models. We quantify the impact of a word on the forecast and analyze the geometric properties of the word embedding we trained ourselves. Note that although multiple time series are discussed in our paper, the main focus of this paper remains electricity consumption. As such, emphasis is put on the predictive results on the load demand time series. The rest of this paper is organized as follows. The following section introduces the two data sets used to conduct our study. Section 3 presents the different machine learning approaches used and how they were tuned. Section 4 highlights the main results of our study, while section 5 concludes this paper and gives insight on future possible work.",Presentation of the data,"In order to prove the consistency of our work, experiments have been conducted on two data sets, one for France and the other for the UK. In this section details about the text and time series data are given, as well as the major preprocessing steps.",Presentation of the data ::: Time Series,"Three types of time series are considered in our work: national net electricity consumption (also referred as load or demand), national temperature and wind speed. The load data sets were retrieved on the websites of the respective grid operators, respectively RTE (Réseau et Transport d'Électricité) for France and National Grid for the UK. For France, the available data ranges from January the 1st 2007 to August the 31st 2018. The default temporal resolution is 30 minutes, but it is averaged to a daily one. For the UK, it is available from January the 1st 2006 to December the 31st 2018 with the same temporal resolution and thus averaging. Due to social factors such as energy policies or new usages of electricity (e.g. Electric Vehicles), the net consumption usually has a long-term trend (fig. FIGREF2). While for France it seems marginal (fig. FIGREF2), there is a strong decreasing trend for the United-Kingdom (fig. FIGREF2). Such a strong non-stationarity of the time series would cause problems for the forecasting process, since the learnt demand levels would differ significantly from the upcoming ones. Therefore a linear regression was used to approximate the decreasing trend of the net consumption in the UK. It is then subtracted before the training of the methods, and then re-added a posteriori for prediction. As for the weather time series, they were extracted from multiple weather stations around France and the UK. The national average is obtained by combining the data from all stations with a weight proportional to the city population the station is located in. For France the stations' data is provided by the French meteorological office, Météo France, while the British ones are scrapped from stations of the National Oceanic and Atmospheric Administration (NOAA). Available on the same time span as the consumption, they usually have a 3 hours temporal resolution but are averaged to a daily one as well. Finally the time series were scaled to the range $[0,1]$ before the training phase, and re-scaled during prediction time.",Presentation of the data ::: Text,"Our work aims at predicting time series using exclusively text. Therefore for both countries the inputs of all our models consist only of written daily weather reports. Under their raw shape, those reports take the form of PDF documents giving a short summary of the country's overall weather, accompanied by pressure, temperature, wind, etc. maps. Note that those reports are written a posteriori, although they could be written in a predictive fashion as well. The reports are published by Météo France and the Met Office, its British counterpart. They are publicly available on the respective websites of the organizations. Both corpora span on the same period as the corresponding time series and given their daily nature, it yields a total of 4,261 and 4,748 documents respectively. An excerpt for each language may be found in tables TABREF6 and TABREF7. The relevant text was extracted from the PDF documents using the Python library PyPDF2. As emphasized in many studies, preprocessing of the text can ease the learning of the methods and improve accuracy BIBREF18. Therefore the following steps are applied: removal of non-alphabetic characters, removal of stop-words and lowercasing. While it was often highlighted that word lemmatization and stemming improve results, initial experiments showed it was not the case for our study. This is probably due to the technical vocabulary used in both corpora pertaining to the field of meteorology. Already limited in size, the aforementioned preprocessing operations do not yield a significant vocabulary size reduction and can even lead to a loss of linguistic meaning. Finally, extremely frequent or rare words may not have high explanatory power and may reduce the different models' accuracy. That is why words appearing less than 7 times or in more than 40% of the (learning) corpus are removed as well. Figure FIGREF8 represents the distribution of the document lengths after preprocessing, while table TABREF11 gives descriptive statistics on both corpora. Note that the preprocessing steps do not heavily rely on the considered language: therefore our pipeline is easily adaptable for other languages.",Modeling and forecasting framework,"A major target of our work is to show the reports contain an intrinsic information relevant for time series, and that the predictive results do not heavily depend on the encoding of the text or the machine learning algorithm used. Therefore in this section we present the text encoding approaches, as well as the forecasting methods used with them.",Modeling and forecasting framework ::: Numerical Encoding of the Text,"Machines and algorithms cannot work with raw text directly. Thus one major step when working with text is the choice of its numerical representation. In our work two significantly different encoding approaches are considered. The first one is the TF-IDF approach. It embeds a corpus of $N$ documents and $V$ words into a matrix $X$ of size $N \times V$. As such, every document is represented by a vector of size $V$. For each word $w$ and document $d$ the associated coefficient $x_{d,w}$ represents the frequency of that word in that document, penalized by its overall frequency in the rest of the corpus. Thus very common words will have a low TF-IDF value, whereas specific ones which will appear often in a handful of documents will have a large TF-IDF score. The exact formula to calculate the TF-IDF value of word $w$ in document $d$ is: where $f_{d,w}$ is the number of appearances of $w$ in $d$ adjusted by the length of $d$ and $\#\lbrace d: w \in d \rbrace $ is the number of documents in which the word $w$ appears. In our work we considered only individual words, also commonly referred as 1-grams in the field of natural language processing (NLP). The methodology can be easily extended to $n$-grams (groups of $n$ consecutive words), but initial experiments showed that it did not bring any significant improvement over 1-grams. The second representation is a neural word embedding. It consists in representing every word in the corpus by a real-valued vector of dimension $q$. Such models are usually obtained by learning a vector representation from word co-occurrences in a very large corpus (typically hundred thousands of documents, such as Wikipedia articles for example). The two most popular embeddings are probably Google's Word2Vec BIBREF19 and Standford's GloVe BIBREF20. In the former, a neural network is trained to predict a word given its context (continuous bag of word model), whereas in the latter a matrix factorization scheme on the log co-occurences of words is applied. In any case, the very nature of the objective function allows the embedding models to learn to translate linguistic similarities into geometric properties in the vector space. For instance the vector $\overrightarrow{king} - \overrightarrow{man} + \overrightarrow{woman}$ is expected to be very close to the vector $\overrightarrow{queen}$. However in our case we want a vector encoding which is tailored for the technical vocabulary of our weather reports and for the subsequent prediction task. This is why we decided to train our own word embedding from scratch during the learning phase of our recurrent or convolutional neural network. Aside from the much more restricted size of our corpora, the major difference with the aforementioned embeddings is that in our case it is obtained by minimizing a squared loss on the prediction. In that framework there is no explicit reason for our representation to display any geometric structure. However as detailed in section SECREF36, our word vectors nonetheless display geometric properties pertaining to the behavior of the time series.",Modeling and forecasting framework ::: Machine Learning Algorithms,"Multiple machine learning algorithms were applied on top of the encoded textual documents. For the TF-IDF representation, the following approaches are applied: random forests (RF), LASSO and multilayer perceptron (MLP) neural networks (NN). We chose these algorithms combined to the TF-IDF representation due to the possibility of interpretation they give. Indeed, considering the novelty of this work, the understanding of the impact of the words on the forecast is of paramount importance, and as opposed to embeddings, TF-IDF has a natural interpretation. Furthermore the RF and LASSO methods give the possibility to interpret marginal effects and analyze the importance of features, and thus to find the words which affect the time series the most. As for the word embedding, recurrent or convolutional neural networks (respectively RNN and CNN) were used with them. MLPs are not used, for they would require to concatenate all the vector representations of a sentence together beforehand and result in a network with too many parameters to be trained correctly with our number of available documents. Recall that we decided to train our own vector representation of words instead of using an already available one. In order to obtain the embedding, the texts are first converted into a sequence of integers: each word is given a number ranging from 1 to $V$, where $V$ is the vocabulary size (0 is used for padding or unknown words in the test set). One must then calculate the maximum sequence length $S$, and sentences of length shorter than $S$ are then padded by zeros. During the training process of the network, for each word a $q$ dimensional real-valued vector representation is calculated simultaneously to the rest of the weights of the network. Ergo a sentence of $S$ words is translated into a sequence of $S$ $q$-sized vectors, which is then fed into a recurrent neural unit. For both languages, $q=20$ seemed to yield the best results. In the case of recurrent units two main possibilities arise, with LSTM (Long Short-Term Memory) BIBREF21 and GRU (Gated Recurrent Unit) BIBREF22. After a few initial trials, no significant performance differences were noticed between the two types of cells. Therefore GRU were systematically used for recurrent networks, since their lower amount of parameters makes them easier to train and reduces overfitting. The output of the recurrent unit is afterwards linked to a fully connected (also referred as dense) layer, leading to the final forecast as output. The rectified linear unit (ReLU) activation in dense layers systematically gave the best results, except on the output layer where we used a sigmoid one considering the time series' normalization. In order to tone down overfitting, dropout layers BIBREF23 with probabilities of 0.25 or 0.33 are set in between the layers. Batch normalization BIBREF24 is also used before the GRU since it stabilized training and improved performance. Figure FIGREF14 represents the architecture of our RNN. The word embedding matrix is therefore learnt jointly with the rest of the parameters of the neural network by minimization of the quadratic loss with respect to the true electricity demand. Note that while above we described the case of the RNN, the same procedure is considered for the case of the CNN, with only the recurrent layers replaced by a combination of 1D convolution and pooling ones. As for the optimization algorithms of the neural networks, traditional stochastic gradient descent with momentum or ADAM BIBREF25 together with a quadratic loss are used. All of the previously mentioned methods were coded with Python. The LASSO and RF were implemented using the library Scikit Learn BIBREF26, while Keras BIBREF27 was used for the neural networks.",Modeling and forecasting framework ::: Hyperparameter Tuning,"While most parameters are trained during the learning optimization process, all methods still involve a certain number of hyperparameters that must be manually set by the user. For instance for random forests it can correspond to the maximum depth of the trees or the fraction of features used at each split step, while for neural networks it can be the number of layers, neurons, the embedding dimension or the activation functions used. This is why the data is split into three sets: The training set, using all data available up to the 31st of December 2013 (2,557 days for France and 2,922 for the UK). It is used to learn the parameters of the algorithms through mathematical optimization. The years 2014 and 2015 serve as validation set (730 days). It is used to tune the hyperparameters of the different approaches. All the data from January the 1st 2016 (974 days for France and 1,096 for the UK) is used as test set, on which the final results are presented. Grid search is applied to find the best combination of values: for each hyperparameter, a range of values is defined, and all the possible combinations are successively tested. The one yielding the lowest RMSE (see section SECREF4) on the validation set is used for the final results on the test one. While relatively straightforward for RFs and the LASSO, the extreme number of possibilities for NNs and their extensive training time compelled us to limit the range of architectures possible. The hyperparameters are tuned per method and per country: ergo the hyperparameters of a given algorithm will be the same for the different time series of a country (e.g. the RNN architecture for temperature and load for France will be the same, but different from the UK one). Finally before application on the testing set, all the methods are re-trained from scratch using both the training and validation data.",Experiments,"The goal of our experiments is to quantify how close one can get using textual data only when compared to numerical data. However the inputs of the numerical benchmark should be hence comparable to the information contained in the weather reports. Considering they mainly contain calendar (day of the week and month) as well as temperature and wind information, the benchmark of comparison is a random forest trained on four features only: the time of the year (whose value is 0 on January the 1st and 1 on December the 31st with a linear growth in between), the day of the week, the national average temperature and wind speed. The metrics of evaluation are the Mean Absolute Percentage Error (MAPE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE) and the $R^2$ coefficient given by: where $T$ is the number of test samples, $y_t$ and $\hat{y}_t$ are respectively the ground truth and the prediction for the document of day $t$, and $\overline{y}$ is the empirical average of the time series over the test sample. A known problem with MAPE is that it unreasonably increases the error score for values close to 0. While for the load it isn't an issue at all, it can be for the meteorological time series. Therefore for the temperature, the MAPE is calculated only when the ground truth is above the 5% empirical quantile. Although we aim at achieving the highest accuracy possible, we focus on the interpretability of our models as well.",Experiments ::: Feature selection,"Many words are obviously irrelevant to the time series in our texts. For instance the day of the week, while playing a significant role for the load demand, is useless for temperature or wind. Such words make the training harder and may decrease the accuracy of the prediction. Therefore a feature selection procedure similar to BIBREF28 is applied to select a subset of useful features for the different algorithms, and for each type of time series. Random forests are naturally able to calculate feature importance through the calculation of error increase in the out-of-bag (OOB) samples. Therefore the following process is applied to select a subset of $V^*$ relevant words to keep: A RF is trained on the whole training & validation set. The OOB feature importance can thus be calculated. The features are then successively added to the RF in decreasing order of feature importance. This process is repeated $B=10$ times to tone down the randomness. The number $V^*$ is then set to the number of features giving the highest median OOB $R^2$ value. The results of this procedure for the French data is represented in figure FIGREF24. The best median $R^2$ is achieved for $V^* = 52$, although one could argue that not much gain is obtained after 36 words. The results are very similar for the UK data set, thus for the sake of simplicity the same value $V^* = 52$ is used. Note that the same subset of words is used for all the different forecasting models, which could be improved in further work using other selection criteria (e.g. mutual information, see BIBREF29). An example of normalized feature importance is given in figure. FIGREF32.",Experiments ::: Main results,"Note that most of the considered algorithms involve randomness during the training phase, with the subsampling in the RFs or the gradient descent in the NNs for instance. In order to tone it down and to increase the consistency of our results, the different models are run $B=10$ times. The results presented hereafter correspond to the average and standard-deviation on those runs. The RF model denoted as ""sel"" is the one with the reduced number of features, whereas the other RF uses the full vocabulary. We also considered an aggregated forecaster (abridged Agg), consisting of the average of the two best individual ones in terms of RMSE. All the neural network methods have a reduced vocabulary size $V^*$. The results for the French and UK data are respectively given by tables TABREF26 and TABREF27. Our empirical results show that for the electricity consumption prediction task, the order of magnitude of the relative error is around 5%, independently of the language, encoding and machine learning method, thus proving the intrinsic value of the information contained in the textual documents for this time series. As expected, all text based methods perform poorer than when using explicitly numerical input features. Indeed, despite containing relevant information, the text is always more fuzzy and less precise than an explicit value for the temperature or the time of the year for instance. Again the aim of this work is not to beat traditional methods with text, but quantifying how close one can come to traditional approaches when using text exclusively. As such achieving less than 5% of MAPE was nonetheless deemed impressive by expert electricity forecasters. Feature selection brings significant improvement in the French case, although it does not yield any improvement in the English one. The reason for this is currently unknown. Nevertheless the feature selection procedure also helps the NNs by dramatically reducing the vocabulary size, and without it the training of the networks was bound to fail. While the errors accross methods are roughly comparable and highlight the valuable information contained within the reports, the best method nonetheless fluctuates between languages. Indeed in the French case there is a hegemony of the NNs, with the embedding RNN edging the MLP TF-IDF one. However for the UK data set the RFs yield significantly better results on the test set than the NNs. This inversion of performance of the algorithms is possibly due to a change in the way the reports were written by the Met Office after August 2017, since the results of the MLP and RNN on the validation set (not shown here) were satisfactory and better than both RFs. For the two languages both the CNN and the LASSO yielded poor results. For the former, it is because despite grid search no satisfactory architecture was found, whereas the latter is a linear approach and was used more for interpretation purposes than strong performance. Finally the naive aggregation of the two best experts always yields improvement, especially for the French case where the two different encodings are combined. This emphasises the specificity of the two representations leading to different types of errors. An example of comparison between ground truth and forecast for the case of electricity consumption is given for the French language with fig. FIGREF29, while another for temperature may be found in the appendix FIGREF51. The sudden ""spikes"" in the forecast are due to the presence of winter related words in a summer report. This is the case when used in comparisons, such as ""The flood will be as severe as in January"" in a June report and is a limit of our approach. Finally, the usual residual $\hat{\varepsilon }_t = y_t - \hat{y}_t$ analyses procedures were applied: Kolmogorov normality test, QQplots comparaison to gaussian quantiles, residual/fit comparison... While not thoroughly gaussian, the residuals were close to normality nonetheless and displayed satisfactory properties such as being generally independent from the fitted and ground truth values. Excerpts of this analysis for France are given in figure FIGREF52 of the appendix. The results for the temperature and wind series are given in appendix. Considering that they have a more stochastic behavior and are thus more difficult to predict, the order of magnitude of the errors differ (the MAPE being around 15% for temperature for instance) but globally the same observations can be made.",Experiments ::: Interpretability of the models,"While accuracy is the most relevant metric to assess forecasts, interpretability of the models is of paramount importance, especially in the field of professional electricity load forecasting and considering the novelty of our work. Therefore in this section we discuss the properties of the RF and LASSO models using the TF-IDF encoding scheme, as well as the RNN word embedding.",Experiments ::: Interpretability of the models ::: TF-IDF representation,"One significant advantage of the TF-IDF encoding when combined with random forests or the LASSO is that it is possible to interpret the behavior of the models. For instance, figure FIGREF32 represents the 20 most important features (in the RF OOB sense) for both data sets when regressing over electricity demand data. As one can see, the random forest naturally extracts calendar information contained in the weather reports, since months or week-end days are among the most important ones. For the former, this is due to the periodic behavior of electricity consumption, which is higher in winter and lower in summer. This is also why characteristic phenomena of summer and winter, such as ""thunderstorms"", ""snow"" or ""freezing"" also have a high feature importance. The fact that August has a much more important role than July also concurs with expert knowledge, especially for France: indeed it is the month when most people go on vacations, and thus when the load drops the most. As for the week-end names, it is due to the significantly different consumer behavior during Saturdays and especially Sundays when most of the businesses are closed and people are usually at home. Therefore the relevant words selected by the random forest are almost all in agreement with expert knowledge. We also performed the analysis of the relevant words for the LASSO. In order to do that, we examined the words $w$ with the largest associated coefficients $\beta _w$ (in absolute value) in the regression. Since the TF-IDF matrix has positive coefficients, it is possible to interpret the sign of the coefficient $\beta _w$ as its impact on the time series. For instance if $\beta _w > 0$ then the presence of the word $w$ causes a rise the time series (respectively if $\beta _w < 0$, it entails a decline). The results are plotted fig. FIGREF35 for the the UK. As one can see, the winter related words have positive coefficients, and thus increase the load demand as expected whereas the summer related ones decrease it. The value of the coefficients also reflects the impact on the load demand. For example January and February have the highest and very similar values, which concurs with the similarity between the months. Sunday has a much more negative coefficient than Saturday, since the demand significantly drops during the last day of the week. The important words also globally match between the LASSO and the RF, which is a proof of the consistency of our results (this is further explored afterwards in figure FIGREF43). Although not presented here, the results are almost identical for the French load, with approximately the same order of relevancy. The important words logically vary in function of the considered time series, but are always coherent. For instance for the wind one, terms such as ""gales"", ""windy"" or ""strong"" have the highest positive coefficients, as seen in the appendix figure FIGREF53. Those results show that a text based approach not only extracts the relevant information by itself, but it may eventually be used to understand which phenomena are relevant to explain the behavior of a time series, and to which extend.",Experiments ::: Interpretability of the models ::: Vector embedding representation,"Word vector embeddings such as Word2Vec and GloVe are known for their vectorial properties translating linguistic ones. However considering the objective function of our problem, there was no obvious reason for such attributes to appear in our own. Nevertheless for both languages we conducted an analysis of the geometric properties of our embedding matrix. We investigated the distances between word vectors, the relevant metric being the cosine distance given by: where $\overrightarrow{w_1}$ and $\overrightarrow{w_2}$ are given word vectors. Thus a cosine distance lower than 1 means similarity between word vectors, whereas a greater than 1 corresponds to opposition. The initial analyses of the embedding matrices for both the UK and France revealed that in general, words were grouped by context or influence on the electricity consumption. For instance, we observed that winter words were together and far away from summer ones. Week days were grouped as well and far from week-end days. However considering the vocabulary was reduced to $V^* = 52$ words, those results lacked of consistency. Therefore for both languages we decided to re-train the RNNs using the same architecture, but with a larger vocabulary of the $V=300$ most relevant words (still in the RF sense) and on all the available data (i.e. everything is used as training) to compensate for the increased size of the vocabulary. We then calculated the distance of a few prominent words to the others. The analysis of the average cosine distance over $B=10$ runs for three major words is given by tables TABREF38 and TABREF39, and three other examples are given in the appendix tables TABREF57 and TABREF58. The first row corresponds to the reference word vector $\overrightarrow{w_1}$ used to calculate the distance from (thus the distance is always zero), while the following ones are the 9 closest to it. The two last rows correspond to words we deemed important to check the distance with (an antagonistic one or relevant one not in the top 9 for instance). The results of the experiments are very similar for both languages again. Indeed, the words are globally embedded in the vector space by topic: winter related words such as ""January"" (""janvier""), ""February"" (""février""), ""snow"" (""neige""), ""freezing"" (""glacial"") are close to each other and almost opposite to summer related ones such as ""July"" (""juillet""), ""August"" (""août""), ""hot"" (""chaud""). For both cases the week days Monday (""lundi"") to Friday (""vendredi"") are grouped very closely to each other, while significantly separated from the week-end ones ""Saturday"" (""samedi"") and ""Sunday"" (""dimanche""). Despite these observations, a few seemingly unrelated words enter the lists of top 10, especially for the English case (such as ""pressure"" or ""dusk"" for ""February""). In fact the French language embedding seems of better quality, which is perhaps linked to the longer length of the French reports in average. This issue could probably be addressed with more data. Another observation made is that the importance of a word $w$ seems related to its euclidean norm in the embedding space ${\overrightarrow{w}}_2$. For both languages the list of the 20 words with the largest norm is given fig. FIGREF40. As one can see, it globally matches the selected ones from the RF or the LASSO (especially for the French language), although the order is quite different. This is further supported by the Venn diagram of common words among the top 50 ones for each word selection method represented in figure FIGREF43 for France. Therefore this observation could also be used as feature selection procedure for the RNN or CNN in further work. In order to achieve a global view of the embeddings, the t-SNE algorithm BIBREF30 is applied to project an embedding matrix into a 2 dimensional space, for both languages. The observations for the few aforementioned words are confirmed by this representation, as plotted in figure FIGREF44. Thematic clusters can be observed, roughly corresponding to winter, summer, week-days, week-end days for both languages. Globally summer and winter seem opposed, although one should keep in mind that the t-SNE representation does not preserve the cosine distance. The clusters of the French embedding appear much more compact than the UK one, comforting the observations made when explicitly calculating the cosine distances.",Conclusion,"In this study, a novel pipeline to predict three types of time series using exclusively a textual source was proposed. Making use of publicly available daily weather reports, we were able to predict the electricity consumption with less than 5% of MAPE for both France and the United-Kingdom. Moreover our average national temperature and wind speed predictions displayed sufficient accuracy to be used to replace missing data or as first approximation in traditional models in case of unavailability of meteorological features. The texts were encoded numerically using either TF-IDF or our own neural word embedding. A plethora of machine learning algorithms such as random forests or neural networks were applied on top of those representations. Our results were consistent over language, numerical representation of the text and prediction algorithm, proving the intrinsic value of the textual sources for the three considered time series. Contrarily to previous works in the field of textual data for time series forecasting, we went in depth and quantified the impact of words on the variations of the series. As such we saw that all the algorithms naturally extract calendar and meteorological information from the texts, and that words impact the time series in the expected way (e.g. winter words increase the consumption and summer ones decrease it). Despite being trained on a regular quadratic loss, our neural word embedding spontaneously builds geometric properties. Not only does the norm of a word vector reflect its significance, but the words are also grouped by topic with for example winter, summer or day of the week clusters. Note that this study was a preliminary work on the use of textual information for time series prediction, especially electricity demand one. The long-term goal is to include multiple sources of textual information to improve the accuracy of state-of-the-art methods or to build a text based forecaster which can be used to increase the diversity in a set of experts for electricity consumption BIBREF31. However due to the redundancy of the information of the considered weather reports with meteorological features, it may be necessary to consider alternative textual sources. The use of social media such as Facebook, Twitter or Instagram may give interesting insight and will therefore be investigated in future work.",,"Additional results for the prediction tasks on temperature and wind speed can be found in tables TABREF47 to TABREF50. An example of forecast for the French temperature is given in figure FIGREF51. While not strictly normally distributed, the residuals for the French electricity demand display an acceptable behavior. This holds also true for the British consumption, and both temperature time series, but is of lesser quality for the wind one. The the UK wind LASSO regression, the words with the highest coefficients $\beta _w$ are indeed related to strong wind phenomena, whereas antagonistic ones such as ""fog"" or ""mist"" have strongly negative ones as expected (fig. FIGREF53). For both languages we represented the evolution of the (normalized) losses for the problem of load regression in fig. FIGREF54. The aspect is a typical one, with the validation loss slightly above the training one. The slightly erratic behavior of the former one is possibly due to a lack of data to train the embeddings. The cosine distances for three other major words and for both corpora have been calculated as well. The results are given in tables TABREF57 and TABREF58. For both languages, the three summer months are grouped together, and so are the two week-end days. However again the results are less clear for the English language. They are especially mediocre for ""hot"", considering that only ""warm"" seems truly relevant and that ""August"" for instance is quite far away. For the French language instead of ""hot"" the distances to ""thunderstorms"" were calculated. The results are quite satisfactory, with ""orageux""/""orageuse"" (""thundery"") coming in the two first places and related meteorological phenomena (""cumulus"" and ""grêle"", meaning ""hail"") relatively close as well. For the French case, Saturday and Sunday are very close to summer related words. This observation probably highlights the fact that the RNN groups load increasing and decreasing words in opposite parts of the embedding space.",,,,,,,,,,,How big is dataset used for training/testing?,07c59824f5e7c5399d15491da3543905cfa5f751,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,,"4,261  days for France and 4,748 for the UK","Our work aims at predicting time series using exclusively text. Therefore for both countries the inputs of all our models consist only of written daily weather reports. Under their raw shape, those reports take the form of PDF documents giving a short summary of the country's overall weather, accompanied by pressure, temperature, wind, etc. maps. Note that those reports are written a posteriori, although they could be written in a predictive fashion as well. The reports are published by Météo France and the Met Office, its British counterpart. They are publicly available on the respective websites of the organizations. Both corpora span on the same period as the corresponding time series and given their daily nature, it yields a total of 4,261 and 4,748 documents respectively. An excerpt for each language may be found in tables TABREF6 and TABREF7. The relevant text was extracted from the PDF documents using the Python library PyPDF2.","The reports are published by Météo France and the Met Office, its British counterpart. They are publicly available on the respective websites of the organizations. Both corpora span on the same period as the corresponding time series and given their daily nature, it yields a total of 4,261 and 4,748 documents respectively.",e6c530042231f1a95608b2495514fe8b5ad08d28,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,,,,,,,,,Is there any example where geometric property is visible for context similarity between words?,77f04cd553df691e8f4ecbe19da89bc32c7ac734,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,True,,"The initial analyses of the embedding matrices for both the UK and France revealed that in general, words were grouped by context or influence on the electricity consumption. For instance, we observed that winter words were together and far away from summer ones. Week days were grouped as well and far from week-end days. However considering the vocabulary was reduced to $V^* = 52$ words, those results lacked of consistency. Therefore for both languages we decided to re-train the RNNs using the same architecture, but with a larger vocabulary of the $V=300$ most relevant words (still in the RF sense) and on all the available data (i.e. everything is used as training) to compensate for the increased size of the vocabulary. We then calculated the distance of a few prominent words to the others. The analysis of the average cosine distance over $B=10$ runs for three major words is given by tables TABREF38 and TABREF39, and three other examples are given in the appendix tables TABREF57 and TABREF58. The first row corresponds to the reference word vector $\overrightarrow{w_1}$ used to calculate the distance from (thus the distance is always zero), while the following ones are the 9 closest to it. The two last rows correspond to words we deemed important to check the distance with (an antagonistic one or relevant one not in the top 9 for instance).","The initial analyses of the embedding matrices for both the UK and France revealed that in general, words were grouped by context or influence on the electricity consumption. For instance, we observed that winter words were together and far away from summer ones. ",5aa11104f6641837a83ea424f900ee683d194b79,71f73551e7aabf873649e8fe97aefc54e6dd14f8,What geometric properties do embeddings display?,728a55c0f628f2133306b6bd88af00eb54017b12,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,,Winter and summer words formed two separate clusters. Week day and week-end day words also formed separate clusters.,"The initial analyses of the embedding matrices for both the UK and France revealed that in general, words were grouped by context or influence on the electricity consumption. For instance, we observed that winter words were together and far away from summer ones. Week days were grouped as well and far from week-end days. However considering the vocabulary was reduced to $V^* = 52$ words, those results lacked of consistency. Therefore for both languages we decided to re-train the RNNs using the same architecture, but with a larger vocabulary of the $V=300$ most relevant words (still in the RF sense) and on all the available data (i.e. everything is used as training) to compensate for the increased size of the vocabulary. We then calculated the distance of a few prominent words to the others. The analysis of the average cosine distance over $B=10$ runs for three major words is given by tables TABREF38 and TABREF39, and three other examples are given in the appendix tables TABREF57 and TABREF58. The first row corresponds to the reference word vector $\overrightarrow{w_1}$ used to calculate the distance from (thus the distance is always zero), while the following ones are the 9 closest to it. The two last rows correspond to words we deemed important to check the distance with (an antagonistic one or relevant one not in the top 9 for instance).","For instance, we observed that winter words were together and far away from summer ones. Week days were grouped as well and far from week-end days.",f704fdce4c0a29cd04b3bd36b5062fd44e16c965,71f73551e7aabf873649e8fe97aefc54e6dd14f8,,,,,,,,How accurate is model trained on text exclusively?,d5498d16e8350c9785782b57b1e5a82212dbdaad,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,,Relative error is less than 5%,"The main contribution of our paper is to suggest the use of a certain type of textual documents, namely daily weather report, to build forecasters of the daily national electricity load, average temperature and wind speed for both France and the United-Kingdom (UK). Consequently this work represents a significant break with traditional methods, and we do not intend to best state-of-the-art approaches. Textual information is naturally more fuzzy than numerical one, and as such the same accuracy is not expected from the presented approaches. With a single text, we were already able to predict the electricity consumption with a relative error of less than 5% for both data sets. Furthermore, the quality of our predictions of temperature and wind speed is satisfying enough to replace missing or unavailable data in traditional models. Two different approaches are considered to represent the text numerically, as well as multiple forecasting algorithms. Our empirical results are consistent across encoding, methods and language, thus proving the intrinsic value weather reports have for the prediction of the aforementioned time series. Moreover, a major distinction between previous works is our interpretation of the models. We quantify the impact of a word on the forecast and analyze the geometric properties of the word embedding we trained ourselves. Note that although multiple time series are discussed in our paper, the main focus of this paper remains electricity consumption. As such, emphasis is put on the predictive results on the load demand time series.","With a single text, we were already able to predict the electricity consumption with a relative error of less than 5% for both data sets.",08426e8d76bfe140f762a3949db74028e5b14163,71f73551e7aabf873649e8fe97aefc54e6dd14f8,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3-Figure1-1.png,Figure 1: Net electricity consumption (Load) over time.,5-Figure2-1.png,Figure 2: Word counts for the two corpora after preprocessing.,5-Table3-1.png,Table 3: Descriptive analysis of the two corpora (after preprocessing),7-Figure3-1.png,Figure 3: Structure of our RNN. Dropout and batch normalization are not represented.,9-Figure4-1.png,Figure 4: Evolution of the OOB R2 during the selection procedure.,9-Table4-1.png,Table 4: Forecast errors on the net load for the French Dataset.,,,,,,,,,,,10-Table5-1.png,Table 5: Forecast errors on the net load for the British Dataset.,11-Table6-1.png,"Table 6: Best (individual, in terms of RMSE) result for each of the considered time series.",11-Figure5-1.png,Figure 5: Overlapping of prediction and real load (France),12-Figure6-1.png,Figure 6: RF feature importance over the B = 10 runs.,13-Figure7-1.png,Figure 7: Coefficients βw in the british load LASSO regression.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,14-Table7-1.png,"Table 7: Closest words (in the cosine sense) to ”february”,”snow” and ”tuesday” for the UK",14-Table8-1.png,"Table 8: Closest words (in the cosine sense) to ”february”,”snow” and ”tuesday” for France",,,,,,,,15-Figure8-1.png,Figure 8: Word vector log-norm over B = 10.,15-Figure9-1.png,Figure 9: Venn diagram of common words among the top 50 ones for each selection procedure (France).,16-Figure10-1.png,Figure 10: 2D t-SNE projections of the embedding matrix for both languages.,17-TableA.9-1.png,Table A.9: Forecast errors on the national temperature for France.,,,,,,,,,,,,,,,,,,,,,,,,,17-TableA.10-1.png,Table A.10: Forecast errors on the national wind for France.,17-TableA.11-1.png,Table A.11: Forecast errors on the national temperature for Great-Britain.,17-TableA.12-1.png,Table A.12: Forecast errors on the national wind for Great-Britain.,18-FigureA.11-1.png,Figure A.11: Overlapping of prediction and national Temperature (France),18-FigureA.12-1.png,Figure A.12: Residual analysis of the French aggregated predictor.,19-FigureA.13-1.png,Figure A.13: Coefficients βw in the British wind LASSO regression.,19-FigureA.14-1.png,Figure A.14: Loss (Mean Squared Error) evolution of the electricity demand RNN for both languages.,20-TableA.13-1.png,"Table A.13: Closest words (in the cosine sense) to ”August”,”Sunday” and ”Hot” for the UK",20-TableA.14-1.png,"Table A.14: Closest words (in the cosine sense) to ”August”,”Sunday and ”thunderstorms” for the France",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Retrieval-based Goal-Oriented Dialogue Generation,"Most research on dialogue has focused either on dialogue generation for openended chit chat or on state tracking for goal-directed dialogue. In this work, we explore a hybrid approach to goal-oriented dialogue generation that combines retrieval from past history with a hierarchical, neural encoder-decoder architecture. We evaluate this approach in the customer support domain using the Multiwoz dataset (Budzianowski et al., 2018). We show that adding this retrieval step to a hierarchical, neural encoder-decoder architecture leads to significant improvements, including responses that are rated more appropriate and fluent by human evaluators. Finally, we compare our retrieval-based model to various semantically conditioned models explicitly using past dialog act information, and find that our proposed model is competitive with the current state of the art (Chen et al., 2019), while not requiring explicit labels about past machine acts.",Introduction,"Dialogue systems have become a very popular research topic in recent years with the rapid improvement of personal assistants and the growing demand for online customer support. However, research has been split in two subfields BIBREF2: models presented for generation of open-ended conversations BIBREF3, BIBREF4, BIBREF5, BIBREF6, BIBREF7 and work on solving goal-oriented dialogue through dialogue management pipelines that include dialogue state tracking and dialogue policy BIBREF8, BIBREF9, BIBREF10, BIBREF11, BIBREF12, BIBREF13, BIBREF14. Dialogue state tracking has often been limited to detection of user intention, as well as learning a dialogue policy to determine what actions the system should take based on the detected user intent. Dialogue generation for open ended conversation, in contrast, has largely relied on transduction architectures originally developed for machine translation (MT) BIBREF15, BIBREF16, BIBREF17. Such architectures offer flexibility because of their ability to encode an utterance into a fixed-sized vector representation, and decoding it into a variable length sequence that is linguistically very different from the input utterance. However, MT-based approaches often lack the ability to encode the context in which the current utterance occurs. This can lead to repetitive and meaningless responses BIBREF18, BIBREF19, BIBREF17. This observation has led researchers to extend simple encoder-decoder models to include context in order to deal with generation of larger structured texts such as paragraphs and documents BIBREF18, BIBREF20, BIBREF21. Many of these models work by encoding information at multiple levels, i.e., using both a context encoder and a last-utterance encoder, passing both encodings to a decoder that predicts the next turn. Such hierarchical methods have proven to be useful for open-ended chit chat, but were not designed for goal-oriented dialogue, where responses need not only be coherent, but also relevant. In goal-directed dialogue generation, there is often one (context-dependent) right answer to a question (e.g., How many types of insurance do you offer?); in chit-chat, there are many good answers to questions (e.g., What do you want to talk about today?). We therefore hypothesize that in personal assistants and customer support, it is beneficial to increase the inductive bias of the dialogue generation model and its dependency on past conversations in order to keep responses relevant. We do so by designing a novel, hybrid dialogue generation model that conditions decoding on retrieved examplars from past history. Although retrieval approaches to dialogue generation have been introduced before, they have typically been used to add more variety to the kind of answers the model can generate in open ended conversations BIBREF7, BIBREF22. Our model, in contrast, is designed for the purpose of goal oriented dialogue in the customer support domain. It is a hierarchical neural model with an information retrieval component that retrieves the most informative prior turns and conditions on those. This simple approach increases inductive bias and alleviates problems arising from long-context dependencies. We show that an information retrieval step leads to improvements over traditional dialogue generation models intended for open ended chit chat when evaluated on BLEU and different embedding metrics. In addition, we evaluate our generated responses based on the Request/Inform success rate typically used in dialogue state tracking, and again, we show performance close to the more complex state-of-the-art model which contrary to our proposed model, makes use of annotated labels. Finally, based on our human evaluations, we show that a simple retrieval step leads to system responses that are more fluent and appropriate than the responses generated by the hierarchical encoder-decoder model.",Model Description,"We want to adapt commonly used models for dialogue generation to the task of goal-oriented dialogue by providing a simple way of integrating past examples in a context-aware dialogue generation model. We extend the Hierarchical Recurrent Encoder-Decoder (HRED) model presented by BIBREF23 for query suggestion, subsequently adopted for dialogue by BIBREF20, which has shown to be a strong baseline for the task of dialogue generation. In line with previous research, we consider a dialogue $D$ between two speakers composed of $n$ utterances so that $D = [U_1, ... , U_n]$ and each utterance $U_i$ composed of $k_i$ tokens so that $U_n = [t_{i,1}, t_{i, 2}, ... , t_{i,k_i}]$. Each token $t_{i, k_i}$ represents a word from a set vocabulary. Preliminary experiments showed that limiting contexts to three utterances gave the best results, therefore all results presented use $n=3$. This is also in line with most previous work on dialogue generation and classification BIBREF20.",Model Description ::: HRED,"HRED BIBREF23 consists of an encoder RNN, which maps an utterance to a vector, a context RNN, which summarizes the dialogue history by keeping track of the previous hidden states, and a decoder RNN, which decodes the hidden state of the context RNN. Given a dialogue consisting of three utterances – a system response, a user response, and a second system response, $\langle s_1,u,s_2\rangle $ – the goal is to predict the system utterance $s_2$ given the context $\langle s_1,u\rangle $. The utterance encoder takes in a single utterance and outputs a vector representation . The representations of each utterance are concatenated so that we end up with an array that is then fed to the context encoder. The context encoder outputs a global context, which is then fed into the decoder. Just as in previous work BIBREF21, BIBREF20, BIBREF3, BIBREF24, we use GRUs BIBREF25 for our encoder, context and decoder RNNs. All modules share parameters.",Model Description ::: Exemplar-HRED,"In this study, we want to enhance HRED with a simple yet efficient information retrieval step. As already mentioned, similar approaches have been presented with the goal of incorporating factual information into open-ended conversations BIBREF22, to add variety and more topics to the conversation. We hypothesize that using exemplar information is also beneficial for multi-domain goal-oriented systems. More specifically, we want to be able to inform our generation model about previous responses to similar utterances, biasing it towards past responses. For each user utterance, we extract the ten most similar past user utterances from the training set using approximate nearest neighbor search BIBREF26. We approximate a point $p \in S$ by specifying some error margin $\epsilon > 0 $ so that $ dist(p,q) \le (1+\epsilon )(dist(p*, q)) $, where $p*$ is the real nearest neighbor. Because we use approximate search, we rerank the retrieved utterances using a feed-forward ranking model, introduced in BIBREF27. Their ranking model is a multi-task model, which relies on simple textual similarity measures combined in a multi-layered perceptron architecture. The model nevertheless achieves state-of-the-art performance on question relevancy ranking. In the end, we take the top user utterance, and return its response as the example to be used in our model. For goal-oriented dialogue generation, our proposed model uses the same architecture as the HRED baseline, however, we include an additional RNN, which encodes the top example response. We feed the representation of the example RNN context into the context RNN and feed this representation into the decoder. Just as in the baseline model, the utterance encoder outputs a vector representation. Additionally, we encode the exemplar into a vector using the example encoder. The representations of each utterance are concatenated so that we end up with an array that includes dialogue context and exemplar information, all of which is then fed to the context encoder. The global context is then fed into the decoder. For all experiments, we use the MultiWoz dataset for goal oriented dialogue BIBREF0, which we describe in more detail in the next section. Our model uses the Adam optimizer BIBREF28 for all encoders. All our encoders are one layer RNNs. In addition, we use a dropout rate of 0.3, and a learning rate of 0.001. We set a maximum of 50 epochs, however, we use early stopping with a patience of 10. Most of our models converge by epoch 30. We use greedy search to generate the response during testing. More implementation details as well as our predicted utterances for each system can be found in the link provided",Experiments ::: Dataset,"We use the MultiWoz dialogue corpus BIBREF0, which consists of 10,438 dialogues spanning several domains and annotated with dialogue states and acts. We train on 8,438 dialogues, and use 1000 dialogues for development and 1000 dialogues for testing. Although the data is primarily intended for dialogue state tracking and learning a dialogue policy, BIBREF0 also mention its potential as a benchmark for end-to-end dialogue due to the fact that it contains about 115k turns in total, which is larger than many structured dialogue corpora available. This makes it a good choice for hybrid approaches in generation and goal oriented dialogue. The MultiWOZ dataset is also a much more difficult dataset than the current benchmarks for goal oriented dialogue, as it spans about 7 different customer support domains and conversations are not limited to a single domain. In line with previous work in goal oriented dialog and recent work using this dataset, we delexicalize the utterances to remove phone numbers, reference numbers and train ids. As opposed to other studies, we only delexicalize these three slots since these were significantly increasing the size of the vocabulary. For delexicalizing, we use the ontology provided with the data and replace the value with the slot names using regular expressions. We do not delexicalize times, prices, postcodes and distinct names of restaurants and hotels. This also makes our generation task more difficult.",Experiments ::: Baselines,"In this study, we are interested in simple ways of providing our dialogue generation model with enough inductive bias in order to generate fluent and on-topic responses for goal oriented dialogue. Encoder-decoder architectures work well when it comes to providing generic fluent responses for open ended conversations, however, in goal-oriented dialogue, it is also necessary for the system to remain on topic. Additionally, when training a single model on different domains, this becomes more difficult. The original HRED model BIBREF23, BIBREF21 adapted for dialogue performs well when it comes to open ended conversations as well as when trained on very large corpora (millions of utterances) BIBREF29, BIBREF30. In our setup however, we train the HRED model using a smaller dataset containing goal oriented dialogues in 7 different domains. We compare this model with our proposed exemplar-based model. In addition, for the BLEU metric we include the results of a transformer modelBIBREF31 that uses dialogue context to condition the decoder as well as a LSTM that uses dialogue context and incorporates belief state and KB results as additional inputs BIBREF0.",Results,"Overall, we found that in most cases, our simple model leads to significant improvements over the standard metrics BIBREF32; see Table 1 for the results. Although we are tackling goal-oriented dialogue, traditional metrics for goal oriented dialogue rely on human-generated supervision i.e. slot-value pair labels or dialogue act labels. Word overlap metrics such as the ones used for machine translation are often used to evaluate the quality of dialogue generation, however, these standard metrics tend to have very weak correlation with human judgment. In any case, we include some of these, as well as word embedding metrics for comparison. For the standard metrics, we use the evaluation scripts from BIBREF20 . We observe that the retrieval model is consistently better across all scenarios and metrics. In addition to these metrics, we assess our performance using the dialogue success metrics typically used in belief tracking BIBREF0. We briefly explain these metrics further.",Results ::: BLEU,"BLEU BIBREF33 is typically used for machine translation and has subsequently been used to evaluate the performance of many dialogue generation systems BIBREF34, BIBREF21, BIBREF20. BLEU analyzes co-occurrences of n-grams in a reference sequence and a hypothesis. For all datasets, we see improvements with BLEU. It uses a modified precision to account for the differences in length between reference and generated output. Given a reference sentence $s$ and a hypothesis sentence $\hat{s}$, we can denote the n-gram precision $P_{n}(s, \hat{s})$ as: where q is the index of all possible n-grams, and h(q,s) is the number of n-grams in s.",Results ::: Average Word Embedding Similarity,"We follow BIBREF32 and obtain the average embedding $e_s$ for the reference sentence $s$ by averaging the word embeddings $e_w$ for each token $w$ in $s$. We do the same for the predicted output $\hat{s}$ and obtain the final similarity score by computing cosine similarity of the two resulting vectors. Again, Exemplar-HRED is consistently superior yielding almost a 2 percent improvement over the best baseline model.",Results ::: Vector Extrema,"We also compute the cosine similarity between the vector extrema of the reference and the hypothesis, again following BIBREF32. The goal of this metric as described by the authors is to consider informative words rather than common words, since the vectors for common words will tend to be pulled towards the zero vector. Our exemplar model achieves the largest improvement for this metric, with a gain of 6 percent over the baseline model.",Results ::: Greedy Matching,"In greedy matching BIBREF32, given two sequences $s$ and $\hat{s}$, each token $w \in s$ is matched with each token $\hat{w} \in \hat{s}$ by computing the cosine similarity of the corresponding word embeddings $emb_w$ and $emb_{\hat{w}}$. The local match $g(s, \hat{s})$ is the word embedding with the maximum cosine similarity. We compute in both directions and the total score is: This metric is used to favour key words. Our best model shows only small improvements on this metric.",Results ::: Human Evaluation,"In addition to the previously mentioned standard metrics, we also evaluate the performance of our baseline and the exemplar-based models using human evaluations. We extract 100 baseline and exemplar model system responses at random. We ask the 7 evaluators to 1) pick the response that is more fluent and grammatically correct and 2) pick the response that achieves the goal given the context of the conversation. We provide the context of System and User utterances, and ask the evaluators to pick one of 4 options: 1) the output of the baseline, 2) the output of the exemplar model, 3) both, 4) none. The order of the options was shuffled. Overall, we found that when it came to fluency, the evaluators perceived that 58% of the time, the exemplar response was better. The baseline beat the exemplar based response for 19 percent of the evaluated dialogs and the rest of the dialogs either both or none were picked. For appropriateness we see a similar pattern. Evaluators perceived the response produced by the exemplar model as the more appropriate one given the context, for 59 percent of the evaluated dialogs. The baseline beat the proposed model only 14 percent of the time. These results can also be found on table TABREF7",Results ::: Dialogue success: inform/request,"Traditional goal-oriented dialogue systems based on prediction of slots and dialogue acts are typically evaluated on the accuracy of predicting these as labels, as well as their success at the end of the dialogue. Dialogue success is measured by how many correct inform/request slots a model can generate in a conversation in comparison to the ground truth. An inform slot is one that provides the user with a specific item for example the inform slots ""food"" and ""area"" i.e. (food=“Chinese”, area=""center”) informs the user that there is a Chinese restaurant in the center. On the other hand, a request slot is a slot that specifies what information is needed for the system to achieve the user goal. For example, for booking a train, the system needs to know the departure location and the destination. The slots ""departure"" and ""destination"" would be the request slots in this case. For goal-oriented generation, many of the models evaluated using the Inform/Request metrics have made use of structured data to semantically condition the generation model in order to generate better responses BIBREF35, BIBREF1, BIBREF0. BIBREF35 proposed to encode each individual dialog act as a unique vector and use it as an extra input feature, in order to influence the generated response. This method has been shown to work well when tested on single domains where the label space is limited to a few dialog acts. However, as the label space grows, using a one-hot encoding representation of the dialog act is not scalable. To deal with this problem, BIBREF1 introduced a semantically conditioned generation model using Hierarchical Disentangled Self-Attention (HDSA) . This model deals with the large label space by representing dialog acts using a multi-layer hierarchical graph that merges cross-branch nodes. For example, the distinct trees for hotel-recommend-area and attraction-recommend-area can be merged at the second and third levels sharing semantic information about actions and slots but maintaining specific information about the domains separate. This information can then be used as part of an attention mechanism when generating a response. This model achieves the state-of-the-art result for generation in both BLEU and Inform/Request metrics. As we are concerned with improving dialogue generation for goal oriented dialogue, we are interested in assessing how our simple approach compares to models explicitly using dialog acts as extra information. We compute the inform/request accuracy and compare to the state-of-the-art BIBREF1 as well as other baseline models. BIBREF1 present experiments conditioning both on predicted acts as well as ground truth past acts. We include both of these as well as the performance of our baseline and proposed model in table TABREF15. We divide the results into models using act information to condition the language generation and models that do not.",Discussion,"As shown in table TABREF7, our simplest proposed model achieved the largest improvements over the baseline when it came to the average embedding similarity and vector extrema similarity. As it is hard to interpret what the difference in performance of each model is based on standard dialogue metrics we examined the output to spot the major differences in response generation of our proposed models versus the baseline. We looked at the responses generated by our proposed models that had the highest score of these metrics and compared to the response generated by the baseline for that same dialogue. Overall we found that the baseline models tend to generate responses containing slots and values for the wrong domain. In addition, by examining the outputs we could see that the vector extrema metric is very sensitive when it comes to slight differences in the references and prediction. We found that this metric was more indicative of model performance than embedding similarity. We present some example outputs in table TABREF16. As mentioned earlier, from manual inspection of the outputs we observed that the the exemplar model is able to stay within the correct domain of the conversation and returns information within that domain that is more appropriate given the conversation context. This was confirmed by the human evaluations and also the Inform/Request metrics. When comparing the performance of the exemplar-based model to models that do not use information about past acts to condition the decoder, we observe that including a simple retrieval step leads to very large gains in the success of providing the inform/request slots.. In addition, the exemplar model performs better than BIBREF0, which uses information of the belief state of the conversation as extra features. More interestingly, our proposed model performs better than the state-of-the-art when it comes to providing the request slots. It also outperforms this same model when evaluated on BLEU; however, it still falls behind the state-of-the-art when it comes to providing inform slots. Overall, we find that our model remains competitive without requiring turn labels.",Related Work,"Dialogue generation has relied on transduction architectures originally developed for machine translation (MT) BIBREF15, BIBREF16, BIBREF17. Open domain dialogue systems aim to generate fluent and meaningful responses, however this has proven a challenging task. Most systems are able to generate coherent responses that are somewhat meaningless and at best entertaining BIBREF19, BIBREF17, BIBREF20. Much of the research on dialogue generation has tried to tackle this problem by predicting an utterance based on some dialogue history BIBREF36, BIBREF37, BIBREF38, BIBREF20. We extend such an architecture to also include past history, in order to avoid generating too generic responses. Most research on goal-oriented dialogue has focused almost exclusively on dialogue state tracking and dialogue policy learning BIBREF12, BIBREF39, BIBREF40, BIBREF41, BIBREF42, BIBREF43, BIBREF9, BIBREF10. Dialogue state tracking consists of detecting the user intent and tends to rely on turn-level supervision and a preset number of possible slot and value pairs which limits the flexibility of such chatbots, including their ability to respond to informal chit chat, as well as transferring knowledge across domains. There has been some work in the past few years that has attempted to address these problems by introducing methods that focus on domain adaptation as well as introducing new data to make this task more accessible BIBREF43, BIBREF44, BIBREF0, BIBREF45. Recent approaches have also introduced methods for representing slot and value pairs that do not rely on a preset ontology BIBREF8, BIBREF46, in an attempt to add flexibility. In our research, we acknowledge the importance of this added flexibility in goal oriented dialogue and propose a method for generating goal oriented responses without having turn level supervision. The idea of combining text generation with past experience has been explored before. BIBREF47 used a set of hand crafted examples in order to generate responses through templates. More recently, BIBREF48 also explored a hybrid system with an information retrieval component, but their system is very different: It uses a complex ranking system at a high computational cost, requires a post-reranking component to exploit previous dialogue turns (about half of which are copied over as predictions), and they only evaluate their system in a chit-chat set-up, reporting only BLEU scores. In a similar paper, BIBREF22 tried to move away from short generic answers in order to make a chit-chat generation model more entertaining by using an information retrieval component, to introduce relevant facts. In addition, a similar method was recently shown to improve other generation tasks such as summarization. In BIBREF49, the authors show that a simple extractive step introduces enough inductive bias for an abstractive summarization system to provide fluent yet precise summaries. In contrast to these works, we integrate a retrieval based method with a context-aware neural dialogue generation model in order to introduce relevant responses in a goal oriented conversation.",Conclusion,"In this study, we have experimented with a simple yet effective way of conditioning the decoder in a dialogue generation model intended for goal oriented dialogue. Generating fluent and precise responses is crucial for creating goal-oriented dialogue systems, however, this can be a very difficult task; particularly, when the system responses are dependent on domain-specific information. We propose adding a simple retrieval step, where we obtain the past conversations that are most relevant to the current one and condition our decoder on these. We find that this method not only improves over multiple strong baseline models on word overlap metrics, it also performs better than the state-of-the-art on BLEU and achieves competitive performance for inform/request metrics without requiring dialog act annotations. Finally, by inspecting the output of the baseline versus our proposed model and through human evaluations, we find that a great advantage of our model is its ability to produce responses that are more fluent and remain on topic.",,,,,,,,,,,what semantically conditioned models did they compare with?,090fd9ce9a21438cdec1ea51ed216941d52eb3b6,,,,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,,"For goal-oriented generation, many of the models evaluated using the Inform/Request metrics have made use of structured data to semantically condition the generation model in order to generate better responses BIBREF35, BIBREF1, BIBREF0. BIBREF35 proposed to encode each individual dialog act as a unique vector and use it as an extra input feature, in order to influence the generated response. This method has been shown to work well when tested on single domains where the label space is limited to a few dialog acts. However, as the label space grows, using a one-hot encoding representation of the dialog act is not scalable. To deal with this problem, BIBREF1 introduced a semantically conditioned generation model using Hierarchical Disentangled Self-Attention (HDSA) . This model deals with the large label space by representing dialog acts using a multi-layer hierarchical graph that merges cross-branch nodes. For example, the distinct trees for hotel-recommend-area and attraction-recommend-area can be merged at the second and third levels sharing semantic information about actions and slots but maintaining specific information about the domains separate. This information can then be used as part of an attention mechanism when generating a response. This model achieves the state-of-the-art result for generation in both BLEU and Inform/Request metrics.","To deal with this problem, BIBREF1 introduced a semantically conditioned generation model using Hierarchical Disentangled Self-Attention (HDSA) .",5fe35a48584df4a9938066eb7c2381d193e3e41a,c1018a31c3272ce74964a3280069f62f314a1a58,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2-Figure1-1.png,"Figure 1: Our model is similar to HRED (Sordoni et al., 2015), we include an utterance encoder, a context encoder and a decoder, however, unlike HRED, our model include a simple, yet effective retrieval step used to condition the decoder to generate responses that are more appropriate for a specific domain and context.",4-Table1-1.png,"Table 1: The results of our dialogue generation experiments comparing HRED Sordoni et al. (2015); Serban et al. (2016) to our proposed exemplar-based model. We present resultd for standard metrics used in dialogue generation. For all the metrics we observe improvements over the strong baseline, with our best improvement of 6 percent in the vector extrema metric",6-Table2-1.png,"Table 2: Inform/request results divided into two section. The top models do not make use of any past dialog acts to condition the decoder to generate a response. The models at the bottom use dialog acts, and belief state in order to generate better responses",8-Table3-1.png,"Table 3: Examples of responses generated by both the baseline and our proposed model. By examining the outputs, it becomes noticeable that the baseline model tends to generate responses that are not precise about the current domain of the conversation (hotel, taxi booking, trains, restaurant, etc). The Exemplar-HRED model on the other hand becomes more accurate in responding for the correct domain. Here we present a few examples of responses given the same context, and provide their scores (similarity to reference) for comparison",,,,,,,,,,,,,,,,,,,,,,,,,,Hierarchical Disentangled Self-Attention,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Build Fast and Accurate Lemmatization for Arabic,"In this paper we describe the complexity of building a lemmatizer for Arabic which has a rich and complex derivational morphology, and we discuss the need for a fast and accurate lammatization to enhance Arabic Information Retrieval (IR) results. We also introduce a new data set that can be used to test lemmatization accuracy, and an efficient lemmatization algorithm that outperforms state-of-the-art Arabic lemmatization in terms of accuracy and speed. We share the data set and the code for public.",Introduction,"Lemmatization is the process of finding the base form (or lemma) of a word by considering its inflected forms. Lemma is also called dictionary form, or citation form, and it refers to all words having the same meaning. Lemmatization is an important preprocessing step for many applications of text mining and question-answering systems, and researches in Arabic Information Retrieval (IR) systems show the need for representing Arabic words at lemma level for many applications, including keyphrase extraction BIBREF0 and machine translation BIBREF1 . In addition, lemmatization provides a productive way to generate generic keywords for search engines (SE) or labels for concept maps BIBREF2 . Word stem is that core part of the word that never changes even with morphological inflections; the part that remains after prefix and suffix removal. Sometimes the stem of the word is different than its lemma, for example the words: believe, believed, believing, and unbelievable share the stem (believ-), and have the normalized word form (believe) standing for the infinitive of the verb (believe). While stemming tries to remove prefixes and suffixes from words that appear with inflections in free text, lemmatization tries to replace word suffixes with (typically) different suffix to get its lemma. This extended abstract is organized as follows: Section SECREF2 shows some complexities in building Arabic lemmatization, and surveys prior work on Arabic stemming and lemmatization; Section SECREF3 introduces the dataset that we created to test lemmatization accuracy; Section SECREF4 describes the algorithm of the system that we built and report results and error analysis in section SECREF5 ; and Section SECREF6 discusses the results and concludes the abstract.",Background,"Arabic is the largest Semitic language spoken by more than 400 million people. It's one of the six official languages in the United Nations, and the fifth most widely spoken language after Chinese, Spanish, English, and Hindi. Arabic has a very rich morphology, both derivational and inflectional. Generally, Arabic words are derived from a root that uses three or more consonants to define a broad meaning or concept, and they follow some templatic morphological patterns. By adding vowels, prefixes and suffixes to the root, word inflections are generated. For instance, the word ÙØ³ÙÙØªØ­ÙÙ> (wsyftHwn) “and they will open” has the triliteral root ÙØªØ­> (ftH), which has the basic meaning of opening, has prefixes ÙØ³> (ws) “and will”, suffixes ÙÙ> (wn) “they”, stem ÙÙØªØ­> (yftH) “open”, and lemma ÙØªØ­> (ftH) “the concept of opening”. IR systems typically cluster words together into groups according to three main levels: root, stem, or lemma. The root level is considered by many researchers in the IR field which leads to high recall but low precision due to language complexity, for example words ÙØªØ¨Ø Ù ÙØªØ¨Ø©Ø ÙØªØ§Ø¨> (ktb, mktbp, ktAb) “wrote, library, book” have the same root ÙØªØ¨> (ktb) with the basic meaning of writing, so searching for any of these words by root, yields getting the other words which may not be desirable for many users. Other researchers show the importance of using stem level for improving retrieval precision and recall as they capture semantic similarity between inflected words. However, in Arabic, stem patterns may not capture similar words having the same semantic meaning. For example, stem patterns for broken plurals are different from their singular patterns, e.g. the plural Ø£ÙÙØ§Ù > (AqlAm) “pens” will not match the stem of its singular form ÙÙÙ > (qlm) “pen”. The same applies to many imperfect verbs that have different stem patterns than their perfect verbs, e.g. the verbs Ø§Ø³ØªØ·Ø§Ø¹Ø ÙØ³ØªØ·ÙØ¹> (AstTAE, ystTyE) “he could, he can” will not match because they have different stems. Indexing using lemmatization can enhance the performance of Arabic IR systems. A lot of work has been done in word stemming and lemmatization in different languages, for example the famous Porter stemmer for English, but for Arabic, there are few work has been done especially in lemmatization, and there is no open-source code and new testing data that can be used by other researchers for word lemmatization. Xerox Arabic Morphological Analysis and Generation BIBREF3 is one of the early Arabic stemmers, and it uses morphological rules to obtain stems for nouns and verbs by looking into a table of thousands of roots. Khoja's stemmer BIBREF4 and Buckwalter morphological analyzer BIBREF5 are other root-based analyzers and stemmers which use tables of valid combinations between prefixes and suffixes, prefixes and stems, and stems and suffixes. Recently, MADAMIRA BIBREF6 system has been evaluated using a blind testset (25K words for Modern Standard Arabic (MSA) selected from Penn Arabic Tree bank (PATB)), and the reported accuracy was 96.2% as the percentage of words where the chosen analysis (provided by SAMA morphological analyzer BIBREF7 ) has the correct lemma. In this paper, we present an open-source Java code to extract Arabic word lemmas, and a new publicly available testset for lemmatization allowing researches to evaluate using the same dataset that we used, and reproduce same experiments.",Data Description,"To make the annotated data publicly available, we selected 70 news articles from Arabic WikiNews site https://ar.wikinews.org/wiki. These articles cover recent news from year 2013 to year 2015 in multiple genres (politics, economics, health, science and technology, sports, arts, and culture.) Articles contain 18,300 words, and they are evenly distributed among these 7 genres with 10 articles per each. Word are white-space and punctuation separated, and some spelling errors are corrected (1.33% of the total words) to have very clean test cases. Lemmatization is done by an expert Arabic linguist where spelling corrections are marked, and lemmas are provided with full diacritization as shown in Figure FIGREF2 . As MSA is usually written without diacritics and IR systems normally remove all diacritics from search queries and indexed data as a basic preprocessing step, so another column for undiacritized lemma is added and it's used for evaluating our lemmatizer and comparing with state-of-the-art system for lemmatization; MADAMIRA.",system Description,"We were inspired by the work done by BIBREF8 for segmenting Arabic words out of context. They achieved an accuracy of almost 99%; slightly better than state-of-the-art system for segmentation (MADAMIRA) which considers surrounding context and many linguistic features. This system shows enhancements in both Machine Translation, and Information Retrieval tasks BIBREF9 . This work can be considered as an extension to word segmentation. From a large diacritized corpus, we constructed a dictionary of words and their possible diacritizations ordered by number of occurrences of each diacritized form. This diacritized corpus was created by a commercial vendor and contains 9.7 million words with almost 200K unique surface words. About 73% of the corpus is in MSA and covers variety of genres like politics, economy, sports, society, etc. and the remaining part is mostly religious texts written in classical Arabic (CA). The effectiveness of using this corpus in building state-of-the-art diacritizer was proven in BIBREF10 .For example, the word ÙØ¨ÙÙØ¯> (wbnwd) “and items” is found 4 times in this corpus with two full diacritization forms ÙÙØ¨ÙÙÙÙØ¯ÙØ ÙÙØ¨ÙÙÙÙØ¯Ù> (wabunudi, wabunudK) “items, with different grammatical case endings” which appeared 3 times and once respectively. All unique undiacritized words in this corpus were analyzed using Buckwalter morphological analyzer which gives all possible word diacritizations, and their segmentation, POS tag and lemma as shown in Figure FIGREF3 . The idea is to take the most frequent diacritized form for words appear in this corpus, and find the morphological analysis with highest matching score between its diacritized form and the corpus word. This means that we search for the most common diacritization of the word regardless of its surrounding context. In the above example, the first solution is preferred and hence its lemma Ø¨ÙØ¯> (banod, bnd after diacritics removal) “item”. While comparing two diacritized forms from the corpus and Buckwalter analysis, special cases were applied to solve inconsistencies between the two diacritization schemas, for example while words are fully diacritized in the corpus, Buckwalter analysis gives diacritics without case ending (i.e. without context), and removes short vowels in some cases, for example before long vowels, and after the definite article Ø§Ù> (Al) “the”, etc. It worths mentioning that there are many cases in Buckwalter analysis where for the input word, there are two or more identical diacritizations with different lemmas, and the analyses of such words are provided without any meaningful order. For example the word Ø³ÙØ§Ø±Ø©> (syArp) “car” has two morphological analyses with different lemmas, namely Ø³ÙØ§Ø±> (syAr) “walker”, and Ø³ÙØ§Ø±Ø©> (syArp) “car” in this order while the second lemma is the most common one. To solve tis problem, all these words are reported and the top frequent words are revised and order of lemmas were changed according to actual usage in modern language. The lemmatization algorithm can be summarized in Figure FIGREF4 , and the online system can be tested through the site http://alt.qcri.org/farasa/segmenter.html",Evaluation,"Data was formatted in a plain text format where sentences are written in separate lines and words are separated by spaces, and the outputs of MADAMIRA and our system are compared against the undiacritized lemma for each word. For accurate results, all differences were revised manually to accept cases that should not be counted as errors (different writings of foreign names entities for example as in ÙÙÙØº ÙÙÙØºØ ÙÙÙØ¬ ÙÙÙØ¬> (hwng kwng, hwnj kwnj) “Hong Kong”, or more than one accepted lemma for some function words, e.g the lemmas ÙÙØ ÙÙÙ Ø§> (fy, fymA) are both valid for the function word ÙÙÙ Ø§> (fymA) “while”). Table TABREF5 shows results of testing our system and MADAMIRA on the WikiNews testset (for undiacritized lemmas). Our approach gives +7% relative gain above MADAMIRA in lemmatization task. In terms of speed, our system was able to lemmatize 7.4 million words on a personal laptop in almost 2 minutes compared to 2.5 hours for MADAMIRA, i.e. 75 times faster. The code is written entirely in Java without any external dependency which makes its integration in other systems quite simple.",Error Analysis,"Most of the lemmatization errors in our system are due to fact that we use the most common diacritization of words without considering their contexts which cannot solve the ambiguity in cases like nouns and adjectives that share the same diacritization forms, for example the word Ø£ÙØ§Ø¯ÙÙ ÙØ©> (AkAdymyp) can be either a noun and its lemma is Ø£ÙØ§Ø¯ÙÙ ÙØ©> (AkAdymyp) “academy”, or an adjective and its lemma is Ø£ÙØ§Ø¯ÙÙ Ù> (AkAdymy) “academic”. Also for MADAMIRA, errors in selecting the correct Part-of-Speech (POS) for ambiguous words, and foreign named entities. In the full paper, we will quantify error cases in our lemmatizer and MADAMIRA and give examples for each case which can help in enhancing both systems.",Discussion,"In this paper, we introduce a new dataset for Arabic lemmatization and a very fast and accurate lemmatization algorithm that performs better than state-of-the art system; MADAMIRA. Both the dataset and the code will be publicly available. We show that to build an effective IR system for complex derivational languages like Arabic, there is a a big need for very fast and accurate lemmatization algorithms, and we show that this can be achieved by considering only the most frequent diacritized form for words and matching this form with the morphological analysis with highest similarity score. We plan to study the performance if the algorithm was modified to provide diacritized lemmas which can be useful for other applications.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,How was speed measured?,da845a2a930fd6a3267950bec5928205b6c6e8e8,,,,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,how long it takes the system to lemmatize a set number of words,"In terms of speed, our system was able to lemmatize 7.4 million words on a personal laptop in almost 2 minutes compared to 2.5 hours for MADAMIRA, i.e. 75 times faster. The code is written entirely in Java without any external dependency which makes its integration in other systems quite simple.","In terms of speed, our system was able to lemmatize 7.4 million words on a personal laptop in almost 2 minutes compared to 2.5 hours for MADAMIRA, i.e. 75 times faster. ",6f8264da377ead4925f48d0637888203343381c7,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,,,,,,,,,What were their accuracy results on the task?,2fa0b9d0cb26e1be8eae7e782ada6820bc2c037f,,,,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,97.32%,FLOAT SELECTED: Table 3: Lemmatization accuracy using WikiNews testset,FLOAT SELECTED: Table 3: Lemmatization accuracy using WikiNews testset,29d8e794ef14a685d9ccba40c66a63baa7352bb0,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,What is the state of the art?,76ce9e02d97e2d77fe28c0fa78526809e7c195c6,,,,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,,"Khoja's stemmer BIBREF4 and Buckwalter morphological analyzer BIBREF5 are other root-based analyzers and stemmers which use tables of valid combinations between prefixes and suffixes, prefixes and stems, and stems and suffixes. Recently, MADAMIRA BIBREF6 system has been evaluated using a blind testset (25K words for Modern Standard Arabic (MSA) selected from Penn Arabic Tree bank (PATB)), and the reported accuracy was 96.2% as the percentage of words where the chosen analysis (provided by SAMA morphological analyzer BIBREF7 ) has the correct lemma. As MSA is usually written without diacritics and IR systems normally remove all diacritics from search queries and indexed data as a basic preprocessing step, so another column for undiacritized lemma is added and it's used for evaluating our lemmatizer and comparing with state-of-the-art system for lemmatization; MADAMIRA.","Recently, MADAMIRA BIBREF6 system has been evaluated using a blind testset (25K words for Modern Standard Arabic (MSA) selected from Penn Arabic Tree bank (PATB)), and the reported accuracy was 96.2% as the percentage of words where the chosen analysis (provided by SAMA morphological analyzer BIBREF7 ) has the correct lemma. As MSA is usually written without diacritics and IR systems normally remove all diacritics from search queries and indexed data as a basic preprocessing step, so another column for undiacritized lemma is added and it's used for evaluating our lemmatizer and comparing with state-of-the-art system for lemmatization; MADAMIRA.",cb5e9f34c283fb1a8638394e7c1060d04fe4ca7f,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,,,,,,,,How was the dataset annotated?,64c7545ce349265e0c97fd6c434a5f8efdc23777,,,,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,,"To make the annotated data publicly available, we selected 70 news articles from Arabic WikiNews site https://ar.wikinews.org/wiki. These articles cover recent news from year 2013 to year 2015 in multiple genres (politics, economics, health, science and technology, sports, arts, and culture.) Articles contain 18,300 words, and they are evenly distributed among these 7 genres with 10 articles per each. Word are white-space and punctuation separated, and some spelling errors are corrected (1.33% of the total words) to have very clean test cases. Lemmatization is done by an expert Arabic linguist where spelling corrections are marked, and lemmas are provided with full diacritization as shown in Figure FIGREF2 .","To make the annotated data publicly available, we selected 70 news articles from Arabic WikiNews site https://ar.wikinews.org/wiki. These articles cover recent news from year 2013 to year 2015 in multiple genres (politics, economics, health, science and technology, sports, arts, and culture.) Articles contain 18,300 words, and they are evenly distributed among these 7 genres with 10 articles per each.

Word are white-space and punctuation separated, and some spelling errors are corrected (1.33% of the total words) to have very clean test cases. Lemmatization is done by an expert Arabic linguist where spelling corrections are marked, and lemmas are provided with full diacritization as shown in Figure FIGREF2 .",c298f88263f2a0d59eba884e9f6ce7b23a3044e8,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,,,,,,,,What is the size of the dataset?,47822fec590e840438a3054b7f512fec09dbd1e1,,,,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,"Articles contain 18,300 words, and they are evenly distributed among these 7 genres with 10 articles per each",,,"To make the annotated data publicly available, we selected 70 news articles from Arabic WikiNews site https://ar.wikinews.org/wiki. These articles cover recent news from year 2013 to year 2015 in multiple genres (politics, economics, health, science and technology, sports, arts, and culture.) Articles contain 18,300 words, and they are evenly distributed among these 7 genres with 10 articles per each.","To make the annotated data publicly available, we selected 70 news articles from Arabic WikiNews site https://ar.wikinews.org/wiki. These articles cover recent news from year 2013 to year 2015 in multiple genres (politics, economics, health, science and technology, sports, arts, and culture.) Articles contain 18,300 words, and they are evenly distributed among these 7 genres with 10 articles per each.",7470eb50839c2a754141ea9429ca4523ce9e64da,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,Where did they collect their dataset from?,989271972b3176d0a5dabd1cc0e4bdb671269c96,,,,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,,"To make the annotated data publicly available, we selected 70 news articles from Arabic WikiNews site https://ar.wikinews.org/wiki. These articles cover recent news from year 2013 to year 2015 in multiple genres (politics, economics, health, science and technology, sports, arts, and culture.) Articles contain 18,300 words, and they are evenly distributed among these 7 genres with 10 articles per each.","To make the annotated data publicly available, we selected 70 news articles from Arabic WikiNews site https://ar.wikinews.org/wiki. These articles cover recent news from year 2013 to year 2015 in multiple genres (politics, economics, health, science and technology, sports, arts, and culture.) Articles contain 18,300 words, and they are evenly distributed among these 7 genres with 10 articles per each.",5c5b74534c190cf078c20a34e0b404737e52f886,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2-Table1-1.png,Table 1: Examples of complex verb lemmatization cases,2-Table2-1.png,Table 2: Examples of complex noun lemmatization cases,3-Figure2-1.png,Figure 2: Buckwalter analysis (diacritization forms and lemmas are highlighted),3-Figure1-1.png,Figure 1: Lemmatization of WikiNews corpus,4-Table3-1.png,Table 3: Lemmatization accuracy using WikiNews testset,5-Figure4-1.png,Figure 4: Lemmatization online demo (part of Farasa Arabic NLP tools),,,,,,,,,, MADAMIRA BIBREF6 system,,,,,,,,,,,from Arabic WikiNews site https://ar.wikinews.org/wiki,,,,,,,,,,"Lemmatization is done by an expert Arabic linguist where spelling corrections are marked, and lemmas are provided with full diacritization",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Semi-supervised sequence tagging with bidirectional language models,"Pre-trained word embeddings learned from unlabeled text have become a standard component of neural network architectures for NLP tasks. However, in most cases, the recurrent network that operates on word-level representations to produce context sensitive representations is trained on relatively little labeled data. In this paper, we demonstrate a general semi-supervised approach for adding pre- trained context embeddings from bidirectional language models to NLP systems and apply it to sequence labeling tasks. We evaluate our model on two standard datasets for named entity recognition (NER) and chunking, and in both cases achieve state of the art results, surpassing previous systems that use other forms of transfer or joint learning with additional labeled data and task specific gazetteers.",Introduction,"Due to their simplicity and efficacy, pre-trained word embedding have become ubiquitous in NLP systems. Many prior studies have shown that they capture useful semantic and syntactic information BIBREF0 , BIBREF1 and including them in NLP systems has been shown to be enormously helpful for a variety of downstream tasks BIBREF2 . However, in many NLP tasks it is essential to represent not just the meaning of a word, but also the word in context. For example, in the two phrases “A Central Bank spokesman” and “The Central African Republic”, the word `Central' is used as part of both an Organization and Location. Accordingly, current state of the art sequence tagging models typically include a bidirectional recurrent neural network (RNN) that encodes token sequences into a context sensitive representation before making token specific predictions BIBREF3 , BIBREF4 , BIBREF5 , BIBREF6 . Although the token representation is initialized with pre-trained embeddings, the parameters of the bidirectional RNN are typically learned only on labeled data. Previous work has explored methods for jointly learning the bidirectional RNN with supplemental labeled data from other tasks BIBREF7 , BIBREF3 . In this paper, we explore an alternate semi-supervised approach which does not require additional labeled data. We use a neural language model (LM), pre-trained on a large, unlabeled corpus to compute an encoding of the context at each position in the sequence (hereafter an LM embedding) and use it in the supervised sequence tagging model. Since the LM embeddings are used to compute the probability of future words in a neural LM, they are likely to encode both the semantic and syntactic roles of words in context. Our main contribution is to show that the context sensitive representation captured in the LM embeddings is useful in the supervised sequence tagging setting. When we include the LM embeddings in our system overall performance increases from 90.87% to 91.93% INLINEFORM0 for the CoNLL 2003 NER task, a more then 1% absolute F1 increase, and a substantial improvement over the previous state of the art. We also establish a new state of the art result (96.37% INLINEFORM1 ) for the CoNLL 2000 Chunking task. As a secondary contribution, we show that using both forward and backward LM embeddings boosts performance over a forward only LM. We also demonstrate that domain specific pre-training is not necessary by applying a LM trained in the news domain to scientific papers.",Overview,"The main components in our language-model-augmented sequence tagger (TagLM) are illustrated in Fig. FIGREF4 . After pre-training word embeddings and a neural LM on large, unlabeled corpora (Step 1), we extract the word and LM embeddings for every token in a given input sequence (Step 2) and use them in the supervised sequence tagging model (Step 3).",Baseline sequence tagging model,"Our baseline sequence tagging model is a hierarchical neural tagging model, closely following a number of recent studies BIBREF4 , BIBREF5 , BIBREF3 , BIBREF8 (left side of Figure FIGREF5 ). Given a sentence of tokens INLINEFORM0 it first forms a representation, INLINEFORM1 , for each token by concatenating a character based representation INLINEFORM2 with a token embedding INLINEFORM3 : DISPLAYFORM0   The character representation INLINEFORM0 captures morphological information and is either a convolutional neural network (CNN) BIBREF4 , BIBREF8 or RNN BIBREF3 , BIBREF5 . It is parameterized by INLINEFORM1 with parameters INLINEFORM2 . The token embeddings, INLINEFORM3 , are obtained as a lookup INLINEFORM4 , initialized using pre-trained word embeddings, and fine tuned during training BIBREF2 . To learn a context sensitive representation, we employ multiple layers of bidirectional RNNs. For each token position, INLINEFORM0 , the hidden state INLINEFORM1 of RNN layer INLINEFORM2 is formed by concatenating the hidden states from the forward ( INLINEFORM3 ) and backward ( INLINEFORM4 ) RNNs. As a result, the bidirectional RNN is able to use both past and future information to make a prediction at token INLINEFORM5 . More formally, for the first RNN layer that operates on INLINEFORM6 to output INLINEFORM7 : DISPLAYFORM0   The second RNN layer is similar and uses INLINEFORM0 to output INLINEFORM1 . In this paper, we use INLINEFORM2 layers of RNNs in all experiments and parameterize INLINEFORM3 as either Gated Recurrent Units (GRU) BIBREF9 or Long Short-Term Memory units (LSTM) BIBREF10 depending on the task. Finally, the output of the final RNN layer INLINEFORM0 is used to predict a score for each possible tag using a single dense layer. Due to the dependencies between successive tags in our sequence labeling tasks (e.g. using the BIOES labeling scheme, it is not possible for I-PER to follow B-LOC), it is beneficial to model and decode each sentence jointly instead of independently predicting the label for each token. Accordingly, we add another layer with parameters for each label bigram, computing the sentence conditional random field (CRF) loss BIBREF11 using the forward-backward algorithm at training time, and using the Viterbi algorithm to find the most likely tag sequence at test time, similar to BIBREF2 .",Bidirectional LM,"A language model computes the probability of a token sequence INLINEFORM0 INLINEFORM1  Recent state of the art neural language models BIBREF12 use a similar architecture to our baseline sequence tagger where they pass a token representation (either from a CNN over characters or as token embeddings) through multiple layers of LSTMs to embed the history INLINEFORM0 into a fixed dimensional vector INLINEFORM1 . This is the forward LM embedding of the token at position INLINEFORM2 and is the output of the top LSTM layer in the language model. Finally, the language model predicts the probability of token INLINEFORM3 using a softmax layer over words in the vocabulary. The need to capture future context in the LM embeddings suggests it is beneficial to also consider a backward LM in additional to the traditional forward LM. A backward LM predicts the previous token given the future context. Given a sentence with INLINEFORM0 tokens, it computes INLINEFORM1  A backward LM can be implemented in an analogous way to a forward LM and produces the backward LM embedding INLINEFORM0 , for the sequence INLINEFORM1 , the output embeddings of the top layer LSTM. In our final system, after pre-training the forward and backward LMs separately, we remove the top layer softmax and concatenate the forward and backward LM embeddings to form bidirectional LM embeddings, i.e., INLINEFORM0 . Note that in our formulation, the forward and backward LMs are independent, without any shared parameters.",Combining LM with sequence model,"Our combined system, TagLM, uses the LM embeddings as additional inputs to the sequence tagging model. In particular, we concatenate the LM embeddings INLINEFORM0 with the output from one of the bidirectional RNN layers in the sequence model. In our experiments, we found that introducing the LM embeddings at the output of the first layer performed the best. More formally, we simply replace ( EQREF6 ) with DISPLAYFORM0  There are alternate possibilities for adding the LM embeddings to the sequence model. One possibility adds a non-linear mapping after the concatenation and before the second RNN (e.g. replacing ( EQREF9 ) with INLINEFORM0 where INLINEFORM1 is a non-linear function). Another possibility introduces an attention-like mechanism that weights the all LM embeddings in a sentence before including them in the sequence model. Our initial results with the simple concatenation were encouraging so we did not explore these alternatives in this study, preferring to leave them for future work.",Experiments,"We evaluate our approach on two well benchmarked sequence tagging tasks, the CoNLL 2003 NER task BIBREF13 and the CoNLL 2000 Chunking task BIBREF14 . We report the official evaluation metric (micro-averaged INLINEFORM0 ). In both cases, we use the BIOES labeling scheme for the output tags, following previous work which showed it outperforms other options BIBREF15 . Following BIBREF8 , we use the Senna word embeddings BIBREF2 and pre-processed the text by lowercasing all tokens and replacing all digits with 0.",Overall system results,"Tables TABREF15 and TABREF16 compare results from TagLM with previously published state of the art results without additional labeled data or task specific gazetteers. Tables TABREF17 and TABREF18 compare results of TagLM to other systems that include additional labeled data or gazetteers. In both tasks, TagLM establishes a new state of the art using bidirectional LMs (the forward CNN-BIG-LSTM and the backward LSTM-2048-512). In the CoNLL 2003 NER task, our model scores 91.93 mean INLINEFORM0 , which is a statistically significant increase over the previous best result of 91.62 INLINEFORM1 from BIBREF8 that used gazetteers (at 95%, two-sided Welch t-test, INLINEFORM2 ). In the CoNLL 2000 Chunking task, TagLM achieves 96.37 mean INLINEFORM0 , exceeding all previously published results without additional labeled data by more then 1% absolute INLINEFORM1 . The improvement over the previous best result of 95.77 in BIBREF6 that jointly trains with Penn Treebank (PTB) POS tags is statistically significant at 95% ( INLINEFORM2 assuming standard deviation of INLINEFORM3 ). Importantly, the LM embeddings amounts to an average absolute improvement of 1.06 and 1.37 INLINEFORM0 in the NER and Chunking tasks, respectively. Although we do not use external labeled data or gazetteers, we found that TagLM outperforms previous state of the art results in both tasks when external resources (labeled data or task specific gazetteers) are available. Furthermore, Tables TABREF17 and TABREF18 show that, in most cases, the improvements we obtain by adding LM embeddings are larger then the improvements previously obtained by adding other forms of transfer or joint learning. For example, BIBREF3 noted an improvement of only 0.06 INLINEFORM0 in the NER task when transfer learning from both CoNLL 2000 chunks and PTB POS tags and BIBREF8 reported an increase of 0.71 INLINEFORM1 when adding gazetteers to their baseline. In the Chunking task, previous work has reported from 0.28 to 0.75 improvement in INLINEFORM2 when including supervised labels from the PTB POS tags or CoNLL 2003 entities BIBREF3 , BIBREF7 , BIBREF6 .",Analysis,"To elucidate the characteristics of our LM augmented sequence tagger, we ran a number of additional experiments on the CoNLL 2003 NER task. In this experiment, we concatenate the LM embeddings at different locations in the baseline sequence tagger. In particular, we used the LM embeddings INLINEFORM0 to: augment the input of the first RNN layer; i.e.,  INLINEFORM0 , augment the output of the first RNN layer; i.e., INLINEFORM0 , and augment the output of the second RNN layer; i.e., INLINEFORM0 . Table TABREF20 shows that the second alternative performs best. We speculate that the second RNN layer in the sequence tagging model is able to capture interactions between task specific context as expressed in the first RNN layer and general context as expressed in the LM embeddings in a way that improves overall system performance. These results are consistent with BIBREF7 who found that chunking performance was sensitive to the level at which additional POS supervision was added. In this experiment, we compare six different configurations of the forward and backward language models (including the baseline model which does not use any language models). The results are reported in Table TABREF21 . We find that adding backward LM embeddings consistently outperforms forward-only LM embeddings, with INLINEFORM0 improvements between 0.22 and 0.27%, even with the relatively small backward LSTM-2048-512 LM. LM size is important, and replacing the forward LSTM-2048-512 with CNN-BIG-LSTM (test perplexities of 47.7 to 30.0 on 1B Word Benchmark) improves INLINEFORM0 by 0.26 - 0.31%, about as much as adding backward LM. Accordingly, we hypothesize (but have not tested) that replacing the backward LSTM-2048-512 with a backward LM analogous to the CNN-BIG-LSTM would further improve performance. To highlight the importance of including language models trained on a large scale data, we also experimented with training a language model on just the CoNLL 2003 training and development data. Due to the much smaller size of this data set, we decreased the model size to 512 hidden units with a 256 dimension projection and normalized tokens in the same manner as input to the sequence tagging model (lower-cased, with all digits replaced with 0). The test set perplexities for the forward and backward models (measured on the CoNLL 2003 test data) were 106.9 and 104.2, respectively. Including embeddings from these language models decreased performance slightly compared to the baseline system without any LM. This result supports the hypothesis that adding language models help because they learn composition functions (i.e., the RNN parameters in the language model) from much larger data compared to the composition functions in the baseline tagger, which are only learned from labeled data. To understand the importance of including a task specific sequence RNN we ran an experiment that removed the task specific sequence RNN and used only the LM embeddings with a dense layer and CRF to predict output tags. In this setup, performance was very low, 88.17 INLINEFORM0 , well below our baseline. This result confirms that the RNNs in the baseline tagger encode essential information which is not encoded in the LM embeddings. This is unsurprising since the RNNs in the baseline tagger are trained on labeled examples, unlike the RNN in the language model which is only trained on unlabeled examples. Note that the LM weights are fixed in this experiment. A priori, we expect the addition of LM embeddings to be most beneficial in cases where the task specific annotated datasets are small. To test this hypothesis, we replicated the setup from BIBREF3 that samples 1% of the CoNLL 2003 training set and compared the performance of TagLM to our baseline without LM. In this scenario, test INLINEFORM0 increased 3.35% (from 67.66 to 71.01%) compared to an increase of 1.06% INLINEFORM1 for a similar comparison with the full training dataset. The analogous increases in BIBREF3 are 3.97% for cross-lingual transfer from CoNLL 2002 Spanish NER and 6.28% INLINEFORM2 for transfer from PTB POS tags. However, they found only a 0.06% INLINEFORM3 increase when using the full training data and transferring from both CoNLL 2000 chunks and PTB POS tags. Taken together, this suggests that for very small labeled training sets, transferring from other tasks yields a large improvement, but this improvement almost disappears when the training data is large. On the other hand, our approach is less dependent on the training set size and significantly improves performance even with larger training sets. Our TagLM formulation increases the number of parameters in the second RNN layer INLINEFORM0 due to the increase in the input dimension INLINEFORM1 if all other hyperparameters are held constant. To confirm that this did not have a material impact on the results, we ran two additional experiments. In the first, we trained a system without a LM but increased the second RNN layer hidden dimension so that number of parameters was the same as in TagLM. In this case, performance decreased slightly (by 0.15% INLINEFORM2 ) compared to the baseline model, indicating that solely increasing parameters does not improve performance. In the second experiment, we decreased the hidden dimension of the second RNN layer in TagLM to give it the same number of parameters as the baseline no LM model. In this case, test INLINEFORM3 increased slightly to INLINEFORM4 indicating that the additional parameters in TagLM are slightly hurting performance. One artifact of our evaluation framework is that both the labeled data in the chunking and NER tasks and the unlabeled text in the 1 Billion Word Benchmark used to train the bidirectional LMs are derived from news articles. To test the sensitivity to the LM training domain, we also applied TagLM with a LM trained on news articles to the SemEval 2017 Shared Task 10, ScienceIE. ScienceIE requires end-to-end joint entity and relationship extraction from scientific publications across three diverse fields (computer science, material sciences, and physics) and defines three broad entity types (Task, Material and Process). For this task, TagLM increased INLINEFORM0 on the development set by 4.12% (from 49.93 to to 54.05%) for entity extraction over our baseline without LM embeddings and it was a major component in our winning submission to ScienceIE, Scenario 1 BIBREF20 . We conclude that LM embeddings can improve the performance of a sequence tagger even when the data comes from a different domain.",Conclusion,"In this paper, we proposed a simple and general semi-supervised method using pre-trained neural language models to augment token representations in sequence tagging models. Our method significantly outperforms current state of the art models in two popular datasets for NER and Chunking. Our analysis shows that adding a backward LM in addition to traditional forward LMs consistently improves performance. The proposed method is robust even when the LM is trained on unlabeled data from a different domain, or when the baseline model is trained on a large number of labeled examples.",Acknowledgments,"We thank Chris Dyer, Julia Hockenmaier, Jayant Krishnamurthy, Matt Gardner and Oren Etzioni for comments on earlier drafts that led to substantial improvements in the final version.",,,,,,,,,,,,,,,,,,,,,,,how are the bidirectional lms obtained?,c000a43aff3cb0ad1cee5379f9388531b5521e9a,,,,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,"They pre-train forward and backward LMs separately, remove top layer softmax, and concatenate to obtain the bidirectional LMs.","In our final system, after pre-training the forward and backward LMs separately, we remove the top layer softmax and concatenate the forward and backward LM embeddings to form bidirectional LM embeddings, i.e., INLINEFORM0 . Note that in our formulation, the forward and backward LMs are independent, without any shared parameters.","In our final system, after pre-training the forward and backward LMs separately, we remove the top layer softmax and concatenate the forward and backward LM embeddings to form bidirectional LM embeddings, i.e., INLINEFORM0 . ",5113446c5057085c6fda63fc2324d5788a26a8b6,c1018a31c3272ce74964a3280069f62f314a1a58,,,,,,,,,what metrics are used in evaluation?,a5b67470a1c4779877f0d8b7724879bbb0a3b313,,,,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,micro-averaged F1,"We evaluate our approach on two well benchmarked sequence tagging tasks, the CoNLL 2003 NER task BIBREF13 and the CoNLL 2000 Chunking task BIBREF14 . We report the official evaluation metric (micro-averaged INLINEFORM0 ). In both cases, we use the BIOES labeling scheme for the output tags, following previous work which showed it outperforms other options BIBREF15 . Following BIBREF8 , we use the Senna word embeddings BIBREF2 and pre-processed the text by lowercasing all tokens and replacing all digits with 0. FLOAT SELECTED: Table 1: Test set F1 comparison on CoNLL 2003 NER task, using only CoNLL 2003 data and unlabeled text.","We report the official evaluation metric (micro-averaged INLINEFORM0 ).  FLOAT SELECTED: Table 1: Test set F1 comparison on CoNLL 2003 NER task, using only CoNLL 2003 data and unlabeled text.",69ffadad2399c928352e38923ee888693f38721e,c1018a31c3272ce74964a3280069f62f314a1a58,what results do they achieve?,12cfbaace49f9363fcc10989cf92a50dfe0a55ea,,,,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,91.93% F1 score on CoNLL 2003 NER task and 96.37% F1 score on CoNLL 2000 Chunking task,"Our main contribution is to show that the context sensitive representation captured in the LM embeddings is useful in the supervised sequence tagging setting. When we include the LM embeddings in our system overall performance increases from 90.87% to 91.93% INLINEFORM0 for the CoNLL 2003 NER task, a more then 1% absolute F1 increase, and a substantial improvement over the previous state of the art. We also establish a new state of the art result (96.37% INLINEFORM1 ) for the CoNLL 2000 Chunking task.","When we include the LM embeddings in our system overall performance increases from 90.87% to 91.93% INLINEFORM0 for the CoNLL 2003 NER task, a more then 1% absolute F1 increase, and a substantial improvement over the previous state of the art. We also establish a new state of the art result (96.37% INLINEFORM1 ) for the CoNLL 2000 Chunking task.",36c3c00b3d17157779f1a8d368f1102d5fd27a06,c1018a31c3272ce74964a3280069f62f314a1a58,,,,,,,,what previous systems were compared to?,4640793d82aa7db30ad7b88c0bf0a1030e636558,,,,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,"Chiu and Nichols (2016), Lample et al. (2016), Ma and Hovy (2016), Yang et al. (2017), Hashimoto et al. (2016), Søgaard and Goldberg (2016) ","Tables TABREF15 and TABREF16 compare results from TagLM with previously published state of the art results without additional labeled data or task specific gazetteers. Tables TABREF17 and TABREF18 compare results of TagLM to other systems that include additional labeled data or gazetteers. In both tasks, TagLM establishes a new state of the art using bidirectional LMs (the forward CNN-BIG-LSTM and the backward LSTM-2048-512). FLOAT SELECTED: Table 1: Test set F1 comparison on CoNLL 2003 NER task, using only CoNLL 2003 data and unlabeled text. FLOAT SELECTED: Table 2: Test set F1 comparison on CoNLL 2000 Chunking task using only CoNLL 2000 data and unlabeled text. FLOAT SELECTED: Table 3: Improvements in test set F1 in CoNLL 2003 NER when including additional labeled data or task specific gazetteers (except the case of TagLM where we do not use additional labeled resources). FLOAT SELECTED: Table 4: Improvements in test set F1 in CoNLL 2000 Chunking when including additional labeled data (except the case of TagLM where we do not use additional labeled data).","Tables TABREF15 and TABREF16 compare results from TagLM with previously published state of the art results without additional labeled data or task specific gazetteers. Tables TABREF17 and TABREF18 compare results of TagLM to other systems that include additional labeled data or gazetteers.  FLOAT SELECTED: Table 1: Test set F1 comparison on CoNLL 2003 NER task, using only CoNLL 2003 data and unlabeled text. FLOAT SELECTED: Table 2: Test set F1 comparison on CoNLL 2000 Chunking task using only CoNLL 2000 data and unlabeled text. FLOAT SELECTED: Table 3: Improvements in test set F1 in CoNLL 2003 NER when including additional labeled data or task specific gazetteers (except the case of TagLM where we do not use additional labeled resources). FLOAT SELECTED: Table 4: Improvements in test set F1 in CoNLL 2000 Chunking when including additional labeled data (except the case of TagLM where we do not use additional labeled data).",303053006f9ee68f6ac62f319b5b7f36a63a28a1,c1018a31c3272ce74964a3280069f62f314a1a58,,,,,,,,what are the evaluation datasets?,a9c5252173d3df1c06c770c180a77520de68531b,,,,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,CoNLL 2003 CoNLL 2000,,,"We evaluate our approach on two well benchmarked sequence tagging tasks, the CoNLL 2003 NER task BIBREF13 and the CoNLL 2000 Chunking task BIBREF14 . We report the official evaluation metric (micro-averaged INLINEFORM0 ). In both cases, we use the BIOES labeling scheme for the output tags, following previous work which showed it outperforms other options BIBREF15 . Following BIBREF8 , we use the Senna word embeddings BIBREF2 and pre-processed the text by lowercasing all tokens and replacing all digits with 0.","We evaluate our approach on two well benchmarked sequence tagging tasks, the CoNLL 2003 NER task BIBREF13 and the CoNLL 2000 Chunking task BIBREF14 . ",46fcdc5cf0ea7f102ea25b575bc63cc2bf3e894c,c1018a31c3272ce74964a3280069f62f314a1a58,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2-Figure1-1.png,"Figure 1: The main components in TagLM, our language-model-augmented sequence tagging system. The language model component (in orange) is used to augment the input token representation in a traditional sequence tagging models (in grey).",3-Figure2-1.png,"Figure 2: Overview of TagLM, our language model augmented sequence tagging architecture. The top level embeddings from a pre-trained bidirectional LM are inserted in a stacked bidirectional RNN sequence tagging model. See text for details.",5-Table1-1.png,"Table 1: Test set F1 comparison on CoNLL 2003 NER task, using only CoNLL 2003 data and unlabeled text.",5-Table2-1.png,Table 2: Test set F1 comparison on CoNLL 2000 Chunking task using only CoNLL 2000 data and unlabeled text.,6-Table3-1.png,Table 3: Improvements in test set F1 in CoNLL 2003 NER when including additional labeled data or task specific gazetteers (except the case of TagLM where we do not use additional labeled resources).,6-Table4-1.png,Table 4: Improvements in test set F1 in CoNLL 2000 Chunking when including additional labeled data (except the case of TagLM where we do not use additional labeled data).,,,,,,,,,,,6-Table5-1.png,Table 5: Comparison of CoNLL-2003 test set F1 when the LM embeddings are included at different layers in the baseline tagger.,7-Table6-1.png,"Table 6: Comparison of CoNLL-2003 test set F1 for different language model combinations. All language models were trained and evaluated on the 1B Word Benchmark, except LSTM-512-256∗ which was trained and evaluated on the standard splits of the NER CoNLL 2003 dataset.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mixed Membership Word Embeddings for Computational Social Science,"Word embeddings improve the performance of NLP systems by revealing the hidden structural relationships between words. Despite their success in many applications, word embeddings have seen very little use in computational social science NLP tasks, presumably due to their reliance on big data, and to a lack of interpretability. I propose a probabilistic model-based word embedding method which can recover interpretable embeddings, without big data. The key insight is to leverage mixed membership modeling, in which global representations are shared, but individual entities (i.e. dictionary words) are free to use these representations to uniquely differing degrees. I show how to train the model using a combination of state-of-the-art training techniques for word embeddings and topic models. The experimental results show an improvement in predictive language modeling of up to 63% in MRR over the skip-gram, and demonstrate that the representations are beneficial for supervised learning. I illustrate the interpretability of the models with computational social science case studies on State of the Union addresses and NIPS articles.",Introduction,"Word embedding models, which learn to encode dictionary words with vector space representations, have been shown to be valuable for a variety of natural language processing (NLP) tasks such as statistical machine translation BIBREF2 , part-of-speech tagging, chunking, and named entity recogition BIBREF3 , as they provide a more nuanced representation of words than a simple indicator vector into a dictionary. These models follow a long line of research in data-driven semantic representations of text, including latent semantic analysis BIBREF4 and its probabilistic extensions BIBREF5 , BIBREF6 . In particular, topic models BIBREF7 have found broad applications in computational social science BIBREF8 , BIBREF9 and the digital humanities BIBREF10 , where interpretable representations reveal meaningful insights. Despite widespread success at NLP tasks, word embeddings have not yet supplanted topic models as the method of choice in computational social science applications. I speculate that this is due to two primary factors: 1) a perceived reliance on big data, and 2) a lack of interpretability. In this work, I develop new models to address both of these limitations. Word embeddings have risen in popularity for NLP applications due to the success of models designed specifically for the big data setting. In particular, BIBREF0 , BIBREF1 showed that very simple word embedding models with high-dimensional representations can scale up to massive datasets, allowing them to outperform more sophisticated neural network language models which can process fewer documents. In this work, I offer a somewhat contrarian perspective to the currently prevailing trend of big data optimism, as exemplified by the work of BIBREF0 , BIBREF1 , BIBREF3 , and others, who argue that massive datasets are sufficient to allow language models to automatically resolve many challenging NLP tasks. Note that “big” datasets are not always available, particularly in computational social science NLP applications, where the data of interest are often not obtained from large scale sources such as the internet and social media, but from sources such as press releases BIBREF11 , academic journals BIBREF10 , books BIBREF12 , and transcripts of recorded speech BIBREF13 , BIBREF14 , BIBREF15 . A standard practice in the literature is to train word embedding models on a generic large corpus such as Wikipedia, and use the embeddings for NLP tasks on the target dataset, cf. BIBREF3 , BIBREF0 , BIBREF16 , BIBREF17 . However, as we shall see here, this standard practice might not always be effective, as the size of a dataset does not correspond to its degree of relevance for a particular analysis. Even very large corpora have idiosyncrasies that can make their embeddings invalid for other domains. For instance, suppose we would like to use word embeddings to analyze scientific articles on machine learning. In Table TABREF1 , I report the most similar words to the word “learning” based on word embedding models trained on two corpora. For embeddings trained on articles from the NIPS conference, the most similar words are related to machine learning, as desired, while for embeddings trained on the massive, generic Google News corpus, the most similar words relate to learning and teaching in the classroom. Evidently, domain-specific data can be important. Even more concerningly, BIBREF18 show that word embeddings can encode implicit sexist assumptions. This suggests that when trained on large generic corpora they could also encode the hegemonic worldview, which is inappropriate for studying, e.g., black female hip-hop artists' lyrics, or poetry by Syrian refugees, and could potentially lead to systematic bias against minorities, women, and people of color in NLP applications with real-world consequences, such as automatic essay grading and college admissions. In order to proactively combat these kinds of biases in large generic datasets, and to address computational social science tasks, there is a need for effective word embeddings for small datasets, so that the most relevant datasets can be used for training, even when they are small. To make word embeddings a viable alternative to topic models for applications in the social sciences, we further desire that the embeddings are semantically meaningful to human analysts. In this paper, I introduce an interpretable word embedding model, and an associated topic model, which are designed to work well when trained on a small to medium-sized corpus of interest. The primary insight is to use a data-efficient parameter sharing scheme via mixed membership modeling, with inspiration from topic models. Mixed membership models provide a flexible yet efficient latent representation, in which entities are associated with shared, global representations, but to uniquely varying degrees. I identify the skip-gram word2vec model of BIBREF0 , BIBREF1 as corresponding to a certain naive Bayes topic model, which leads to mixed membership extensions, allowing the use of fewer vectors than words. I show that this leads to better modeling performance without big data, as measured by predictive performance (when the context is leveraged for prediction), as well as to interpretable latent representations that are highly valuable for computational social science applications. The interpretability of the representations arises from defining embeddings for words (and hence, documents) in terms of embeddings for topics. My experiments also shed light on the relative merits of training embeddings on generic big data corpora versus domain-specific data.",Background,"In this section, I provide the necessary background on word embeddings, as well as on topic models and mixed membership models. Traditional language models aim to predict words given the contexts that they are found in, thereby forming a joint probabilistic model for sequences of words in a language. BIBREF19 developed improved language models by using distributed representations BIBREF20 , in which words are represented by neural network synapse weights, or equivalently, vector space embeddings. Later authors have noted that these word embeddings are useful for semantic representations of words, independently of whether a full joint probabilistic language model is learned, and that alternative training schemes can be beneficial for learning the embeddings. In particular, BIBREF0 , BIBREF1 proposed the skip-gram model, which inverts the language model prediction task and aims to predict the context given an input word. The skip-gram model is a log-bilinear discriminative probabilistic classifier parameterized by “input” word embedding vectors INLINEFORM0 for the input words INLINEFORM1 , and “output” word embedding vectors INLINEFORM2 for context words INLINEFORM3 , as shown in Table TABREF2 , top-left. Topic models such as latent Dirichlet allocation (LDA) BIBREF7 are another class of probabilistic language models that have been used for semantic representation BIBREF6 . A straightforward way to model text corpora is via unsupervised multinomial naive Bayes, in which a latent cluster assignment for each document selects a multinomial distribution over words, referred to as a topic, with which the documents' words are assumed to be generated. LDA topic models improve over naive Bayes by using a mixed membership model, in which the assumption that all words in a document INLINEFORM0 belong to the same topic is relaxed, and replaced with a distribution over topics INLINEFORM1 . In the model's assumed generative process, for each word INLINEFORM2 in document INLINEFORM3 , a topic assignment INLINEFORM4 is drawn via INLINEFORM5 , then the word is drawn from the chosen topic INLINEFORM6 . The mixed membership formalism provides a useful compromise between model flexibility and statistical efficiency: the INLINEFORM7 topics INLINEFORM8 are shared across all documents, thereby sharing statistical strength, but each document is free to use the topics to its own unique degree. Bayesian inference further aids data efficiency, as uncertainty over INLINEFORM9 can be managed for shorter documents. Some recent papers have aimed to combine topic models and word embeddings BIBREF21 , BIBREF22 , but they do not aim to address the small data problem for computational social science, which I focus on here. I provide a more detailed discussion of related work in the supplementary.",The Mixed Membership Skip-Gram,"To design an interpretable word embedding model for small corpora, we identify novel connections between word embeddings and topic models, and adapt advances from topic modeling. Following the distributional hypothesis BIBREF23 , the skip-gram's word embeddings parameterize discrete probability distributions over words INLINEFORM0 which tend to co-occur, and tend to be semantically coherent – a property leveraged by the Gaussian LDA model of BIBREF21 . This suggests that these discrete distributions can be reinterpreted as topics INLINEFORM1 . We thus reinterpret the skip-gram as a parameterization of a certain supervised naive Bayes topic model (Table TABREF2 , top-right). In this topic model, input words INLINEFORM2 are fully observed “cluster assignments,” and the words in INLINEFORM3 's contexts are a “document.” The skip-gram differs from this supervised topic model only in the parameterization of the “topics” via word vectors which encode the distributions with a log-bilinear model. Note that although the skip-gram is discriminative, in the sense that it does not jointly model the input words INLINEFORM4 , we are here equivalently interpreting it as encoding a “conditionally generative” process for the context given the words, in order to develop probabilistic models that extend the skip-gram. As in LDA, this model can be improved by replacing the naive Bayes assumption with a mixed membership assumption. By applying the mixed membership representation to this topic model version of the skip-gram, we obtain the model in the bottom-right of Table TABREF2 . After once again parameterizing this model with word embeddings, we obtain our final model, the mixed membership skip-gram (MMSG) (Table TABREF2 , bottom-left). In the model, each input word has a distribution over topics INLINEFORM0 . Each topic has a vector-space embedding INLINEFORM1 and each output word has a vector INLINEFORM2 (a parameter, not an embedding for INLINEFORM3 ). A topic INLINEFORM4 is drawn for each context, and the words in the context are drawn from the log-bilinear model using INLINEFORM5 : DISPLAYFORM0   We can expect that the resulting mixed membership word embeddings are beneficial in the small-to-medium data regime for the following reasons: Of course, the model also requires some new parameters to be learned, namely the mixed membership proportions INLINEFORM0 . Based on topic modeling, I hypothesized that with care, these added parameters need not adversely affect performance in the small-medium data regime, for two reasons: 1) we can use a Bayesian approach to effectively manage uncertainty in them, and to marginalize them out, which prevents them being a bottleneck during training; and 2) at test time, using the posterior for INLINEFORM1 given the context, instead of the “prior” INLINEFORM2 , mitigates the impact of uncertainty in INLINEFORM3 due to limited training data: DISPLAYFORM0   To obtain a vector for a word type INLINEFORM0 , we can use the prior mean, INLINEFORM1 . For a word token INLINEFORM2 , we can leverage its context via the posterior mean, INLINEFORM3 . These embeddings are convex combinations of topic vectors (see Figure FIGREF23 for an example). With fewer vectors than words, some model capacity is lost, but the flexibility of the mixed membership representation allows the model to compensate. When the number of shared vectors equals the number of words, the mixed membership skip-gram is strictly more representationally powerful than the skip-gram. With more vectors than words, we can expect that the increased representational power would be beneficial in the big data regime. As this is not my goal, I leave this for future work.",Experimental Results,"The goals of our experiments were to study the relative merits of big data and domain-specific small data, to validate the proposed methods, and to study their applicability for computational social science research.",Quantitative Experiments,"I first measured the effectiveness of the embeddings at the skip-gram's training task, predicting context words INLINEFORM0 given input words INLINEFORM1 . This task measures the methods' performance for predictive language modeling. I used four datasets of sociopolitical, scientific, and literary interest: the corpus of NIPS articles from 1987 – 1999 ( INLINEFORM2 million), the U.S. presidential state of the Union addresses from 1790 – 2015 ( INLINEFORM3 ), the complete works of Shakespeare ( INLINEFORM4 ; this version did not contain the Sonnets), and the writings of black scholar and activist W.E.B. Du Bois, as digitized by Project Gutenberg ( INLINEFORM5 ). For each dataset, I held out 10,000 INLINEFORM6 pairs uniformly at random, where INLINEFORM7 , and aimed to predict INLINEFORM8 given INLINEFORM9 (and optionally, INLINEFORM10 ). Since there are a large number of classes, I treat this as a ranking problem, and report the mean reciprocal rank. The experiments were repeated and averaged over 5 train/test splits. The results are shown in Table TABREF25 . I compared to a word frequency baseline, the skip-gram (SG), and Tomas Mikolov/Google's vectors trained on Google News, INLINEFORM0 billion, via CBOW. Simulated annealing was performed for 1,000 iterations, NCE was performed for 1 million minibatches of size 128, and 128-dimensional embeddings were used (300 for Google). I used INLINEFORM1 for NIPS, INLINEFORM2 for state of the Union, and INLINEFORM3 for the two smaller datasets. Methods were able to leverage the remainder of the context, either by adding the context's vectors, or via the posterior (Equation EQREF22 ), which helped for all methods except the naive skip-gram. We can identify several noteworthy findings. First, the generic big data vectors (Google+context) were outperformed by the skip-gram on 3 out of 4 datasets (and by the skip-gram topic model on the other), by a large margin, indicating that domain-specific embeddings are often important. Second, the mixed membership models, using posterior inference, beat or matched their naive Bayes counterparts, for both the word embedding models and the topic models. As hypothesized, posterior inference on INLINEFORM4 at test time was important for good performance. Finally, the topic models beat their corresponding word embedding models at prediction. I therefore recommend the use of our MMSG topic model variant for predictive language modeling in the small data regime. I tested the performance of the representations as features for document categorization and regression tasks. The results are given in Table TABREF26 . For document categorization, I used three standard benchmark datasets: 20 Newsgroups (19,997 newsgroup posts), Reuters-150 newswire articles (15,500 articles and 150 classes), and Ohsumed medical abstracts on 23 cardiovascular diseases (20,000 articles). I held out 4,000 test documents for 20 Newsgroups, and used the standard train/test splits from the literature in the other corpora (e.g. for Ohsumed, 50% of documents were assigned to training and to test sets). I obtained document embeddings for the MMSG, in the same latent space as the topic embeddings, by summing the posterior mean vectors INLINEFORM0 for each token. Vector addition was similarly used to construct document vectors for the other embedding models. All vectors were normalized to unit length. I also considered a tf-idf baseline. Logistic regression models were trained on the features extracted on the training set for each method. Across the three datasets, several clear trends emerged (Table TABREF26 ). First, the generic Google vectors were consistently and substantially outperformed in classification performance by the skipgram (SG) and MMSG vectors, highlighting the importance of corpus-specific embeddings. Second, despite the MMSG's superior performance at language modeling on small datasets, the SG features outperformed the MMSG's at the document categorization task. By encoding vectors at the topic level instead of the word level, the MMSG loses word level resolution in the embeddings, which turned out to be valuable for these particular classification tasks. We are not, however, restricted to use only one type of embedding to construct features for classification. Interestingly, when the SG and MMSG features were concatenated (SG+MMSG), this improved classification performance over these vectors individually. This suggests that the topic-level MMSG vectors and word-level SG vectors encode complementary information, and both are beneficial for performance. Finally, further concatenating the generic Google vectors' features (SG+MMSG+Google) improved performance again, despite the fact that these vectors performed poorly on their own. It should be noted that tf-idf, which is notoriously effective for document categorization, outperformed the embedding methods on these datasets. I also analyzed the regression task of predicting the year of a state of the Union address based on its text information. I used lasso-regularized linear regression models, evaluated via a leave-one-out cross-validation experimental setup. Root-mean-square error (RMSE) results are reported in Table TABREF26 (bottom). Unlike for the other tasks, the Google big data vectors were the best individual features in this case, outperforming the domain-specific SG and MMSG embeddings individually. On the other hand, SG+MMSG+Google performed the best overall, showing that domain-specific embeddings can improve performance even when big data embeddings are successful. The tf-idf baseline was beaten by all of the embedding models on this task.",Computational Social Science Case Studies: State of the Union and NIPS,"I also performed several case studies. I obtained document embeddings, in the same latent space as the topic embeddings, by summing the posterior mean vectors INLINEFORM0 for each token, and visualized them in two dimensions using INLINEFORM1 -SNE BIBREF24 (all vectors were normalized to unit length). The state of the Union addresses (Figure FIGREF27 ) are embedded almost linearly by year, with a major jump around the New Deal (1930s), and are well separated by party at any given time period. The embedded topics (gray) allow us to interpret the space. The George W. Bush addresses are embedded near a “war on terror” topic (“weapons, war...”), and the Barack Obama addresses are embedded near a “stimulus” topic (“people, work...”). On the NIPS corpus, for the input word “Bayesian” (Table ), the naive Bayes and skip-gram models learned a topic with words that refer to Bayesian networks, probabilistic models, and neural networks. The mixed membership models are able to separate this into more coherent and specific topics including Bayesian modeling, Bayesian training of neural networks (for which Sir David MacKay was a strong proponent, and Andreas Weigend wrote an influential early paper), and Monte Carlo methods. By performing the additive composition of word vectors, which we obtain by finding the prior mean vector for each word type INLINEFORM0 , INLINEFORM1 (and then normalizing), we obtain relevant topics INLINEFORM2 as nearest neighbors (Figure FIGREF28 ). Similarly, we find that the additive composition of topic and word vectors works correctly: INLINEFORM3 , and INLINEFORM4 . The INLINEFORM0 -SNE visualization of NIPS documents (Figure FIGREF28 ) shows some temporal clustering patterns (blue documents are more recent, red documents are older, and gray points are topics). I provide a more detailed case study on NIPS in the supplementary material.",Conclusion,"I have proposed a model-based method for training interpretable corpus-specific word embeddings for computational social science, using mixed membership representations, Metropolis-Hastings-Walker sampling, and NCE. Experimental results for prediction, supervised learning, and case studies on state of the Union addresses and NIPS articles, indicate that high-quality embeddings and topics can be obtained using the method. The results highlight the fact that big data is not always best, as domain-specific data can be very valuable, even when it is small. I plan to use this approach for substantive social science applications, and to address algorithmic bias and fairness issues. Acknowledgements I thank Eric Nalisnick and Padhraic Smyth for many helpful discussions.",Supplementary Material,],Related Work,"In this supplementary document, we discuss related work in the literature and its relation to our proposed methods, provide a case study on NIPS articles, and derive the collapsed Gibbs sampling update for the MMSGTM, which we leverage when training the MMSG.",Topic Modeling and Word Embeddings,"The Gaussian LDA model of BIBREF21 improves the performance of topic modeling by leveraging the semantic information encoded in word embeddings. Gaussian LDA modifies the generative process of LDA such that each topic is assumed to generate the vectors via its own Gaussian distribution. Similarly to our MMSG model, in Gaussian LDA each topic is encoded with a vector, in this case the mean of the Gaussian. It takes pre-trained word embeddings as input, rather than learning the embeddings from data within the same model, and does not aim to perform word embedding. The topical word embedding (TWE) models of BIBREF22 reverse this, as they take LDA topic assignments of words as input, and aim to use them to improve the resultant word embeddings. The authors propose three variants, each of which modifies the skip-gram training objective to use LDA topic assignments together with words. In the best performing variant, called TWE-1, a standard skip-gram word embedding model is trained independently with another skip-gram variant, which tries to predict context words given the input word's topic assignment. The skip-gram embedding and the topic embeddings are concatenated to form the final embedding. At test time, a distribution over topics for the word given the context, INLINEFORM0 is estimated according to the topic counts over the other context words. Using this as a prior, a posterior over topics given both the input word and the context is calculated, and similarities between pairs of words (with their contexts) are averaged over this posterior, in a procedure inspired by those used by BIBREF43 , BIBREF36 . The primary similarity to our MMSG approach is the use of a training algorithm involving the prediction of context words, given a topic. Our method does this as part of an overall model-based inference procedure, and we learn mixed membership proportions INLINEFORM1 rather than using empirical counts as the prior over topics for a word token. In accordance with the skip-gram's prediction model, we are thus able to model the context words in the data likelihood term when computing the posterior probability of the topic assignment. TWE-1 requires that topic assignments are available at test time. It provides a mechanism to predict contextual similarity, but not to predict held-out context words, so we are unable to compare to it in our experiments. Other neurally-inspired topic models include replicated softmax BIBREF34 , and its successor, DocNADE BIBREF37 . Replicated softmax extends the restricted Boltzmann machine to handle multinomial counts for document modeling. DocNADE builds on the ideas of replicated softmax, but uses the NADE architecture, where observations (i.e. words) are modeled sequentially given the previous observations.",Multi-Prototype Embedding Models,"Multi-prototype embeddings models are another relevant line of work. These models address lexical ambiguity by assigning multiple vectors to each word type, each corresponding to a different meaning of that word. BIBREF43 propose to cluster the occurrences of each word type, based on features extracted from its context. Embeddings are then learned for each cluster. BIBREF36 apply a similar approach, but they use initial single-prototype word embeddings to provide the features used for clustering. These clustering methods have some resemblance to our topic model pre-clustering step, although their clustering is applied within instances of a given word type, rather than globally across all word types, as in our methods. This results in models with more vectors than words, while we aim to find fewer vectors than words, to reduce the model's complexity for small datasets. Rather than employing an off-the-shelf clustering algorithm and then applying an unrelated embedding model to its output, our approach aims to perform model-based clustering within an overall joint model of topic/cluster assignments and word vectors. Perhaps the most similar model to ours in the literature is the probabilistic multi-prototype embedding model of BIBREF45 , who treat the prototype assignment of a word as a latent variable, assumed drawn from a mixture over prototypes for each word. The embeddings are then trained using EM. Our MMSG model can be understood as the mixed membership version of this model, in which the prototypes (vectors) are shared across all word types, and each word type has its own mixed membership proportions across the shared prototypes. While a similar EM algorithm can be applied to the MMSG, the E-step is much more expensive, as we typically desire many more shared vectors (often in the thousands) than we would prototypes per a single word type (Tian et al. use ten in their experiments). We use the Metropolis-Hastings-Walker algorithm with the topic model reparameterization of our model in order to address this by efficiently pre-solving the E-step.",Mixed Membership Modeling,"Mixed membership modeling is a flexible alternative to traditional clustering, in which each data point is assigned to a single cluster. Instead, mixed membership models posit that individual entities are associated with multiple underlying clusters, to differing degrees, as encoded by a mixed membership vector that sums to one across the clusters BIBREF28 , BIBREF26 . These mixed membership proportions are generally used to model lower-level grouped data, such as the words inside a document. Each lower-level data point inside a group is assumed to be assigned to one of the shared, global clusters according to the group-level membership proportions. Thus, a mixed membership model consists of a mixture model for each group, which share common mixture component parameters, but with differing mixture proportions. This formalism has lead to probabilistic models for a variety of applications, including medical diagnosis BIBREF39 , population genetics BIBREF42 , survey analysis BIBREF29 , computer vision BIBREF27 , BIBREF30 , text documents BIBREF35 , BIBREF7 , and social network analysis BIBREF25 . Nonparametric Bayesian extensions, in which the number of underlying clusters is learned from data via Bayesian inference, have also been proposed BIBREF44 . In this work, dictionary words are assigned a mixed membership distribution over a set of shared latent vector space embeddings. Each instantiation of a dictionary word (an “input” word) is assigned to one of the shared embeddings based on its dictionary word's membership vector. The words in its context (“output” words) are assumed to be drawn based on the chosen embedding.",Case Study on NIPS,"In Figure FIGREF33 , we show a zoomed in INLINEFORM0 -SNE visualization of NIPS document embeddings. We can see regions of the space corresponding to learning algorithms (bottom), data space and latent space (center), training neural networks (top), and nearest neighbors (bottom-left). We also visualized the authors' embeddings via INLINEFORM1 -SNE (Figure FIGREF34 ). We find regions of latent space for reinforcement learning authors (left: “state, action,...,” Singh, Barto,Sutton), probabilistic methods (right: “mixture, model,” “monte, carlo,” Bishop, Williams, Barber, Opper, Jordan, Ghahramani, Tresp, Smyth), and evaluation (top-right: “results, performance, experiments,...”).",Derivation of the Collapsed Gibbs Update,"Let INLINEFORM0 be the number of output words in the INLINEFORM1 th context, let INLINEFORM2 be those output words, and let INLINEFORM3 be the input words other that INLINEFORM4 (similarly, topic assignments INLINEFORM5 and output words INLINEFORM6 ). Then the collapsed Gibbs update samples from the conditional distribution INLINEFORM7   We recognize the first integral as the mean of a Dirichlet distribution which we obtain via conjugacy: INLINEFORM0   The above can also be understood as the probability of the next ball drawn from a multivariate Polya urn model, also known as the Dirichlet-compound multinomial distribution, arising from the posterior predictive distribution of a discrete likelihood with a Dirichlet prior. We will need the full form of such a distribution to analyze the second integral. Once again leveraging conjugacy, we have: INLINEFORM0   INLINEFORM0   where INLINEFORM0 is the number of times that output word INLINEFORM1 occurs in the INLINEFORM2 th context, since the final integral is over the full support of a Dirichlet distribution, which integrates to one. Eliminating terms that aren't affected by the INLINEFORM3 assignment, the above is INLINEFORM4   where we have used the fact that INLINEFORM0 for any INLINEFORM1 , and integer INLINEFORM2 . We can interpret this as the probability of drawing the context words under the multivariate Polya urn model, in which the number of “colored balls” (word counts plus prior counts) is increased by one each time a certain color (word) is selected. In other words, in each step, corresponding to the selection of each context word, we draw a ball from the urn, then put it back, along with another ball of the same color. The INLINEFORM3 and INLINEFORM4 terms reflect that the counts have been changed by adding these extra balls into the urn in each step. The second to last equation shows that this process is exchangeable: it does not matter which order the balls were drawn in when determining the probability of the sequence. Multiplying this with the term from the first integral, calculated earlier, gives us the final form of the update equation, INLINEFORM5  ",,,,,,,,,,,,,,,What supervised learning tasks are attempted with these representations?,4c71ed7d30ee44cf85ffbd7756b985e32e8e07da,five,unfamiliar,no,,e8b24c3133e0bec0a6465e1f13acd3a5ed816b37,False,,,"In this paper, I introduce an interpretable word embedding model, and an associated topic model, which are designed to work well when trained on a small to medium-sized corpus of interest. The primary insight is to use a data-efficient parameter sharing scheme via mixed membership modeling, with inspiration from topic models. Mixed membership models provide a flexible yet efficient latent representation, in which entities are associated with shared, global representations, but to uniquely varying degrees. I identify the skip-gram word2vec model of BIBREF0 , BIBREF1 as corresponding to a certain naive Bayes topic model, which leads to mixed membership extensions, allowing the use of fewer vectors than words. I show that this leads to better modeling performance without big data, as measured by predictive performance (when the context is leveraged for prediction), as well as to interpretable latent representations that are highly valuable for computational social science applications. The interpretability of the representations arises from defining embeddings for words (and hence, documents) in terms of embeddings for topics. My experiments also shed light on the relative merits of training embeddings on generic big data corpora versus domain-specific data. I tested the performance of the representations as features for document categorization and regression tasks. The results are given in Table TABREF26 . For document categorization, I used three standard benchmark datasets: 20 Newsgroups (19,997 newsgroup posts), Reuters-150 newswire articles (15,500 articles and 150 classes), and Ohsumed medical abstracts on 23 cardiovascular diseases (20,000 articles). I held out 4,000 test documents for 20 Newsgroups, and used the standard train/test splits from the literature in the other corpora (e.g. for Ohsumed, 50% of documents were assigned to training and to test sets). I obtained document embeddings for the MMSG, in the same latent space as the topic embeddings, by summing the posterior mean vectors INLINEFORM0 for each token. Vector addition was similarly used to construct document vectors for the other embedding models. All vectors were normalized to unit length. I also considered a tf-idf baseline. Logistic regression models were trained on the features extracted on the training set for each method. I also analyzed the regression task of predicting the year of a state of the Union address based on its text information. I used lasso-regularized linear regression models, evaluated via a leave-one-out cross-validation experimental setup. Root-mean-square error (RMSE) results are reported in Table TABREF26 (bottom). Unlike for the other tasks, the Google big data vectors were the best individual features in this case, outperforming the domain-specific SG and MMSG embeddings individually. On the other hand, SG+MMSG+Google performed the best overall, showing that domain-specific embeddings can improve performance even when big data embeddings are successful. The tf-idf baseline was beaten by all of the embedding models on this task.","In this paper, I introduce an interpretable word embedding model, and an associated topic model, which are designed to work well when trained on a small to medium-sized corpus of interest. I tested the performance of the representations as features for document categorization and regression tasks. The results are given in Table TABREF26 . For document categorization, I used three standard benchmark datasets: 20 Newsgroups (19,997 newsgroup posts), Reuters-150 newswire articles (15,500 articles and 150 classes), and Ohsumed medical abstracts on 23 cardiovascular diseases (20,000 articles). I I also analyzed the regression task of predicting the year of a state of the Union address based on its text information. ",638a79523ddc482d96be422ad091c20c92ccf7d9,5053f146237e8fc8859ed3984b5d3f02f39266b7,,,,,,,,,What is MRR?,1949d84653562fa9e83413796ae55980ab7318f2,five,unfamiliar,no,,e8b24c3133e0bec0a6465e1f13acd3a5ed816b37,False,,,"I first measured the effectiveness of the embeddings at the skip-gram's training task, predicting context words INLINEFORM0 given input words INLINEFORM1 . This task measures the methods' performance for predictive language modeling. I used four datasets of sociopolitical, scientific, and literary interest: the corpus of NIPS articles from 1987 – 1999 ( INLINEFORM2 million), the U.S. presidential state of the Union addresses from 1790 – 2015 ( INLINEFORM3 ), the complete works of Shakespeare ( INLINEFORM4 ; this version did not contain the Sonnets), and the writings of black scholar and activist W.E.B. Du Bois, as digitized by Project Gutenberg ( INLINEFORM5 ). For each dataset, I held out 10,000 INLINEFORM6 pairs uniformly at random, where INLINEFORM7 , and aimed to predict INLINEFORM8 given INLINEFORM9 (and optionally, INLINEFORM10 ). Since there are a large number of classes, I treat this as a ranking problem, and report the mean reciprocal rank. The experiments were repeated and averaged over 5 train/test splits.","I first measured the effectiveness of the embeddings at the skip-gram's training task, predicting context words INLINEFORM0 given input words INLINEFORM1 . This task measures the methods' performance for predictive language modeling. I used four datasets of sociopolitical, scientific, and literary interest: the corpus of NIPS articles from 1987 – 1999 ( INLINEFORM2 million), the U.S. presidential state of the Union addresses from 1790 – 2015 ( INLINEFORM3 ), the complete works of Shakespeare ( INLINEFORM4 ; this version did not contain the Sonnets), and the writings of black scholar and activist W.E.B. Du Bois, as digitized by Project Gutenberg ( INLINEFORM5 ). For each dataset, I held out 10,000 INLINEFORM6 pairs uniformly at random, where INLINEFORM7 , and aimed to predict INLINEFORM8 given INLINEFORM9 (and optionally, INLINEFORM10 ). Since there are a large number of classes, I treat this as a ranking problem, and report the mean reciprocal rank. The experiments were repeated and averaged over 5 train/test splits.",3d2d234552afa12b52e6c2caef65925286d400cc,5053f146237e8fc8859ed3984b5d3f02f39266b7,Which techniques for word embeddings and topic models are used?,7ee660927e2b202376849e489faa7341518adaf9,five,unfamiliar,no,,e8b24c3133e0bec0a6465e1f13acd3a5ed816b37,False,,,"To design an interpretable word embedding model for small corpora, we identify novel connections between word embeddings and topic models, and adapt advances from topic modeling. Following the distributional hypothesis BIBREF23 , the skip-gram's word embeddings parameterize discrete probability distributions over words INLINEFORM0 which tend to co-occur, and tend to be semantically coherent – a property leveraged by the Gaussian LDA model of BIBREF21 . This suggests that these discrete distributions can be reinterpreted as topics INLINEFORM1 . We thus reinterpret the skip-gram as a parameterization of a certain supervised naive Bayes topic model (Table TABREF2 , top-right). In this topic model, input words INLINEFORM2 are fully observed “cluster assignments,” and the words in INLINEFORM3 's contexts are a “document.” The skip-gram differs from this supervised topic model only in the parameterization of the “topics” via word vectors which encode the distributions with a log-bilinear model. Note that although the skip-gram is discriminative, in the sense that it does not jointly model the input words INLINEFORM4 , we are here equivalently interpreting it as encoding a “conditionally generative” process for the context given the words, in order to develop probabilistic models that extend the skip-gram. As in LDA, this model can be improved by replacing the naive Bayes assumption with a mixed membership assumption. By applying the mixed membership representation to this topic model version of the skip-gram, we obtain the model in the bottom-right of Table TABREF2 . After once again parameterizing this model with word embeddings, we obtain our final model, the mixed membership skip-gram (MMSG) (Table TABREF2 , bottom-left). In the model, each input word has a distribution over topics INLINEFORM0 . Each topic has a vector-space embedding INLINEFORM1 and each output word has a vector INLINEFORM2 (a parameter, not an embedding for INLINEFORM3 ). A topic INLINEFORM4 is drawn for each context, and the words in the context are drawn from the log-bilinear model using INLINEFORM5 : DISPLAYFORM0","To design an interpretable word embedding model for small corpora, we identify novel connections between word embeddings and topic models, and adapt advances from topic modeling. Following the distributional hypothesis BIBREF23 , the skip-gram's word embeddings parameterize discrete probability distributions over words INLINEFORM0 which tend to co-occur, and tend to be semantically coherent – a property leveraged by the Gaussian LDA model of BIBREF21  We thus reinterpret the skip-gram as a parameterization of a certain supervised naive Bayes topic model (Table TABREF2 , top-right). As in LDA, this model can be improved by replacing the naive Bayes assumption with a mixed membership assumption. By applying the mixed membership representation to this topic model version of the skip-gram, we obtain the model in the bottom-right of Table TABREF2 . After once again parameterizing this model with word embeddings, we obtain our final model, the mixed membership skip-gram (MMSG) ",acb298246a4b3036c79e8e6e9618762cc82683de,5053f146237e8fc8859ed3984b5d3f02f39266b7,,,,,,,,Why is big data not appropriate for this task?,f6380c60e2eb32cb3a9d3bca17cf4dc5ae584eca,five,unfamiliar,no,,e8b24c3133e0bec0a6465e1f13acd3a5ed816b37,False,,Training embeddings from small-corpora can increase the performance of some tasks,"Word embeddings have risen in popularity for NLP applications due to the success of models designed specifically for the big data setting. In particular, BIBREF0 , BIBREF1 showed that very simple word embedding models with high-dimensional representations can scale up to massive datasets, allowing them to outperform more sophisticated neural network language models which can process fewer documents. In this work, I offer a somewhat contrarian perspective to the currently prevailing trend of big data optimism, as exemplified by the work of BIBREF0 , BIBREF1 , BIBREF3 , and others, who argue that massive datasets are sufficient to allow language models to automatically resolve many challenging NLP tasks. Note that “big” datasets are not always available, particularly in computational social science NLP applications, where the data of interest are often not obtained from large scale sources such as the internet and social media, but from sources such as press releases BIBREF11 , academic journals BIBREF10 , books BIBREF12 , and transcripts of recorded speech BIBREF13 , BIBREF14 , BIBREF15 . A standard practice in the literature is to train word embedding models on a generic large corpus such as Wikipedia, and use the embeddings for NLP tasks on the target dataset, cf. BIBREF3 , BIBREF0 , BIBREF16 , BIBREF17 . However, as we shall see here, this standard practice might not always be effective, as the size of a dataset does not correspond to its degree of relevance for a particular analysis. Even very large corpora have idiosyncrasies that can make their embeddings invalid for other domains. For instance, suppose we would like to use word embeddings to analyze scientific articles on machine learning. In Table TABREF1 , I report the most similar words to the word “learning” based on word embedding models trained on two corpora. For embeddings trained on articles from the NIPS conference, the most similar words are related to machine learning, as desired, while for embeddings trained on the massive, generic Google News corpus, the most similar words relate to learning and teaching in the classroom. Evidently, domain-specific data can be important. I have proposed a model-based method for training interpretable corpus-specific word embeddings for computational social science, using mixed membership representations, Metropolis-Hastings-Walker sampling, and NCE. Experimental results for prediction, supervised learning, and case studies on state of the Union addresses and NIPS articles, indicate that high-quality embeddings and topics can be obtained using the method. The results highlight the fact that big data is not always best, as domain-specific data can be very valuable, even when it is small. I plan to use this approach for substantive social science applications, and to address algorithmic bias and fairness issues.","Word embeddings have risen in popularity for NLP applications due to the success of models designed specifically for the big data setting.  Note that “big” datasets are not always available, particularly in computational social science NLP applications, where the data of interest are often not obtained from large scale sources such as the internet and social media, but from sources such as press releases BIBREF11 , academic journals BIBREF10 , books BIBREF12 , and transcripts of recorded speech BIBREF13 , BIBREF14 , BIBREF15  have proposed a model-based method for training interpretable corpus-specific word embeddings for computational social science, using mixed membership representations, Metropolis-Hastings-Walker sampling, and NCE. Experimental results for prediction, supervised learning, and case studies on state of the Union addresses and NIPS articles, indicate that high-quality embeddings and topics can be obtained using the method. The results highlight the fact that big data is not always best, as domain-specific data can be very valuable, even when it is small.",321ae98d153c58df5e886904482062fd2717be2f,5053f146237e8fc8859ed3984b5d3f02f39266b7,,,,,,,,What is an example of a computational social science NLP task?,c7d99e66c4ab555fe3d616b15a5048f3fe1f3f0e,five,unfamiliar,no,,e8b24c3133e0bec0a6465e1f13acd3a5ed816b37,False,,,Visualization of State of the union addresses,"I also performed several case studies. I obtained document embeddings, in the same latent space as the topic embeddings, by summing the posterior mean vectors INLINEFORM0 for each token, and visualized them in two dimensions using INLINEFORM1 -SNE BIBREF24 (all vectors were normalized to unit length). The state of the Union addresses (Figure FIGREF27 ) are embedded almost linearly by year, with a major jump around the New Deal (1930s), and are well separated by party at any given time period. The embedded topics (gray) allow us to interpret the space. The George W. Bush addresses are embedded near a “war on terror” topic (“weapons, war...”), and the Barack Obama addresses are embedded near a “stimulus” topic (“people, work...”).","I also performed several case studies. I obtained document embeddings, in the same latent space as the topic embeddings, by summing the posterior mean vectors INLINEFORM0 for each token, and visualized them in two dimensions using INLINEFORM1 -SNE BIBREF24 (all vectors were normalized to unit length). The state of the Union addresses (Figure FIGREF27 ) are embedded almost linearly by year, with a major jump around the New Deal (1930s), and are well separated by party at any given time period. ",28f72bd8e3448e8b49513cae46bd426476b54693,5053f146237e8fc8859ed3984b5d3f02f39266b7,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2-Table1-1.png,"Table 1: Most similar words to “learning,” based on word embeddings trained on NIPS articles, and on the large generic Google News corpus (Mikolov et al., 2013a,b).",3-Table2-1.png,"Table 2: “Generative” models. Identifying the skip-gram (top-left)’s word distributions with topics yields analogous topic models (right), and mixed membership modeling extensions (bottom).",4-Figure1-1.png,"Figure 1: Mixed membership word embeddings v̄w for word type w (prior) and v̂wi for word token wi (posterior), are convex combinations of topic embeddings vk.",5-Table3-1.png,"Table 3: SG = skip-gram, TM = topic model, MM = mixed membership.",7-Table4-1.png,"Table 4: Mean reciprocal rank of held-out context words. SG = skip-gram, TM = topic model, MM = mixed membership. Bold indicates statistically significant improvement versus SG.",7-Table5-1.png,"Table 5: Document categorization (top, classification accuracy, larger is better), and predicting the year of State of the Union addresses (bottom, RMSE, LOO cross-validation, smaller is better).",,,,,,,,,mean reciprocal rank, skip-gram LDA,8-Figure2-1.png,"Figure 2: State of the Union (SOTU) addresses. Colored circles are t-SNE projected embeddings for SOTU addresses. Color = party (red = GOP, blue = Democrats, light green = Whigs, pink = Democratic-Republicans, orange = Federalists (John Adams), green = George Washington), size = recency (year, see dates in green). Gray circles correspond to topics.",8-Figure3-1.png,"Figure 3: Left: Vector compositionality examples, NIPS. Right: NIPS documents/ topics, t-SNE.",12-Figure4-1.png,"Figure 4: NIPS documents/topics, t-SNE, zoomed in. Blue/red = more recent/older, gray = topics.",,,,,,document categorization regression tasks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Efficient Vector Representation for Documents through Corruption,"We present an efficient document representation learning framework, Document Vector through Corruption (Doc2VecC). Doc2VecC represents each document as a simple average of word embeddings. It ensures a representation generated as such captures the semantic meanings of the document during learning. A corruption model is included, which introduces a data-dependent regularization that favors informative or rare words while forcing the embeddings of common and non-discriminative ones to be close to zero. Doc2VecC produces significantly better word embeddings than Word2Vec. We compare Doc2VecC with several state-of-the-art document representation learning algorithms. The simple model architecture introduced by Doc2VecC matches or out-performs the state-of-the-art in generating high-quality document representations for sentiment analysis, document classification as well as semantic relatedness tasks. The simplicity of the model enables training on billions of words per hour on a single machine. At the same time, the model is very efficient in generating representations of unseen documents at test time.",Introduction,"Text understanding starts with the challenge of finding machine-understandable representation that captures the semantics of texts. Bag-of-words (BoW) and its N-gram extensions are arguably the most commonly used document representations. Despite its simplicity, BoW works surprisingly well for many tasks BIBREF0 . However, by treating words and phrases as unique and discrete symbols, BoW often fails to capture the similarity between words or phrases and also suffers from sparsity and high dimensionality. Recent works on using neural networks to learn distributed vector representations of words have gained great popularity. The well celebrated Word2Vec BIBREF1 , by learning to predict the target word using its neighboring words, maps words of similar meanings to nearby points in the continuous vector space. The surprisingly simple model has succeeded in generating high-quality word embeddings for tasks such as language modeling, text understanding and machine translation. Word2Vec naturally scales to large datasets thanks to its simple model architecture. It can be trained on billions of words per hour on a single machine. Paragraph Vectors BIBREF2 generalize the idea to learn vector representation for documents. A target word is predicted by the word embeddings of its neighbors in together with a unique document vector learned for each document. It outperforms established document representations, such as BoW and Latent Dirichlet Allocation BIBREF3 , on various text understanding tasks BIBREF4 . However, two caveats come with this approach: 1) the number of parameters grows with the size of the training corpus, which can easily go to billions; and 2) it is expensive to generate vector representations for unseen documents at test time. We propose an efficient model architecture, referred to as Document Vector through Corruption (Doc2VecC), to learn vector representations for documents. It is motivated by the observation that linear operations on the word embeddings learned by Word2Vec can sustain substantial amount of syntactic and semantic meanings of a phrase or a sentence BIBREF5 . For example, vec(“Russia”) + vec(“river”) is close to vec(“Volga River”) BIBREF6 , and vec(“king”) - vec(“man”) + vec(“women”) is close to vec(“queen”) BIBREF5 . In Doc2VecC, we represent each document as a simple average of the word embeddings of all the words in the document. In contrast to existing approaches which post-process learned word embeddings to form document representation BIBREF7 , BIBREF8 , Doc2VecC enforces a meaningful document representation can be formed by averaging the word embeddings during learning. Furthermore, we include a corruption model that randomly remove words from a document during learning, a mechanism that is critical to the performance and learning speed of our algorithm. Doc2VecC has several desirable properties: 1. The model complexity of Doc2VecC is decoupled from the size of the training corpus, depending only on the size of the vocabulary; 2. The model architecture of Doc2VecC resembles that of Word2Vec, and can be trained very efficiently; 3. The new framework implicitly introduces a data-dependent regularization, which favors rare or informative words and suppresses words that are common but not discriminative; 4. Vector representation of a document can be generated by simply averaging the learned word embeddings of all the words in the document, which significantly boost test efficiency; 5. The vector representation generated by Doc2VecC matches or beats the state-of-the-art for sentiment analysis, document classification as well as semantic relatedness tasks.",Related Works and Notations,"Text representation learning has been extensively studied. Popular representations range from the simplest BoW and its term-frequency based variants BIBREF9 , language model based methods BIBREF10 , BIBREF11 , BIBREF12 , topic models BIBREF13 , BIBREF3 , Denoising Autoencoders and its variants BIBREF14 , BIBREF15 , and distributed vector representations BIBREF8 , BIBREF2 , BIBREF16 . Another prominent line of work includes learning task-specific document representation with deep neural networks, such as CNN BIBREF17 or LSTM based approaches BIBREF18 , BIBREF19 . In this section, we briefly introduce Word2Vec and Paragraph Vectors, the two approaches that are most similar to ours. There are two well-know model architectures used for both methods, referred to as Continuous Bag-of-Words (CBoW) and Skipgram models BIBREF1 . In this work, we focus on CBoW. Extending to Skipgram is straightforward. Here are the notations we are going to use throughout the paper:",Method,"Several works BIBREF6 , BIBREF5 showcased that syntactic and semantic regularities of phrases and sentences are reasonably well preserved by adding or subtracting word embeddings learned through Word2Vec. It prompts us to explore the option of simply representing a document as an average of word embeddings. Figure FIGREF9 illustrates the new model architecture. Similar to Word2Vec or Paragraph Vectors, Doc2VecC consists of an input layer, a projection layer as well as an output layer to predict the target word, “ceremony” in this example. The embeddings of neighboring words (“opening”, “for”, “the”) provide local context while the vector representation of the entire document (shown in grey) serves as the global context. In contrast to Paragraph Vectors, which directly learns a unique vector for each document, Doc2VecC represents each document as an average of the embeddings of words randomly sampled from the document (“performance” at position INLINEFORM0 , “praised” at position INLINEFORM1 , and “brazil” at position INLINEFORM2 ). BIBREF25 also proposed the idea of using average of word embeddings to represent the global context of a document. Different from their work, we choose to corrupt the original document by randomly removing significant portion of words, and represent the document using only the embeddings of the words remained. This corruption mechanism offers us great speedup during training as it significantly reduces the number of parameters to update in back propagation. At the same time, as we are going to detail in the next section, it introduces a special form of regularization, which brings great performance improvement. Here we describe the stochastic process we used to generate a global context at each update. The global context, which we denote as INLINEFORM0 , is generated through a unbiased mask-out/drop-out corruption, in which we randomly overwrites each dimension of the original document INLINEFORM1 with probability INLINEFORM2 . To make the corruption unbiased, we set the uncorrupted dimensions to INLINEFORM3 times its original value. Formally, DISPLAYFORM0  Doc2VecC then defines the probability of observing a target word INLINEFORM0 given its local context INLINEFORM1 as well as the global context INLINEFORM2 as DISPLAYFORM0  Here INLINEFORM0 is the length of the document. Exactly computing the probability is impractical, instead we approximate it with negative sampling BIBREF1 . DISPLAYFORM0  here INLINEFORM0 stands for a uniform distribution over the terms in the vocabulary. The two projection matrices INLINEFORM1 and INLINEFORM2 are then learned to minimize the loss: DISPLAYFORM0  Given the learned projection matrix INLINEFORM0 , we then represent each document simply as an average of the embeddings of the words in the document, DISPLAYFORM0  We are going to elaborate next why we choose to corrupt the original document with the corruption model in eq.( EQREF10 ) during learning, and how it enables us to simply use the average word embeddings as the vector representation for documents at test time.",Corruption as data-dependent regularization,"We approximate the log likelihood for each instance INLINEFORM0 in eq.( EQREF13 ) with its Taylor expansion with respect to INLINEFORM1 up to the second-order BIBREF26 , BIBREF27 , BIBREF28 . Concretely, we choose to expand at the mean of the corruption INLINEFORM2 : INLINEFORM3  where INLINEFORM0 and INLINEFORM1 are the first-order (i.e., gradient) and second-order (i.e., Hessian) of the log likelihood with respect to INLINEFORM2 . Expansion at the mean INLINEFORM3 is crucial as shown in the following steps. Let us assume that for each instance, we are going to sample the global context INLINEFORM4 infinitely many times, and thus compute the expected log likelihood with respect to the corrupted INLINEFORM5 . INLINEFORM6  The linear term disappears as INLINEFORM0 . We substitute in INLINEFORM1 for the mean INLINEFORM2 of the corrupting distribution (unbiased corruption) and the matrix INLINEFORM3 for the variance, and obtain DISPLAYFORM0  As each word in a document is corrupted independently of others, the variance matrix INLINEFORM0 is simplified to a diagonal matrix with INLINEFORM1 element equals INLINEFORM2 . As a result, we only need to compute the diagonal terms of the Hessian matrix INLINEFORM3 . The INLINEFORM0 dimension of the Hessian's diagonal evaluated at the mean INLINEFORM1 is given by INLINEFORM2  Plug the Hessian matrix and the variance matrix back into eq.( EQREF16 ), and then back to the loss defined in eq.( EQREF13 ), we can see that Doc2VecC intrinsically minimizes DISPLAYFORM0  Each INLINEFORM0 in the first term measures the log likelihood of observing the target word INLINEFORM1 given its local context INLINEFORM2 and the document vector INLINEFORM3 . As such, Doc2VecC enforces that a document vector generated by averaging word embeddings can capture the global semantics of the document, and fill in information missed in the local context. The second term here is a data-dependent regularization. The regularization on the embedding INLINEFORM4 of each word INLINEFORM5 takes the following form, INLINEFORM6  where INLINEFORM0 prescribes the confidence of predicting the target word INLINEFORM1 given its neighboring context INLINEFORM2 as well as the document vector INLINEFORM3 . Closely examining INLINEFORM0 leads to several interesting findings: 1. the regularizer penalizes more on the embeddings of common words. A word INLINEFORM1 that frequently appears across the training corpus, i.e, INLINEFORM2 often, will have a bigger regularization than a rare word; 2. on the other hand, the regularization is modulated by INLINEFORM3 , which is small if INLINEFORM4 . In other words, if INLINEFORM5 is critical to a confident prediction INLINEFORM6 when it is active, then the regularization is diminished. Similar effect was observed for dropout training for logistic regression model BIBREF27 and denoising autoencoders BIBREF28 .",Experiments,"We evaluate Doc2VecC on a sentiment analysis task, a document classification task and a semantic relatedness task, along with several document representation learning algorithms. All experiments can be reproduced using the code available at https://github.com/mchen24/iclr2017",Baselines,"We compare against the following document representation baselines: bag-of-words (BoW); Denoising Autoencoders (DEA) BIBREF14 , a representation learned from reconstructing original document INLINEFORM0 using corrupted one INLINEFORM1 . SDAs have been shown to be the state-of-the-art for sentiment analysis tasks BIBREF29 . We used Kullback-Liebler divergence as the reconstruction error and an affine encoder. To scale up the algorithm to large vocabulary, we only take into account the non-zero elements of INLINEFORM2 in the reconstruction error and employed negative sampling for the remainings; Word2Vec BIBREF1 +IDF, a representation generated through weighted average of word vectors learned using Word2Vec; Doc2Vec BIBREF2 ; Skip-thought Vectors BIBREF16 , a generic, distributed sentence encoder that extends the Word2Vec skip-gram model to sentence level. It has been shown to produce highly generic sentence representations that apply to various natural language processing tasks. We also include RNNLM BIBREF11 , a recurrent neural network based language model in the comparison. In the semantic relatedness task, we further compare to LSTM-based methods BIBREF18 that have been reported on this dataset.",Sentiment analysis,"For sentiment analysis, we use the IMDB movie review dataset. It contains 100,000 movies reviews categorized as either positive or negative. It comes with predefined train/test split BIBREF30 : 25,000 reviews are used for training, 25,000 for testing, and the rest as unlabeled data. The two classes are balanced in the training and testing sets. We remove words that appear less than 10 times in the training set, resulting in a vocabulary of 43,375 distinct words and symbols. Setup. We test the various representation learning algorithms under two settings: one follows the same protocol proposed in BIBREF8 , where representation is learned using all the available data, including the test set; another one where the representation is learned using training and unlabeled set only. For both settings, a linear support vector machine (SVM) BIBREF31 is trained afterwards on the learned representation for classification. For Skip-thought Vectors, we used the generic model trained on a much bigger book corpus to encode the documents. A vector of 4800 dimensions, first 2400 from the uni-skip model, and the last 2400 from the bi-skip model, are generated for each document. In comparison, all the other algorithms produce a vector representation of size 100. The supervised RNN-LM is learned on the training set only. The hyper-parameters are tuned on a validation set subsampled from the training set. Accuracy. Comparing the two columns in Table TABREF20 , we can see that all the representation learning algorithms benefits from including the testing data during the representation learning phrase. Doc2VecC achieved similar or even better performance than Paragraph Vectors. Both methods outperforms the other baselines, beating the BOW representation by 15%. In comparison with Word2Vec+IDF, which applies post-processing on learned word embeddings to form document representation, Doc2VecC naturally enforces document semantics to be captured by averaged word embeddings during training. This leads to better performance. Doc2VecC reduces to Denoising Autoencoders (DEA) if the local context words are removed from the paradigm shown in Figure FIGREF9 . By including the context words, Doc2VecC allows the document vector to focus more on capturing the global context. Skip-thought vectors perform surprisingly poor on this dataset comparing to other methods. We hypothesized that it is due to the length of paragraphs in this dataset. The average length of paragraphs in the IMDB movie review dataset is INLINEFORM0 , much longer than the ones used for training and testing in the original paper, which is in the order of 10. As noted in BIBREF18 , the performance of LSTM based method (similarly, the gated RNN used in Skip-thought vectors) drops significantly with increasing paragraph length, as it is hard to preserve state over long sequences of words. Time. Table TABREF22 summarizes the time required by these algorithms to learn and generate the document representation. Word2Vec is the fastest one to train. Denoising Autoencoders and Doc2VecC second that. The number of parameters that needs to be back-propagated in each update was increased by the number of surviving words in INLINEFORM0 . We found that both models are not sensitive to the corruption rate INLINEFORM1 in the noise model. Since the learning time decreases with higher corruption rate, we used INLINEFORM2 throughout the experiments. Paragraph Vectors takes longer time to train as there are more parameters (linear to the number of document in the learning set) to learn. At test time, Word2Vec+IDF, DEA and Doc2VecC all use (weighted) averaging of word embeddings as document representation. Paragraph Vectors, on the other hand, requires another round of inference to produce the vector representation of unseen test documents. It takes Paragraph Vectors 4 minutes and 17 seconds to infer the vector representations for the 25,000 test documents, in comparison to 7 seconds for the other methods. As we did not re-train the Skip-thought vector models on this dataset, the training time reported in the table is the time it takes to generate the embeddings for the 25,000 training documents. Due to repeated high-dimensional matrix operations required for encoding long paragraphs, it takes fairly long time to generate the representations for these documents. Similarly for testing. The experiments were conducted on a desktop with Intel i7 2.2Ghz cpu. Data dependent regularization. As explained in Section SECREF15 , the corruption introduced in Doc2VecC acts as a data-dependent regularization that suppresses the embeddings of frequent but uninformative words. Here we conduct an experiment to exam the effect. We used a cutoff of 100 in this experiment. Table TABREF24 lists the words having the smallest INLINEFORM0 norm of embeddings found by different algorithms. The number inside the parenthesis after each word is the number of times this word appears in the learning set. In word2Vec or Paragraph Vectors, the least frequent words have embeddings that are close to zero, despite some of them being indicative of sentiment such as debacle, bliss and shabby. In contrast, Doc2VecC manages to clamp down the representation of words frequently appear in the training set, but are uninformative, such as symbols and stop words. Subsampling frequent words. Note that for all the numbers reported, we applied the trick of subsampling of frequent words introduced in BIBREF6 to counter the imbalance between frequent and rare words. It is critical to the performance of simple Word2Vec+AVG as the sole remedy to diminish the contribution of common words in the final document representation. If we were to remove this step, the error rate of Word2Vec+AVG will increases from INLINEFORM0 to INLINEFORM1 . Doc2VecC on the other hand naturally exerts a stronger regularization toward embeddings of words that are frequent but uninformative, therefore does not rely on this trick.",Word analogy,"In table TABREF24 , we demonstrated that the corruption model introduced in Doc2VecC dampens the embeddings of words which are common and non-discriminative (stop words). In this experiment, we are going to quantatively compare the word embeddings generated by Doc2VecC to the ones generated by Word2Vec, or Paragraph Vectors on the word analogy task introduced by BIBREF1 . The dataset contains five types of semantic questions, and nine types of syntactic questions, with a total of 8,869 semantic and 10,675 syntactic questions. The questions are answered through simple linear algebraic operations on the word embeddings generated by different methods. Please refer to the original paper for more details on the evaluation protocol. We trained the word embeddings of different methods using the English news dataset released under the ACL workshop on statistical machine translation. The training set includes close to 15M paragraphs with 355M tokens. We compare the performance of word embeddings trained by different methods with increasing embedding dimensionality as well as increasing training data. We observe similar trends as in BIBREF1 . Increasing embedding dimensionality as well as training data size improves performance of the word embeddings on this task. However, the improvement is diminishing. Doc2VecC produces word embeddings which performs significantly better than the ones generated by Word2Vec. We observe close to INLINEFORM0 uplift when we train on the full training corpus. Paragraph vectors on the other hand performs surprisingly bad on this dataset. Our hypothesis is that due to the large capacity of the model architecture, Paragraph Vectors relies mostly on the unique document vectors to capture the information in a text document instead of learning the word semantic or syntactic similarities. This also explains why the PV-DBOW BIBREF2 model architecture proposed in the original work, which completely removes word embedding layers, performs comparable to the distributed memory version. In table 5, we list a detailed comparison of the performance of word embeddings generated by Word2Vec and Doc2VecC on the 14 subtasks, when trained on the full dataset with embedding of size 100. We can see that Doc2VecC significantly outperforms the word embeddings produced by Word2Vec across almost all the subtasks.",Document Classification,"For the document classification task, we use a subset of the wikipedia dump, which contains over 300,000 wikipedia pages in 100 categories. The 100 categories includes categories under sports, entertainment, literature, and politics etc. Examples of categories include American drama films, Directorial debut films, Major League Baseball pitchers and Sydney Swans players. Body texts (the second paragraph) were extracted for each page as a document. For each category, we select 1,000 documents with unique category label, and 100 documents were used for training and 900 documents for testing. The remaining documents are used as unlabeled data. The 100 classes are balanced in the training and testing sets. For this data set, we learn the word embedding and document representation for all the algorithms using all the available data. We apply a cutoff of 10, resulting in a vocabulary of size INLINEFORM0 . Table TABREF29 summarizes the classification error of a linear SVM trained on representations of different sizes. We can see that most of the algorithms are not sensitive to the size of the vector representation. Doc2Vec benefits most from increasing representation size. Across all sizes of representations, Doc2VecC outperform the existing algorithms by a significant margin. In fact, Doc2VecC can achieve same or better performance with a much smaller representation vector. Figure FIGREF30 visualizes the document representations learned by Doc2Vec (left) and Doc2VecC (right) using t-SNE BIBREF32 . We can see that documents from the same category are nicely clustered using the representation generated by Doc2VecC. Doc2Vec, on the other hand, does not produce a clear separation between different categories, which explains its worse performance reported in Table TABREF29 . Figure FIGREF31 visualizes the vector representation generated by Doc2VecC w.r.t. coarser categorization. we manually grouped the 100 categories into 7 coarse categories, television, albums, writers, musicians, athletes, species and actors. Categories that do no belong to any of these 7 groups are not included in the figure. We can see that documents belonging to a coarser category are grouped together. This subset includes is a wide range of sports descriptions, ranging from football, crickets, baseball, and cycling etc., which explains why the athletes category are less concentrated. In the projection, we can see documents belonging to the musician category are closer to those belonging to albums category than those of athletes or species.",Semantic relatedness,"We test Doc2VecC on the SemEval 2014 Task 1: semantic relatedness SICK dataset BIBREF33 . Given two sentences, the task is to determine how closely they are semantically related. The set contains 9,927 pairs of sentences with human annotated relatedness score, ranging from 1 to 5. A score of 1 indicates that the two sentences are not related, while 5 indicates high relatedness. The set is splitted into a training set of 4,500 instances, a validation set of 500, and a test set of 4,927. We compare Doc2VecC with several winning solutions of the competition as well as several more recent techniques reported on this dataset, including bi-directional LSTM and Tree-LSTM trained from scratch on this dataset, Skip-thought vectors learned a large book corpus BIBREF34 and produced sentence embeddings of 4,800 dimensions on this dataset. We follow the same protocol as in skip-thought vectors, and train Doc2VecC on the larger book corpus dataset. Contrary to the vocabulary expansion technique used in BIBREF16 to handle out-of-vocabulary words, we extend the vocabulary of the learned model directly on the target dataset in the following way: we use the pre-trained word embedding as an initialization, and fine-tune the word and sentence representation on the SICK dataset. Notice that the fine-tuning is done for sentence representation learning only, and we did not use the relatedness score in the learning. This step brings small improvement to the performance of our algorithm. Given the sentence embeddings, we used the exact same training and testing protocol as in BIBREF16 to score each pair of sentences: with two sentence embedding INLINEFORM0 and INLINEFORM1 , we concatenate their component-wise product, INLINEFORM2 and their absolute difference, INLINEFORM3 as the feature representation. Table TABREF35 summarizes the performance of various algorithms on this dataset. Despite its simplicity, Doc2VecC significantly out-performs the winning solutions of the competition, which are heavily feature engineered toward this dataset and several baseline methods, noticeably the dependency-tree RNNs introduced in BIBREF35 , which relies on expensive dependency parsers to compose sentence vectors from word embeddings. The performance of Doc2VecC is slightly worse than the LSTM based methods or skip-thought vectors on this dataset, while it significantly outperforms skip-thought vectors on the IMDB movie review dataset ( INLINEFORM0 error rate vs INLINEFORM1 ). As we hypothesized in previous section, while Doc2VecC is better at handling longer paragraphs, LSTM-based methods are superior for relatively short sentences (of length in the order of 10s). We would like to point out that Doc2VecC is much faster to train and test comparing to skip-thought vectors. It takes less than 2 hours to learn the embeddings on the large book corpus for Doc2VecC on a desktop with Intel i7 2.2Ghz cpu, in comparison to the 2 weeks on GPU required by skip-thought vectors.",Conclusion,"We introduce a new model architecture Doc2VecC for document representation learning. It is very efficient to train and test thanks to its simple model architecture. Doc2VecC intrinsically makes sure document representation generated by averaging word embeddings capture semantics of document during learning. It also introduces a data-dependent regularization which favors informative or rare words while dampening the embeddings of common and non-discriminative words. As such, each document can be efficiently represented as a simple average of the learned word embeddings. In comparison to several existing document representation learning algorithms, Doc2VecC outperforms not only in testing efficiency, but also in the expressiveness of the generated representations.",,,,,,,,,,,,,,,,,,,,,Which language models do they compare against?,52f1a91f546b8a25a5d72325c503ec8f9c72de23,infinity,familiar,no,,2cfd959e433f290bb50b55722370f0d22fe090b7,False,,,"We compare against the following document representation baselines: bag-of-words (BoW); Denoising Autoencoders (DEA) BIBREF14 , a representation learned from reconstructing original document INLINEFORM0 using corrupted one INLINEFORM1 . SDAs have been shown to be the state-of-the-art for sentiment analysis tasks BIBREF29 . We used Kullback-Liebler divergence as the reconstruction error and an affine encoder. To scale up the algorithm to large vocabulary, we only take into account the non-zero elements of INLINEFORM2 in the reconstruction error and employed negative sampling for the remainings; Word2Vec BIBREF1 +IDF, a representation generated through weighted average of word vectors learned using Word2Vec; Doc2Vec BIBREF2 ; Skip-thought Vectors BIBREF16 , a generic, distributed sentence encoder that extends the Word2Vec skip-gram model to sentence level. It has been shown to produce highly generic sentence representations that apply to various natural language processing tasks. We also include RNNLM BIBREF11 , a recurrent neural network based language model in the comparison. In the semantic relatedness task, we further compare to LSTM-based methods BIBREF18 that have been reported on this dataset.","We also include RNNLM BIBREF11 , a recurrent neural network based language model in the comparison.",6db29a269f42efdb89beabbd9c34bc64102f33af,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,"Is their approach similar to making an averaged weighted sum of word vectors, where weights reflect word frequencies?",bb5697cf352dd608edf119ca9b82a6b7e51c8d21,infinity,familiar,no,,2cfd959e433f290bb50b55722370f0d22fe090b7,False,,,"Similar to Word2Vec or Paragraph Vectors, Doc2VecC consists of an input layer, a projection layer as well as an output layer to predict the target word, “ceremony” in this example. The embeddings of neighboring words (“opening”, “for”, “the”) provide local context while the vector representation of the entire document (shown in grey) serves as the global context. In contrast to Paragraph Vectors, which directly learns a unique vector for each document, Doc2VecC represents each document as an average of the embeddings of words randomly sampled from the document (“performance” at position INLINEFORM0 , “praised” at position INLINEFORM1 , and “brazil” at position INLINEFORM2 ). BIBREF25 also proposed the idea of using average of word embeddings to represent the global context of a document. Different from their work, we choose to corrupt the original document by randomly removing significant portion of words, and represent the document using only the embeddings of the words remained. This corruption mechanism offers us great speedup during training as it significantly reduces the number of parameters to update in back propagation. At the same time, as we are going to detail in the next section, it introduces a special form of regularization, which brings great performance improvement.","BIBREF25 also proposed the idea of using average of word embeddings to represent the global context of a document. Different from their work, we choose to corrupt the original document by randomly removing significant portion of words, and represent the document using only the embeddings of the words remained.",1ae7eca7804e1547227cce6d43ad9b403f8832ad,258ee4069f740c400c0049a2580945a1cc7f044c,How do they determine which words are informative?,98785bf06e60fcf0a6fe8921edab6190d0c2cec1,infinity,familiar,no,,2cfd959e433f290bb50b55722370f0d22fe090b7,False,,Informative are those that will not be suppressed by regularization performed.,"Data dependent regularization. As explained in Section SECREF15 , the corruption introduced in Doc2VecC acts as a data-dependent regularization that suppresses the embeddings of frequent but uninformative words. Here we conduct an experiment to exam the effect. We used a cutoff of 100 in this experiment. Table TABREF24 lists the words having the smallest INLINEFORM0 norm of embeddings found by different algorithms. The number inside the parenthesis after each word is the number of times this word appears in the learning set. In word2Vec or Paragraph Vectors, the least frequent words have embeddings that are close to zero, despite some of them being indicative of sentiment such as debacle, bliss and shabby. In contrast, Doc2VecC manages to clamp down the representation of words frequently appear in the training set, but are uninformative, such as symbols and stop words.","As explained in Section SECREF15 , the corruption introduced in Doc2VecC acts as a data-dependent regularization that suppresses the embeddings of frequent but uninformative words. In contrast, Doc2VecC manages to clamp down the representation of words frequently appear in the training set, but are uninformative, such as symbols and stop words.",9335468572f5556bcdc53f49d72dc01c47d6814b,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3-Figure1-1.png,Figure 1: A new framework for learning document vectors.,6-Table1-1.png,Table 1: Classification error of a linear classifier trained on various document representations on the Imdb dataset.,7-Table2-1.png,Table 2: Learning time and representation generation time required by different representation learning algorithms.,7-Table3-1.png,Table 3: Words with embeddings closest to 0 learned by different algorithms.,8-Figure2-1.png,Figure 2: Accuracy on subset of the Semantic-Syntactic Word Relationship test set. Only questions containing words from the most frequent 30k words are included in the test.,8-Table4-1.png,Table 4: Top 1 accuracy on the 5 type of semantics and 9 types of syntactic questions.,,,,,,,,,"Different from their work, we choose to corrupt the original document by randomly removing significant portion of words, and represent the document using only the embeddings of the words remained.",,9-Table5-1.png,Table 5: Classification error (%) of a linear classifier trained on various document representations on the Wikipedia dataset.,9-Figure3-1.png,Figure 3: Visualization of document vectors on Wikipedia dataset using t-SNE.,9-Figure4-1.png,Figure 4: Visualization of Wikipedia Doc2VecC vectors using t-SNE.,11-Table6-1.png,"Table 6: Test set results on the SICK semantic relatedness task. The first group of results are from the submission to the 2014 SemEval competition; the second group includes several baseline methods reported in (Tai et al., 2015); the third group are methods based on LSTM reported in (Tai et al., 2015) as well as the skip-thought vectors (Kiros et al., 2015).",,,,RNNLM BIBREF11,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Community Question Answering Platforms vs. Twitter for Predicting Characteristics of Urban Neighbourhoods,"In this paper, we investigate whether text from a Community Question Answering (QA) platform can be used to predict and describe real-world attributes. We experiment with predicting a wide range of 62 demographic attributes for neighbourhoods of London. We use the text from QA platform of Yahoo! Answers and compare our results to the ones obtained from Twitter microblogs. Outcomes show that the correlation between the predicted demographic attributes using text from Yahoo! Answers discussions and the observed demographic attributes can reach an average Pearson correlation coefficient of \r{ho} = 0.54, slightly higher than the predictions obtained using Twitter data. Our qualitative analysis indicates that there is semantic relatedness between the highest correlated terms extracted from both datasets and their relative demographic attributes. Furthermore, the correlations highlight the different natures of the information contained in Yahoo! Answers and Twitter. While the former seems to offer a more encyclopedic content, the latter provides information related to the current sociocultural aspects or phenomena.",Introduction,"Recent years have seen a huge boom in the number of different social media platforms available to users. People are increasingly using these platforms to voice their opinions or let others know about their whereabouts and activities. Each of these platforms has its own characteristics and is used for different purposes. The availability of a huge amount of data from many social media platforms has inspired researchers to study the relation between the data generated through the use of these platforms and real-world attributes. Many recent studies in this field are particularly inspired by the availability of text-based social media platforms such as blogs and Twitter. Text from Twitter microblogs, in particular, has been widely used as data source to make predictions in many domains. For example, box-office revenues are predicted using text from Twitter BIBREF0 . Twitter data has also been used to find correlations between the mood stated in tweets and the value of Dow Jones Industrial Average (DJIA) BIBREF1 . Predicting demographics of individual users using their language on social media platforms, especially Twitter, has been the focus of many research works: text from blogs and on-line forum posts are utilised to predict user's age through the analysis of linguistic features. Results show that the age of users can be predicted where the predicted and observed values reach a Pearson correlation coefficient of almost $0.7$ . Sociolinguistic associations using geo-tagged Twitter data have been discovered BIBREF2 and the results indicate that the demographic information of users such as first language, race, and ethnicity can be predicted by using text from Twitter with a correlation up to $0.3$ . Other research shows that users' income can also be predicted using tweets with a good prediction accuracy BIBREF3 . Text from Twitter microblogs has also been used to discover the relation between the language of users and the deprivation index of neighbourhoods. The collective sentiment extracted from the tweets of users has been shown BIBREF4 to have significant correlation ( $0.35$ ) with the deprivation index of the communities the users belong to. Data generated on QA platforms have not been used in the past for predicting real-world attributes. Most research work that utilise QA data aim to increase the performance of such platforms in analysing question quality BIBREF5 , predicting the best answers BIBREF6 , BIBREF7 or the best responder BIBREF8 . In this paper, we use the text from the discussions on the QA platform of Yahoo! Answers about neighbourhoods of London to show that the QA text can be used to predict the demographic attributes of the population of those neighbourhoods. We compare the performance of Yahoo! Answers data to the performance of data from Twitter, a platform that has been widely used for predicting many real-world attributes. Unlike many current works that focus on predicting one or few selected attributes (e.g. deprivation, race or income) using social media data, we study a wide range of 62 demographic attributes. Furthermore, we test whether terms extracted from both Yahoo! Answers and Twitter are semantically related to these attributes and provide examples of sociocultural profiles of neighbourhoods through the interpretation of the coefficients of the predictive models. The contributions of this paper can be summarised as follows:",Spatial Unit of Analysis,"The spatial unit of analysis chosen for this work is the neighbourhood. This is identified with a unique name (e.g., Camden) and people normally use this name in QA discussions to refer to specific neighbourhoods. A list of neighbourhoods for London is extracted from the GeoNames gazetteer, a dataset containing names of geographic places including place names. For each neighbourhood, GeoNames provides its name and a set of geographic coordinates (i.e., latitude and longitude) which roughly represents its centre. Note that geographical boundaries are not provided. GeoNames contains 589 neighbourhoods that fall within the boundaries of the Greater London metropolitan area. In the remainder of the paper, we use the terms “neighbourhood” or “area” to refer to our spatial unit of analysis.","Pre-processing, Filtering, and Spatial Aggregation","We collect questions and answers (QAs) from Yahoo! Answers using its public API. For each neighbourhood, the query consists of the name of the neighbourhood together with the keywords London and area. This is to prevent obtaining irrelevant QAs for ambiguous entity names such as Victoria. For each neighbourhood, we then take all the QAs that are returned by the API. Each QA consists of a title and a content which is an elaboration on the title. This is followed by a number of answers. In total, we collect $12,947$ QAs across all London neighbourhoods. These QAs span over the last 5 years. It is common for users to discuss characteristics of several neighbourhoods in the same QA thread. This means that the same QA can be assigned to more than one neighbourhood. Figure 1 shows the histogram of the number of QAs for each neighbourhood. As the figure shows, the majority of areas have less than 100 QAs with some areas having less than 10. Only few areas have over 100 QAs. For each neighbourhood, we create one single document by combining all the QA discussions that have been retrieved using the name of such neighbourhood. This document may or may not contain names of other neighbourhoods. We split each document into sentences and remove those neighbourhoods containing less than 40 sentences. We then remove URLs from each document. The document is then converted to tokens and stop words are removed. All the tokens in all the documents are then stemmed. The goal of stemming is to reduce the different grammatical forms of a word to a common base form. Stemming is a special case of text normalisation. For example, a stemmer will transform the word “presumably” to “presum” and “provision” to “provis”. To keep the most frequent words, we remove any token that has appeared less than 5 times in less than 5 unique QAs. This leaves us with 8k distinct tokens. To collect data from Twitter, we use the geographical bounding box of London, defined by the northwest and southeast points of the Greater London region. We then use this bounding box to obtain the tweets that are geotagged and are created within this box through the official Twitter API. We stream Twitter data for 6 months between December 2015 and July 2016. At the end, we have around $2,000,000$ tweets in our dataset. To assign tweets to different neighbourhoods, for each tweet, we calculate the distance between the location that it was blogged from and the centre points of all the neighbourhoods in our dataset. Note that the centre point for each neighbourhood is provided in the gazetteer. We then assign the tweet to the closest neighbourhood that is not further than 1 km from the tweet's geolocation. At the end of this process, we have a collection of tweets per each neighbourhood and we combine them to create a single document. Figure 2 shows the number of tweets per each neighbourhood. As we can see, the majority of neighbourhoods have less than 1000 tweets. We remove all the target words (words starting with @) from the documents. The pre-processing is then similar to the QA documents. At the end of this process, we obtain 17k distinct frequent tokens for the Twitter corpus. As we previously explained, each attribute in census data is assigned to spatial units called LSOAs. However, these units do not geographically match our units of analysis which are the neighbourhoods defined trough the gazetteer. A map showing the spatial mismatch is presented in Figure 3 . To aggregate the data contained in the LSOAs at the neighbourhood level, we use the following approach. Often, when people talk about a neighbourhood, they refer to the area around its centre point. Therefore, the information provided for neighbourhoods in QA discussions should be very related to this geographic point. To keep this level of local information, for each demographic attribute, we assign only the values of the nearby LSOAs to the respective neighbourhood. To do this, we calculate the distance between each neighbourhood and all the LSOAs in London. The distance is calculated between the coordinates of a neighbourhood and the coordinates of each LSOA's centroid. For each neighbourhood, we then select the 10 closest LSOAs that are not further than one kilometre away. The value of each demographic attribute for each neighbourhood is then computed by averaging the values associated with the LSOAs assigned to it. We apply this procedure to all the demographic attributes.",Document Representation,"A very popular method for representing a document using its words is the tf-idf approach BIBREF9 . Tf-idf is short for term frequency-inverse document frequency where tf indicates the frequency of a term in the document and idf is a function of the number of documents that a terms has appeared in. In a tf-idf representation, the order of the words in the document is not preserved. For each term in a document, the tf-idf value is calculated as below:  $$\small \text{tf-idf} (d,t) = \frac{\text{tf}\ (d,t)}{\log (\frac{\text{Total number of documents}}{\text{Number of documents containing the term}\ t })} $$   (Eq. 21)  To discount the bias for areas that have a high number of QAs or tweets, we normalise tf values by the length of each document as below. The length of a document is defined by the number of its tokens (non-distinctive words).  $$\small \text{Normalised\ tf} (d,t) = \frac{\text{Frequency of Term t in Document d}}{\text{Number of Tokens in Document d}}$$   (Eq. 22) ",Correlation,"To investigate the extent to which the text obtained from the two platforms of Yahoo! Answers and Twitter reflect the true attributes of neighbourhoods, we first study whether there are significant, strong and meaningful correlations between the terms present in each corpus and the many neighbourhood attributes through the Pearson correlation coefficient $\rho $ . For each term in each corpus, we calculate the correlation between the term and all the selected demographic attributes. To do so, for each term, we define a vector with the dimension of the number of neighbourhoods. The value of each cell in this vector represents the normalised tf-idf value of the term for the corresponding neighbourhood. For each demographic attribute, we also define a vector with the dimension of the number of neighbourhoods. Each cell represents the value for the demographic attribute of the corresponding neighbourhood. We then calculate the Pearson correlation coefficient ( $\rho $ ) between these two vectors to measure the strength of the association between each term and each attribute. Since we perform many correlation tests simultaneously, we need to correct the significance values (p-values) for multiple testing. We do so by implementing the Bonferroni correction, a multiple-comparison p-value correction, which is used when several dependent or independent statistical tests are being performed simultaneously. The Bonferroni adjustment ensures an upper bound for the probability of having an erroneous significant result among all the tests. All the p-values showed in this paper are adjusted through the use of the Bonferroni correction. The number of significantly correlated terms from both Yahoo! Answers and the Twitter with the selected demographic attributes are shown in Table 2 . Note that the number of unique (frequent) words in Twitter (17k) is almost twice as in Yahoo! Answers (8k). The first column shows a demographic attribute and the second column indicates the source, i.e. Yahoo! Answers (Y!A for short) or Twitter. The third column (“All”) shows the total number of terms that have a significant correlation with each attribute (p-value $<0.01$ ). The following columns show the number of terms that have a significant correlation with the attribute with a $\rho $ in the given ranges. The last column shows the number of terms that are significantly correlated with the attribute with a negative $\rho $ . The data source that has the highest number of correlated terms with each attribute is highlighted in bold. As the table shows, terms extracted from Yahoo! Answers tend to be more related, in terms of the number of correlated terms, to attributes related to religion or ethnicity compared to terms from Twitter. However, for two particular attributes (i.e., Price and Buddhist), the number of correlated terms from Twitter is higher than the ones from Yahoo! Answers . These results collectively suggest that there is a wealth of terms, both in Yahoo! Answers and Twitter, which can be used to predict the population demographics. In this section, we observe whether the correlations between terms and attributes are semantically meaningful. Due to the limited space, we select three attributes and their relative top correlated terms extracted from Yahoo! Answers (Table 3 ) and Twitter (Table 4 ). We choose the attributes Price and IMD as they show the highest number of correlated terms for both sources. For each source, we then choose one more attribute that has the highest number of strongly correlated terms ( $\rho >0.4$ ), i.e. Jewish% for Yahoo! Anwsers and Buddhist% for Twitter. We first examine Table 3 and provide examples of semantic similarity between Yahoo! Answers terms and the selected attributes. Words highlighted in bold are, in our view, the ones most associated with their respective attribute. For the attribute Deprivation, the majority of the terms seem to be linked to issues of deprived areas. “Poverty”, “drug”, “victim”, all refer to social issues. “Rundown” and “slum” may be associated with the degradation of the surrounding environment. “Cockney” is a dialect traditionally spoken by working class, and thus less advantaged, Londoners. For the attribute (High) Price, most terms seem to be related to aspects of places which may offer more expensive housing. Terms such as “fortune”, “diplomat”, and “aristocratic” are often associated with wealth. Others seem to reflect a posh lifestyle and status symbol: “townhouse”, “exclusive”, “celeb”, “fashionable”, “desirable”. For the attribute Jewish%, most of the terms seem to reflect aspects of this religion or be linguistically associated with it (i.e., “Jew” and “Jewish”). “Matzo” and “Kosher” are associated with the traditional Jewish cuisine; the former is a type of flat-bread, the latter is a way of preparing food. The “ark” is a specific part of the synagogue which contains sacred texts. We now examine Table 4 . For the attribute Deprivation, nine words out of ten seem to be linked to more deprived areas. “East”, “eastlondon”, and “eastend”, for example, provide geographical information on where deprivation is more concentrated in London (i.e., East End). Other terms seem to be related to the presence of younger generation of creatives and artists in more deprived neighbourhoods. “Yeah”, “shit”, “ass”, may all be jargons commonly used by this section of population. “Studio”, “craftbeer”, “music” may instead refer to their main activities and occupations. For what concerns (high) “Price”, all the terms seem to relate to aspects of expensive areas, e.g. “luxury”, “classy”, and “stylish”. “Tea”, “teatime”, “delight”, “truffle” seem to relate to social activities of the upper class. For the attribute “Buddhist%”, five terms out of ten are, in our view, associated with neighbourhoods where the majority of people is Buddhist or practise Buddhism. These terms seems to relate to aspects of this religion, e.g. “think”, “learn”, “mind”, etc. Interestingly, terms extracted from Yahoo! Answers and Twitter seem to offer two different kinds of knowledge. On one side, terms extracted from Yahoo! Answers are more encyclopedic as they tend to offer definitions or renowned aspects for each attribute. “Jewish%” is, for example, related to aspects of the Jewish culture such as “matzo”, “harmony”, and “kosher”. “Deprivation” is associated with social issues such as “poverty” and “drug”, but also with a degraded urban environment (e.g., “rundown”, “slum”). On the other, Twitter words provide a kind of knowledge more related to current sociocultural aspects. This is the case, for example, of the jargon associated with “Deprivation” (e.g., “yeah”, “shit”), or of the culinary habits related to “High Prices” (e.g., “tea”, “truffle”).",Prediction,"We investigate how well the demographic attributes can be predicted by using using Yahoo! Ansewrs and Twitter data. We define the task of predicting a continuous-valued demographic attribute for unseen neighbourhoods as a regression task given their normalised tf-idf document representation. A separate regression task is defined for each demographic attribute. We choose linear regression for the prediction tasks as it has been widely used for predictions from text in the literature BIBREF10 , BIBREF11 . Due to the high number of features (size of vocabulary) and a small number of training points, over-fitting can occur. To avoid this issue, we use elastic net regularisation, a technique that combines the regularisation of the ridge and lasso regressions. The parameters $\theta $ are estimated by minimising the following loss function. Here, $y_i$ is the value of an attribute for the $i$ -th neighbourhood, vector $\mathbf {x}_i$ is its document representation and $N$ is the number of neighbourhoods in the training set.  $$\mathfrak {L} = \frac{1}{N} \sum _{i=1}^N (y_i- \mathbf {x}_{i}^T \theta )^2 + \lambda _1 ||\mathbf {\theta }|| + \lambda _2 ||\mathbf {\theta }||^2$$   (Eq. 25)  To measure the performance of a regression model, residual-based methods such as mean squared error are commonly used. Ranking metrics such as Pearson correlation coefficient have also been used in the literature BIBREF12 , BIBREF2 . Using a ranking measure has some advantages compared to a residual-based measure. First, ranking evaluation is more robust against extreme outliers compared to an additive residual-based evaluation measure. Second, ranking metrics are more interpretable than measures such as mean squared error BIBREF13 . We thus use this method for evaluating the performance of the regression models in this work. As further performance check, we apply a 10 folds cross-validation to each regression task. In each fold, we use $75\%$ of the data for training and the remaining $25\%$ for validation. At the end, we report the average performance over all folds together with the standard deviation. For each demographic attribute, i.e. target value, training and validation sets are sampled using Stratified Sampling. This is a sampling method from a population, when sub-populations within this population vary. For instance, in London, there are areas with very high or very low deprivation. In these cases, it is advantageous to sample each sub-population independently and proportionally to its size. The results of the regression tasks performed over the selected set of demographic attributes, in terms of Pearson correlation coefficient ( $\rho $ ), are presented in Table 4 . Results are averaged over 10 folds and standard deviations are displayed in parenthesis. We can see that on average, performances of Yahoo! Answers and Twitter are very similarly with Yahoo! Answers having a slightly higher performance ( $4\%$ ). Twitter data can predict the majority of the religion-related attributes with a higher correlation coefficient with the exception of population of Jewish%. On the other hand, Yahoo! Answers is superior to Twitter when predicting ethnicity related attributes such as population of White% and Black%. We have seen in Table 2 that Twitter has very few correlated terms with the attributes White (0) and Black (2). We also observe that IMD and Price can be predicted with a high correlation coefficient using both Yahoo! Answers and Twitter. This can be due to the fact that there are many words in our dataset that can be related to the deprivation of a neighbourhood or to how expensive a neighbourhood is. This is also evident in Table 2 where the number of correlated terms from both Yahoo! Answers and Twitter with these attributes are very high. On the other hand, terms that describe a religion or an ethnicity are more specific and lower in frequency. Therefore attributes that are related to religion or ethnicity are predicted with a lower accuracy. Table 5 further shows two terms that have the highest coefficients in the regressions models (across the majority of folds) for each attribute and source in the column Terms. These terms are among the strong predictors of their respective attribute. Many of these terms appear to be related to the given demographic attribute (for both Twitter and Yahoo! Answers ) and are also often amongst the top correlated terms presented in Tables 3 and 4 . We follow with some examples. According to the regression coefficients for the attribute Muslim%, neighbourhoods inhabited by a Muslim majority may be located in Mile End, an East London district (i.e., Twitter terms “mileend” and “eastlondon”), see the presence of Asian population and barber shops (i.e., Yahoo! Answers terms “asian” and “barber”). According to the terms for Black%, neighbourhoods with a black majority tend to be located in the southern part of London (i.e., Twitter term “southlondon”) and experience social issues such as presence of criminal groups and drug use (i.e., Yahoo! Answers terms “gang” and “drug”). According to the terms for IMD, more deprived areas seem to be located in the East End of London (i.e., Twitter term “eastlondon”) where the Cockney dialect is dominant (i.e., Yahoo! Answers term “cockney”). Yahoo! Answers and Twitter seem to complement one another in terms of information they provide through the terms associated with each attribute which in most cases are different. One noticeable difference is that Twitter tends to offer geographical information (e.g., “mileend”, “southlondon”, “essex”). On the other hand, terms from Yahoo! Answers sometimes match the name of the attribute (i.e. “asian” and “Jewish”). In the Appendix, in Tables 6 and 7 , we show the prediction results for a wide range of 62 demographic attributes using Yahoo! Answers and Twitter. For each attribute, we display two terms with the highest coefficient common between the majority of the folds. Attributes are divided into categories such as Religion, Employment, Education, etc. Overall, the results show that Yahoo! Answers performs slightly better than Twitter with an average $1\%$ increase over all the attributes. Wilxocon signed rank test shows that their results are significantly different from each other (p-value $ < 0.01$ ). Outcomes in these tables show that on average, a wide range of demographic attributes of the population of neighbourhoods can be predicted using both Yahoo! Answers and Twitter with high performances of $0.54$ and $0.53$ respectively. While Yahoo! Answers outperforms Twitter in predicting attributes related to Ethnicity and Employment, Twitter performs better when predicting attributes relating to the Age Group, and Car Ownership.",Related Work,"The availability of a huge amount of data from many social media platforms has inspired researchers to study the relation between the data on these platforms and many real-world attributes. Twitter data, in particular, has been widely used as a social media source to make predictions in many domains. For example, box-office revenues are predicted using text from Twitter microblogs BIBREF0 . Prediction results have been predicted by performing content analysis on tweets BIBREF14 . It is shown that correlations exist between mood states of the collective tweets to the value of Dow Jones Industrial Average (DJIA) BIBREF1 . Predicting demographics of individual users using their language on social media platforms, especially Twitter, has been the focus of many research. Text from blogs, telephone conversations, and forum posts are utilised for predicting author's age BIBREF15 with a Pearson's correlation of $0.7$ . Geo-tagged Twitter data have been used to predict the demographic information of authors such as first language, race, and ethnicity with correlations up to $~0.3$ BIBREF2 . One aspect of urban area life that has been the focus of many research work in urban data mining is finding correlations between different sources of data and the deprivation index (IMD), of neighbourhoods across a city or a country BIBREF16 , BIBREF4 . Cellular data BIBREF17 and the elements present in an urban area BIBREF18 are among non-textual data sources that are shown to have correlations with a deprivation index. Also, flow of public transport data has been used to find correlations (with a correlation coefficient of $r = 0.21$ ) with IMD of urban areas available in UK census BIBREF16 . Research shows that correlations of $r = 0.35$ exists between the sentiment expressed in tweets of users in a community and the deprivation index of the community BIBREF4 . Social media data has been used in many domains to find links to the real-world attributes. Data generated on QA platforms, however, has not been used in the past for predicting such attributes. In this paper, we use discussions on Yahoo! Answers QA platform to make predictions of demographic attribute of city neighbourhoods. Previous work in this domain has mainly focused on predicting the deprivation index of areas BIBREF4 . In this work, we look at a wide range of attributes and report prediction results on 62 demographic attributes. Additionally, work in urban prediction uses geolocation-based platforms such as Twitter. QA data that has been utilised in this paper does not include geolocation information. Utilising such data presents its own challenges.",Discussion,"In this paper, we investigate predicting values for real-world entities such as demographic attributes of neighbourhoods using discussions from QA platforms. We show that these attributes can be predicted using text features based on Yahoo! Answers discussions about neighbourhoods with a slightly higher correlation coefficient than predictions made using Twitter data.",Limitations,"Here, we present some of the limitations of our work. To unify the units of analysis, we take a heuristic approach. We do not cross-validate our results with other approaches. This is because of the lack of work in using non-geotagged text for predicting attributes of neighbourhoods in the current literature. Our experiments in this paper is limited to the city of London. London is a cosmopolitan city and a popular destination for travellers and settlers. Therefore, many discussions can be found on Yahoo! Answers regarding its neighbourhoods. The coverage of discussions on QA platforms may not be sufficient for all cities of interest.",,,,,,,,,,,,,,,,,,,,,,,,,What do the correlation demonstrate? ,39cfb8473c8be4e5d8ecc3227b800a10477c5f80,five,familiar,no,Question Answering,50d8b4a941c26b89482c94ab324b5a274f9ced66,False,,,"To investigate the extent to which the text obtained from the two platforms of Yahoo! Answers and Twitter reflect the true attributes of neighbourhoods, we first study whether there are significant, strong and meaningful correlations between the terms present in each corpus and the many neighbourhood attributes through the Pearson correlation coefficient $\rho $ . For each term in each corpus, we calculate the correlation between the term and all the selected demographic attributes. To do so, for each term, we define a vector with the dimension of the number of neighbourhoods. The value of each cell in this vector represents the normalised tf-idf value of the term for the corresponding neighbourhood. For each demographic attribute, we also define a vector with the dimension of the number of neighbourhoods. Each cell represents the value for the demographic attribute of the corresponding neighbourhood. We then calculate the Pearson correlation coefficient ( $\rho $ ) between these two vectors to measure the strength of the association between each term and each attribute.","To investigate the extent to which the text obtained from the two platforms of Yahoo! Answers and Twitter reflect the true attributes of neighbourhoods, we first study whether there are significant, strong and meaningful correlations between the terms present in each corpus and the many neighbourhood attributes through the Pearson correlation coefficient $\rho $ .",85ac6dd0bf8410cbd2280738282a009b41e3bd23,1ba1b5b562aef9cd264cace5b7bdd46a7c065c0a,,,,,,,,,"On Twitter, do the demographic attributes and answers show more correlations than on Yahoo! Answers?",f981781021d4bacbaf3d076c895dc42d7fa108ba,five,familiar,no,Question Answering,50d8b4a941c26b89482c94ab324b5a274f9ced66,False,False,,"As the table shows, terms extracted from Yahoo! Answers tend to be more related, in terms of the number of correlated terms, to attributes related to religion or ethnicity compared to terms from Twitter. However, for two particular attributes (i.e., Price and Buddhist), the number of correlated terms from Twitter is higher than the ones from Yahoo! Answers . These results collectively suggest that there is a wealth of terms, both in Yahoo! Answers and Twitter, which can be used to predict the population demographics.","terms extracted from Yahoo! Answers tend to be more related, in terms of the number of correlated terms, to attributes related to religion or ethnicity compared to terms from Twitter. However, for two particular attributes (i.e., Price and Buddhist), the number of correlated terms from Twitter is higher than the ones from Yahoo! Answers . ",c4f56d085fc64a43fd4fc8a95dee51bc71c2ac77,1ba1b5b562aef9cd264cace5b7bdd46a7c065c0a,How many demographic attributes they try to predict?,d5105a6a6d5d1931b0729dcf15ca862d6eac770f,five,familiar,no,Question Answering,50d8b4a941c26b89482c94ab324b5a274f9ced66,False,,,"Social media data has been used in many domains to find links to the real-world attributes. Data generated on QA platforms, however, has not been used in the past for predicting such attributes. In this paper, we use discussions on Yahoo! Answers QA platform to make predictions of demographic attribute of city neighbourhoods. Previous work in this domain has mainly focused on predicting the deprivation index of areas BIBREF4 . In this work, we look at a wide range of attributes and report prediction results on 62 demographic attributes. Additionally, work in urban prediction uses geolocation-based platforms such as Twitter. QA data that has been utilised in this paper does not include geolocation information. Utilising such data presents its own challenges.","In this work, we look at a wide range of attributes and report prediction results on 62 demographic attributes.",5c485df7aff0d57d78a2eaa1eb53dfd4a70a9c41,1ba1b5b562aef9cd264cace5b7bdd46a7c065c0a,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2-Table1-1.png,Table 1: Examples of Yahoo! Answers discussions and Twitter microblogs about neighbourhoods that contain the term “Jewish”.,3-Figure1-1.png,Figure 1: Histogram of the number of QAs per each London neighbourhood.,4-Figure2-1.png,Figure 2: Histogram of the number of tweets per each London neighbourhood.,4-Figure3-1.png,Figure 3: The geographic relation between LSOAs and neighbourhoods identified through the gazetteer. LSOAs are geographical shapes and their centroids are marked with dark dots. Neighbourhoods are marked with green circles.,5-Table2-1.png,Table 2: Number of significantly correlated terms (p-value < 0.01) from both Yahoo! Answers (“Y! A”) and Twitter.,6-Table3-1.png,Table 3: Terms from Yahoo! Answers with the highest Pearson correlation coefficients for the selected demographic attributes. Correlations are statistically significant (p-value < 0.001).,,,,,,,,,,62,6-Table4-1.png,Table 4: Terms from Twitter with the highest Pearson correlation coefficients for the selected demographic attributes. Correlations are statistically significant (p-value < 0.001).,7-Table5-1.png,Table 5: Prediction results in terms of ρ using Yahoo! Answers and Twitter data. Results are averaged over 10 folds and standard deviations are shown in parenthesis. Correlations are statistically significant (p-value < 0.01). Terms with the highest coefficients in regressions models are also provided.,9-Table6-1.png,"Table 6: Prediction results for different categories and attributes in terms of ρ using Yahoo! Answers and Twitter data. Results are averaged over 10 folds and standard deviations are shown in parenthesis. All correlations are statistically significant with a p-value < 0.01. For each category, the difference in performance between the two sources are highlighted in the column related to the outperforming (i.e. upward arrow) source.",10-Table7-1.png,Table 7: cont.,,,,the extent to which the text obtained from the two platforms of Yahoo! Answers and Twitter reflect the true attributes of neighbourhoods,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exploring Hate Speech Detection in Multimodal Publications,"In this work we target the problem of hate speech detection in multimodal publications formed by a text and an image. We gather and annotate a large scale dataset from Twitter, MMHS150K, and propose different models that jointly analyze textual and visual information for hate speech detection, comparing them with unimodal detection. We provide quantitative and qualitative results and analyze the challenges of the proposed task. We find that, even though images are useful for the hate speech detection task, current multimodal models cannot outperform models analyzing only text. We discuss why and open the field and the dataset for further research.",Introduction,"Social Media platforms such as Facebook, Twitter or Reddit have empowered individuals' voices and facilitated freedom of expression. However they have also been a breeding ground for hate speech and other types of online harassment. Hate speech is defined in legal literature as speech (or any form of expression) that expresses (or seeks to promote, or has the capacity to increase) hatred against a person or a group of people because of a characteristic they share, or a group to which they belong BIBREF0. Twitter develops this definition in its hateful conduct policy as violence against or directly attack or threaten other people on the basis of race, ethnicity, national origin, sexual orientation, gender, gender identity, religious affiliation, age, disability, or serious disease. In this work we focus on hate speech detection. Due to the inherent complexity of this task, it is important to distinguish hate speech from other types of online harassment. In particular, although it might be offensive to many people, the sole presence of insulting terms does not itself signify or convey hate speech. And, the other way around, hate speech may denigrate or threaten an individual or a group of people without the use of any profanities. People from the african-american community, for example, often use the term nigga online, in everyday language, without malicious intentions to refer to folks within their community, and the word cunt is often used in non hate speech publications and without any sexist purpose. The goal of this work is not to discuss if racial slur, such as nigga, should be pursued. The goal is to distinguish between publications using offensive terms and publications attacking communities, which we call hate speech. Modern social media content usually include images and text. Some of these multimodal publications are only hate speech because of the combination of the text with a certain image. That is because, as we have stated, the presence of offensive terms does not itself signify hate speech, and the presence of hate speech is often determined by the context of a publication. Moreover, users authoring hate speech tend to intentionally construct publications where the text is not enough to determine they are hate speech. This happens especially in Twitter, where multimodal tweets are formed by an image and a short text, which in many cases is not enough to judge them. In those cases, the image might give extra context to make a proper judgement. Fig. FIGREF5 shows some of such examples in MMHS150K. The contributions of this work are as follows: [noitemsep,leftmargin=*] We propose the novel task of hate speech detection in multimodal publications, collect, annotate and publish a large scale dataset. We evaluate state of the art multimodal models on this specific task and compare their performance with unimodal detection. Even though images are proved to be useful for hate speech detection, the proposed multimodal models do not outperform unimodal textual models. We study the challenges of the proposed task, and open the field for future research.",Related Work ::: Hate Speech Detection,"The literature on detecting hate speech on online textual publications is extensive. Schmidt and Wiegand BIBREF1 recently provided a good survey of it, where they review the terminology used over time, the features used, the existing datasets and the different approaches. However, the field lacks a consistent dataset and evaluation protocol to compare proposed methods. Saleem et al. BIBREF2 compare different classification methods detecting hate speech in Reddit and other forums. Wassem and Hovy BIBREF3 worked on hate speech detection on twitter, published a manually annotated dataset and studied its hate distribution. Later Wassem BIBREF4 extended the previous published dataset and compared amateur and expert annotations, concluding that amateur annotators are more likely than expert annotators to label items as hate speech. Park and Fung BIBREF5 worked on Wassem datasets and proposed a classification method using a CNN over Word2Vec BIBREF6 word embeddings, showing also classification results on racism and sexism hate sub-classes. Davidson et al. BIBREF7 also worked on hate speech detection on twitter, publishing another manually annotated dataset. They test different classifiers such as SVMs and decision trees and provide a performance comparison. Malmasi and Zampieri BIBREF8 worked on Davidson's dataset improving his results using more elaborated features. ElSherief et al. BIBREF9 studied hate speech on twitter and selected the most frequent terms in hate tweets based on Hatebase, a hate expression repository. They propose a big hate dataset but it lacks manual annotations, and all the tweets containing certain hate expressions are considered hate speech. Zhang et al. BIBREF10 recently proposed a more sophisticated approach for hate speech detection, using a CNN and a GRU BIBREF11 over Word2Vec BIBREF6 word embeddings. They show experiments in different datasets outperforming previous methods. Next, we summarize existing hate speech datasets: [noitemsep,leftmargin=*] RM BIBREF10: Formed by $2,435$ tweets discussing Refugees and Muslims, annotated as hate or non-hate. DT BIBREF7: Formed by $24,783$ tweets annotated as hate, offensive language or neither. In our work, offensive language tweets are considered as non-hate. WZ-LS BIBREF5: A combination of Wassem datasets BIBREF4, BIBREF3 labeled as racism, sexism, neither or both that make a total of $18,624$ tweets. Semi-Supervised BIBREF9: Contains $27,330$ general hate speech Twitter tweets crawled in a semi-supervised manner. Although often modern social media publications include images, not too many contributions exist that exploit visual information. Zhong et al. BIBREF12 worked on classifying Instagram images as potential cyberbullying targets, exploiting both the image content, the image caption and the comments. However, their visual information processing is limited to the use of features extracted by a pre-trained CNN, the use of which does not achieve any improvement. Hosseinmardi et al. BIBREF13 also address the problem of detecting cyberbullying incidents on Instagram exploiting both textual and image content. But, again, their visual information processing is limited to use the features of a pre-trained CNN, and the improvement when using visual features on cyberbullying classification is only of 0.01%.",Related Work ::: Visual and Textual Data Fusion,"A typical task in multimodal visual and textual analysis is to learn an alignment between feature spaces. To do that, usually a CNN and a RNN are trained jointly to learn a joint embedding space from aligned multimodal data. This approach is applied in tasks such as image captioning BIBREF14, BIBREF15 and multimodal image retrieval BIBREF16, BIBREF17. On the other hand, instead of explicitly learning an alignment between two spaces, the goal of Visual Question Answering (VQA) is to merge both data modalities in order to decide which answer is correct. This problem requires modeling very precise correlations between the image and the question representations. The VQA task requirements are similar to our hate speech detection problem in multimodal publications, where we have a visual and a textual input and we need to combine both sources of information to understand the global context and make a decision. We thus take inspiration from the VQA literature for the tested models. Early VQA methods BIBREF18 fuse textual and visual information by feature concatenation. Later methods, such as Multimodal Compact Bilinear pooling BIBREF19, utilize bilinear pooling to learn multimodal features. An important limitation of these methods is that the multimodal features are fused in the latter model stage, so the textual and visual relationships are modeled only in the last layers. Another limitation is that the visual features are obtained by representing the output of the CNN as a one dimensional vector, which losses the spatial information of the input images. In a recent work, Gao et al. BIBREF20 propose a feature fusion scheme to overcome these limitations. They learn convolution kernels from the textual information –which they call question-guided kernels– and convolve them with the visual information in an earlier stage to get the multimodal features. Margffoy-Tuay et al. BIBREF21 use a similar approach to combine visual and textual information, but they address a different task: instance segmentation guided by natural language queries. We inspire in these latest feature fusion works to build the models for hate speech detection.",The MMHS150K dataset,"Existing hate speech datasets contain only textual data. Moreover, a reference benchmark does not exists. Most of the published datasets are crawled from Twitter and distributed as tweet IDs but, since Twitter removes reported user accounts, an important amount of their hate tweets is no longer accessible. We create a new manually annotated multimodal hate speech dataset formed by $150,000$ tweets, each one of them containing text and an image. We call the dataset MMHS150K, and made it available online . In this section, we explain the dataset creation steps.",The MMHS150K dataset ::: Tweets Gathering,"We used the Twitter API to gather real-time tweets from September 2018 until February 2019, selecting the ones containing any of the 51 Hatebase terms that are more common in hate speech tweets, as studied in BIBREF9. We filtered out retweets, tweets containing less than three words and tweets containing porn related terms. From that selection, we kept the ones that included images and downloaded them. Twitter applies hate speech filters and other kinds of content control based on its policy, although the supervision is based on users' reports. Therefore, as we are gathering tweets from real-time posting, the content we get has not yet passed any filter.",The MMHS150K dataset ::: Textual Image Filtering,"We aim to create a multimodal hate speech database where all the instances contain visual and textual information that we can later process to determine if a tweet is hate speech or not. But a considerable amount of the images of the selected tweets contain only textual information, such as screenshots of other tweets. To ensure that all the dataset instances contain both visual and textual information, we remove those tweets. To do that, we use TextFCN BIBREF22, BIBREF23 , a Fully Convolutional Network that produces a pixel wise text probability map of an image. We set empirical thresholds to discard images that have a substantial total text probability, filtering out $23\%$ of the collected tweets.",The MMHS150K dataset ::: Annotation,"We annotate the gathered tweets using the crowdsourcing platform Amazon Mechanical Turk. There, we give the workers the definition of hate speech and show some examples to make the task clearer. We then show the tweet text and image and we ask them to classify it in one of 6 categories: No attacks to any community, racist, sexist, homophobic, religion based attacks or attacks to other communities. Each one of the $150,000$ tweets is labeled by 3 different workers to palliate discrepancies among workers. We received a lot of valuable feedback from the annotators. Most of them had understood the task correctly, but they were worried because of its subjectivity. This is indeed a subjective task, highly dependent on the annotator convictions and sensitivity. However, we expect to get cleaner annotations the more strong the attack is, which are the publications we are more interested on detecting. We also detected that several users annotate tweets for hate speech just by spotting slur. As already said previously, just the use of particular words can be offensive to many people, but this is not the task we aim to solve. We have not included in our experiments those hits that were made in less than 3 seconds, understanding that it takes more time to grasp the multimodal context and make a decision. We do a majority voting between the three annotations to get the tweets category. At the end, we obtain $112,845$ not hate tweets and $36,978$ hate tweets. The latest are divided in $11,925$ racist, $3,495$ sexist, $3,870$ homophobic, 163 religion-based hate and $5,811$ other hate tweets (Fig. FIGREF17). In this work, we do not use hate sub-categories, and stick to the hate / not hate split. We separate balanced validation ($5,000$) and test ($10,000$) sets. The remaining tweets are used for training. We also experimented using hate scores for each tweet computed given the different votes by the three annotators instead of binary labels. The results did not present significant differences to those shown in the experimental part of this work, but the raw annotations will be published nonetheless for further research. As far as we know, this dataset is the biggest hate speech dataset to date, and the first multimodal hate speech dataset. One of its challenges is to distinguish between tweets using the same key offensive words that constitute or not an attack to a community (hate speech). Fig. FIGREF18 shows the percentage of hate and not hate tweets of the top keywords.",Methodology ::: Unimodal Treatment ::: Images.,"All images are resized such that their shortest size has 500 pixels. During training, online data augmentation is applied as random cropping of $299\times 299$ patches and mirroring. We use a CNN as the image features extractor which is an Imagenet BIBREF24 pre-trained Google Inception v3 architecture BIBREF25. The fine-tuning process of the Inception v3 layers aims to modify its weights to extract the features that, combined with the textual information, are optimal for hate speech detection.",Methodology ::: Unimodal Treatment ::: Tweet Text.,"We train a single layer LSTM with a 150-dimensional hidden state for hate / not hate classification. The input dimensionality is set to 100 and GloVe BIBREF26 embeddings are used as word input representations. Since our dataset is not big enough to train a GloVe word embedding model, we used a pre-trained model that has been trained in two billion tweets. This ensures that the model will be able to produce word embeddings for slang and other words typically used in Twitter. To process the tweets text before generating the word embeddings, we use the same pipeline as the model authors, which includes generating symbols to encode Twitter special interactions such as user mentions (@user) or hashtags (#hashtag). To encode the tweet text and input it later to multimodal models, we use the LSTM hidden state after processing the last tweet word. Since the LSTM has been trained for hate speech classification, it extracts the most useful information for this task from the text, which is encoded in the hidden state after inputting the last tweet word.",Methodology ::: Unimodal Treatment ::: Image Text.,"The text in the image can also contain important information to decide if a publication is hate speech or not, so we extract it and also input it to our model. To do so, we use Google Vision API Text Detection module BIBREF27. We input the tweet text and the text from the image separately to the multimodal models, so it might learn different relations between them and between them and the image. For instance, the model could learn to relate the image text with the area in the image where the text appears, so it could learn to interpret the text in a different way depending on the location where it is written in the image. The image text is also encoded by the LSTM as the hidden state after processing its last word.",Methodology ::: Multimodal Architectures,"The objective of this work is to build a hate speech detector that leverages both textual and visual data and detects hate speech publications based on the context given by both data modalities. To study how the multimodal context can boost the performance compared to an unimodal context we evaluate different models: a Feature Concatenation Model (FCM), a Spatial Concatenation Model (SCM) and a Textual Kernels Model (TKM). All of them are CNN+RNN models with three inputs: the tweet image, the tweet text and the text appearing in the image (if any).",Methodology ::: Multimodal Architectures ::: Feature Concatenation Model (FCM),"The image is fed to the Inception v3 architecture and the 2048 dimensional feature vector after the last average pooling layer is used as the visual representation. This vector is then concatenated with the 150 dimension vectors of the LSTM last word hidden states of the image text and the tweet text, resulting in a 2348 feature vector. This vector is then processed by three fully connected layers of decreasing dimensionality $(2348, 1024, 512)$ with following corresponding batch normalization and ReLu layers until the dimensions are reduced to two, the number of classes, in the last classification layer. The FCM architecture is illustrated in Fig. FIGREF26.",Methodology ::: Multimodal Architectures ::: Spatial Concatenation Model (SCM),"Instead of using the latest feature vector before classification of the Inception v3 as the visual representation, in the SCM we use the $8\times 8\times 2048$ feature map after the last Inception module. Then we concatenate the 150 dimension vectors encoding the tweet text and the tweet image text at each spatial location of that feature map. The resulting multimodal feature map is processed by two Inception-E blocks BIBREF28. After that, dropout and average pooling are applied and, as in the FCM model, three fully connected layers are used to reduce the dimensionality until the classification layer.",Methodology ::: Multimodal Architectures ::: Textual Kernels Model (TKM),"The TKM design, inspired by BIBREF20 and BIBREF21, aims to capture interactions between the two modalities more expressively than concatenation models. As in SCM we use the $8\times 8\times 2048$ feature map after the last Inception module as the visual representation. From the 150 dimension vector encoding the tweet text, we learn $K_t$ text dependent kernels using independent fully connected layers that are trained together with the rest of the model. The resulting $K_t$ text dependent kernels will have dimensionality of $1\times 1\times 2048$. We do the same with the feature vector encoding the image text, learning $K_{it}$ kernels. The textual kernels are convolved with the visual feature map in the channel dimension at each spatial location, resulting in a $8\times 8\times (K_i+K_{it})$ multimodal feature map, and batch normalization is applied. Then, as in the SCM, the 150 dimension vectors encoding the tweet text and the tweet image text are concatenated at each spatial dimension. The rest of the architecture is the same as in SCM: two Inception-E blocks, dropout, average pooling and three fully connected layers until the classification layer. The number of tweet textual kernels $K_t$ and tweet image textual kernels $K_it$ is set to $K_t = 10$ and $K_it = 5$. The TKM architecture is illustrated in Fig. FIGREF29.",Methodology ::: Multimodal Architectures ::: Training,"We train the multimodal models with a Cross-Entropy loss with Softmax activations and an ADAM optimizer with an initial learning rate of $1e-4$. Our dataset suffers from a high class imbalance, so we weight the contribution to the loss of the samples to totally compensate for it. One of the goals of this work is to explore how every one of the inputs contributes to the classification and to prove that the proposed model can learn concurrences between visual and textual data useful to improve the hate speech classification results on multimodal data. To do that we train different models where all or only some inputs are available. When an input is not available, we set it to zeros, and we do the same when an image has no text.",Results,"Table TABREF31 shows the F-score, the Area Under the ROC Curve (AUC) and the mean accuracy (ACC) of the proposed models when different inputs are available. $TT$ refers to the tweet text, $IT$ to the image text and $I$ to the image. It also shows results for the LSTM, for the Davison method proposed in BIBREF7 trained with MMHS150K, and for random scores. Fig. FIGREF32 shows the Precision vs Recall plot and the ROC curve (which plots the True Positive Rate vs the False Positive Rate) of the different models. First, notice that given the subjectivity of the task and the discrepancies between annotators, getting optimal scores in the evaluation metrics is virtually impossible. However, a system with relatively low metric scores can still be very useful for hate speech detection in a real application: it will fire on publications for which most annotators agree they are hate, which are often the stronger attacks. The proposed LSTM to detect hate speech when only text is available, gets similar results as the method presented in BIBREF7, which we trained with MMHS150K and the same splits. However, more than substantially advancing the state of the art on hate speech detection in textual publications, our key purpose in this work is to introduce and work on its detection on multimodal publications. We use LSTM because it provides a strong representation of the tweet texts. The FCM trained only with images gets decent results, considering that in many publications the images might not give any useful information for the task. Fig. FIGREF33 shows some representative examples of the top hate and not hate scored images of this model. Many hate tweets are accompanied by demeaning nudity images, being sexist or homophobic. Other racist tweets are accompanied by images caricaturing black people. Finally, MEMES are also typically used in hate speech publications. The top scored images for not hate are portraits of people belonging to minorities. This is due to the use of slur inside these communities without an offensive intention, such as the word nigga inside the afro-american community or the word dyke inside the lesbian community. These results show that images can be effectively used to discriminate between offensive and non-offensive uses of those words. Despite the model trained only with images proves that they are useful for hate speech detection, the proposed multimodal models are not able to improve the detection compared to the textual models. Besides the different architectures, we have tried different training strategies, such as initializing the CNN weights with a model already trained solely with MMHS150K images or using dropout to force the multimodal models to use the visual information. Eventually, though, these models end up using almost only the text input for the prediction and producing very similar results to those of the textual models. The proposed multimodal models, such as TKM, have shown good performance in other tasks, such as VQA. Next, we analyze why they do not perform well in this task and with this data: [noitemsep,leftmargin=*] Noisy data. A major challenge of this task is the discrepancy between annotations due to subjective judgement. Although this affects also detection using only text, its repercussion is bigger in more complex tasks, such as detection using images or multimodal detection. Complexity and diversity of multimodal relations. Hate speech multimodal publications employ a lot of background knowledge which makes the relations between visual and textual elements they use very complex and diverse, and therefore difficult to learn by a neural network. Small set of multimodal examples. Fig. FIGREF5 shows some of the challenging multimodal hate examples that we aimed to detect. But although we have collected a big dataset of $150K$ tweets, the subset of multimodal hate there is still too small to learn the complex multimodal relations needed to identify multimodal hate.",Conclusions,"In this work we have explored the task of hate speech detection on multimodal publications. We have created MMHS150K, to our knowledge the biggest available hate speech dataset, and the first one composed of multimodal data, namely tweets formed by image and text. We have trained different textual, visual and multimodal models with that data, and found out that, despite the fact that images are useful for hate speech detection, the multimodal models do not outperform the textual models. Finally, we have analyzed the challenges of the proposed task and dataset. Given that most of the content in Social Media nowadays is multimodal, we truly believe on the importance of pushing forward this research. The code used in this work is available in .",,,,,,,,,What models do they propose?,6976296126e4a5c518e6b57de70f8dc8d8fde292,two,unfamiliar,no,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,,"The objective of this work is to build a hate speech detector that leverages both textual and visual data and detects hate speech publications based on the context given by both data modalities. To study how the multimodal context can boost the performance compared to an unimodal context we evaluate different models: a Feature Concatenation Model (FCM), a Spatial Concatenation Model (SCM) and a Textual Kernels Model (TKM). All of them are CNN+RNN models with three inputs: the tweet image, the tweet text and the text appearing in the image (if any).","To study how the multimodal context can boost the performance compared to an unimodal context we evaluate different models: a Feature Concatenation Model (FCM), a Spatial Concatenation Model (SCM) and a Textual Kernels Model (TKM)",e759a4245a5ac52632d3fbc424192e9e72b16350,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,Are all tweets in English?,53640834d68cf3b86cf735ca31f1c70aa0006b72,two,unfamiliar,no,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,True,,,,,594bca16b30968bbe0e3b0f68318f1788f732491,258ee4069f740c400c0049a2580945a1cc7f044c,How large is the dataset?,b2b0321b0aaf58c3aa9050906ade6ef35874c5c1,two,unfamiliar,no,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,,"Existing hate speech datasets contain only textual data. Moreover, a reference benchmark does not exists. Most of the published datasets are crawled from Twitter and distributed as tweet IDs but, since Twitter removes reported user accounts, an important amount of their hate tweets is no longer accessible. We create a new manually annotated multimodal hate speech dataset formed by $150,000$ tweets, each one of them containing text and an image. We call the dataset MMHS150K, and made it available online . In this section, we explain the dataset creation steps.","We create a new manually annotated multimodal hate speech dataset formed by $150,000$ tweets, each one of them containing text and an image. ",374b00290fe0a9a6f8f123d6dc04c1c2cb7ce619,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,What is the results of multimodal compared to unimodal models?,4e9684fd68a242cb354fa6961b0e3b5c35aae4b6,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,,"Unimodal LSTM vs Best Multimodal (FCM)
- F score: 0.703 vs 0.704
- AUC: 0.732 vs 0.734 
- Mean Accuracy: 68.3 vs 68.4 ","Table TABREF31 shows the F-score, the Area Under the ROC Curve (AUC) and the mean accuracy (ACC) of the proposed models when different inputs are available. $TT$ refers to the tweet text, $IT$ to the image text and $I$ to the image. It also shows results for the LSTM, for the Davison method proposed in BIBREF7 trained with MMHS150K, and for random scores. Fig. FIGREF32 shows the Precision vs Recall plot and the ROC curve (which plots the True Positive Rate vs the False Positive Rate) of the different models. FLOAT SELECTED: Table 1. Performance of the proposed models, the LSTM and random scores. The Inputs column indicate which inputs are available at training and testing time.","Table TABREF31 shows the F-score, the Area Under the ROC Curve (AUC) and the mean accuracy (ACC) of the proposed models when different inputs are available. FLOAT SELECTED: Table 1. Performance of the proposed models, the LSTM and random scores. The Inputs column indicate which inputs are available at training and testing time.",01747abc86fa3933552919b030e74fc9d6515178,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,What is author's opinion on why current multimodal models cannot outperform models analyzing only text?,2e632eb5ad611bbd16174824de0ae5efe4892daf,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,Noisy data Complexity and diversity of multimodal relations Small set of multimodal examples,,,"Despite the model trained only with images proves that they are useful for hate speech detection, the proposed multimodal models are not able to improve the detection compared to the textual models. Besides the different architectures, we have tried different training strategies, such as initializing the CNN weights with a model already trained solely with MMHS150K images or using dropout to force the multimodal models to use the visual information. Eventually, though, these models end up using almost only the text input for the prediction and producing very similar results to those of the textual models. The proposed multimodal models, such as TKM, have shown good performance in other tasks, such as VQA. Next, we analyze why they do not perform well in this task and with this data: [noitemsep,leftmargin=*] Noisy data. A major challenge of this task is the discrepancy between annotations due to subjective judgement. Although this affects also detection using only text, its repercussion is bigger in more complex tasks, such as detection using images or multimodal detection. Complexity and diversity of multimodal relations. Hate speech multimodal publications employ a lot of background knowledge which makes the relations between visual and textual elements they use very complex and diverse, and therefore difficult to learn by a neural network. Small set of multimodal examples. Fig. FIGREF5 shows some of the challenging multimodal hate examples that we aimed to detect. But although we have collected a big dataset of $150K$ tweets, the subset of multimodal hate there is still too small to learn the complex multimodal relations needed to identify multimodal hate.","Next, we analyze why they do not perform well in this task and with this data:

[noitemsep,leftmargin=*]

Noisy data. A major challenge of this task is the discrepancy between annotations due to subjective judgement. Although this affects also detection using only text, its repercussion is bigger in more complex tasks, such as detection using images or multimodal detection.

Complexity and diversity of multimodal relations. Hate speech multimodal publications employ a lot of background knowledge which makes the relations between visual and textual elements they use very complex and diverse, and therefore difficult to learn by a neural network.

Small set of multimodal examples. Fig. FIGREF5 shows some of the challenging multimodal hate examples that we aimed to detect. But although we have collected a big dataset of $150K$ tweets, the subset of multimodal hate there is still too small to learn the complex multimodal relations needed to identify multimodal hate.",06bfc3c0173c2bf9e8f0e7a34d8857be185f1310,258ee4069f740c400c0049a2580945a1cc7f044c,What metrics are used to benchmark the results?,d1ff6cba8c37e25ac6b261a25ea804d8e58e09c0,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,,,"Table TABREF31 shows the F-score, the Area Under the ROC Curve (AUC) and the mean accuracy (ACC) of the proposed models when different inputs are available. $TT$ refers to the tweet text, $IT$ to the image text and $I$ to the image. It also shows results for the LSTM, for the Davison method proposed in BIBREF7 trained with MMHS150K, and for random scores. Fig. FIGREF32 shows the Precision vs Recall plot and the ROC curve (which plots the True Positive Rate vs the False Positive Rate) of the different models.","Table TABREF31 shows the F-score, the Area Under the ROC Curve (AUC) and the mean accuracy (ACC) of the proposed models when different inputs are available. Fig. FIGREF32 shows the Precision vs Recall plot and the ROC curve (which plots the True Positive Rate vs the False Positive Rate) of the different models.",e054bc12188dfd93e3491fde76dc37247f91051d,258ee4069f740c400c0049a2580945a1cc7f044c,"How is data collected, manual collection or Twitter api?",24c0f3d6170623385283dfda7f2b6ca2c7169238,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,,,"We used the Twitter API to gather real-time tweets from September 2018 until February 2019, selecting the ones containing any of the 51 Hatebase terms that are more common in hate speech tweets, as studied in BIBREF9. We filtered out retweets, tweets containing less than three words and tweets containing porn related terms. From that selection, we kept the ones that included images and downloaded them. Twitter applies hate speech filters and other kinds of content control based on its policy, although the supervision is based on users' reports. Therefore, as we are gathering tweets from real-time posting, the content we get has not yet passed any filter.","We used the Twitter API to gather real-time tweets from September 2018 until February 2019, selecting the ones containing any of the 51 Hatebase terms that are more common in hate speech tweets, as studied in BIBREF9.",e2962aa33290adc42fdac994cdf8f77b90532666,258ee4069f740c400c0049a2580945a1cc7f044c,"How many tweats does MMHS150k contains, 150000?",21a9f1cddd7cb65d5d48ec4f33fe2221b2a8f62e,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,,,"Existing hate speech datasets contain only textual data. Moreover, a reference benchmark does not exists. Most of the published datasets are crawled from Twitter and distributed as tweet IDs but, since Twitter removes reported user accounts, an important amount of their hate tweets is no longer accessible. We create a new manually annotated multimodal hate speech dataset formed by $150,000$ tweets, each one of them containing text and an image. We call the dataset MMHS150K, and made it available online . In this section, we explain the dataset creation steps.","We create a new manually annotated multimodal hate speech dataset formed by $150,000$ tweets, each one of them containing text and an image. We call the dataset MMHS150K, and made it available online . In this section, we explain the dataset creation steps.",8b3d8e719caa03403c1779308c410d875d34f065,258ee4069f740c400c0049a2580945a1cc7f044c,What unimodal detection models were used?,a0ef0633d8b4040bf7cdc5e254d8adf82c8eed5e,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False, single layer LSTM with a 150-dimensional hidden state for hate / not hate classification,,,"We train a single layer LSTM with a 150-dimensional hidden state for hate / not hate classification. The input dimensionality is set to 100 and GloVe BIBREF26 embeddings are used as word input representations. Since our dataset is not big enough to train a GloVe word embedding model, we used a pre-trained model that has been trained in two billion tweets. This ensures that the model will be able to produce word embeddings for slang and other words typically used in Twitter. To process the tweets text before generating the word embeddings, we use the same pipeline as the model authors, which includes generating symbols to encode Twitter special interactions such as user mentions (@user) or hashtags (#hashtag). To encode the tweet text and input it later to multimodal models, we use the LSTM hidden state after processing the last tweet word. Since the LSTM has been trained for hate speech classification, it extracts the most useful information for this task from the text, which is encoded in the hidden state after inputting the last tweet word.",We train a single layer LSTM with a 150-dimensional hidden state for hate / not hate classification. The input dimensionality is set to 100 and GloVe BIBREF26 embeddings are used as word input representations.,fef9d96af320e166ea80854dd890bffc92143437,258ee4069f740c400c0049a2580945a1cc7f044c,2-Figure1-1.png,Figure 1. Tweets from MMHS150K where the visual information adds relevant context for the hate speech detection task.,3-Figure2-1.png,Figure 2. Percentage of tweets per class in MMHS150K.,4-Figure3-1.png,Figure 3. Percentage of hate and not hate tweets for top keywords of MMHS150K.,6-Figure4-1.png,Figure 4. FCM architecture. Image and text representations are concatenated and processed by a set of fully connected layers.,6-Figure5-1.png,"Figure 5. TKM architecture. Textual kernels are learnt from the text representations, and convolved with the image representation.",7-Table1-1.png,"Table 1. Performance of the proposed models, the LSTM and random scores. The Inputs column indicate which inputs are available at training and testing time.",,,,,,,,,," $150,000$ tweets",7-Figure7-1.png,Figure 7. Top scored examples for hate (top) and for not hate (bottom) for the FCM model trained only with images.,7-Figure6-1.png,"Figure 6. Precision vs Recall (left) and ROC curve (True Positive Rate vs False Positive Rate) (right) plots of the proposed models trained with the different inputs, the LSTM and random scores.",,,,,,,F-score Area Under the ROC Curve (AUC) mean accuracy (ACC) Precision vs Recall plot ROC curve (which plots the True Positive Rate vs the False Positive Rate),Feature Concatenation Model (FCM) Spatial Concatenation Model (SCM) Textual Kernels Model (TKM),,,,,,,,,,What different models for multimodal detection were proposed?,b0799e26152197aeb3aa3b11687a6cc9f6c31011,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,Feature Concatenation Model (FCM) Spatial Concatenation Model (SCM) Textual Kernels Model (TKM),,,"The objective of this work is to build a hate speech detector that leverages both textual and visual data and detects hate speech publications based on the context given by both data modalities. To study how the multimodal context can boost the performance compared to an unimodal context we evaluate different models: a Feature Concatenation Model (FCM), a Spatial Concatenation Model (SCM) and a Textual Kernels Model (TKM). All of them are CNN+RNN models with three inputs: the tweet image, the tweet text and the text appearing in the image (if any).","To study how the multimodal context can boost the performance compared to an unimodal context we evaluate different models: a Feature Concatenation Model (FCM), a Spatial Concatenation Model (SCM) and a Textual Kernels Model (TKM).",8c2edb685d8f82b80bc60d335a6b53a86b855bd1,258ee4069f740c400c0049a2580945a1cc7f044c,What annotations are available in the dataset - tweat used hate speach or not?,4ce4db7f277a06595014db181342f8cb5cb94626,zero,unfamiliar,no,,258ee4069f740c400c0049a2580945a1cc7f044c,False,No attacks to any community  racist sexist homophobic religion based attacks attacks to other communities,,,"We annotate the gathered tweets using the crowdsourcing platform Amazon Mechanical Turk. There, we give the workers the definition of hate speech and show some examples to make the task clearer. We then show the tweet text and image and we ask them to classify it in one of 6 categories: No attacks to any community, racist, sexist, homophobic, religion based attacks or attacks to other communities. Each one of the $150,000$ tweets is labeled by 3 different workers to palliate discrepancies among workers.","We then show the tweet text and image and we ask them to classify it in one of 6 categories: No attacks to any community, racist, sexist, homophobic, religion based attacks or attacks to other communities.",c528a0f56b7aa65eeafa53dcc5747d171f526879,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"$150,000$ tweets",Twitter API,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Drug-drug Interaction Extraction via Recurrent Neural Network with Multiple Attention Layers,"Drug-drug interaction (DDI) is a vital information when physicians and pharmacists intend to co-administer two or more drugs. Thus, several DDI databases are constructed to avoid mistakenly combined use. In recent years, automatically extracting DDIs from biomedical text has drawn researchers' attention. However, the existing work utilize either complex feature engineering or NLP tools, both of which are insufficient for sentence comprehension. Inspired by the deep learning approaches in natural language processing, we propose a recur- rent neural network model with multiple attention layers for DDI classification. We evaluate our model on 2013 SemEval DDIExtraction dataset. The experiments show that our model classifies most of the drug pairs into correct DDI categories, which outperforms the existing NLP or deep learning methods.",Introduction,"Drug-drug interaction (DDI) is a situation when one drug increases or decreases the effect of another drug BIBREF0 . Adverse drug reactions may cause severe side effect, if two or more medicines were taken and their DDI were not investigated in detail. DDI is a common cause of illness, even a cause of death BIBREF1 . Thus, DDI databases for clinical medication decisions are proposed by some researchers. These databases such as SFINX BIBREF2 , KEGG BIBREF3 , CredibleMeds BIBREF4 help physicians and pharmacists avoid most adverse drug reactions. Traditional DDI databases are manually constructed according to clinical records, scientific research and drug specifications. For instance, The sentence “With combined use, clinicians should be aware, when phenytoin is added, of the potential for reexacerbation of pulmonary symptomatology due to lowered serum theophylline concentrations BIBREF5 ”, which is from a pharmacotherapy report, describe the side effect of phenytoin and theophylline's combined use. Then this information on specific medicines will be added to DDI databases. As drug-drug interactions have being increasingly found, manually constructing DDI database would consume a lot of manpower and resources. There has been many efforts to automatically extract DDIs from natural language BIBREF0 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 , BIBREF12 , mainly medical literature and clinical records. These works can be divided into the following categories: To avoid complex feature engineering and NLP toolkits' usage, we employ deep learning approaches for sentence comprehension as a whole. Our model takes in a sentence from biomedical literature which contains a drug pair and outputs what kind of DDI this drug pair belongs. This assists physicians refrain from improper combined use of drugs. In addition, the word and sentence level attentions are introduced to our model for better DDI predictions. We train our language comprehension model with labeled instances. Figure FIGREF5 shows partial records in DDI corpus BIBREF16 . We extract the sentence and drug pairs in the records. There are 3 drug pairs in this example thus we have 3 instances. The DDI corpus annotate each drug pair in the sentence with a DDI type. The DDI type, which is the most concerned information, is described in table TABREF4 . The details about how we train our model and extract the DDI type from text are described in the remaining sections.",Related Work,"In DDI extraction task, NLP methods or machine learning approaches are proposed by most of the work. Chowdhury BIBREF14 and Thomas et al. BIBREF11 proposed methods that use linguistic phenomenons and two-stage SVM to classify DDIs. FBK-irst BIBREF10 is a follow-on work which applies kernel method to the existing model and outperforms it. Neural network based approaches have been proposed by several works. Liu et al. BIBREF9 employ CNN for DDI extraction for the first time which outperforms the traditional machine learning based methods. Limited by the convolutional kernel size, the CNN can only extracted features of continuous 3 to 5 words rather than distant words. Liu et al. BIBREF8 proposed dependency-based CNN to handle distant but relevant words. Sahu et al. BIBREF12 proposed LSTM based DDI extraction approach and outperforms CNN based approach, since LSTM handles sentence as a sequence instead of slide windows. To conclude, Neural network based approaches have advantages of 1) less reliance on extra NLP toolkits, 2) simpler preprocessing procedure, 3) better performance than text analysis and machine learning methods. Drug-drug interaction extraction is a relation extraction task of natural language processing. Relation extraction aims to determine the relation between two given entities in a sentence. In recent years, attention mechanism and various neural networks are applied to relation extraction BIBREF17 , BIBREF18 , BIBREF19 , BIBREF20 , BIBREF21 . Convolutional deep neural network are utilized for extracting sentence level features in BIBREF19 . Then the sentence level features are concatenated with lexical level features, which are obtained by NLP toolkit WordNet BIBREF22 , followed by a multilayer perceptron (MLP) to classify the entities' relation. A fixed work is proposed by Nguyen et al. BIBREF21 . The convolutional kernel is set various size to capture more n-gram features. In addition, the word and position embedding are trained automatically instead of keeping constant as in BIBREF19 . Wang et al. BIBREF20 introduce multi-level attention mechanism to CNN in order to emphasize the keywords and ignore the non-critical words during relation detection. The attention CNN model outperforms previous state-of-the-art methods. Besides CNN, Recurrent neural network (RNN) has been applied to relation extraction as well. Zhang et al. BIBREF18 utilize long short-term memory network (LSTM), a typical RNN model, to represent sentence. The bidirectional LSTM chronologically captures the previous and future information, after which a pooling layer and MLP have been set to extract feature and classify the relation. Attention mechanism is added to bidirectional LSTM in BIBREF17 for relation extraction. An attention layer gives each memory cell a weight so that classifier can catch the principal feature for the relation detection. The Attention based bidirectional LSTM has been proven better than previous work.",Proposed Model,"In this section, we present our bidirectional recurrent neural network with multiple attention layer model. The overview of our architecture is shown in figure FIGREF15 . For a given instance, which describes the details about two or more drugs, the model represents each word as a vector in embedding layer. Then the bidirectional RNN layer generates a sentence matrix, each column vector in which is the semantic representation of the corresponding word. The word level attention layer transforms the sentence matrix to vector representation. Then sentence level attention layer generates final representation for the instance by combining several relevant sentences in view of the fact that these sentences have the same drug pair. Followed by a softmax classifier, the model classifies the drug pair in the given instance as specific DDI.",Preprocessing,"The DDI corpus contains thousands of XML files, each of which are constructed by several records. For a sentence containing INLINEFORM0 drugs, there are INLINEFORM1 drug pairs. We replace the interested two drugs with “drug1” and “drug2” while the other drugs are replaced by “durg0”, as in BIBREF9 did. This step is called drug blinding. For example, the sentence in figure FIGREF5 generates 3 instances after drug blinding: “drug1: an increased risk of hepatitis has been reported to result from combined use of drug2 and drug0”, “drug1: an increased risk of hepatitis has been reported to result from combined use of drug0 and drug2”, “drug0: an increased risk of hepatitis has been reported to result from combined use of drug1 and drug2”. The drug blinded sentences are the instances that are fed to our model. We put the sentences with the same drug pairs together as a set, since the sentence level attention layer (will be described in Section SECREF21 ) will use the sentences which contain the same drugs.",Embedding Layer,"Given an instance INLINEFORM0 which contains specified two drugs INLINEFORM1 , INLINEFORM2 , each word is embedded in a INLINEFORM3 dimensional space ( INLINEFORM4 , INLINEFORM5 are the dimension of word embedding and position embedding). The look up table function INLINEFORM6 maps a word or a relative position to a column vector. After embedding layer the sentence is represented by INLINEFORM7 , where DISPLAYFORM0  The INLINEFORM0 function is usually implemented with matrix-vector product. Let INLINEFORM1 , INLINEFORM2 denote the one-hot representation (column vector) of word and relative distance. INLINEFORM3 , INLINEFORM4 are word and position embedding query matrix. The look up functions are implemented by DISPLAYFORM0  Then the word sequence INLINEFORM0 is fed to the RNN layer. Note that the sentence will be filled with INLINEFORM1 if its length is less than INLINEFORM2 .",Bidirectional RNN Encoding Layer,"The words in the sequence are read by RNN's gated recurrent unit (GRU) one by one. The GRU takes the current word INLINEFORM0 and the previous GRU's hidden state INLINEFORM1 as input. The current GRU encodes INLINEFORM2 and INLINEFORM3 into a new hidden state INLINEFORM4 (its dimension is INLINEFORM5 , a hyperparameter), which can be regarded as informations the GRU remembered. Figure FIGREF25 shows the details in GRU. The reset gate INLINEFORM0 selectively forgets informations delivered by previous GRU. Then the hidden state becomes INLINEFORM1 . The update gate INLINEFORM2 updates the informations according to INLINEFORM3 and INLINEFORM4 . The equations below describe these procedures. Note that INLINEFORM5 stands for element wise multiplication. DISPLAYFORM0 DISPLAYFORM1  The bidirectional RNN contains forward RNN and backward RNN. Forward RNN reads sentence from INLINEFORM0 to INLINEFORM1 , generating INLINEFORM2 , INLINEFORM3 , ..., INLINEFORM4 . Backward RNN reads sentence from INLINEFORM5 to INLINEFORM6 , generating INLINEFORM7 , INLINEFORM8 , ..., INLINEFORM9 . Then the encode result of this layer is DISPLAYFORM0  We apply dropout technique in RNN layer to avoid overfitting. Each GRU have a probability (denoted by INLINEFORM0 , also a hyperparameter) of being dropped. The dropped GRU has no output and will not affect the subsequent GRUs. With bidirectional RNN and dropout technique, the input INLINEFORM1 is encoded into sentence matrix INLINEFORM2 .",Word Level Attention,"The purpose of word level attention layer is to extract sentence representation (also known as feature vector) from encoded matrix. We use word level attention instead of max pooling, since attention mechanism can determine the importance of individual encoded word in each row of INLINEFORM0 . Let INLINEFORM1 denotes the attention vector (column vector), INLINEFORM2 denotes the filter that gives each element in the row of INLINEFORM3 a weight. The following equations shows the attention operation, which is also illustrated in figure FIGREF15 . DISPLAYFORM0 DISPLAYFORM1  The softmax function takes a vector INLINEFORM0 as input and outputs a vector, DISPLAYFORM0   INLINEFORM0 denotes the feature vector captured by this layer. Several approaches BIBREF12 , BIBREF17 use this vector and softmax classifier for classification. Inspired by BIBREF23 we propose the sentence level attention to combine the information of other sentences for a improved DDI classification.",Sentence Level Attention,"The previous layers captures the features only from the given sentence. However, other sentences may contains informations that contribute to the understanding of this sentence. It is reasonable to look over other relevant instances when determine two drugs' interaction from the given sentence. In our implementation, the instances that have the same drug pair are believed to be relevant. The relevant instances set is denoted by INLINEFORM0 , where INLINEFORM1 is the sentence feature vector. INLINEFORM2 stands for how well the instance INLINEFORM3 matches its DDI INLINEFORM4 (Vector representation of a specific DDI). INLINEFORM5 is a diagonal attention matrix, multiplied by which the feature vector INLINEFORM6 can concentrate on those most representative features. DISPLAYFORM0 DISPLAYFORM1   INLINEFORM0 is the softmax result of INLINEFORM1 . The final sentence representation is decided by all of the relevant sentences' feature vector, as Equation EQREF24 shows. DISPLAYFORM0  Note that the set INLINEFORM0 is gradually growing as new sentence with the same drugs pairs is found when training. An instance INLINEFORM1 is represented by INLINEFORM2 before sentence level attention. The sentence level attention layer finds the set INLINEFORM3 , instances in which have the same drug pair as in INLINEFORM4 , and put INLINEFORM5 in INLINEFORM6 . Then the final sentence representation INLINEFORM7 is calculated in this layer.",Classification and Training,"A given sentence INLINEFORM0 is finally represented by the feature vector INLINEFORM1 . Then we feed it to a softmax classifier. Let INLINEFORM2 denotes the set of all kinds of DDI. The output INLINEFORM3 is the probabilities of each class INLINEFORM4 belongs. DISPLAYFORM0  We use cross entropy cost function and INLINEFORM0 regularization as the optimization objective. For INLINEFORM1 -th instance, INLINEFORM2 denotes the one-hot representation of it's label, where the model outputs INLINEFORM3 . The cross entropy cost is: DISPLAYFORM0  For a mini-batch INLINEFORM0 , the optimization objective is: DISPLAYFORM0  All parameters in this model is: DISPLAYFORM0  We optimize the parameters of objective function INLINEFORM0 with Adam BIBREF24 , which is a variant of mini-batch stochastic gradient descent. During each train step, the gradient of INLINEFORM1 is calculated. Then INLINEFORM2 is adjusted according to the gradient. After the end of training, we have a model that is able to predict two drugs' interactions when a sentence about these drugs is given.",DDI Prediction,"The model is trained for DDI classification. The parameters in list INLINEFORM0 are tuned during the training process. Given a new sentence with two drugs, we can use this model to classify the DDI type. The DDI prediction follows the procedure described in Section SECREF6 - SECREF26 . The given sentence is eventually represented by feature vector INLINEFORM0 . Then INLINEFORM1 is classified to a specific DDI type with a softmax classifier. In next section, we will evaluate our model's DDI prediction performance and see the advantages and shortcomings of our model.",Datasets and Evaluation Metrics,"We use the DDI corpus of the 2013 DDIExtraction challenge BIBREF16 to train and test our model. The DDIs in this corpus are classified as five types. We give the definitions of these types and their example sentences, as shown in table TABREF4 . This standard dataset is made up of training set and testing set. We use the same metrics as in other drug-drug interaction extraction literature BIBREF11 , BIBREF10 , BIBREF25 , BIBREF9 , BIBREF8 , BIBREF12 : the overall precision, recall, and F1 score on testing set. INLINEFORM0 denotes the set of {False, Mechanism, Effect, Advise, Int}. The precision and recall of each INLINEFORM1 are calculated by DISPLAYFORM0 DISPLAYFORM1  Then the overall precision, recall, and F1 score are calculated by DISPLAYFORM0  Besides, we evaluate the captured feature vectors with t-SNE BIBREF26 , a visualizing and intuitive way to map a high dimensional vector into a 2 or 3-dimensional space. If the points in a low dimensional space are easy to be split, the feature vectors are believed to be more distinguishable.",Hyperparameter Settings and Training,"We use TensorFlow BIBREF27 r0.11 to implement the proposed model. The input of each word is an ordered triple (word, relative distance from drug1, relative distance from drug2). The sentence, which is represented as a matrix, is fed to the model. The output of the model is a INLINEFORM0 -dimensional vector representing the probabilities of being corresponding DDI. It is the network, parameters, and hyperparameters which decides the output vector. The network's parameters are adjusted during training, where the hyperparameters are tuned by hand. The hyperparameters after tuning are as follows. The word embedding's dimension INLINEFORM1 , the position embedding's dimension INLINEFORM2 , the hidden state's dimension INLINEFORM3 , the probability of dropout INLINEFORM4 , other hyperparameters which are not shown here are set to TensorFlow's default values. The word embedding is initialized by pre-trained word vectors using GloVe BIBREF28 , while other parameters are initialized randomly. During each training step, a mini-batch (the mini-batch size INLINEFORM0 in our implementation) of sentences is selected from training set. The gradient of objective function is calculated for parameters updating (See Section SECREF26 ). Figure FIGREF32 shows the training process. The objective function INLINEFORM0 is declining as the training mini-batches continuously sent to the model. As the testing mini-batches, the INLINEFORM1 function is fluctuating while its overall trend is descending. The instances in testing set are not participated in training so that INLINEFORM2 function is not descending so fast. However, training and testing instances have similar distribution in sample space, causing that testing instances' INLINEFORM3 tends to be smaller along with the training process. INLINEFORM4 has inverse relationship with the performance measurement. The F1 score is getting fluctuating around a specific value after enough training steps. The reason why fluctuating range is considerable is that only a tiny part of the whole training or testing set has been calculated the F1 score. Testing the whole set during every step is time consuming and not necessary. We will evaluate the model on the whole testing set in Section SECREF47 .",Experimental Results,"We save our model every 100 step and predict all the DDIs of the instances in the testing set. These predictions' F1 score is shown in figure FIGREF40 . To demonstrate the sentence level attention layer is effective, we drop this layer and then directly use INLINEFORM0 for softmax classification (See figure FIGREF15 ). The result is shown with “RNN + dynamic word embedding + ATT” curve, which illustrates that the sentence level attention layer contributes to a more accurate model. Whether a dynamic or static word embedding is better for a DDI extraction task is under consideration. Nguyen et al. BIBREF21 shows that updating word embedding at the time of other parameters being trained makes a better performance in relation extraction task. We let the embedding be static when training, while other conditions are all the same. The “RNN + static word embedding + 2ATT” curve shows this case. We can draw a conclusion that updating the initialized word embedding trains more suitable word vectors for the task, which promotes the performance. We compare our best F1 score with other state-of-the-art approaches in table TABREF39 , which shows our model has competitive advantage in dealing with drug-drug interaction extraction. The predictions confusion matrix is shown in table TABREF46 . The DDIs other than false being classified as false makes most of the classification error. It may perform better if a classifier which can tells true and false DDI apart is trained. We leave this two-stage classifier to our future work. Another phenomenon is that the “Int” type is often classified as “Effect”. The “Int” sentence describes there exists interaction between two drugs and this information implies the two drugs' combination will have good or bed effect. That's the reason why “Int” and “Effect” are often obfuscated. To evaluate the features our model captured, we employ scikit-learn BIBREF29 's t-SNE class to map high dimensional feature vectors to 2-dimensional vectors, which can be depicted on a plane. We depict all the features of the instances in testing set, as shown in figure FIGREF41 . The RNN model using dynamic word embedding and 2 layers of attention is the most distinguishable one. Unfortunately, the classifier can not classify all the instances into correct classes. Comparing table TABREF46 with figure UID44 , both of which are from the best performed model, we can observe some conclusions. The “Int” DDIs are often misclassified as “Effect”, for the reason that some of the “Int” points are in the “Effect” cluster. The “Effect” points are too scattered so that plenty of “Effect” DDIs are classified to other types. The “Mechanism” points are gathered around two clusters, causing that most of the “mechanism” DDIs are classified to two types: “False” and “Mechanism”. In short, the visualizability of feature mapping gives better explanations for the prediction results and the quality of captured features.",Conclusion and Future Work,"To conclude, we propose a recurrent neural network with multiple attention layers to extract DDIs from biomedical text. The sentence level attention layer, which combines other sentences containing the same drugs, has been added to our model. The experiments shows that our model outperforms the state-of-the-art DDI extraction systems. Task relevant word embedding and two attention layers improved the performance to some extent. The imbalance of the classes and the ambiguity of semantics cause most of the misclassifications. We consider that instance generation using generative adversarial networks would cover the instance shortage in specific category. It is also reasonable to use distant supervision learning (which utilize other relevant material) for knowledge supplement and obtain a better performed DDI extraction system.",Acknowledgment,"This work is supported by the NSFC under Grant 61303191, 61303190, 61402504, 61103015.",,,,,,,,,,,,,How big is the evaluated dataset?,b42323d60827ecf0d9e478c9a31f90940cfae975,,,,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,,"The DDI corpus contains thousands of XML files, each of which are constructed by several records. For a sentence containing INLINEFORM0 drugs, there are INLINEFORM1 drug pairs. We replace the interested two drugs with “drug1” and “drug2” while the other drugs are replaced by “durg0”, as in BIBREF9 did. This step is called drug blinding. For example, the sentence in figure FIGREF5 generates 3 instances after drug blinding: “drug1: an increased risk of hepatitis has been reported to result from combined use of drug2 and drug0”, “drug1: an increased risk of hepatitis has been reported to result from combined use of drug0 and drug2”, “drug0: an increased risk of hepatitis has been reported to result from combined use of drug1 and drug2”. The drug blinded sentences are the instances that are fed to our model.","The DDI corpus contains thousands of XML files, each of which are constructed by several records. For a sentence containing INLINEFORM0 drugs, there are INLINEFORM1 drug pairs.",a7063c73663ca3470b2f8c60c0a294efee32a10b,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,By how much does their model outperform existing methods?,1a69696034f70fb76cd7bb30494b2f5ab97e134d,,,,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,Answer with content missing: (Table II) Proposed model has F1 score of  0.7220 compared to 0.7148 best state-state-of-the-art result.,"We compare our best F1 score with other state-of-the-art approaches in table TABREF39 , which shows our model has competitive advantage in dealing with drug-drug interaction extraction. The predictions confusion matrix is shown in table TABREF46 . The DDIs other than false being classified as false makes most of the classification error. It may perform better if a classifier which can tells true and false DDI apart is trained. We leave this two-stage classifier to our future work. Another phenomenon is that the “Int” type is often classified as “Effect”. The “Int” sentence describes there exists interaction between two drugs and this information implies the two drugs' combination will have good or bed effect. That's the reason why “Int” and “Effect” are often obfuscated.","We compare our best F1 score with other state-of-the-art approaches in table TABREF39 , which shows our model has competitive advantage in dealing with drug-drug interaction extraction.",ba9d206b09685047828f3f50ccbd464fbadee940,258ee4069f740c400c0049a2580945a1cc7f044c,What is the performance of their model?,9a596bd3a1b504601d49c2bec92d1592d7635042,,,,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,Answer with content missing: (Table II) Proposed model has F1 score of  0.7220.,"We compare our best F1 score with other state-of-the-art approaches in table TABREF39 , which shows our model has competitive advantage in dealing with drug-drug interaction extraction. The predictions confusion matrix is shown in table TABREF46 . The DDIs other than false being classified as false makes most of the classification error. It may perform better if a classifier which can tells true and false DDI apart is trained. We leave this two-stage classifier to our future work. Another phenomenon is that the “Int” type is often classified as “Effect”. The “Int” sentence describes there exists interaction between two drugs and this information implies the two drugs' combination will have good or bed effect. That's the reason why “Int” and “Effect” are often obfuscated.","We compare our best F1 score with other state-of-the-art approaches in table TABREF39 , which shows our model has competitive advantage in dealing with drug-drug interaction extraction.",2e5e8cf9a787c14ed9bdbdf552c24a103e2ff467,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,What are the existing methods mentioned in the paper?,1ba28338d3f993674a19d2ee2ec35447e361505b,,,,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,,"In DDI extraction task, NLP methods or machine learning approaches are proposed by most of the work. Chowdhury BIBREF14 and Thomas et al. BIBREF11 proposed methods that use linguistic phenomenons and two-stage SVM to classify DDIs. FBK-irst BIBREF10 is a follow-on work which applies kernel method to the existing model and outperforms it. Neural network based approaches have been proposed by several works. Liu et al. BIBREF9 employ CNN for DDI extraction for the first time which outperforms the traditional machine learning based methods. Limited by the convolutional kernel size, the CNN can only extracted features of continuous 3 to 5 words rather than distant words. Liu et al. BIBREF8 proposed dependency-based CNN to handle distant but relevant words. Sahu et al. BIBREF12 proposed LSTM based DDI extraction approach and outperforms CNN based approach, since LSTM handles sentence as a sequence instead of slide windows. To conclude, Neural network based approaches have advantages of 1) less reliance on extra NLP toolkits, 2) simpler preprocessing procedure, 3) better performance than text analysis and machine learning methods.","Chowdhury BIBREF14 and Thomas et al. BIBREF11 proposed methods that use linguistic phenomenons and two-stage SVM to classify DDIs. FBK-irst BIBREF10 is a follow-on work which applies kernel method to the existing model and outperforms it. Neural network based approaches have been proposed by several works. Liu et al. BIBREF9 employ CNN for DDI extraction for the first time which outperforms the traditional machine learning based methods.  Sahu et al. BIBREF12 proposed LSTM based DDI extraction approach and outperforms CNN based approach, since LSTM handles sentence as a sequence instead of slide windows.",938e10deafaf39e71819eaf3d98e10b47f81b839,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2-TableI-1.png,TABLE I THE DDI TYPES AND CORRESPONDING EXAMPLES,2-Figure1-1.png,Fig. 1. Partial records in DDI corpus,3-Figure2-1.png,Fig. 2. The bidirectional recurrent neural network with multiple attentions,4-Figure3-1.png,Fig. 3. The Gated Recurrent Unit,5-Figure4-1.png,Fig. 4. The objective function and F1 in the train process,5-Figure5-1.png,Fig. 5. The F1 scores on the whole testing set,,,,,,,,,,,6-TableII-1.png,TABLE II PERFORMANCE COMPARISON WITH OTHER APPROACHES,6-Figure6-1.png,Fig. 6. The features which mapped to 2D,6-TableIII-1.png,TABLE III PREDICTION RESULTS,,,,,,"contains thousands of XML files, each of which are constructed by several records",,,,,,,,,Chowdhury BIBREF14 and Thomas et al. BIBREF11 FBK-irst BIBREF10 Liu et al. BIBREF9 Sahu et al. BIBREF12,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Component Analysis for Visual Question Answering Architectures,"Recent research advances in Computer Vision and Natural Language Processing have introduced novel tasks that are paving the way for solving AI-complete problems. One of those tasks is called Visual Question Answering (VQA). A VQA system must take an image and a free-form, open-ended natural language question about the image, and produce a natural language answer as the output. Such a task has drawn great attention from the scientific community, which generated a plethora of approaches that aim to improve the VQA predictive accuracy. Most of them comprise three major components: (i) independent representation learning of images and questions; (ii) feature fusion so the model can use information from both sources to answer visual questions; and (iii) the generation of the correct answer in natural language. With so many approaches being recently introduced, it became unclear the real contribution of each component for the ultimate performance of the model. The main goal of this paper is to provide a comprehensive analysis regarding the impact of each component in VQA models. Our extensive set of experiments cover both visual and textual elements, as well as the combination of these representations in form of fusion and attention mechanisms. Our major contribution is to identify core components for training VQA models so as to maximize their predictive performance.",Introduction,"Recent research advances in Computer Vision (CV) and Natural Language Processing (NLP) introduced several tasks that are quite challenging to be solved, the so-called AI-complete problems. Most of those tasks require systems that understand information from multiple sources, i.e., semantics from visual and textual data, in order to provide some kind of reasoning. For instance, image captioning BIBREF0, BIBREF1, BIBREF2 presents itself as a hard task to solve, though it is actually challenging to quantitatively evaluate models on that task, and that recent studies BIBREF3 have raised questions on its AI-completeness. The Visual Question Answering (VQA) BIBREF3 task was introduced as an attempt to solve that issue: to be an actual AI-complete problem whose performance is easy to evaluate. It requires a system that receives as input an image and a free-form, open-ended, natural-language question to produce a natural-language answer as the output BIBREF3. It is a multidisciplinary topic that is gaining popularity by encompassing CV and NLP into a single architecture, what is usually regarded as a multimodal model BIBREF4, BIBREF5, BIBREF6. There are many real-world applications for models trained for Visual Question Answering, such as automatic surveillance video queries BIBREF7 and visually-impaired aiding BIBREF8, BIBREF9. Models trained for VQA are required to understand the semantics from images while finding relationships with the asked question. Therefore, those models must present a deep understanding of the image to properly perform inference and produce a reasonable answer to the visual question BIBREF10. In addition, it is much easier to evaluate this task since there is a finite set of possible answers for each image-question pair. Traditionally, VQA approaches comprise three major steps: (i) representation learning of the image and the question; (ii) projection of a single multimodal representation through fusion and attention modules that are capable of leveraging both visual and textual information; and (iii) the generation of the natural language answer to the question at hand. This task often requires sophisticated models that are able to understand a question expressed in text, identify relevant elements of the image, and evaluate how these two inputs correlate. Given the current interest of the scientific community in VQA, many recent advances try to improve individual components such as the image encoder, the question representation, or the fusion and attention strategies to better leverage both information sources. With so many approaches currently being introduced at the same time, it becomes unclear the real contribution and importance of each component within the proposed models. Thus, the main goal of this work is to understand the impact of each component on a proposed baseline architecture, which draws inspiration from the pioneer VQA model BIBREF3 (Fig. FIGREF1). Each component within that architecture is then systematically tested, allowing us to understand its impact on the system's final performance through a thorough set of experiments and ablation analysis. More specifically, we observe the impact of: (i) pre-trained word embeddings BIBREF11, BIBREF12, recurrent BIBREF13 and transformer-based sentence encoders BIBREF14 as question representation strategies; (ii) distinct convolutional neural networks used for visual feature extraction BIBREF15, BIBREF16, BIBREF17; and (iii) standard fusion strategies, as well as the importance of two main attention mechanisms BIBREF18, BIBREF19. We notice that even using a relatively simple baseline architecture, our best models are competitive to the (maybe overly-complex) state-of-the-art models BIBREF20, BIBREF21. Given the experimental nature of this work, we have trained over 130 neural network models, accounting for more than 600 GPU processing hours. We expect our findings to be useful as guidelines for training novel VQA models, and that they serve as a basis for the development of future architectures that seek to maximize predictive performance.",Related Work,"The task of VAQ has gained attention since Antol et al. BIBREF3 presented a large-scale dataset with open-ended questions. Many of the developed VQA models employ a very similar architecture BIBREF3, BIBREF22, BIBREF23, BIBREF24, BIBREF25, BIBREF26, BIBREF27: they represent images with features from pre-trained convolutional neural networks; they use word embeddings or recurrent neural networks to represent questions and/or answers; and they combine those features in a classification model over possible answers. Despite their wide adoption, RNN-based models suffer from their limited representation power BIBREF28, BIBREF29, BIBREF30, BIBREF31. Some recent approaches have investigated the application of the Transformer model BIBREF32 to tasks that incorporate visual and textual knowledge, as image captioning BIBREF28. Attention-based methods are also being continuously investigated since they enable reasoning by focusing on relevant objects or regions in original input features. They allow models to pay attention on important parts of visual or textual inputs at each step of a task. Visual attention models focus on small regions within an image to extract important features. A number of methods have adopted visual attention to benefit visual question answering BIBREF27, BIBREF33, BIBREF34. Recently, dynamic memory networks BIBREF27 integrate an attention mechanism with a memory module, and multimodal bilinear pooling BIBREF22, BIBREF20, BIBREF35 is exploited to expressively combine multimodal features and predict attention over the image. These methods commonly employ visual attention to find critical regions, but textual attention has been rarely incorporated into VQA systems. While all the aforementioned approaches have exploited those kind of mechanisms, in this paper we study the impact of such choices specifically for the task of VQA, and create a simple yet effective model. Burns et al. BIBREF36 conducted experiments comparing different word embeddings, language models, and embedding augmentation steps on five multimodal tasks: image-sentence retrieval, image captioning, visual question answering, phrase grounding, and text-to-clip retrieval. While their work focuses on textual experiments, our experiments cover both visual and textual elements, as well as the combination of these representations in form of fusion and attention mechanisms. To the best of our knowledge, this is the first paper that provides a comprehensive analysis on the impact of each major component within a VQA architecture.",Impact of VQA Components,"In this section we first introduce the baseline approach, with default image and text encoders, alongside a pre-defined fusion strategy. That base approach is inspired by the pioneer of Antol et al. on VQA BIBREF3. To understand the importance of each component, we update the base architecture according to each component we are investigating. In our baseline model we replace the VGG network from BIBREF19 by a Faster RCNN pre-trained in the Visual Genome dataset BIBREF37. The default text encoding is given by the last hidden-state of a Bidirectional LSTM network, instead of the concatenation of the last hidden-state and memory cell used in the original work. Fig. FIGREF1 illustrates the proposed baseline architecture, which is subdivided into three major segments: independent feature extraction from (1) images and (2) questions, as well as (3) the fusion mechanism responsible to learn cross-modal features. The default text encoder (denoted by the pink rectangle in Fig. FIGREF1) employed in this work comprises a randomly initialized word-embedding module that takes a tokenized question and returns a continuum vector for each token. Those vectors are used to feed an LSTM network. The last hidden-state is used as the question encoding, which is projected with a linear layer into a $d$-dimensional space so it can be fused along to the visual features. As the default option for the LSTM network, we use a single layer with 2048 hidden units. Given that this text encoding approach is fully trainable, we hereby name it Learnable Word Embedding (LWE). For the question encoding, we explore pre-trained and randomly initialized word-embeddings in various settings, including Word2Vec (W2V) BIBREF12 and GloVe BIBREF11. We also explore the use of hidden-states of Skip-Thoughts Vector BIBREF13 and BERT BIBREF14 as replacements for word-embeddings and sentence encoding approaches. Regarding the visual feature extraction (depicted as the green rectangle in Fig. FIGREF1), we decided to use the pre-computed features proposed in BIBREF19. Such an architecture employs a ResNet-152 with a Faster-RCNN BIBREF15 fine-tuned on the Visual Genome dataset. We opted for this approach due to the fact that using pre-computed features is far more computationally efficient, allowing us to train several models with distinct configurations. Moreover, several recent approaches BIBREF20, BIBREF21, BIBREF38 employ that same strategy as well, making it easier to provide fair comparison to the state-of-the-art approaches. In this study we perform experiments with two additional networks widely used for the task at hand, namely VGG-16 BIBREF16 and ReSNet-101 BIBREF17. Given the multimodal nature of the problem we are dealing with, it is quite challenging to train proper image and question encoders so as to capture relevant semantic information from both of them. Nevertheless, another essential aspect of the architecture is the component that merges them altogether, allowing for the model to generate answers based on both information sources BIBREF39. The process of multimodal fusion consists itself in a research area with many approaches being recently proposed BIBREF20, BIBREF40, BIBREF22, BIBREF41. The fusion module receives the extracted image and query features, and provides multimodal features that theoretically present information that allows the system to answer to the visual question. There are many fusion strategies that can either assume quite simple forms, such as vector multiplication or concatenation, or be really complex, involving multilayered neural networks, tensor decomposition, and bi-linear pooling, just to name a few. Following BIBREF3, we adopt the element-wise vector multiplication (also referred as Hadamard product) as the default fusion strategy. This approach requires the feature representations to be fused to have the same dimensionality. Therefore, we project them using a fully-connected layer to reduce their dimension from 2048 to 1024. After being fused together, the multimodal features are finally passed through a fully-connected layer that provides scores (logits) further converted into probabilities via a softmax function ($S$). We want to maximize the probability $P(Y=y|X=x,Q=q)$ of the correct answer $y$ given the image $X$ and the provided question $Q$. Our models are trained to choose within a set comprised by the 3000 most frequent answers extracted from both training and validation sets of the VQA v2.0 dataset BIBREF42.",Experimental Setup ::: Dataset,"For conducting this study we decided to use the VQA v2.0 dataset BIBREF42. It is one of the largest and most frequently used datasets for training and evaluation of models in this task, being the official dataset used in yearly challenges hosted by mainstream computer vision venues . This dataset enhances the original one BIBREF3 by alleviating bias problems within the data and increasing the original number of instances. VQA v2.0 contains over $200,000$ images from MSCOCO BIBREF43, over 1 million questions and $\approx 11$ million answers. In addition, it has at least two questions per image, which prevents the model from answering the question without considering the input image. We follow VQA v2.0 standards and adopt the official provided splits allowing for fair comparison with other approaches. The splits we use are Validation, Test-Dev, Test-Standard. In this work, results of the ablation experiments are reported on the Validation set, which is the default option used for this kind of experiment. In some experiments we also report the training set accuracy to verify evidence of overfitting due to excessive model complexity. Training data has a total of $443,757$ questions labeled with 4 million answers, while the Test-Dev has a total of $214,354$ questions. Note that the validation size is about 4-fold larger than ImageNet's, which contains about $50,000$ samples. Therefore, one must keep in mind that even small performance gaps might indicate quite significant results improvement. For instance, 1% accuracy gains depict $\approx 2,000$ additional instances being correctly classified. We submit the predictions of our best models to the online evaluation servers BIBREF44 so as to obtain results for the Test-Standard split, allowing for a fair comparison to state-of-the-art approaches.",Experimental Setup ::: Evaluation Metric,"Free and open-ended questions result in a diverse set of possible answers BIBREF3. For some questions, a simple yes or no answer may be sufficient. Other questions, however, may require more complex answers. In addition, it is worth noticing that multiple answers may be considered correct, such as gray and light gray. Therefore, VQA v2.0 provides ten ground-truth answers for each question. These answers were collected from ten different randomly-chosen humans. The evaluation metric used to measure model performance in the open-ended Visual Question Answering task is a particular kind of accuracy. For each question in the input dataset, the model's most likely response is compared to the ten possible answers provided by humans in the dataset associated with that question BIBREF3, and evaluated according to Equation DISPLAY_FORM7. In this approach, the prediction is considered totally correct only if at least 3 out of 10 people provided that same answer.",Experimental Setup ::: Hyper-parameters,"As in BIBREF20 we train our models in a classification-based manner, in which we minimize the cross-entropy loss calculated with an image-question-answer triplet sampled from the training set. We optimize the parameters of all VQA models using Adamax BIBREF45 optimizer with a base learning rate of $7 \times 10^{-4}$, with exception of BERT BIBREF14 in which we apply a 10-fold reduction as suggested in the original paper. We used a learning rate warm-up schedule in which we halve the base learning rate and linearly increase it until the fourth epoch where it reaches twice its base value. It remains the same until the tenth epoch, where we start applying a 25% decay every two epochs. Gradients are calculated using batch sizes of 64 instances, and we train all models for 20 epochs.",Experimental Analysis,"In this section we show the experimental analysis for each component in the baseline VQA model. We also provide a summary of our findings regarding the impact of each part. Finally, we train a model with all the components that provide top results and compare it against state-of-the-art approaches.",Experimental Analysis ::: Text Encoder,"In our first experiment, we analyze the impact of different embeddings for the textual representation of the questions. To this end, we evaluate: (i) the impact of word-embeddings (pre-trained, or trained from scratch); and (ii) the role of the temporal encoding function, i.e., distinct RNN types, as well as pre-trained sentence encoders (e.g., Skip-Thoughts, BERT). The word-embedding strategies we evaluate are Learnable Word Embedding (randomly initialized and trained from scratch), Word2Vec BIBREF12, and GloVe BIBREF11. We also use word-level representations from widely used sentence embeddings strategies, namely Skip-Thoughts BIBREF13 and BERT BIBREF14. To do so, we use the hidden-states from the Skip-thoughts GRU network, while for BERT we use the activations of the last layer as word-level information. Those vectors feed an RNN that encodes the temporal sequence into a single global vector. Different types of RNNs are also investigated for encoding textual representation, including LSTM BIBREF46, Bidirectional LSTM BIBREF47, GRU BIBREF48, and Bidirectional GRU. For bidirectional architectures we concatenate both forward and backward hidden-states so as to aggregate information from both directions. Those approaches are also compared to a linear strategy, where we use a fully-connected layer followed by a global average pooling on the temporal dimension. The linear strategy discards any order information so we can demonstrate the role of the recurrent network as a temporal encoder to improve model performance. Figure FIGREF5 shows the performance variation of different types of word-embeddings, recurrent networks, initialization strategies, and the effect of fine-tuning the textual encoder. Clearly, the linear layer is outperformed by any type of recurrent layer. When using Skip-Thoughts the difference reaches $2.22\%$, which accounts for almost $5,000$ instances that the linear model mistakenly labeled. The only case in which the linear approach performed well is when trained with BERT. That is expected since Transformer-based architectures employ several attention layers that present the advantage of achieving the total receptive field size in all layers. While doing so, BERT also encodes temporal information with special positional vectors that allow for learning temporal relations. Hence, it is easier for the model to encode order information within word-level vectors without using recurrent layers. For the Skip-Thoughts vector model, considering that its original architecture is based on GRUs, we evaluate both the randomly initialized and the pre-trained GRU of the original model, described as [GRU] and [GRU (skip)], respectively. We noticed that both options present virtually the same performance. In fact, GRU trained from scratch performed $0.13\%$ better than its pre-trained version. Analyzing the results obtained with pre-trained word embeddings, it is clear that GloVe obtained consistently better results than the Word2Vec counterpart. We believe that GloVe vectors perform better given that they capture not only local context statistics as in Word2Vec, but they also incorporate global statistics such as co-occurrence of words. One can also observe that the use of different RNNs models inflicts minor effects on the results. It might be more advisable to use GRU networks since they halve the number of trainable parameters when compared to the LSTMs, albeit being faster and consistently presenting top results. Note also that the best results for Skip-Thoughts, Word2Vec, and GloVe were all quite similar, without any major variation regarding accuracy. The best overall result is achieved when using BERT to extract the textual features. BERT versions using either the linear layer or the RNNs outperformed all other pre-trained embeddings and sentence encoders. In addition, the overall training accuracy for BERT models is not so high compared to all other approaches. That might be an indication that BERT models are less prone to overfit training data, and therefore present better generalization ability. Results make it clear that when using BERT, one must fine-tune it for achieving top performance. Figure FIGREF5 shows that it is possible to achieve a $3\%$ to $4\%$ accuracy improvement when updating BERT weights with $1/10$ of the base learning rate. Moreover, Figure FIGREF6 shows that the use of a pre-training strategy is helpful, once Skip-thoughts and BERT outperform trainable word-embeddings in most of the evaluated settings. Is also make clear that using a single-layered RNNs provide best results, and are far more efficient in terms of parameters.",Experimental Analysis ::: Image Encoder,"Experiments in this section analyze the visual feature extraction layers. The baseline uses the Faster-RCNN BIBREF15 network, and we will also experiment with other pre-trained neural networks to encode image information so we can observe their impact on predictive performance. Additionally to Faster-RCNN, we experiment with two widely used networks for VQA, namely ResNet-101 BIBREF17 and VGG-16 BIBREF16. Table TABREF11 illustrates the result of this experiment. Intuitively, visual features provide a larger impact on model's performance. The accuracy difference between the best and the worst performing approaches is $\approx 5\%$. That difference accounts for roughly $10,000$ validation set instances. VGG-16 visual features presented the worst accuracy, but that was expected since it is the oldest network used in this study. In addition, it is only sixteen layers deep, and it has been shown that the depth of the network is quite important to hierarchically encode complex structures. Moreover, VGG-16 architecture encodes all the information in a 4096 dimensional vector that is extracted after the second fully-connected layer at the end. That vector encodes little to none spatial information, which makes it almost impossible for the network to answer questions on the spatial positioning of objects. ResNet-101 obtained intermediate results. It is a much deeper network than VGG-16 and it achieves much better results on ImageNet, which shows the difference of the the learning capacity of both networks. ResNet-101 provides information encoded in 2048 dimensional vectors, extracted from the global average pooling layer, which also summarizes spatial information into a fixed-sized representation. The best result as a visual feature extractor was achieved by the Faster-RCNN fine-tuned on the Visual Genome dataset. Such a network employs a ResNet-152 as backbone for training an RPN-based object detector. In addition, given that it was fine-tuned on the Visual Genome dataset, it allows for the training of robust models suited for general feature extraction. Hence, differently from the previous ResNet and VGG approaches, the Faster-RCNN approach is trained to detect objects, and therefore one can use it to extract features from the most relevant image regions. Each region is encoded as a 2048 dimensional vector. They contain rich information regarding regions and objects, since object detectors often operate over high-dimensional images, instead of resized ones (e.g., $256 \times 256$) as in typical classification networks. Hence, even after applying global pooling over regions, the network still has access to spatial information because of the pre-extracted regions of interest from each image.",Experimental Analysis ::: Fusion strategy,"In order to analyze the impact that the different fusion methods have on the network performance, three simple fusion mechanisms were analyzed: element-wise multiplication, concatenation, and summation of the textual and visual features. The choice of the fusion component is essential in VQA architectures, since its output generates multi-modal features used for answering the given visual question. The resulting multi-modal vector is projected into a 3000-dimensional label space, which provides a probability distribution over each possible answer to the question at hand BIBREF39. Table presents the experimental results with the fusion strategies. The best result is obtained using the element-wise multiplication. Such an approach functions as a filtering strategy that is able to scale down the importance of irrelevant dimensions from the visual-question feature vectors. In other words, vector dimensions with high cross-modal affinity will have their magnitudes increased, differently from the uncorrelated ones that will have their values reduced. Summation does provide the worst results overall, closely followed by the concatenation operator. Moreover, among all the fusion strategies used in this study, multiplication seems to ease the training process as it presents a much higher training set accuracy ($\approx 11\% $ improvement) as well.",Experimental Analysis ::: Attention Mechanism,"Finally, we analyze the impact of different attention mechanisms, such as Top-Down Attention BIBREF19 and Co-Attention BIBREF18. These mechanisms are used to provide distinct image representations according to the asked questions. Attention allows the model to focus on the most relevant visual information required to generate proper answers to the given questions. Hence, it is possible to generate several distinct representations of the same image, which also has a data augmentation effect.",Experimental Analysis ::: Attention Mechanism ::: Top-Down Attention,"Top-down attention, as the name suggests, uses global features from questions to weight local visual information. The global textual features $\mathbf {q} \in \mathbb {R}^{2048}$ are selected from the last internal state of the RNN, and the image features $V \in \mathbb {R}^{k \times 2048}$ are extracted from the Faster-RCNN, where $k$ represents the number of regions extracted from the image. In the present work we used $k=36$. The question features are linearly projected so as to reduce its dimension to 512, which is the size used in the original paper BIBREF19. Image features are concatenated with the textual features, generating a matrix $C$ of dimensions $k \times 2560$. Features resulting from that concatenation are first non-linearly projected with a trainable weight matrix $W_1^{2560 \times 512}$ generating a novel multimodal representation for each image region: Therefore, such a layer learns image-question relations, generating $k \times 512 $ features that are transformed by an activation function $\phi $. Often, $\phi $ is ReLU BIBREF49, Tanh BIBREF50, or Gated Tanh BIBREF51. The latter employs both the logistic Sigmoid and Tanh, in a gating scheme $\sigma (x) \times \textsc {tanh}(x)$. A second fully-connected layer is employed to summarize the 512-dimensional vectors into $h$ values per region ($k \times h$). It is usual to use a small value for $h$ such as $\lbrace 1, 2\rbrace $. The role of $h$ is to allow the model to produce distinct attention maps, which is useful for understanding complex sentences that require distinct viewpoints. Values produced by this layer are normalized with a softmax function applied on the columns of the matrix, as follows. It generates an attention mask $A^{k \times h}$ used to weight image regions, producing the image vector $\hat{\mathbf {v}}$, as shown in Equation DISPLAY_FORM17. Note that when $h>1$, the dimensionality of the visual features increases $h$-fold. Hence, $\hat{\mathbf {v}}^{h \times 2048}$, which we reshape to be a $(2048\times h)\times 1$ vector, constitutes the final question-aware image representation.",Experimental Analysis ::: Attention Mechanism ::: Co-Attention,"Unlike the Top-Down attention mechanism, Co-Attention is based on the computation of local similarities between all questions words and image regions. It expects two inputs: an image feature matrix $V^{k \times 2048}$, such that each image feature vector encodes an image region out of $k$; and a set of word-level features $Q^{n \times 2048}$. Both $V$ and $Q$ are normalized to have unit $L_2$ norm, so their multiplication $VQ^T$ results in the cosine similarity matrix used as guidance for generating the filtered image features. A context feature matrix $C^{k \times 2048}$ is given by: Finally, $C$ is normalized with a $\textsc {softmax}$ function, and the $k$ regions are summed so as to generate a 1024-sized vector $\hat{\mathbf {v}}$ to represent relevant visual features $V$ based on question $Q$: Table depicts the results obtained by adding the attention mechanisms to the baseline model. For these experiments we used only element-wise multiplication as fusion strategy, given that it presented the best performance in our previous experiments. We observe that attention is a crucial mechanism for VQA, leading to an $\approx 6\%$ accuracy improvement. The best performing attention approach was Top-Down attention with ReLU activation, followed closely by Co-Attention. We noticed that when using Gated Tanh within Top-Down attention, results degraded 2%. In addition, experiments show that $L_2$ normalization is quite important in Co-Attention, providing an improvement of almost $6\%$.",Findings Summary,"The experiments presented in Section SECREF9 have shown that the best text encoder approach is fine-tuning a pre-trained BERT model with a GRU network trained from scratch. In Section SECREF10 we performed experiments for analyzing the impact of pre-trained networks to extract visual features, among them Faster-RCNN, ResNet-101, and VGG-16. The best result was using a Faster-RCNN, reaching a $3\%$ improvement in the overall accuracy. We analyzed different ways to perform multimodal feature fusion in Section SECREF12. In this sense, the fusion mechanism that obtained the best result was the element-wise product. It provides $\approx 3\%$ higher overall accuracy when compared to the other fusion approaches. Finally, in Section SECREF13 we have studied two main attention mechanisms and their variations. They aim to provide question-aware image representation by attending to the most important spatial features. The top performing mechanism is the Top-Down attention with the ReLU activation function, which provided an $\approx 6\%$ overall accuracy improvement when compared to the base architecture.",Comparison to state-of-the-art methods,"After evaluating individually each component in a typical VQA architecture, our goal in this section is to compare the approach that combines the best performing components into a single model with the current state-of-the-art in VQA. Our comparison involves the following VQA models: Deeper-lstm-q BIBREF3, MCB BIBREF22, ReasonNet BIBREF52, Tips&Tricks BIBREF53, and the recent block BIBREF20. Tables TABREF21 and show that our best architecture outperforms all competitors but block, in both Test-Standard (Table TABREF21) and Test-Dev sets (Table ). Despite block presenting a marginal advantage in accuracy, we have shown in this paper that by carefully analyzing each individual component we are capable of generating a method, without any bells and whistles, that is on par with much more complex methods. For instance, block and MCB require 18M and 32M parameters respectively for the fusion scheme alone, while our fusion approach is parameter-free. Moreover, our model performs far better than BIBREF22, BIBREF52, and BIBREF53, which are also arguably much more complex methods.",Conclusion,"In this study we observed the actual impact of several components within VQA models. We have shown that transformer-based encoders together with GRU models provide the best performance for question representation. Notably, we demonstrated that using pre-trained text representations provide consistent performance improvements across several hyper-parameter configurations. We have also shown that using an object detector fine-tuned with external data provides large improvements in accuracy. Our experiments have demonstrated that even simple fusion strategies can achieve performance on par with the state-of-the-art. Moreover, we have shown that attention mechanisms are paramount for learning top performing networks, once they allow producing question-aware image representations that are capable of encoding spatial relations. It became clear that Top-Down is the preferred attention method, given its results with ReLU activation. It is is now clear that some configurations used in some architectures (e.g., additional RNN layers) are actually irrelevant and can be removed altogether without harming accuracy. For future work, we expect to expand this study in two main ways: (i) cover additional datasets, such as Visual Genome BIBREF37; and (ii) study in an exhaustive fashion how distinct components interact with each other, instead of observing their impact alone on the classification performance.",Acknowledgment,This study was financed in part by the Coordenação de Aperfeiçoamento de Pessoal de Nivel Superior – Brasil (CAPES) – Finance Code 001. We also would like to thank FAPERGS for funding this research. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the graphics cards used for this research.,,,,,,,,,What are least important components identified in the the training of VQA models?,649d6dc076251547aece6532f75d00fc99081d2b,zero,familiar,no,computer vision,258ee4069f740c400c0049a2580945a1cc7f044c,False,,,"In this study we observed the actual impact of several components within VQA models. We have shown that transformer-based encoders together with GRU models provide the best performance for question representation. Notably, we demonstrated that using pre-trained text representations provide consistent performance improvements across several hyper-parameter configurations. We have also shown that using an object detector fine-tuned with external data provides large improvements in accuracy. Our experiments have demonstrated that even simple fusion strategies can achieve performance on par with the state-of-the-art. Moreover, we have shown that attention mechanisms are paramount for learning top performing networks, once they allow producing question-aware image representations that are capable of encoding spatial relations. It became clear that Top-Down is the preferred attention method, given its results with ReLU activation. It is is now clear that some configurations used in some architectures (e.g., additional RNN layers) are actually irrelevant and can be removed altogether without harming accuracy. For future work, we expect to expand this study in two main ways: (i) cover additional datasets, such as Visual Genome BIBREF37; and (ii) study in an exhaustive fashion how distinct components interact with each other, instead of observing their impact alone on the classification performance.","It is is now clear that some configurations used in some architectures (e.g., additional RNN layers) are actually irrelevant and can be removed altogether without harming accuracy.",5ecfac6251f72c2ef0d897974998b989bae2ea49,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,What type of experiments are performed?,92412a449c28b9121a4a4f4acca996563f107131,zero,familiar,no,computer vision,258ee4069f740c400c0049a2580945a1cc7f044c,False,,,"More specifically, we observe the impact of: (i) pre-trained word embeddings BIBREF11, BIBREF12, recurrent BIBREF13 and transformer-based sentence encoders BIBREF14 as question representation strategies; (ii) distinct convolutional neural networks used for visual feature extraction BIBREF15, BIBREF16, BIBREF17; and (iii) standard fusion strategies, as well as the importance of two main attention mechanisms BIBREF18, BIBREF19. We notice that even using a relatively simple baseline architecture, our best models are competitive to the (maybe overly-complex) state-of-the-art models BIBREF20, BIBREF21. Given the experimental nature of this work, we have trained over 130 neural network models, accounting for more than 600 GPU processing hours. We expect our findings to be useful as guidelines for training novel VQA models, and that they serve as a basis for the development of future architectures that seek to maximize predictive performance.","More specifically, we observe the impact of: (i) pre-trained word embeddings BIBREF11, BIBREF12, recurrent BIBREF13 and transformer-based sentence encoders BIBREF14 as question representation strategies; (ii) distinct convolutional neural networks used for visual feature extraction BIBREF15, BIBREF16, BIBREF17; and (iii) standard fusion strategies, as well as the importance of two main attention mechanisms BIBREF18, BIBREF19.",e73d9321bc958db1aa52b52eaf4e7bcb8aa7405c,258ee4069f740c400c0049a2580945a1cc7f044c,What components are identified as core components for training VQA models?,76405b76b930a5bbe895e9e96ce4a3cff1b0b1a1,zero,familiar,no,computer vision,258ee4069f740c400c0049a2580945a1cc7f044c,False,,,"In this study we observed the actual impact of several components within VQA models. We have shown that transformer-based encoders together with GRU models provide the best performance for question representation. Notably, we demonstrated that using pre-trained text representations provide consistent performance improvements across several hyper-parameter configurations. We have also shown that using an object detector fine-tuned with external data provides large improvements in accuracy. Our experiments have demonstrated that even simple fusion strategies can achieve performance on par with the state-of-the-art. Moreover, we have shown that attention mechanisms are paramount for learning top performing networks, once they allow producing question-aware image representations that are capable of encoding spatial relations. It became clear that Top-Down is the preferred attention method, given its results with ReLU activation. It is is now clear that some configurations used in some architectures (e.g., additional RNN layers) are actually irrelevant and can be removed altogether without harming accuracy. For future work, we expect to expand this study in two main ways: (i) cover additional datasets, such as Visual Genome BIBREF37; and (ii) study in an exhaustive fashion how distinct components interact with each other, instead of observing their impact alone on the classification performance.","We have shown that transformer-based encoders together with GRU models provide the best performance for question representation. Notably, we demonstrated that using pre-trained text representations provide consistent performance improvements across several hyper-parameter configurations. We have also shown that using an object detector fine-tuned with external data provides large improvements in accuracy. Moreover, we have shown that attention mechanisms are paramount for learning top performing networks, once they allow producing question-aware image representations that are capable of encoding spatial relations. It became clear that Top-Down is the preferred attention method, given its results with ReLU activation.",8032212acafce9a630684bd69bc29e079221f43e,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2-Figure1-1.png,Fig. 1. Baseline architecture proposed for the experimental setup.,5-Figure2-1.png,"Fig. 2. Overall validation accuracy improvement (∆) over the baseline architecture. Models denoted with * present fixed word-embedding representations, i.e., they are not updated via back-propagation.",5-Figure3-1.png,"Fig. 3. Overall accuracy vs. number of parameters trade-off analysis. Circled markers denote two-layered RNNs. Number of parameters increases due to the number of hidden units H within the RNN. In this experiment we vary H ∈ {128, 256, 512, 1024, 2048}.",6-TableIII-1.png,TABLE III EXPERIMENT USING DIFFERENT ATTENTION MECHANISMS.,8-TableIV-1.png,"TABLE IV COMPARISON OF THE MODELS ON VQA2 TEST-STANDARD SET. THE MODELS WERE TRAINED ON THE UNION OF VQA 2.0 TRAINVAL SPLIT AND VISUALGENOME [38] TRAIN SPLIT. All IS THE OVERALL OPENENDED ACCURACY (HIGHER IS BETTER). Yes/No, Numbers, AND Others ARE SUBSETS THAT CORRESPOND TO ANSWERS TYPES. * SCORES REPORTED FROM [21].",8-TableV-1.png,"TABLE V COMPARISON OF THE MODELS ON VQA2 TEST-DEV SET. All IS THE OVERALL OPENENDED ACCURACY (HIGHER IS BETTER). Yes/No, Numbers, AND Others ARE SUBSETS THAT CORRESPOND TO ANSWERS TYPES. * SCORES REPORTED FROM [21].",,,,,,,,,"pre-trained word embeddings BIBREF11, BIBREF12 recurrent BIBREF13 transformer-based sentence encoders BIBREF14 distinct convolutional neural networks standard fusion strategies  two main attention mechanisms BIBREF18, BIBREF19",pre-trained text representations transformer-based encoders together with GRU models attention mechanisms are paramount for learning top performing networks Top-Down is the preferred attention method,,,,,,,,,,,," some configurations used in some architectures (e.g., additional RNN layers) are actually irrelevant",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improving Information Retrieval Results for Persian Documents using FarsNet,"In this paper, we propose a new method for query expansion, which uses FarsNet (Persian WordNet) to find similar tokens related to the query and expand the semantic meaning of the query. For this purpose, we use synonymy relations in FarsNet and extract the related synonyms to query words. This algorithm is used to enhance information retrieval systems and improve search results. The overall evaluation of this system in comparison to the baseline method (without using query expansion) shows an improvement of about 9 percent in Mean Average Precision (MAP).",Introduction,"In Information Retrieval (IR), the searched query has always been an integral part. When a user enters a query in the information retrieval system the keywords they use might be different from the ones used in the documents or they might be expressing it in a different form. Considering this situation, the information retrieval systems should be intelligent and provide the requested information to the user. According to Spink (2001), each user in the web uses 2.4 words in their query; having said that, the probability of the input query being close to those of the documents is extremely low [22]. The latest algorithms implement query indexing techniques and covers only the user's history of search. This simply brings the problem of keywords mismatch; the queries entered by user don't match with the ones in the documents, this problem is called the lexical problem. The lexical problem originates from synonymy. Synonymy is the state that two or more words have the same meaning. Thus, expanding the query by enriching each word with their synonyms will enhance the IR results. This paper is organized as follows. In section II, we discuss some previous researches conducted on IR. In section III, the proposed method is described. Section IV, represents the evaluation and results of proposed method; and finally, in section V, we conclude the remarks and discuss some possible future works.",Previous Works,"One of the first researchers who used the method for indexing was Maron (1960) [11]. Aforementioned paper described a meticulous and novel method to retrieve information from the books in the library. This paper is also one of the pioneers of the relevance and using probabilistic indexing. Relevance feedback is the process to involve user in the retrieved documents. It was mentioned in Rocchio (1971) [15], Ide (1971) [8], and Salton (1971) [19]. In the Relevance feedback the user's opinion for the retrieved documents is asked, then by the help of the user's feedbacks the relevance and irrelevance of the documents is decided. In the later researches, relevance feedback has been used in combination with other methods. For instance, Rahimi (2014) [14] used relevance feedback and Latent Semantic Analysis (LSA) to increase user's satisfaction. Other researches regarding the usage of relevance feedback are Salton (1997) [18], Rui (1997) [16], and Rui (1998) [17]. In the next approaches, the usage of thesauri was increased. Zazo used thesauri to ""reformulate"" user's input query [23]. Then came the WordNet. WordNet was one the paradigm shifting resources. It was first created at Princeton University's Cognitive Science Laboratory in 1995 [12]. It is a lexical database of English which includes: Nouns, Adjectives, Verbs, and Adverbs. The structure of WordNet is a semantic network which has several relations such as: synonymy, hypernymy, hyponymy, meronymy, holonymy, and etc. WordNet contains more than 155,000 entries. Using WordNet for query expansion was first introduced in Gong (2005) [5]. They implemented query expansion via WordNet to improve one token search in images and improved precision. Another research conducted by Pal (2014) showed that the results from query expansion using standard TREC collections improves the results on overall [13]. Zhang (2009) reported 7 percent improvement in precision in comparison to the queries without being expanded [24]. Using WordNet for query expansion improved 23 to 31 percent improvement on TREC 9, 10, and 12 [10]. Liu (2004) used a knowledge database called ConceptNet which contained 1.6 million commonsense knowledge [9]. ConceptNet is used for Topic Gisting, Analogy-Making, and other context-oriented inferences. Later, Hsu (2006) used WordNet and ConceptNet to expand queries and the results were better than not using query expansion method [6]. FarsNet [20] [21] is the first WordNet for Persian, developed by the NLP Lab at Shahid Beheshti University and it follows the same structure as the original WordNet. The first version of FarsNet contained more than 10,000 synsets while version 2.0 and 2.5 contained 20,000 synsets. Currently, FarsNet version 3 is under release and contains more than 40,000 synsets [7].",Proposed Method,"Each word in FarsNet has a Word ID (WID). Each WID is then related to other WIDs e.g. words and their synonyms are related to each other in groups called synsets. As mentioned before, often the user input doesn't match with the ones used in the documents and therefore the information retrieval system fails to fulfil user's request. Having said that; the present paper utilizes FarsNet and its synonymy relations to use in query expansion. We use the original synsets of FarsNet 2.5 as dataset. However, the data is first cleaned and normalized. Normalization refers to the process where the /ی/ is replaced with Unicode code point of 06CC and /ک/ is replaced by Unicode code point of 06A9. The input of the algorithm is the string of input queries. Then the input string is tokenized. Tokenization is the process of separating each word token by white space characters. In the next step, each token is searched in FarsNet and if it is found, the WID of the token will be searched in the database of synonyms; in other words, FarsNet Synsets. Finally, each word is concatenated to its synonyms and they are searched in the collection. Snippet below shows the pseudo code of the query expansion method. Sample input and output are: Input: [Casualties of drought] Output: [drought Casualties waterless dry dried up] بي آب خشک خشکيده خسارات خشك سالي GET input_query L <- an empty list FOR token IN input_query: Wid <- find token's WID in FarsNet INSERT(Wid , L) Expanded_Query <- input@query FOR wid IN L: Syns <- find synonym of wid in Synset CONCAT(Expanded_Query, Syns) Search Expanded_Query in Collection END",Experimental Results,"In the evaluation phase, we used Hamshahri Corpus [2] which is one of the biggest collections of documents for Persian, suitable for Information Retrieval tasks. This corpus was first created by Database Research Group at Tehran University. The name Hamshahri comes from the Persian newspaper Hamshahri, one of the biggest Persian language newspapers. Hamshahri corpus contains 166,000 documents from Hamshahri newspaper in 65 categories. On average, each document contains 380 words and in general the corpus contains 400,000 distinct words. This corpus is built with TREC standards and contains list of standard judged queries. These queries are judged to be relevant or irrelevant to the queries based on real judgments. The judgment list contains 65 standard queries along with the judgements and some descriptions of the queries. Sample queries include: [women basketball] [teaching gardening flower] [news about jungles' fires] [status of Iran's carpet export] [air bicycle] In the present paper, the information retrieval experiments are based on standard queries of Hamshahri corpus. For assessment of the proposed algorithm in a real information retrieval situation we used Elasticsearch database [1]. Elasticsearch is a noSQL database which its base is document, hence called document based database. Elasticsearch uses Lucene as its engine. The evaluation process started with normalizing all the documents in Hamshahri corpus. Then some articles that were incomplete or had errors were removed so that they could be indexed in Elasticsearch. In the end, the total number of 165,000 documents were indexed in Elasticsearch. Code snippet below shows a sample of index structure in Elasticsearch database. _index: ""Hamshahri"" [Default-Elasticsearch Index] _type: ""articles"" [Default-All our types are Hamshahri document] _id : ""AV9Np3YfvUqJXrCluoHe"" [random generated ID] DID: ""1S1"" [Document ID in Hamshahri Corpus] Date: ""75\\04\\02"" [Document date in Iranian Calendar, \\ is for character escape] Cat: ""adabh"" [Document category e.g. adab-honar] Body: ""&"" [Document body] We arranged two sets of experiments for evaluation of the algorithm: without query expansion (baseline) and with query expansion (proposed). First, for each query in the standard query list of Hamshahri corpus, we searched in Elasticsearch database and retrieved the results. In the next step, we expanded each query using proposed method and searched each expanded query in Elasticsearch. In order to evaluate the precision of the retrieved documents in each experiment, we used ""TREC_Eval"" tool [3]. TREC_Eval is a standard tool for evaluation of IR tasks and its name is a short form of Text REtrieval Conference (TREC) Evaluation tool. The Mean Average Precision (MAP) reported by TREC_Eval was 27.99% without query expansion and 37.10% with query expansion which shows more than 9 percent improvement. Table 1 and Figure 1 show the precision at the first n retrieved documents (P@n) for different numbers of n in two sets of experiments. In all P@n states the precision of Query Expansion algorithm was higher than the baseline. Figure 1 shows the plot of precision vs recall for two sets of experiments. This plot shows that our method will improve the overall quality of Information Retrieval system.",Conclusions,"In this paper, we proposed a method for query expansion in IR systems using FarsNet. Results from this approach showed about 9% improvement in Mean Average Precision (MAP) for document retrieval. In the future researches, we will use FarsNet 3.0 and also, we will modify and revise some synsets in the FarsNet, in order toincrease the precision for Information Retrieval.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Which evaluation metric has been measured?,70c2dc170a73185c9d1a16953f85aca834ead6d3,two,unfamiliar,no,,50d8b4a941c26b89482c94ab324b5a274f9ced66,False,,,"In order to evaluate the precision of the retrieved documents in each experiment, we used ""TREC_Eval"" tool [3]. TREC_Eval is a standard tool for evaluation of IR tasks and its name is a short form of Text REtrieval Conference (TREC) Evaluation tool. The Mean Average Precision (MAP) reported by TREC_Eval was 27.99% without query expansion and 37.10% with query expansion which shows more than 9 percent improvement.","In order to evaluate the precision of the retrieved documents in each experiment, we used ""TREC_Eval"" tool [3]. TREC_Eval is a standard tool for evaluation of IR tasks and its name is a short form of Text REtrieval Conference (TREC) Evaluation tool. The Mean Average Precision (MAP) reported by TREC_Eval was 27.99% without query expansion and 37.10% with query expansion which shows more than 9 percent improvement.",d70cd9ded4d7f67f0a3678d230c641d87235f672,7dd5db428d7a43d2945b97c0c07fa56af4eb02ae,,,,,,,,,What is the WordNet counterpart for Persian?,38854255dbdf2f36eebefc0d9826aa76df9637c6,two,unfamiliar,no,,50d8b4a941c26b89482c94ab324b5a274f9ced66,False,,,"FarsNet [20] [21] is the first WordNet for Persian, developed by the NLP Lab at Shahid Beheshti University and it follows the same structure as the original WordNet. The first version of FarsNet contained more than 10,000 synsets while version 2.0 and 2.5 contained 20,000 synsets. Currently, FarsNet version 3 is under release and contains more than 40,000 synsets [7].","FarsNet [20] [21] is the first WordNet for Persian, developed by the NLP Lab at Shahid Beheshti University and it follows the same structure as the original WordNet.",4276f0816e7cb81006ef71ccb921788704cf9591,7dd5db428d7a43d2945b97c0c07fa56af4eb02ae,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3-TableI-1.png,TABLE I PRECISION AT THE FIRST N RETRIEVED DOCUMENTS (P@N) FOR DIFFERENT NUMBERS OF N. “PLUS” DENOTES WITH QUERY EXPANSION AND “MINUS” DENOTES WITHOUT QUERY EXPANSION,4-Figure1-1.png,Fig. 1. Precision comparison for different numbers of retrieved documents in two sets of experiments,4-Figure2-1.png,Fig. 2. Plot of precision vs recall for two sets of experiments,,,,,,,,,,,,,,,FarsNet,,,,,,,,,,,,,Mean Average Precision,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Integration of Japanese Papers Into the DBLP Data Set,"If someone is looking for a certain publication in the field of computer science, the searching person is likely to use the DBLP to find the desired publication. The DBLP data set is continuously extended with new publications, or rather their metadata, for example the names of involved authors, the title and the publication date. While the size of the data set is already remarkable, specific areas can still be improved. The DBLP offers a huge collection of English papers because most papers concerning computer science are published in English. Nevertheless, there are official publications in other languages which are supposed to be added to the data set. One kind of these are Japanese papers. This diploma thesis will show a way to automatically process publication lists of Japanese papers and to make them ready for an import into the DBLP data set. Especially important are the problems along the way of processing, such as transcription handling and Personal Name Matching with Japanese names.",List of Acronyms,tocchapterList of Acronyms [OAI-PMH] ACMAssociation for Computing Machinery ASCIIAmerican Standard Code for Information Interchange APIApplication Programming Interface BHTBibliography HyperText DBLPDigital Bibliography & Library Project (former meaning: DataBase systems and Logic Programming) FAQFrequently Asked Questions GBGigaByte HTMLHyperText Markup Language HTTPHyperText Transfer Protocol IDIdentifier IEEEInstitute of Electrical and Electronics Engineers IFIPInternational Federation for Information Processing IPSJInformation Processing Society of Japan IPSJ DLDigital Library of the Information Processing Society of Japan ISOInternational Organization for Standardization JARJava ARchive JDBCJava DataBase Connectivity JDKJava Development Kit OAIOpen Archives Initiative OAI-PMHOpen Archives Initiative - Protocol for Metadata Harvesting PDFPortable Document Format RAMRandom Access Memory SAXSimple API for XML SQLStructured Query Language SPFSingle Publication Format TOCTables Of Contents URLUniform Resource Locator XMLeXtensible Markup Language ,About This Diploma Thesis,"The idea for this work was born when the author was searching for a possibility to combine computer science with his minor subject Japan studies in his diploma thesis. After dismissing some ideas leaning towards Named Entity Recognition and computer linguistics the author chose “Integration of Japanese Papers Into the DBLP Data Set” as his subject. The DBLP is a well-known and useful tool for finding papers published in the context of computer science. The challenge to deal with such a huge database and the problems that occur when processing Japanese input data was the reason why this idea has been chosen. The hope is that, in the future, many Japanese papers can be added by the responsible people of the DBLP project.",Motivation,"Computer scientists are likely to use the DBLP to find information about certain papers or authors. Therefore, the DBLP is supposed to provide information about as many papers as possible. For example, one could be interested in the paper “Analysis of an Entry Term Set of a Civil Engineering Dictionary and Its Application to Information Retrieval Systems” by Akiko Aizawa et al. (2005) but DBLP does not include it yet. Japanese scientists might look for the original (Japanese) title “土木関連用語辞典の見出し語の分析と検索システムにおける活用に関する考察” or use Aizawa's name in Japanese characters (相澤彰子) for a search in DBLP. The DBLP contains the author “Akiko Aizawa” but does not contain this specific paper or the author's original name in Japanese characters. Our work is to implement a tool which addresses these questions, support the DBLP team in the integration of Japanese papers and reveal the difficulties of realizing the integration.",Composition of the Diploma Thesis,"Dates are displayed in the ISO 8601 standard format YYYY-MM-DD, e.g. 2012-10-19. Although scientific works about the Japanese language often display the Sino-Japanese reading of kanji (a Japanese character set) with uppercase letters to distinguish them from the other “pure” Japanese reading, we will not use uppercase letters to distinguish them in this work. When a Japanese word is used in its plural form in this work, the word always stays unmodified. The reason is that in the Japanese language there is no differentiation between a singular and plural form. We use a macron instead of a circumflex to display a long vowel of a Japanese word in Latin transcription (see section SECREF14 ).",Acknowledgement,"First I would like to thank Prof. Dr. Bernd Walter and Prof. Dr. Peter Sturm for making this diploma thesis possible. Special thanks go to Florian Reitz for the great support and the useful answers for the questions I had while I have been working on this diploma thesis. I also want to acknowledge the help of Peter Sommerhoff, Daniel Fett, David Christ and Kana Matsumoto for proofreading my work. I thank Dr. Michael Ley, Oliver Hoffmann, Peter Birke and the other members of the Chair of Database and Information Systems of the University of Trier. Last but not least I want to tell some personal words to my family in my and their native language German: Ich möchte nun noch meinen Eltern und meinem Bruder Peter dafür danken, dass sie mich in meiner Diplomarbeitsphase, meinem Studium und auch schon davor immer unterstützt haben und immer für mich da waren, wenn ich sie brauchte. Ich weiß es zu schätzen.",Writing in Japanese,"“My view is that if your philosophy is not unsettled daily then you are blind to all the universe has to offer.” (Neil deGrasse Tyson) First we need to understand some aspects of the Japanese language and especially the different ways of writing Japanese because the peculiarities of the Japanese writing system are a crucial point of our work. It lays the foundation for all Japanese-related subjects such as the structure of Japanese names (discussed in section SECREF19 ), a dictionary for Japanese names (discussed in section SECREF36 ) or the publication metadata source for Japanese publications (discussed in section SECREF39 ). Hadamitzky ( BIBREF0 , p. 8-57) gives an overview about the basics of Japanese writing. The Japanese writing system includes kanji, hiragana, katakana and the possibility to use Latin characters.",Kanji,"Kanji is the Japanese script which consists of traditional Chinese characters. It came to Japan around the 4th century. Since the Japanese had not developed an own writing system yet they began to use the Chinese characters. At the beginning, the characters were linked phonetically with a certain sound, so that they could write down all existing words by their sound. Applying this principle the man'yōgana were created. Every character had one defined way to pronounce it. In addition to this, a second principle was introduced to write Japanese. This time the people orientated themselves on the meaning of the Chinese characters to choose a writing for a word. Applying the second principle, the kanji were created. While the man'yōgana were simplified to hiragana and katakana (see following sections SECREF7 and SECREF11 ) the general usage of kanji did not change. Due to an increase in number and possible readings of characters, the government began to try to simplify the Japanese writing system after the Meiji Restoration at the end of the 19th century. The last important reform took place after World War II. Along with some other changes and regulations, the permitted characters in official documents (tōyō kanji) were limited to 1850 in 1946 and increased to 1900 in a draft from 1977. In 1981 they were replaced by the “List of Characters for General Use” (jōyō kanji) containing 1945 characters. In 1951 the government published a list of additional 92 kanji permitted for personal names. The number of kanji permitted for personal names increased with time passing by. Eschbach-Szabo ( BIBREF2 , p. 175) says the last change permitted 983 kanji for personal names in 2004. The press tries to abide by the jōyō kanji. Japanese literature (science, fiction, etc.) uses about 4000 characters (comprehensive Sino-Japanese kanji dictionaries contain ca. 10000 characters). Japanese people know approximately 3000 kanji on average. Due to their capability to give a word a meaning, kanji are used in substantives, verbs, adjectives and Japanese personal names. An important aspect is reading a kanji because there are several possibilities to read one. Saitō and Silberstein ( BIBREF3 , p. 31-34) describe how to read a kanji. There is a Japanese reading kun and a Sino-Japanese reading on. Depending on the text and grammar context either the kun or on reading is required. For example the kanji 生 is read sei in 学生 (gakusei, meaning: student, on reading) but is read INLINEFORM0 in 生まれる (umareru, meaning: being born, kun reading). A single kanji can have several kun and several on readings. For our work it is important to know that one character can have several readings in names too.",Hiragana,"The syllabary hiragana evolved from the man'yōgana by simplifying the characters. Every syllable is phonetically assigned to one sound of the spoken language (with two exceptions which can have two sounds each). The gojūon table shown in figure FIGREF9 lists the 46 syllables used today in a certain way (it can be compared with the ABC for letters). Another but obsolete way to order the syllables is iroha which is a poem containing all syllables. Although the name implies 50 sounds (gojū means “50”, on means “sound”) there are only 46 syllables left in modern Japanese. Actually, only 45 syllables belong to the gojūon table. The INLINEFORM0 counts as extra symbol (see gojūon tables in figures FIGREF9 and FIGREF12 ). Other additional syllables are dakuon (e.g. だ/ INLINEFORM0 , recognizable by two little strokes), handakuon (e.g. ぱ/ INLINEFORM1 , recognizable by a little circle) and yōon (e.g. しゃ/ INLINEFORM2 , recognizable by a normally sized character that is followed by a smaller character). You can write every Japanese word in hiragana but if possible, kanji are usually preferred to avoid problems with homonyms (we take a look at homonyms in chapter SECREF5 ). Hiragana is mainly used to write words not covered by kanji and as inflected endings. Kanji and hiragana are often combined within one word. For example 読む (yomu) is the basic form of the verb “to read”. The kanji 読 means reading by itself and in combination with the hiragana syllable む it becomes the verb “to read” in a special grammatical form specifying tense, politeness level and other properties.",Katakana,"The syllabary katakana also evolved from the man'yōgana by simplifying the characters, consists of 46 characters nowadays (representing the same syllables as hiragana) and is usually ordered by the gojūon table. Figure FIGREF12 presents the katakana in a gojūon table. Besides optical differences with hiragana, katakana are used in other contexts. Japanese mostly use them to write foreign words including foreign personal names. So foreigners often apply katakana for their names. For example, the author's name can be transcribed as パウル·ソマホフ. The dot · in the middle separates family and given name. Foreign names are often written with the given name preceding the family name.",Latin Characters/Transcription,"Transcription systems which convert kanji, hiragana and katakana to Latin characters are usually called rōmaji. Japanese can be easily transcribed by 22 letters and two additional signs. Due to many words having the same pronunciation, the meaning of words is sometimes ambiguous if they are transcribed into Latin characters. In 1954 the government released recommendations for transcribing Japanese. It recommended following two transcription systems:  The kunreishiki rōmaji assigns transcriptions according to the order in the gojūon table without regard to phonetic divergences of some consonants (we will discuss these divergences later). It has been introduced for official usage by the government only slightly different in 1937. It became the preferred transcription system in the standard ISO 3602 “Documentation - Romanization of Japanese (kana script)” BIBREF6 .  The hebonshiki rōmaji was developed by a council of Japanese and foreign erudites in 1885 and spread by the American missionary James C. Hepburn (Hebon in Japanese), especially thanks to his Japanese-English dictionary published one year later. This work also employs hebonshiki. Kunreishiki would lead to transcriptions like kunreisiki, hebonsiki and kanzi. Although the kunreishiki became the preferred system of the government, the international community often prefers the Hepburn system because the written words suggest a more intuitive pronunciation than kunreishiki. There are also language-related transcription systems that are rarely used. Kaneko and Stickel ( BIBREF7 , p. 53-55) mention them: The important aspect are the system differences because we need to know where they occur when we deal with Personal Name Matching problems later. Figure FIGREF165 in the appendix reveals the differences between the transcription systems. It summarizes 18 differences in all syllables including INLINEFORM0 , INLINEFORM1 and INLINEFORM2 . Unfortunately, there can be even more transcription differences. ISO 3602 highlights some more special cases when it comes to transcribing Japanese. One is the question whether to put an apostrophe after an INLINEFORM3 . To avoid misunderstandings, one should put an apostrophe behind an INLINEFORM4 in certain cases. Otherwise, people could misinterpret the syllable INLINEFORM5 followed by a syllable composed of a vowel or “y” and a vowel as syllables na, ni, nu, ne, no, nya, nyu or nyo. We will outline a practical example of this case in section UID99 . A second irregularity occurs when the same vowel appears right after another. If there is a morpheme boundary between the vowels, they should be transcribed as “aa”, “ii”, etc. but should be transcribed by an additional circumflex otherwise. Koop and Inada BIBREF4 write about another difficulty called nigori. “The nigori (濁, literally, `turbidity', `impurity') ... [means] modifying the pronunciation of the consonant in certain of the kana sounds. It may be either (1) inherent, as in suge (`sedge'), suzu (`grelot'), go (`five'), or (2) applied incidentally to the initial consonant of a word or name-element following another in composition, e.g., Shimabara from shima and hara, nenjū from nen and chū, Harada from hara and ta.” ( BIBREF4 , p. 34) So, if we want to derive a transcription from the family name 中田, we cannot tell whether to take Nakata or Nakada as the rightful transcription.",Japanese Personal Names,"七転び、八起き。 Nana korobi, ya oki. (Fall seven times, get up eight times.) Japanese saying One of the central problems in this work is to deal with Japanese personal names. We need to get a picture of Japanese personal names in general to deal with multiple data sources (like the introduced publication metadata sources in chapter SECREF4 ) which may represent the same name with different scripts or transcription methods. The dictionary ENAMDICT will be very helpful when it comes to extracting and verifying name information.",Structure of Japanese Names,"Having the urge to name things is part of the human nature. Names make it easy to refer to things, people or any other object in this world. When it comes to name giving, history shows a development in the Japanese society. Japanese names are divided into family and given name, similar to the system in the Western culture. When Japanese write their name in kanji they put the family name first, followed by the given name (usually without leaving spaces between them), for example 中村武志 (Takeshi Nakamura). While introducing themselves, they often tell their family name and skip the given name. When Japanese refer to others, they have many name particles they put after a name to express the relationship to the other person. There is the neutral san, chan for children, kun particular for boys or sensei for teachers and doctors. ( BIBREF5 , p. 18-19) Kagami ( BIBREF8 , p. 913) writes about Japanese personal names. Only the samurai and nobility were allowed to carry family names before the Meiji Restoration in 1868. Merchants carried shop names instead (recognizable by the suffix -ya), for example Kinokuniya (shop name) Bunzaemon (given name). Then everybody had to pick a family name after the Meiji Restoration. Approximately 135000 family names are recognized now. The most common family names are Suzuki, Satō, Tanaka, Yamamoto, Watanabe, Takahashi, Kobayashi, Nakamura, Itō, Saitō and others. “In the feudal age, first and second given names were used as male names. The first name was Kemyoo which was the order of brothers, and the second name was the formal name given at the coming of age ceremony (genpuku), e.g. the name of a famous general in 12c.: Minamoto (family name) no (of) Kuroo (kemyoo) Yoshitune (formal given name), and before the genpuku ceremony, he was called by Yoomyoo (child name) Ushiwakamaru.” ( BIBREF8 , p. 913) While there were no restrictions to the number of personal names visible until the Meiji Restoration, due to modernization, Japanese people got the restriction to carry only one given and one family name. ( BIBREF2 , p. 167-169) Some indicators for assigning the gender to a name also exist. The suffixes -ko (e.g. Hanako), -mi (Natsumi) and -yo (Yachiyo) indicate a female name. Male names are harder to identify because they have no fixed pattern. The suffix -o (Kazuo) mostly belongs to a male name though. Family names often consist of two kanji characters, rarely of one or three characters. ( BIBREF8 , p. 913) Eschbach-Szabo ( BIBREF2 , p. 157-309) dedicates an elaborate chapter to Japanese personal names. Compared to the Chinese system, the Japanese naming system shows more tolerance. Several readings are left besides each other, formal rules are not always applied in practice. Japanese apprehend names mainly visually by the characters, secondarily by the reading and sound. This is why several readings for a written name are still acceptable in the modern Japanese world. In the feudal system, names were needed to determine the position and roles of a person in the family and the society rather than characterizing him or her as an individual. Japan has an open naming system which allows adding new names. This is a difference to the exclusive name lists in Germany or France. ( BIBREF2 , p. 157-166) Even the apparently simple kanji 正 has a lot of possible readings: Akira, Kami, Sada, Taka, Tadashi, Tsura, Nao, Nobu, Masa. We can see the same phenomenon in recently approved kanji too. When we see 昴 we cannot be sure whether it is read Kō or Subaru. ( BIBREF9 ) “Conversely, it often happens that one does not know to write a name of given pronunciation. For example, Ogawa can be written 尾川 or 小川. In Japan, when two people meet for the first time, they exchange business cards. This custom often baffles foreigners, but for Japanese it is a ritual with practical purpose: Japanese do not feel at ease until they see how a name is spelled out in kanji.” ( BIBREF9 ) Figure FIGREF22 illustrates the problem. The cashier tries to read the customer's name and cannot determine the right name. According to the customer's reaction, his first two trials Hiroko and Yūko seem to be wrong. Ogawa considers the name polygraphy as a reason why the creation of new name characters is still allowed. Some characteristics of the Japanese naming system are: only little renaming of people semantic variance (names indicate different meanings/attributes) admission of foreign elements (foreign names get assimilated) possibility of polygraphic writing diversity of writing (many scripts usable, weak orthographic normalization) number of personal names for one person          In academic circles a Sino-Japanese reading led to a more reputable name. So the famous linguist 上田万年 from the Meiji era became known as Kazutoshi Ueda AND Mannen Ueda (Mannen is the Sino-Japanese on reading, Kazutoshi is the Japanese kun reading). Modern guidebooks underline that maybe one has to take a loan word from another language to find the corresponding reading for a name in kanji. For example, 宇宙 could be read as Kosumo (from the Greek word for cosmos) instead of Uchū. Also ノイ (Noi), derived from the German word “neu” (new), became a Japanese given name. Another imaginable name is “Sky” written as 空海 (meanings: 空 Sky, 海 sea) and transcribed as Sukai (actually kūkai). This would finally show the impact of globalization also on the Japanese naming system. If one has lived in Japan for a while and wants to adapt or register his or her Western name, one can choose corresponding kanji either by meaning or reading of the original name. Another possibility is transcribing the name with katakana. ( BIBREF2 , p. 170-171, 305-309) The name Anna exists in many cultures. The girls in figure FIGREF29 are both called Anna. Both turn around when they hear their name and respond in their mother tongue (“Yes!” and “Hai!”, respectively). One principle of Japanese name giving is ateji. Ateji (当て字) means “appropriate characters”. It says Japanese try to find characters with good, positive meanings for their children's name. Examples are 愛子 (愛: ai, love; 子: ko, child), 夏美 (夏: natsu, summer; 美: mi, beauty) or 正 (Tadashi, correct, honest). There is also a list with characters that are allowed but should be avoided because of bad associations. Characters like 蟻 (ari, ant), 苺 (ichigo, strawberry), 陰 (kage, shadow), 悪 (aku, bad/evil) belong to this list. ( BIBREF2 , p. 172-176) A particular case drew public attention from June 1993 to February 1994 when Shigeru Satō wanted to call his son Akuma, written as 悪魔 (devil/demon). The civil registry office declined the registration after some discussion because they were worried about other children teasing him. The father went to court but the judges also declined the wish. Although the father wanted to give his son a unique, rememberable name, the judges saw a possible problem in his individual identification process and also getting teased (ijime) by other children in school someday. Then Satō tried to choose other characters while keeping the reading Akuma. But also changing the name partly into man'yōgana (亜久魔) did not change anything about the declination because of the phonological equality implying the same negative associations. Thereupon the father picked the character 神 (god) and its unusual reading Jin. Even though Shintoistic gods can be good or evil, the civil registry office accepted the name. Satō announced his intention to keep calling his son Akuma anyway. So a new (yet unofficial) reading for a character might be established. ( BIBREF2 , p. 271-278) An article of “Japan Today” from December 2012 shows that there is still a debate about this subject. “[...]Shinzo Abe, the leader of the Liberal Democratic Party made a stand against kirakira names last week when he stated that giving a child a name like Pikachu, which could be written something like 光宙 (`light' and `space'), is tantamount to child abuse, saying: `Children are not pets; we have to provide guidance for parents who would name their child in such a way.' ”( BIBREF11 ) Despite regulations, the discussion about the culture of name giving does not seem to have ended yet. Japanese comics like the one in figure FIGREF34 suggest a happy-go-lucky life if one has a common everyday name like Keiko. Today's registration of names allows 2983 kanji for given names, 4000 kanji for family names, 700 man'yōgana, 46 hiragana and 46 katakana. There are still people whose names are written with the obsolete kana syllabary hentaigana which has been prohibited in 1948 ( BIBREF2 , p. 176-177; BIBREF12 ). Regarding this variety of characters (and readings) it is not surprising that even well educated Japanese have problems reading certain names too, respectively they cannot be sure that the chosen reading is the correct reading in the current situation. Forbidden is the usage of geometrical and punctuation signs. The sign ◯ (maru) is an example of such a forbidden one. Also forbidden is the usage of Latin characters (rōmaji) at the registration of a name. Rōmaji can be used privately, though. ( BIBREF2 , p. 176-177) Names can be changed by marriage, adoption or getting a pseudonym or special posthumous name. Titles can be acquired too. ( BIBREF2 , p. 251) After disestablishing the patriarchal ie system in which a man (for example the husband) is the dominating householder of a family, the family name has not been focused on the affiliation to a family anymore but has been focused on the couple living together in joint lives. ( BIBREF2 , p. 253-255) Writing a Japanese name can be ambiguous. While the name written in kanji is definite, displaying it in Latin characters leads to several possibilities. Japanese themselves usually write their name using kanji. To find matching authors in the DBLP, it will be crucial for us to have names in Latin characters later on (in chapter SECREF6 ) because the standard encoding format of the file containing the main data of the DBLP project is ISO 8859-1 (Latin-1). We sometimes talk about “kanji names” or “names in kanji representation” in this work. Although the expression does not suggest it, they shall include all names in Japanese characters, ergo names in kanji, hiragana and katakana.",ENAMDICT,"To automatically detect where a Japanese family name in kanji notation ends and the given name begins, we should factor a name dictionary into our work. It is important that this dictionary includes the names written in kanji and a clear transcription for them in Latin characters. A useful dictionary for our purposes is ENAMDICT. ENAMDICT BIBREF13 is a free dictionary for Japanese proper names, maintained by the Monash University in Victoria (Australia). The Electronic Dictionary Research and Development Group owns the copyright. In 1995, ENAMDICT became an independent project by dividing the universal dictionary EDICT into two projects. ENAMDICT contains person names and non-person names like places and companies as well. Table TABREF38 shows the online statistics about the content of the ENAMDICT file. We will call the categories “name types” in subsequent chapters. “A proper name is a word or group of words which is recognized as having identification as its specific purpose, and which achieves, or tends to achieve that purpose by means of its distinctive sound alone, without regard to any meaning possessed by that sound from the start, or aquired by it through association with the object thereby identified.” ( BIBREF14 , p. 73) these intern abbreviations occur again when we construct a database for Japanese names in chapter SECREF74 ",Publication Metadata Sources,百語より一笑 Hyaku go yori isshō (A smile is more worth than a hundred words.) Japanese saying This chapter gives an overview of the publication metadata sources that we will need later. We take a look at these sources because we will discuss a way to extract metadata information from one source containing Japanese papers and import them into another source in chapter SECREF6 .,Digital Library of the IPSJ,"The IPSJ is a Japanese society in the area of information processing and computer science. It was founded in April 1960 and, by its own account, helps evolving computer science and technology and contributes new ideas in the digital age. It regularly publishes the magazine “Information Processing” (jōhō shori) and a journal, holds symposiums and seminars, Special Interest Groups issue technical reports and hold conferences. It is also the Japan representative member of the IFIP and established partnerships with the IEEE, ACM and other organizations. -2 IPSJ develops drafts of international standards and Japanese industrial standards as well. Eight regional research sections are widespread over Japan. IPSJ had over 17000 members in March 2011. ( BIBREF15 ; BIBREF16 ) The IPSJ provides a Digital Library (referenced as IPSJ DL in this work) where everybody can search Japanese papers in the field of computer science. The search page can be displayed in Japanese and English, most papers are written in Japanese. Free papers are accessible in PDF format, non-free can be bought. A tree view provides the order structure of the papers and there is a keyword search available. We are especially interested in the metadata export functions, though. The online application offers following export formats: OAI-PMH BibTeX OWL SWRC WEKO Export For our purposes the OAI-PMH is the most suitable solution because we can send simple HTTP requests to the server and get publication metadata as a result. It “provides an application-independent interoperability framework based on metadata harvesting” ( BIBREF17 ) and consists of two groups of participants. Data Providers can be servers hosting and supplying the metadata. Service Providers take the harvester role and process the recieved metadata from the Data Provider. The application-independent interoperability is achieved by using XML as basic exchange format. Arbitrary programs can parse XML input data very easily, so can we. While accessing the server, the data can be extracted in several ways. We can either access an OAI-PMH repository by the repository name, the metadata format prefix of the record and a unique identifier or get a list of records with only one request. A request for a list of records looks like this: 1.5 em1.5 em(*@@*)false6pt http: //ipsj.ixsq.nii.ac.jp/ej/ ?action=repository_oaipmh&verb=ListRecords &metadataPrefix=oai_dc It may also contain a start date and an end date or a resumption token. The headers of records include a corresponding time stamp. The server's response to a request offers only 100 publications. We need this resumption token because it determines the point where we resume the harvest.       In the beginning and for debugging, it was more comfortable to increment a counter that acts as the unique identifier and send requests for single entries with the respective ID multiple times. Fortunately, the entries can be addressed by such an integer ID (plus some constant name): 1.5 em1.5 em(*@@*)false6pt http: //ipsj.ixsq.nii.ac.jp/ej/  ?action=repository_oaipmh&verb=GetRecord&metadataPrefix=oai_dc  &(*@\textbf{identifier}@*)=oai:ipsj.ixsq.nii.ac.jp:(*@\textbf{27130} @*) The last entry containing real publication metadata has the suffix integer 87045 in its ID. After that some entries with status INLINEFORM0 follow. If we continue requesting even higher IDs, we soon get only a reply with the error code INLINEFORM1 anymore, implying there are no publications with higher IDs. We will discuss the implementation of an OAI-PMH harvester for the IPSJ DL in section UID99 .",DBLP Project,"The DBLP is a worldwide known database for publication metadata in the field of computer science. Ley BIBREF19 gives a brief explanation of the DBLP, additional information is extracted from the online DBLP FAQ BIBREF20 . It was started in 1993 as a test server for web technologies and named “Database systems and Logic Programming” in the beginning. But it grew and became a popular web application for computer scientists. The Computer Science department of the University of Trier founded the project, since summer 2011 it is a joint project of Schloss Dagstuhl - Leibniz Center for Informatics and the University of Trier. “For computer science researchers the DBLP web site is a popular tool to trace the work of colleagues and to retrieve bibliographic details when composing the lists of references for new papers. Ranking and profiling of persons, institutions, journals, or conferences is another sometimes controversial usage of DBLP.” ( BIBREF19 ) The publication metadata is stored in the XML file INLINEFORM0 containing more than 2 million publications and exceeding a size of 1 GB (state of October 2012). An excerpt of the beginning of INLINEFORM1 can be found in the appendix section SECREF171 . The header dictates ISO-8859-1 (Latin-1) as encoding of the file. Considering that we want to import Japanese names in kanji (which are not included in Latin-1) we must handle that issue somehow. We will discuss the solution in section UID121 . The web front end of the DBLP provides an overview of coauthor relationships by a Coauthor Index (see figure FIGREF53 ). The Coauthor Index can be found at the author's page after the list of the author's publications itself. It shows all coauthors, common papers and categorizes the coauthors into groups that worked together by giving the author names corresponding background colors. In his diploma thesis Vollmer BIBREF23 gives useful hints in terms of converting the INLINEFORM0 file to a relational database. He also compares the performance of several relational database management systems for this conversion. The DBLP team developed a special format for the integration of new publications. It is called Bibliography Hypertext (BHT), is based on HTML and similar to the HTML code of the tables of contents (TOCs) at the DBLP website. An example of a publication list in BHT format can be found in the appendix in section SECREF168 . A BHT file has the following structure. The header (text between h2 tags) contains the volume, the number/issue and the date of issue. A list of corresponding publications follows next. The list is surrounded by a beginning and a closing INLINEFORM0 tag, single publication entries start with a INLINEFORM1 tag. A comma is used for the separation of authors while there should be a colon after the last author name. Then comes the title which has to end with a period, question mark or exclamation point. The next line provides the start and end page in the volume/issue. At last, an optional URL can be added by an INLINEFORM2 element to specify an “electronic edition” for a paper. Some guidelines need to be considered, too: there is no closing INLINEFORM0 tag initials should be avoided (full name is preferred) titles with only upper case letters should be avoided “0-” is the default page number value if the page information is missing The BHT file may contain additional information. For example, conference proceedings may have more headers to achieve a better clarity. But it should be as close to the proposed format as possible to guarantee an easy import without unnecessary burdens. ( BIBREF24 ; BIBREF20 , “What is the preferred format to enter publications into DBLP?”) We will extend the original format in section UID121 to satisfy our needs in the context of Japanese papers.",Personal Name Matching,"“The important thing is not to stop questioning; curiosity has its own reason for existing.” (Albert Einstein) After looking at transcription systems, Japanese personal names and publication metadata sources, we will now have to look at Personal Name Matching to enable us to deal with the Japanese names extracted from the metadata sources. First we will discuss Personal Name Matching in general and then problems of Personal Name Matching for Japanese names in particular. The expression Personal Name Matching comes from the work by Borgman and Siegfried BIBREF25 and is used here as in the extended definition from Reuther's work ( BIBREF26 , p. 48-51). Borgman and Siegfried only talk about synonyms. Synonyms are possible names for the same person. Reuther extended the definition by also including homonyms. A name is a homonym if it can belong to several persons. Personal Name Matching is known by other titles in literature, too. Niu et al. BIBREF27 discuss Cross Document Name Disambiguation: “Cross document name disambiguation is required for various tasks of knowledge discovery from textual documents, such as entity tracking, link discovery, information fusion and event tracking. This task is part of the co-reference task: if two mentions of the same name refer to same (different) entities, by definition, they should (should not) be co-referenced. As far as names are concerned, co-reference consists of two sub-tasks: On et al. BIBREF28 formally express their Name Disambiguation problem as follows: “Given two long lists of author names, INLINEFORM0 and INLINEFORM1 , for each author name INLINEFORM2 , find a set of author names, INLINEFORM3 such that both INLINEFORM4 and INLINEFORM5 are name variants of the same author.” ( BIBREF28 ) In contrast to the previous definitions Han et al. BIBREF29 define Name Disambiguation like this: “Name disambiguation can have several causes. Because of name variations, identical names, name misspellings or pseudonyms, two types of name ambiguities in research papers and bibliographies (citations) can be observed. The first type is that an author has multiple name labels. For example, the author `David S. Johnson' may appear in multiple publications under different name abbreviations such as `David Johnson', `D. Johnson', or `D. S. Johnson', or a misspelled name such as `Davad Johnson'. The second type is that multiple authors may share the same name label. For example, 'D. Johnson' may refer to `David B. Johnson' from Rice University, `David S. Johnson' from AT&T research lab, or `David E. Johnson' from Utah University (assuming the authors still have these affiliations).”( BIBREF29 ) The citations above show that there are many expressions for Personal Name Matching (or sub-categories) which are not equally used by different authors. Niu et al. and On et al. restrict Name Disambiguation to finding synonyms, Han et al. include homonyms in their definition. Even more related expressions can be found in literature. As mentioned, we will use Personal Name Matching in this work as Reuther uses it. The main aspect of Personal Name Matching is handling synonyms and homonyms. Trying to express the problems formally leads to the following description: Let INLINEFORM0 be a set of persons, especially characterized by their names, in a certain data set and INLINEFORM1 a set of all existing persons. We are also being given a function INLINEFORM2 and a relation INLINEFORM3 . The actual problems can be described as with INLINEFORM0 ; INLINEFORM1 ; INLINEFORM2 . Case UID60 checks for each person INLINEFORM0 from the person set INLINEFORM1 whether another person INLINEFORM2 from INLINEFORM3 exists, so that their name labels are different ( INLINEFORM4 ) but the person is the same ( INLINEFORM5 ). So this case covers the synonym problem because the same person has several names here. Case UID61 checks for each person INLINEFORM0 from the person set INLINEFORM1 whether another person INLINEFORM2 exists in INLINEFORM3 , so that their name labels are equal ( INLINEFORM4 ) but the persons behind the names differ ( INLINEFORM5 ). So this case covers the homonym problem because the same name is taken by several people. The problem Personal Name Matching arises because such a relation INLINEFORM0 usually does not exist and needs to be approximated as good as possible: INLINEFORM1  Thanks to appropriate similarity measurements and a matching threshold INLINEFORM0 , we can find such a relation INLINEFORM1 which is approximately equivalent to the original relation INLINEFORM2 . The main task in Personal Name Matching is finding a good similarity measure for the described problem. ( BIBREF26 , p. 52) Let us have a look at a vivid example. The birth name of the famous actor Michael Keaton is Michael John Douglas. Keaton took a pseudonym because he could have been confused with the more famous actor Michael Douglas. Synonyms for Keaton are “Michael Keaton”, “Michael Douglas”, “Michael John Douglas”, “Michael J. Douglas”, “M. Keaton” or “M. J. Douglas”. -1 On the other hand, when we hear the name “Michael Douglas” we cannot be sure which famous actor is referred to, because Michael Douglas is a valid name for both of them. Figure FIGREF62 illustrates this Personal Name Matching problem with Michael Keaton. The process of Personal Name Matching can be divided into the following steps ( BIBREF26 , p. 56-87): Criteria for the evaluation of such a process are Precision and Recall ( BIBREF35 , p. 75-81; BIBREF26 , p. 83-85). Let INLINEFORM0 be a set of items, INLINEFORM1 be the set of relevant items (e.g. synonyms) with INLINEFORM2 and INLINEFORM3 be the answer of a request. In our scenario, the request is usually the question “Is the item INLINEFORM4 a synonym, or accordingly INLINEFORM5 ?”. Then we can define: INLINEFORM6 INLINEFORM7  Precision testifies whether the reported synonyms during the Name Matching process are really synonyms, Recall allows us to say whether there are synonyms which have not been found. We use a combination of the Jaccard Similarity Coefficient and Levenshtein Distance in our tool. Bilenko et al. BIBREF36 explain these string matching methods isolated. Given two word sets INLINEFORM0 and INLINEFORM1 , the simple Jaccard Similarity Coefficient is: INLINEFORM2  The Levenshtein Distance uses the operations replacement, insertion and deletion of a character and is defined by a matrix. Let INLINEFORM0 and INLINEFORM1 be words, INLINEFORM2 and INLINEFORM3 their lengths. Then we can define: DISPLAYFORM0  We modify the Jaccard Similarity Coefficient in a way that it classifies two set items as intersected if their Levenshtein Distance is lower than a certain threshold. In addition to the general Personal Name Matching, we must take the characteristics of Japanese names into account. Particularly the usage of kanji and several possibilities to transcribe a name make it hard to compare Japanese names. For example, we cannot compare kanji names from the IPSJ DL with the author names in DBLP. Even though kanji are suited best for name comparison it does not work here because the standard encoding of names in DBLP is “Latin-1” which does not support kanji natively. A big problem for our work is revealed by looking at the given name Akiko with its kanji representation 章子. As we can see in table TABREF71 章子 has several possible readings besides Akiko (left column) and Akiko written in Latin characters does not determine a nonambiguous match in kanji (right column). The same problem applies to Japanese family names. Table TABREF72 presents the problem with Kojima as a family name example.",Preparation of Japanese Papers for the Import Into the DBLP Data Set,大事の前の小事 Daiji no mae no shōji (Who wants to achieve big things must do the little things first.) Japanese saying This chapter explains the approach to process and combine the various data sources so that we can import Japanese publications in the end. We will proceed step by step to make the ideas behind the solution as comprehensible as possible.,General Approach,"First we will construct a table in a relational database containing information about Japanese names and their transcriptions by converting the ENAMDICT name dictionary. Then we set up a data structure for Japanese names that handles the problem of assigning a given and a family name to a newly instantiated author during parsing the publications of IPSJ DL. At last, we will discuss the actual and titular integration of Japanese papers into the DBLP data set including an explanation that shows how to create a harvester for the OAI-PMH protocol.",Converting an ENAMDICT File to a Relational Database,"The first step towards being able to handle Japanese names is distinguishing given and family name in the input text. A relational database containing information about Japanese names and their transcriptions is useful for this task. The database should contain names in kanji, their transcriptions in hiragana and Latin characters and the name type to have a good match with the data source ENAMDICT and to provide all necessary name information we need. To fill the empty database, the ENAMDICT file needs to be analyzed and its data needs to be extracted. The entries usually have the form KANJI [TRANSCRIPTION] /LATIN (TYPE)/. We can take the following line as an example of an existing entry: 森田 [もりだ] /Morida (s)/ A parser should export the single entries. First it saves the text between the slashes and searches for the type of the entry. It must be assured that all person name types and no undesired or alleged types will be stored. Types can consist of the characters “s” (surname), “g” (given name), “f” (female name), “m” (male name), “u” (unclassified name), “p” (place name), “h” (full name of a particular person), “pr” (product name), “co” (company name) or “st” (station name). But only the types “s”, “g”, “f” and “m” are important in this case because the parser should only store person names in the database. One exception are the unclassified names and they need to be stored too because they can also contain person names. Using unclassified names carelessly leads to problems, though. On the one hand it is useful if you find a match for the given name but not for the assumed family name. Then it helps to find an unclassified name matching the assumed family name. On the other hand some unclassified names in the ENAMDICT file decrease the data quality of the database. The entry スターウォーズ /(u) Star Wars (film)/ shows that there are undesired names like film titles in the category “unclassified”. The example also reveals that there is no overall standard for an entry format. Analyzing the file leads to following observations: text in round brackets might be type or additional commentary (see entry example above) when only hiragana or katakana are used instead of kanji to display the Japanese name the transcription part is missing because it is not required (see entry example above) the type information in brackets might actually consist of several type declarations, separated by commas the type information might be placed before or after the transcription in Latin characters one entry line might contain several possibilities to interpret the name, the example イブ /(f) Eve/(u) Ib/Ibu (f)/(m) Yves/ clarifies this aspect We must consider these observations when we implement the parser. To handle the problems in UID76 and UID78 we can filter the contents in round brackets. One possibility is using a regular expression like (,|s|u|g|f|m|p|h|pr|co|st) INLINEFORM0 to filter all valid types. Regular expressions are powerful and popular tools for pattern matching. In our case we are looking for valid type expressions including commas to get rid of commentaries. After eliminating commentaries we also want to get rid of unwanted types like place names. So we filter again and only process desired types this way. To handle UID77 we just ignore missing transcriptions in square brackets. Our parser also needs to be flexible enough to deal with observation UID79 which means that it must expect the type(s) at two possible places (before and after the transcription in Latin characters). We can handle the last observation UID80 by using recursive function calls. We call the function that exports one entry with a modified parameter value within the function itself when there is more than one entry in the input line (noticeable by additional slashes). Before parsing we need to change the original encoding of the ENAMDICT file from “EUC-JP” to “UTF-8” to make it compatible with our program. During parsing a few inconsistencies in the syntax of the ENAMDICT file occurred: there were four times no slash in the end of the entry: 甲子太郎 [かしたろう] /Kashitarou (m) there was once an unnecessary closing bracket without an opening bracket: 近松秋江 [ちかまつしゅうこう] /Chikamatsu Shuukou) (h)/ there was once a backslash where a square bracket was supposed to be put: キルギス共和国 [キルギスきょうわこく\ /(p) Kyrgyz Republic/Kirghiz Republic/ Instead of constructing a workaround for these problems we should rather correct the only few inconsistencies manually.",A Data Structure for Japanese Names,"We will construct a class which is responsible for handling Japanese names and representing them in a convenient way. Therefore, it must be able to save the name in kanji and in at least one Latin transcription. The transcription is necessary to compare found authors in IPSJ DL with authors in the DBLP. The kanji name can be stored as additional author metadata in the DBLP later. Our goal is a standardized representation of a Japanese person. So first we can construct a simple helper class for a single name containing given and family name as strings. This class can be applied to both kanji and Latin names. Our Japanese person usually has these two name representations. When getting an input name from the IPSJ DL we try to determine the separation point and categorize the tokens into given and family names. The separation point can mostly be identified by white space or a comma between the words. The categorization is done by including information from ENAMDICT. Thanks to ENAMDICT's classification into name types we can use this information to categorize our input name tokens into given and family names. However, we have to cover some unusual cases too because IPSJ DL has no standardized way to provide names. So we get names in various formats. For example, there are entries in which the family name follows the given name directly without any separation markers. Then we can try to take advantage of upper and lower case letters assuming that an uppercase letter means the beginning of a new name token. But we must also be aware of existing input names like “KenjiTODA”. If we get a longer sequence of uppercase letters, this sequence is probably a family name. We can filter these names with a regular expression like [A-Z][a-z]{1,}[A-Z]{3,} (first character is an uppercase letter, followed by at least one lowercase letter, followed by at least three uppercase letters). We also have to recognize abbreviated names and normalize Latin names. Let us have a look at what we can observe about necessary transcription customizations. One peculiarity is that Japanese like to transcribe their names with an INLINEFORM0 instead of a double vowel. An example is “Hitoshi Gotoh”. The INLINEFORM1 symbolizes the lengthening of a vowel and is a substitute for INLINEFORM2 or INLINEFORM3 in this case. To enable our class to find names like this in ENAMDICT, we have to replace the INLINEFORM4 's lengthening a vowel by the vowel itself because ENAMDICT entries contain double vowels instead of INLINEFORM5 's with this semantic function. Another observation is ENAMDICT's usage of the Hepburn transcription system throughout the entire dictionary. So we have to convert the name to match the Hepburn system and to check a name via ENAMDICT. The needed character replacements for a conversion into the Hepburn system are shown in table TABREF86 (see also figure FIGREF165 in the appendix). In addition to the replacements from table TABREF86 , we must consider that names usually start with uppercase letters and replace “Tu”, “Ti”, “Sya” and so on by “Tsu”, “Chi”, “Sha”, etc. as well. The Japanese INLINEFORM0 is sometimes transcribed as INLINEFORM1 . If INLINEFORM2 is followed by INLINEFORM3 or INLINEFORM4 , this INLINEFORM5 is likely to be transcribed as INLINEFORM6 . The reason is a correlative modification in the pronunciation of INLINEFORM7 in these cases. For example, the family name Kanbe is often transcribed as Kambe in the IPSJ DL data set. -1 Double vowels are sometimes completely dropped in some IPSJ DL author elements. While this might be okay for aesthetic reasons when transcribing the own name, it becomes a problem when we try to find a matching name in a dictionary like ENAMDICT. So we also have to check additional modified names. If there is a single vowel in the name, we must also check the same name whose vowel has become a double vowel. If several single vowels occur in a name, the number of names to be checked rapidly increases too. We have to pay special attention to the doubling of the vowel INLINEFORM0 because INLINEFORM1 AND INLINEFORM2 are possible doublings for the single INLINEFORM3 . Doubling the vowel INLINEFORM4 leads either to INLINEFORM5 or INLINEFORM6 . All other double vowels are intuitive: INLINEFORM7 becomes INLINEFORM8 , INLINEFORM9 becomes INLINEFORM10 , INLINEFORM11 becomes INLINEFORM12 . Taking “Gotoh” as an example we remove the INLINEFORM13 first and check a list of names via ENAMDICT. The list of names consists of “Goto”, “Gooto”, “Gouto”, “Gotoo”, “Gotou”, “Gootoo”, “Goutoo”, “Gootou” and “Goutou”. We can remove “Goto”, “Gooto” and “Gouto” from the list if we know that the INLINEFORM14 (representing a double vowel) has been removed before. If the input metadata contains a Latin and kanji representation of the author's name, we will try to find a match for these. Names in kanji usually do not have any separation mark, so we must distinguish given and family name by taking advantage of the ENAMDICT dictionary and checking the possible name combinations. Processing author names without kanji representation is okay but a missing Latin representation becomes a problem when it comes to actually integrating the publication into the DBLP data set because all DBLP data are supposed to have a Latin representation. The solution is a search for name candidates (we will discuss it more detailed in section UID121 ). We cannot be sure that our name matching for Latin and kanji names always succeeds. Therefore, we add some status information to our Japanese name to get a chance to evaluate the outcome of the program. Possible status types are:  The status “ok” means that given and family name have successfully been found in the name dictionary and (if available) the kanji names have successfully been assigned to their corresponding name in Latin characters.  An undefined status usually means that the Latin name is missing. A missing Latin name leads to a never changed name status. In these cases, the name in kanji usually exists anyway.  This is the status type for an abbreviated name like “T. Nakamura”.  If this status occurs, the Latin name could not be found in the name dictionary.  If a kanji name has not been found in the name dictionary or could not be assigned to the Latin name, this status will occur.  As the name suggests, this status means that the data quality of the publication metadata source is most likely bad. Our tool can handle some of these cases well by normalizing the name.  We could have stumbled upon a name anomaly when we see this status type. During implementation this status was narrowed down to a possible name anomaly for abbreviated names.  This status indicates a critical name anomaly. This is the only case in which the tool cannot even give a recommendation for given and family name. The output is the full name of the input data for both given and family name. In chapter SECREF5 we discussed synonyms and homonyms. With the strategies from above we can deal with synonyms pretty well. Yet, homonyms cannot be recognized this way and are not covered at all by our tool.",How successful are they at matching names of authors in Japanese and English?,70e596dd4334a94844454fa7b565889556e2358d,infinity,unfamiliar,no,,2cfd959e433f290bb50b55722370f0d22fe090b7,False,,,"Fortunately, 180221 of 231162 author names could be matched successfully. There are many reasons for the remaining uncovered cases. 9073 Latin names could not be found in the name dictionary ENAMDICT and 14827 name matchings between the names' Latin and kanji representations did not succeed. These names might be missing at all in the dictionary, delivered in a very unusual format that the tool does not cover, or might not be Japanese or human names at all. Of course, Japanese computer scientists sometimes also cooperate with foreign colleagues but our tool expects Japanese names and is optimized for them. Both IPSJ DL and ENAMDICT provide katakana representations for some Western names. However, katakana representations for Western names are irrelevant for projects like DBLP. But for instance, Chinese names in Chinese characters are relevant. Understandably, our tool does not support any special Personal Name Matching for Chinese names yet because our work is focused on Japanese names. The tool does not take account of the unclassified names of ENAMDICT by default. We can increase the general success rate of the Name Matching process by enabling the inclusion of unclassified names in the configuration file but the quality of the Name Matching process will decrease because the correct differentiation between given and family name cannot be guaranteed anymore. An unclassified name may substitute a given or a family name.","Fortunately, 180221 of 231162 author names could be matched successfully. There are many reasons for the remaining uncovered cases. 9073 Latin names could not be found in the name dictionary ENAMDICT and 14827 name matchings between the names' Latin and kanji representations did not succeed. These names might be missing at all in the dictionary, delivered in a very unusual format that the tool does not cover, or might not be Japanese or human names at all. Of course, Japanese computer scientists sometimes also cooperate with foreign colleagues but our tool expects Japanese names and is optimized for them.",ee8cf7ffcb084cacb2cfdf59494131c46ae79b21,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,Is their approach applicable to papers outside computer science?,18dab362ae4587408a291a55299f347f8870e9f1,infinity,unfamiliar,no,,2cfd959e433f290bb50b55722370f0d22fe090b7,True,,,,,40c5035eebe87745a6ce8f1c71483dd15138d078,258ee4069f740c400c0049a2580945a1cc7f044c,Do they translate metadata from Japanese papers to English?,9c2de35d07f0d536bfdefe4828d66dd450de2b61,infinity,unfamiliar,no,,2cfd959e433f290bb50b55722370f0d22fe090b7,False,False,,,,5bf5650a2ceaa12ee9e867b310bdf1dcd78847b8,258ee4069f740c400c0049a2580945a1cc7f044c,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,12-Figure2.1-1.png,Figure 2.1: Hiragana gojūon table,13-Figure2.2-1.png,Figure 2.2: Katakana gojūon table,14-Figure2.3-1.png,"Figure 2.3: “Hiragana” written in hiragana, “katakana” written in katakana, “kanji” written in kanji",15-Table2.1-1.png,"Table 2.1: Language-related transcription systems, source: [KS87], p. 53",20-Figure3.1-1.png,Figure 3.1: “Legibility is particularly important!!”,21-Figure3.2-1.png,Figure 3.2: Anna! Yes! Hai!,Import Into the DBLP Data Set,"To be able to import the harvested data into the DBLP, we still need to make the existing publication data processable in an appropriate way for our program, construct a coauthor table for these data, compare publications from the Digital Library of the IPSJ with those available in the DBLP project and provide the new publication metadata for the DBLP adequately. It is important to convert the DBLP file INLINEFORM0 to a relational database to gain an easier and more efficient access to the data while running our program. We are mainly interested in the basic publication metadata. So we will skip some non-publication records of the DBLP like INLINEFORM1 elements. Our publication database table shall contain columns for an ID, the authors, title, publication year, journal title, journal pages and the volume. Whenever we come across the beginning of a publication type element ( INLINEFORM3 , INLINEFORM4 , INLINEFORM5 , INLINEFORM6 , INLINEFORM7 , INLINEFORM8 , INLINEFORM9 ) during parsing, we reinitialize the variables which store this metadata for the table columns. When we encounter the according XML end tag of the publication we add an SQL INSERT command to a batch of commands. This batch is regularly executed after processing a certain amount of publications. The regular execution of batches allows a better performance than sending single INSERT commands to the database server. There are some recommendations in the DBLP FAQ BIBREF20 for parsing the INLINEFORM10 file. We use the Apache Xerces parser instead of the standard Java SAX parser and need to increase the allocatable heap space for our parser. While parsing the DBLP file we can construct a table with coauthor relationships along with the DBLP publication table. This coauthor table stores two author names and a publication ID. The ID shows which publication has been written together by the authors and matches the ID in the DBLP publication table. New coauthor relationships will only be inserted if there are at least two authors mentioned in the metadata. If the metadata mentions more than two authors, every possible pair of authors will be inserted into the database. As already explained in section SECREF39 , we access the OAI-PMH repository by the repository name and the metadata format prefix to get a list of publication metadata entries. The specification of OAI-PMH 2.0 BIBREF17 describes a possibility to retrieve a list of all metadata formats which a Data Provider has to offer. The HTTP request 1.5 em1.5 em(*@@*)false6pt http: //ipsj.ixsq.nii.ac.jp/ej/?action=repository_oaipmh  &verb=ListMetadataFormats informs us that there are two metadata formats called oai_dc and junii2. oai_dc is the standard Dublin Core format all Data Providers provide, also traceable in the protocol specification. The “Implementation Guidelines for the Open Archives Initiative Protocol for Metadata Harvesting” BIBREF37 classify the metadata format oai_dc as mandatory. The name junii2 suggests that it is a self-developed format of the National Institute of Informatics (in Tokyo). Comparing these two in IPSJ DL, we notice that junii2 provides a more accurate description of the data, for example regarding additional XML attributes telling us whether the element value is English or Japanese. This additional information is helpful when we process the data in a later step and is missing in the oai_dc representation of the IPSJ server's data. So we will take the metadata prefix junii2 as initial point for harvesting the server's metadata. Figure FIGREF102 shows an according metadata example (also compare figure FIGREF46 ). The harvesting includes the following steps: we load the DBLP publication, coauthor relationship and the ENAMDICT data into the RAM we access the IPSJ server to get publication metadata we parse the accessed XML metadata (concerning the thoughts from section SECREF85 ) and store the needed publication data temporarily in the RAM. we add the parsed publication to an SQL command batch to insert the metadata into a relational database (the batch is regularly executed) we create a BHT file for the parsed publication at the end we go into all directories with BHT files and concatenate them to one bigger BHT file During the implementation and testing, some exceptional incidents occurred. We try to cover them besides the expected difficulties like Personal Name Matching and transcriptions. For example, we get “NobukazuYOSHIOKA” as a full input name. Algorithm UID99 shows a way to handle these unusual input data. Japanese sometimes write their family names in upper case letters to distinguish given and family name. [htb]  INLINEFORM0 : full input name  INLINEFORM0 : list of name representations for a Japanese person function split( INLINEFORM0 ): searches for regular expression and splits text,  splitted text does not contain text that matches the regular expression function normalize( INLINEFORM0 ): normalizes personal name new name for person found and added (given and family name separated)  INLINEFORM0 matches regular expression INLINEFORM1 INLINEFORM2 split INLINEFORM3 INLINEFORM4 split INLINEFORM5 normalize INLINEFORM6 INLINEFORM7 BAD_DATA_QUALITY_IN_SOURCE INLINEFORM8 add(new PersonName INLINEFORM9 Categorizing names like “NobukazuYOSHIOKA” Another observation during testing the program and checking the data is the following. Searching the Japanese given name “Shin'ichi” in the DBLP we notice that there is no uniform way to store certain names in the database. We find “Shin'ichi Aihara” but also “Shin-ichi Adachi” along with other results indicating the same phenomenon. So we see the apostrophe and the hyphen are used equally as syllable separators (we discussed the syllable separation in chapter SECREF14 ). Comparing the author “Shinichi Horiden” from the IPSJ data set and the one from the DBLP data set we can assume they are the same person because they have common coauthors (e.g. Kenji Taguchi and Kiyoshi Itoh) in both databases. The IPSJ data set tells us that the name written in kanji is 本位田真一. We are interested in the part 真一 (Shin'ichi) because we get to know that the separator symbol is sometimes missing. The kanji indicates the syllables INLINEFORM0 , especially focused on INLINEFORM1 and INLINEFORM2 instead of INLINEFORM3 . We would expect an additional separator symbol for a clear (nonambiguous) transcription; but obviously, it has been dropped in this case. A separator symbol can also be found when some double vowels occur. For example, we find “Toru Moto'oka” (元岡達) instead of “Toru Motooka”. This makes it easier to identify the reading of a single kanji (元 moto, 岡 oka, 達 toru). When a separator symbol is needed for a clear transcription, an apostrophe is used as separator symbol in ENAMDICT. While ENAMDICT always uses an apostrophe as separator symbol, DBLP and IPSJ DL use an apostrophe, a hyphen or the separator symbol is missing. We must consider these differences in the data sources for a successful import. For an easier name matching between names in the ENAMDICT and IPSJ DL data set we can add names containing an apostrophe once as they are and once without apostrophes to the relational database when we parse the ENAMDICT file to store person names in a relational database. Our tool has a statistics class to get an overview over the parsed input data and the quality of the output data. We will have a look at these statistics created after the harvest. There are 81597 records with publication metadata and 8562 records which are marked as INLINEFORM0 in the parsed data. Figure FIGREF114 shows a visualization in pie chart form. The publication types are declared as “Technical Report”, “Conference Paper”, “Journal Article”, “Departmental Bulletin Paper” or “Article” (compare the table TABREF115 and figure FIGREF116 ). The statistics also reveal that 74971 publications are published in Japanese, only 4456 in English (compare the pie chart in figure FIGREF117 ). Our tool detects 1325 publications which are already included in DBLP. A publication is considered found in both databases if the title is the same and at least one author is the same. The most interesting statistics for our work are these about the evaluation of the quality of author name assignments (compare the bar chart in figure FIGREF119 ): Fortunately, 180221 of 231162 author names could be matched successfully. There are many reasons for the remaining uncovered cases. 9073 Latin names could not be found in the name dictionary ENAMDICT and 14827 name matchings between the names' Latin and kanji representations did not succeed. These names might be missing at all in the dictionary, delivered in a very unusual format that the tool does not cover, or might not be Japanese or human names at all. Of course, Japanese computer scientists sometimes also cooperate with foreign colleagues but our tool expects Japanese names and is optimized for them. Both IPSJ DL and ENAMDICT provide katakana representations for some Western names. However, katakana representations for Western names are irrelevant for projects like DBLP. But for instance, Chinese names in Chinese characters are relevant. Understandably, our tool does not support any special Personal Name Matching for Chinese names yet because our work is focused on Japanese names. The tool does not take account of the unclassified names of ENAMDICT by default. We can increase the general success rate of the Name Matching process by enabling the inclusion of unclassified names in the configuration file but the quality of the Name Matching process will decrease because the correct differentiation between given and family name cannot be guaranteed anymore. An unclassified name may substitute a given or a family name. There are 1203 entries that were qualified as “bad data quality in publication metadata source”. They might be handled alright but they are particularly marked to indicate that these cases should also be reviewed manually before any import action is performed. The numbers of abbreviated names, possible name anomalies and name anomalies are very low. While processing author names which will be later qualified as “possible name anomaly”, the tool cannot decide whether the assignment has been correct or the name is an anomaly. “Name anomalies” are critical anomalies that could not be categorized into any other status. There could be a few uncovered flaws, for example HTML or code in titles. We must be aware of those when we do the actual import into the DBLP data set. We will discuss the creation of BHT files and important extensions for the BHT format that fit the requirements of Japanese papers well, based on our knowledge from section SECREF49 . As mentioned, the header dictates ISO-8859-1 (Latin-1) as encoding of the file INLINEFORM0 . Ley's work BIBREF19 reveals that we can use XML/HTML entities to solve this problem. Authors have person records in the DBLP providing additional information. For example, we can find the following entry for Atsuyuki Morishima (森嶋厚行) in the XML file: 1.5 em1.5 em(*@@*)false6pt <www mdate=""2008-02-20"" key=""homepages/m/AtsuyukiMorishima"">  <author>Atsuyuki Morishima</author>  <title>Home Page</title>  <url>http://www.kc.tsukuba.ac.jp/~mori/index.html</url>  <note>&#x68EE;&#x5D8B;&#x539A;&#x884C;</note> </www> We must extend the BHT format to fulfill the requirements and add extra metadata for authors, title and relevant process information. The author talked to members of the DBLP team personally and got the permission to extend the original BHT format to enable us to adapt the format to Japanese papers. Our additions are well formed XML elements. We must substitute all non-ASCII characters by escape characters (XML entities) to ensure the compatibility for DBLP. The additional elements are: Every author that has a kanji representation in its metadata gets an originalname element: 1.5 em1.5 em(*@@*)false6pt  <originalname latin=""Shinsuke Mori"">&#x68EE;,&#x4FE1;&#x4ECB;  </originalname> If available, the Latin representation is added as an attribute INLINEFORM0 to avoid confusion on assigning the extra information to the right author later on. The element content has a fixed structure. The family name comes first, followed by a comma and the given name. Every author gets a status information that evaluates the author name assignment. It is displayed by a status element: 1.5 em1.5 em(*@@*)false6pt  <status name=""Shinsuke Mori"">ok</status> The connected author is added as an attribute INLINEFORM0 . If there is no Latin representation of the name of an author, we will add Latin name candidates to the BHT file: 1.5 em1.5 em(*@@*)false6pt  <namecandidates kanji=""&#x83C5;&#x8C37;&#x6B63;&#x5F18;"">Shougu Sugatani, Seihiro Sugatani, Tadahiro Sugatani, Masahiro Sugatani, Shougu Suganoya, Seihiro Suganoya, Tadahiro Suganoya, Masahiro Suganoya, Shougu Sugaya, Seihiro Sugaya, Tadahiro Sugaya, Masahiro Sugaya, Shougu Sugetani, Seihiro Sugetani, Tadahiro Sugetani, Masahiro Sugetani, Shougu Sugenoya, Seihiro Sugenoya, Tadahiro Sugenoya, Masahiro Sugenoya</namecandidates> The connected kanji representation is added as an attribute kanji in the namecandidates element. We seek the kanji in ENAMDICT and output all possible name combinations in a comma separated list. If the original language of the title is Japanese, we will add this title to the BHT file: 1.5 em1.5 em(*@@*)false6pt  <originaltitle lang=""ja"" type=""Journal Article"">&#x70B9;&#x4E88;&#x6E2C;&#x306B;&#x3088;&#x308B;&#x81EA;&#x52D5;&#x5358;&#x8A9E;&#x5206;&#x5272;</originaltitle> The XML element originaltitle has the attributes lang (for the paper language) and type (for the publication type). The tool searches the authors in DBLP and tries to find additional common coauthors in DBLP. If at least two of the main authors of the paper also worked with a certain other person (that is retrieved from DBLP), this person is added to the comma separated list. The Personal Name Matching of author names uses a combination of Levenshtein Distance and Jaccard Similarity Coefficient here. 1.5 em1.5 em(*@@*)false6pt  <commoncoauthors>Masato Mimura</commoncoauthors> If the tool finds the paper in DBLP, we also add the DBLP key. Records, such as elements with publication metadata, have a unique key in DBLP. 1.5 em1.5 em(*@@*)false6pt  <dblpkey>conf/iscas/HiratsukaGI06</dblpkey> An example of a BHT file in SPF can be found in the appendix in section SECREF170 (also compare with the original BHT format in section SECREF168 ). After we have finished parsing all Japanese papers, we concatenate the BHT files in SPF that belong together to one bigger BHT file INLINEFORM0 . Publications, respectively BHT files, that belong together are recognizable by the directory structure. If they belong together, they will be in the same directory. We must simply go through the BHT root directory recursively.",Conclusion and Future Work,"“Creativity is seeing what everyone else sees, but then thinking a new thought that has never been thought before and expressing it somehow.” (Neil deGrasse Tyson) The integration of Japanese papers into the DBLP data set has revealed some major problems. The nonambiguous representation of Japanese names (and paper titles, etc.) is done by kanji while DBLP's standard encoding is Latin-1 and Japanese characters are only optionally added to the publications' metadata. This leads to the need of transcribing the Japanese names which in turn also evokes new problems because there is not the transcription but rather a lot of transcription possibilities. In addition to that, we must ensure a certain data quality even if one data source sometimes lacks this quality. Due to name matching with a name dictionary, format checking and conversions (if necessary), we can actually correct some flaws or at least assimilate the data into our project. The problem of synonyms is dealt with by transcription manipulations, homonyms could not be addressed in this work. Reuther ( BIBREF26 , p. 159-164) describes an idea to handle homonyms. We could extend our tool by a Coauthor Index as in DBLP for the publications of the IPSJ DL. The idea is based on the assumption that scientists often publish their papers with the same people as coauthors. If the coauthors match a certain coauthor group, the author is considered the same. -1 If the author's coauthors are not members of the expected coauthor groups, the author could be a different person than we expected and we might have a homonym here. The developed tool is usable and provides among relational databases customized Bibliography Hypertext (BHT) files as output data. Customizations were necessary to optimize the BHT files for Japanese papers and additional important metadata information. Desired but missing metadata like contributors or a short description of the content of a paper can be added without much effort because the relational database already contains these data, only the source code of Kankoukanyuu (our tool) needs to be extended by a few lines. Though having been created with care regarding correct and well-formed output data, it is not recommended to import the newly created BHT files unchecked. The DBLP team should check the files not to compromise the data quality of DBLP. There might still be undesired format anomalies in the BHT files. The DBLP team also needs to adapt their import system to the extended BHT format developed in this work for the actual import into DBLP. Titles might be in uppercase letters. This could be improved but we have to pay attention because a primitive solution will not work well. For example, we have to be aware of the popular usage of acronyms in computer science. So some words in uppercase letters can be correct. Our tool is optimized for the Digital Library of the IPSJ and their OAI-PMH metadata prefix junii2. It can easily be adapted to support the similar and commonly used metadata prefix oai_dc. So the tool would be able to handle other publication metadata sources that support OAI-PMH. The algorithm for detecting common papers in DBLP and IPSJ DL may be modified to achieve an even better comparison between the databases and detect more common papers. It would be useful to include a Chinese name dictionary in the future and extend the name search of our tool to cover Chinese names as well. -1 One improvement in the future could be storing the most common names (for example, the 100 most common given and family names) in a separate data structure in the RAM. This way we can improve the runtime by often skipping the search in the huge name data. We can still increase the success rate of the Name Matching process too. One way is swapping kanji. A typical Japanese name has two kanji for the given name and two kanji for the family name. The family name shall precede the given name. However, this principle could be violated by the publication source. If the Name Matching is not successful, we may swap the first two for the last two characters and try to find a match again. A second advancement is the additional support of a special Latin character set that is used by Japanese. For instance, we can find the name “Ｋａｉ” instead of “Kai” in the metadata of IPSJ DL. They look very similar and both represent simple Latin letters but their character codes are different. So programs handle them differently. A simple (but yet unimplemented) substitution function can cover these rare and unusual cases. Another possibility to take advantage of this work is extracting the author names in kanji from the relational database. So the DBLP team can insert author metadata for already existing authors in DBLP. We can also have a look at what phases of the Personal Name Matching process have been implemented in this work and to which degree. There are actually different types of Personal Name Matching included in our tool: The “Standardization” is accomplished by a normalization of the Latin input names at the beginning of the process. Kanji input names get trimmed by removing all whitespace. We do not have a “Blocking” phase as it is proposed by Reuther BIBREF26 . When searching a match between transcribed Japanese names with their original kanji representation we even go a contrary way and increase the number of comparisons by adding reasonable other transcriptions to the matching process. Due to efficient data structures and a comparatively small amount of Japanese papers (less than 100000), our tool has an acceptable runtime (the retrieval of the publication metadata from the IPSJ server takes much longer than processing it). In addition, the search for common coauthors will only be done if the author exists in DBLP. The phases “Analysis” and “Decision Model” are entangled in our tool. If we find a match between a (normalized or modified) input name and a name in the name dictionary, we will immediately consider them a successful match and continue parsing the metadata. When we try to find coauthors in DBLP, we take advantage of the combined Jaccard Levenshtein Distance as explained in chapter SECREF5 . Instead of checking the complete output data in the “Performance Measurement” phase, we could only take control samples while implementing, debugging, testing and improving our program. A broad manual check of approximately 90000 publications is not possible within the scope of a diploma thesis. The control samples had the expected and desired content but we cannot guarantee the correctness of the output. Under the assumption that ENAMDICT's entries are correct, the predicted Precision should be about INLINEFORM0 because the tool probably does not produce many false positives. But we cannot say anything about the Recall because ENAMDICT does not cover all names that occur in IPSJ DL. All exceptions resulting from the limits of a name dictionary and a bad data quality are supposed to be handled by the status for author name assignments (described in section UID99 ). This gives us the chance to manually handle the noted exceptions afterwards. All in all, this work is a first approach for an integration of Japanese papers into the DBLP data set and provides a not yet perfect but usable tool for this task. Some major obstacles are overcome.",About the Tool,"The developed tool that is also part of this project is named Kankoukanyuu (刊行加入). Kankou means publication, kanyuu means admission. The whole name indicates the ability to import publications. The tool also allows the assimilation of imported publications, of course. The usable functionalities are: Parsing the DBLP file INLINEFORM0 and converting it to a MySQL database Converting an ENAMDICT name dictionary file to a MySQL database Harvesting the IPSJ server, processing the publication metadata and storing it in a MySQL database Making the harvested publications ready for an import into the DBLP data set by making BHT files",Usage,"The tool has been developed and tested on a Linux system with Intel Core 2 Quad and 8 GB RAM in the local computer pool. It has to be executed by command line like this: 1.5 em1.5 em(*@@*)false6pt java -Xmx5400M -jar kankoukanyuu.jar The parameter -Xmx5400M allows our program to allocate more than 5 GB RAM and store all necessary data in the RAM for an unproblematic execution. Possible command line arguments are:  Parse dplb.xml and fill database tables  Convert ENAMDICT dictionary file to a relational database  Harvest the IPSJ server, fill OAI-PMH data into databases and create BHT files (in SPF) - requires DBLP and ENAMDICT database tables from steps above  Concatenate BHT files in Single Publication Format to one bigger file (file all.bht will be created in every folder with BHT files) - requires BHT files in SPF from step above  Do all of the above  Show help text about usage of the tool The configuration file INLINEFORM0 allows us to change following parameters: Database related parameters (in INLINEFORM0 section): URL ( INLINEFORM1 ), database name ( INLINEFORM2 ), user name ( INLINEFORM3 ) and password ( INLINEFORM4 ) ENAMDICT related parameter (in INLINEFORM0 section): location of ENAMDICT file ( INLINEFORM1 ) ENAMDICT database related parameters (in INLINEFORM0 section): database table name ( INLINEFORM1 ), decision whether to use unclassified names ( INLINEFORM2 ) DBLP related parameter (in INLINEFORM0 section): location of INLINEFORM1 ( INLINEFORM2 ) DBLP database related parameters (in INLINEFORM0 section): database table name for publications ( INLINEFORM1 ), database table name for coauthor relationships (authorscounttable) OAI-PMH database (contains output after harvest and parsing process) related parameters (in INLINEFORM0 section): publication table ( INLINEFORM1 ), authors table ( INLINEFORM2 ), titles table ( INLINEFORM3 ), contributors table ( INLINEFORM4 ), descriptions table ( INLINEFORM5 ) Harvester related parameters (in INLINEFORM0 section): location for storing the harvest ( INLINEFORM1 ), start ID for harvester ( INLINEFORM2 ), end ID for harvester ( INLINEFORM3 ), decision whether to use record lists ( INLINEFORM4 ) BHT export related parameters (in INLINEFORM0 section): location for BHT output files ( INLINEFORM1 ), decision whether to compute and show common coauthors (showcommoncoauthors) Log related parameter (in INLINEFORM0 section): location of log files ( INLINEFORM1 ) A configuration example can be found in the appendix section SECREF172 . The system must support the Japanese language (meaning Japanese characters) to ensure a successful run. Kankoukanyuu does not use any Linux-only commands but has not been tested on Microsoft Windows yet.",,,23-Figure3.3-1.png,Figure 3.3: “With an everyday name you can step through life steadily”,25-Table3.1-1.png,"Table 3.1: Statistics about the ENAMDICT file, data source: [Bre23], self-made creation",30-Figure4.2-1.png,Figure 4.2: New logo of the DBLP project,30-Figure4.3-1.png,Figure 4.3: Screenshot of the DBLP Coauthor Index of Atsuyuki Morishima,35-Figure5.1-1.png,Figure 5.1: Synonyms and homonyms: Michael Keaton,,180221 of 231162 author names could be matched successfully,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,38-Table5.1-1.png,"Table 5.1: Problems with the given name Akiko, sources: ENAMDICT; [ES09], p. 172- 173",38-Table5.2-1.png,"Table 5.2: Problems with the family name Kojima, sources: ENAMDICT; [ES09], p. 223",,,,,,,,44-Table6.1-1.png,Table 6.1: Conversion into the Hepburn transcription system,48-Figure6.1-1.png,Figure 6.1: Part of publication metadata in junii2 format,50-Figure6.2-1.png,Figure 6.2: Statistics about the parsed records,51-Table6.2-1.png,Table 6.2: The different publication types and the number of occurrences,,,Used Technologies,"The tool itself has been written in Java, using the OpenJDK 6. The handling of databases is done by MySQL 5 and JDBC is used to provide MySQL functionalities within Java. External libraries are the Apache Xerces parser and the MySQL Connector/J. The Fat Jar Eclipse Plug-In is used to deploy the complete project into one executable Java JAR file. The execution of Kankoukanyuu becomes more user-friendly this way because external libraries are already included and class paths for external libraries does not need to be specified anymore.",Runtime,"Measurement indicates the following approximated runtimes of Kankoukanyuu: We can make some observations. During the harvest, only ca. 30 minutes were spent on processing the harvested data, the rest is needed to retrieve the data from the Japanese server. Depending on whether the local file system or network file system was used, the runtime for the concatenation differs immensely.",BHT Example Proposed By DBLP,"1.5 em1.5 em(*@@*)false6pt Computer Languages, Systems &amp; Structures (journals/cl)   <h2>Volume 34, Numbers 2-3, July-October 2008</h2> Best Papers 2006 International Smalltalk Conference <ul> <li>Wolfgang De Meuter: Preface. 45 <ee>http://dx.doi.org/10.1016/j.cl.2007.07.001</ee> <li>David R&ouml;thlisberger, Marcus Denker, &Eacute;ric Tanter: Unanticipated partial behavioral reflection: Adapting applications at runtime. 46-65 <ee>http://dx.doi.org/10.1016/j.cl.2007.05.001</ee> <li>Johan Brichau, Andy Kellens, Kris Gybels, Kim Mens, Robert Hirschfeld, Theo D'Hondt: Application-specific models and pointcuts using a logic metalanguage. 66-82 <ee>http://dx.doi.org/10.1016/j.cl.2007.05.004</ee> <li>Alexandre Bergel, St&eacute;phane Ducasse, Oscar Nierstrasz, Roel Wuyts: Stateful traits and their formalization. 83-108 <ee>http://dx.doi.org/10.1016/j.cl.2007.05.003</ee> <li>Alexandre Bergel, St&eacute;phane Ducasse, Colin Putney, Roel Wuyts: Creating sophisticated development tools with OmniBrowser. 109-129 <ee>http://dx.doi.org/10.1016/j.cl.2007.05.005</ee> <li>Luc Fabresse, Christophe Dony, Marianne Huchard: Foundations of a simple and unified component-oriented language. 130-149 <ee>http://dx.doi.org/10.1016/j.cl.2007.05.002</ee> </ul> This is a BHT example proposed by the DBLP team in the DBLP FAQ BIBREF20 .",BHT Example File Created By Kankoukanyuu,"1.5 em1.5 em(*@@*)false6pt <h2>Volume 52, Number 10, October 2011</h2> <ul> <li>Shinsuke Mori, Graham Neubig, Yuuta Tsuboi: A Pointwise Approach to Automatic Word Segmentation. 2944-2952 <ee>http://id.nii.ac.jp/1001/00078161/</ee> <originalname latin=""Shinsuke Mori"">&#x68EE;,&#x4FE1;&#x4ECB;</originalname> <status name=""Shinsuke Mori"">ok</status> <originalname latin=""Graham Neubig"">&#x30CB;&#x30E5;&#x30FC;&#x30D3;&#x30C3;&#x30B0;&#x30B0;&#x30E9;&#x30E0;,</originalname> <status name=""Graham Neubig"">no kanji matching found</status> <originalname latin=""Yuuta Tsuboi"">&#x576A;&#x4E95;,&#x7950;&#x592A;</originalname> <status name=""Yuuta Tsuboi"">ok</status> <originaltitle lang=""ja"" type=""Journal Article"">&#x70B9;&#x4E88;&#x6E2C;&#x306B;&#x3088;&#x308B;&#x81EA;&#x52D5;&#x5358;&#x8A9E;&#x5206;&#x5272;</originaltitle> <commoncoauthors>Masato Mimura</commoncoauthors> </ul> This is an output example of a BHT file in Single Publication Format (before the concatenation step), created by our tool.",Excerpt From dblp.xml,"1.5 em1.5 em(*@@*)false6pt <?xml version=""1.0"" encoding=""ISO-8859-1""?> <!DOCTYPE dblp SYSTEM ""dblp.dtd""> <dblp> <article mdate=""2002-01-03"" key=""persons/Codd71a""> <author>E. F. Codd</author> <title>Further Normalization of the Data Base Relational Model.</title> <journal>IBM Research Report, San Jose, California</journal> <volume>RJ909</volume> <month>August</month> <year>1971</year> <cdrom>ibmTR/rj909.pdf</cdrom> <ee>db/labs/ibm/RJ909.html</ee> </article>   <article mdate=""2002-01-03"" key=""persons/Hall74""> <author>Patrick A. V. Hall</author> <title>Common Subexpression Identification in General Algebraic Systems.</title> <journal>Technical Rep. UKSC 0060, IBM United Kingdom Scientific Centre</journal> <month>November</month> <year>1974</year> </article>   <article mdate=""2002-01-03"" key=""persons/Tresch96""> <author>Markus Tresch</author> <title>Principles of Distributed Object Database Languages.</title> <journal>technical Report 248, ETH Z&uuml;rich, Dept. of Computer Science</journal> <month>July</month> <year>1996</year> </article> ...",Configuration File of Our Tool,1.5 em1.5 em(*@@*)false6pt [db] url=myserver db=mydbname user=myusername password=mypassword [japnamesdb] table=japnames useunclassifiednames=false [dblpdb] authorscounttable=dblpauthors dblptable=dblp [oaidb] publicationtable=oai_publications authorstable=oai_authors titlestable=oai_titles contributorstable=oai_contributors descriptionstable=oai_descriptions [enamdict] file=./enamdict [harvester] filespath=./files-harvester minid=1 maxid=100000 uselistrecords=true [dblp] xmlfile=/dblp/dblp.xml [bhtexport] path=./bht showcommoncoauthors=true [log] path=./log,,,,,,,,,,,51-Figure6.3-1.png,Figure 6.3: Statistics about the types of parsed publications,52-Figure6.4-1.png,Figure 6.4: Statistics about the language of parsed publications,53-Figure6.5-1.png,Figure 6.5: Statistics about the evaluation of the quality of author name assignments,70-FigureB.1-1.png,Figure B.1: Differences between the transcription systems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enriching Existing Conversational Emotion Datasets with Dialogue Acts using Neural Annotators.,"The recognition of emotion and dialogue acts enrich conversational analysis and help to build natural dialogue systems. Emotion makes us understand feelings and dialogue acts reflect the intentions and performative functions in the utterances. However, most of the textual and multi-modal conversational emotion datasets contain only emotion labels but not dialogue acts. To address this problem, we propose to use a pool of various recurrent neural models trained on a dialogue act corpus, with or without context. These neural models annotate the emotion corpus with dialogue act labels and an ensemble annotator extracts the final dialogue act label. We annotated two popular multi-modal emotion datasets: IEMOCAP and MELD. We analysed the co-occurrence of emotion and dialogue act labels and discovered specific relations. For example, Accept/Agree dialogue acts often occur with the Joy emotion, Apology with Sadness, and Thanking with Joy. We make the Emotional Dialogue Act (EDA) corpus publicly available to the research community for further study and analysis.",Introduction,"With the growing demand for human-computer/robot interaction systems, detecting the emotional state of the user can heavily benefit a conversational agent to respond at an appropriate emotional level. Emotion recognition in conversations has proven important for potential applications such as response recommendation or generation, emotion-based text-to-speech, personalisation, etc. Human emotional states can be expressed verbally and non-verbally BIBREF0, BIBREF1, however, while building an interactive dialogue system, the interface needs dialogue acts. A typical dialogue system consists of a language understanding module which requires to determine the meaning of and intention in the human input utterances BIBREF2, BIBREF3. Also, in discourse or conversational analysis, dialogue acts are the main linguistic features to consider BIBREF4. A dialogue act provides an intention and performative function in an utterance of the dialogue. For example, it can infer a user's intention by distinguishing Question, Answer, Request, Agree/Reject, etc. and performative functions such as Acknowledgement, Conversational-opening or -closing, Thanking, etc. The dialogue act information together with emotional states can be very useful for a spoken dialogue system to produce natural interaction BIBREF5. The research in emotion recognition is growing very rapidly and many datasets are available, such as text-based, speech- or vision-level, and multimodal emotion data. Emotion expression recognition is a challenging task and hence multimodality is crucial BIBREF0. However, few conversational multi-modal emotion recognition datasets are available, for example, IEMOCAP BIBREF6, SEMAINE BIBREF7, MELD BIBREF8. They are multi-modal dyadic conversational datasets containing audio-visual and conversational transcripts. Every utterance in these datasets is labeled with an emotion label. In this work, we apply an automated neural ensemble annotation process for dialogue act labeling. Several neural models are trained with the Switchboard Dialogue Act (SwDA) Corpus BIBREF9, BIBREF10 and used for inferring dialogue acts on the emotion datasets. We ensemble five model output labels by checking majority occurrences (most of the model labels are the same) and ranking confidence values of the models. We have annotated two potential multi-modal conversation datasets for emotion recognition: IEMOCAP (Interactive Emotional dyadic MOtion CAPture database) BIBREF6 and MELD (Multimodal EmotionLines Dataset) BIBREF8. Figure FIGREF2, shows an example of dialogue acts with emotion and sentiment labels from the MELD dataset. We confirmed the reliability of annotations with inter-annotator metrics. We analysed the co-occurrences of the dialogue act and emotion labels and discovered a key relationship between them; certain dialogue acts of the utterances show significant and useful association with respective emotional states. For example, Accept/Agree dialogue act often occurs with the Joy emotion while Reject with Anger, Acknowledgements with Surprise, Thanking with Joy, and Apology with Sadness, etc. The detailed analysis of the emotional dialogue acts (EDAs) and annotated datasets are being made available at the SECURE EU Project website.",Annotation of Emotional Dialogue Acts ::: Data for Conversational Emotion Analysis,"There are two emotion taxonomies: (1) discrete emotion categories (DEC) and (2) fined-grained dimensional basis of emotion states (DBE). The DECs are Joy, Sadness, Fear, Surprise, Disgust, Anger and Neutral; identified by Ekman et al. ekman1987universalemos. The DBE of the emotion is usually elicited from two or three dimensions BIBREF1, BIBREF11, BIBREF12. A two-dimensional model is commonly used with Valence and Arousal (also called activation), and in the three-dimensional model, the third dimension is Dominance. IEMOCAP is annotated with all DECs and two additional emotion classes, Frustration and Excited. IEMOCAP is also annotated with three DBE, that includes Valance, Arousal and Dominance BIBREF6. MELD BIBREF8, which is an evolved version of the Emotionlines dataset developed by BIBREF13, is annotated with exactly 7 DECs and sentiments (positive, negative and neutral).",Annotation of Emotional Dialogue Acts ::: Dialogue Act Tagset and SwDA Corpus,"There have been many taxonomies for dialogue acts: speech acts BIBREF14 refer to the utterance, not only to present information but to the action at is performed. Speech acts were later modified into five classes (Assertive, Directive, Commissive, Expressive, Declarative) BIBREF15. There are many such standard taxonomies and schemes to annotate conversational data, and most of them follow the discourse compositionality. These schemes have proven their importance for discourse or conversational analysis BIBREF16. During the increased development of dialogue systems and discourse analysis, the standard taxonomy was introduced in recent decades, called Dialogue Act Markup in Several Layers (DAMSL) tag set. According to DAMSL, each DA has a forward-looking function (such as Statement, Info-request, Thanking) and a backwards-looking function (such as Accept, Reject, Answer) BIBREF17. The DAMSL annotation includes not only the utterance-level but also segmented-utterance labelling. However, in the emotion datasets, the utterances are not segmented, as we can see in Figure FIGREF2 first or fourth utterances are not segmented as two separate. The fourth utterance, it could be segmented to have two dialogue act labels, for example, a statement (sd) and a question (qy). That provides very fine-grained DA classes and follows the concept of discourse compositionality. DAMSL distinguishes wh-question (qw), yes-no question (qy), open-ended (qo), and or-question (qr) classes, not just because these questions are syntactically distinct, but also because they have different forward functions BIBREF18. For example, yes-no question is more likely to get a “yes"" answer than a wh-question (qw). This also gives an intuition that the answers follow the syntactic formulation of question, providing a context. For example, qy is used for a question that, from a discourse perspective, expects a Yes (ny) or No (nn) answer. We have investigated the annotation method and trained our neural models with the Switchboard Dialogue Act (SwDA) Corpus BIBREF9, BIBREF10. SwDA Corpus is annotated with the DAMSL tag set and it is been used for reporting and bench-marking state-of-the-art results in dialogue act recognition tasks BIBREF19, BIBREF20, BIBREF21 which makes it ideal for our use case. The Switchboard DAMSL Coders Manual can be followed for knowing more about the dialogue act labels.",Annotation of Emotional Dialogue Acts ::: Neural Model Annotators,"We adopted the neural architectures based on Bothe et al. bothe2018discourse where two variants are: non-context model (classifying at utterance level) and context model (recognizing the dialogue act of the current utterance given a few preceding utterances). From conversational analysis using dialogue acts in Bothe et al. bothe2018interspeech, we learned that the preceding two utterances contribute significantly to recognizing the dialogue act of the current utterance. Hence, we adapt this setting for the context model and create a pool of annotators using recurrent neural networks (RNNs). RNNs can model the contextual information in the sequence of words of an utterance and in the sequence of utterances of a dialogue. Each word in an utterance is represented with a word embedding vector of dimension 1024. We use the word embedding vectors from pre-trained ELMo (Embeddings from Language Models) embeddings BIBREF22. We have a pool of five neural annotators as shown in Figure FIGREF6. Our online tool called Discourse-Wizard is available to practice automated dialogue act labeling. In this tool we use the same neural architectures but model-trained embeddings (while, in this work we use pre-trained ELMo embeddings, as they are better performant but computationally and size-wise expensive to be hosted in the online tool). The annotators are: Utt-level 1 Dialogue Act Neural Annotator (DANA) is an utterance-level classifier that uses word embeddings ($w$) as an input to an RNN layer, attention mechanism and computes the probability of dialogue acts ($da$) using the softmax function (see in Figure FIGREF10, dotted line utt-l1). This model achieved 75.13% accuracy on the SwDA corpus test set. Context 1 DANA is a context model that uses 2 preceding utterances while recognizing the dialogue act of the current utterance (see context model with con1 line in Figure FIGREF10). It uses a hierarchical RNN with the first RNN layer to encode the utterance from word embeddings ($w$) and the second RNN layer is provided with three utterances ($u$) (current and two preceding) composed from the first layer followed by the attention mechanism ($a$), where $\sum _{n=0}^{n} a_{t-n} = 1$. Finally, the softmax function is used to compute the probability distribution. This model achieved 77.55% accuracy on the SwDA corpus test set. Utt-level 2 DANA is another utterance-level classifier which takes an average of the word embeddings in the input utterance and uses a feedforward neural network hidden layer (see utt-l2 line in Figure FIGREF10, where $mean$ passed to $softmax$ directly). Similar to the previous model, it computes the probability of dialogue acts using the softmax function. This model achieved 72.59% accuracy on the test set of the SwDA corpus. Context 2 DANA is another context model that uses three utterances similar to the Context 1 DANA model, but the utterances are composed as the mean of the word embeddings over each utterance, similar to the Utt-level 2 model ($mean$ passed to context model in Figure FIGREF10 with con2 line). Hence, the Context 2 DANA model is composed of one RNN layer with three input vectors, finally topped with the softmax function for computing the probability distribution of the dialogue acts. This model achieved 75.97% accuracy on the test set of the SwDA corpus. Context 3 DANA is a context model that uses three utterances similar to the previous models, but the utterance representations combine both features from the Context 1 and Context 2 models (con1 and con2 together in Figure FIGREF10). Hence, the Context 3 DANA model combines features of almost all the previous four models to provide the recognition of the dialogue acts. This model achieves 75.91% accuracy on the SwDA corpus test set.",Annotation of Emotional Dialogue Acts ::: Ensemble of Neural Annotators,"First preference is given to the labels that are perfectly matching in all the neural annotators. In Table TABREF11, we can see that both datasets have about 40% of exactly matching labels over all models (AM). Then priority is given to the context-based models to check if the label in all context models is matching perfectly. In case two out of three context models are correct, then it is being checked if that label is also produced by at least one of the non-context models. Then, we allow labels to rely on these at least two context models. As a result, about 47% of the labels are taken based on the context models (CM). When we see that none of the context models is producing the same results, then we rank the labels with their respective confidence values produced as a probability distribution using the $softmax$ function. The labels are sorted in descending order according to confidence values. Then we check if the first three (case when one context model and both non-context models produce the same label) or at least two labels are matching, then we allow to pick that one. There are about 3% in IEMOCAP and 5% in MELD (BM). Finally, when none the above conditions are fulfilled, we leave out the label with an unknown category. This unknown category of the dialogue act is labeled with `xx' in the final annotations, and they are about 7% in IEMOCAP and 11% in MELD (NM). The statistics of the EDAs is reported in Table TABREF13 for both datasets. Total utterances in MELD includes training, validation and test datasets.",Annotation of Emotional Dialogue Acts ::: Reliability of Neural Annotators,"The pool of neural annotators provides a fair range of annotations, and we checked the reliability with the following metrics BIBREF23. Krippendorff's Alpha ($\alpha $) is a reliability coefficient developed to measure the agreement among observers, annotators, and raters, and is often used in emotion annotation BIBREF24. We apply it on the five neural annotators at the nominal level of measurement of dialogue act categories. $\alpha $ is computed as follows: where $D_{o}$ is the observed disagreement and $D_{e}$ is the disagreement that is expected by chance. $\alpha =1$ means all annotators produce the same label, while $\alpha =0$ would mean none agreed on any label. As we can see in Table TABREF20, both datasets IEMOCAP and MELD produce significant inter-neural annotator agreement, 0.553 and 0.494, respectively. A very popular inter-annotator metric is Fleiss' Kappa score, also reported in Table TABREF20, which determines consistency in the ratings. The kappa $k$ can be defined as, where the denominator $1 -\bar{P}_e$ elicits the degree of agreement that is attainable above chance, and the numerator $\bar{P} -\bar{P}_e$ provides the degree of the agreement actually achieved above chance. Hence, $k = 1$ if the raters agree completely, and $k = 0$ when none reach any agreement. We got 0.556 and 0.502 for IEOMOCAP and MELD respectively with our five neural annotators. This indicated that the annotators are labeling the dialogue acts reliably and consistently. We also report the Spearman's correlation between context-based models (Context1 and Context2), and it shows a strong correlation between them (Table TABREF20). While using the labels we checked the absolute match between all context-based models and hence their strong correlation indicates their robustness.",EDAs Analysis,"We can see emotional dialogue act co-occurrences with respect to emotion labels in Figure FIGREF12 for both datasets. There are sets of three bars per dialogue act in the figure, the first and second bar represent emotion labels of IEMOCAP (IE) and MELD (ME), and the third bar is for MELD sentiment (MS) labels. MELD emotion and sentiment statistics are interesting as they are strongly correlated to each other. The bars contain the normalized number of utterances for emotion labels with respect to the total number of utterances for that particular dialogue act category. The statements without-opinion (sd) and with-opinion (sv) contain utterances with almost all emotions. Many neutral utterances are spanning over all the dialogue acts. Quotation (⌃q) dialogue acts, on the other hand, are mostly used with `Anger' and `Frustration' (in case of IEMOCAP), however, some utterances with `Joy' or `Sadness' as well (see examples in Table TABREF21). Action Directive (ad) dialogue act utterances, which are usually orders, frequently occur with `Anger' or `Frustration' although many with `Happy' emotion in case of the MELD dataset. Acknowledgements (b) are mostly with positive or neutral, however, Appreciation (ba) and Rhetorical (bh) backchannels often occur with a greater number in `Surprise', `Joy' and/or with `Excited' (in case of IEMOCAP). Questions (qh, qw, qy and qy⌃d) are mostly asked with emotions `Surprise', `Excited', `Frustration' or `Disgust' (in case of MELD) and many are neutral. No-answers (nn) are mostly `Sad' or `Frustrated' as compared to yes-answers (ny). Forward-functions such as Apology (fa) are mostly with `Sadness' whereas Thanking (ft) and Conventional-closing or -opening (fc or fp) are usually with `Joy' or `Excited'. We also noticed that both datasets exhibit a similar relation between dialogue act and emotion. It is important to notice that the dialogue act annotation is based on the given transcripts, however, the emotional expressions are better perceived with audio or video BIBREF6. We report some examples where we mark the utterances with an determined label (xx) in the last row of Table TABREF21. They are skipped from the final annotation because of not fulfilling the conditions explained in Section SECREF14 It is also interesting to see the previous utterance dialogue acts (P-DA) of those skipped utterances, and the sequence of the labels can be followed from Figure FIGREF6 (utt-l1, utt-l2, con1, con2, con3). In the first example, the previous utterance was b, and three DANA models produced labels of the current utterance as b, but it is skipped because the confidence values were not sufficient to bring it as a final label. The second utterance can be challenging even for humans to perceive with any of the dialogue acts. However, the third and fourth utterances are followed by a yes-no question (qy), and hence, we can see in the third example, that context models tried their best to at least perceive it as an answer (ng, ny, nn). The last utterance, “I'm so sorry!"", has been completely disagreed by all the five annotators. Similar apology phrases are mostly found with `Sadness' emotion label's, and the correct dialogue act is Apology (fa). However, they are placed either in the sd or in ba dialogue act category. We believe that with human annotator's help those labels of the utterances can be corrected with very limited efforts.",Conclusion and Future Work,"In this work, we presented a method to extend conversational multi-modal emotion datasets with dialogue act labels. We successfully show this on two well-established emotion datasets: IEMOCAP and MELD, which we labeled with dialogue acts and made publicly available for further study and research. As a first insight, we found that many of the dialogue acts and emotion labels follow certain relations. These relations can be useful to learn about the emotional behaviours with dialogue acts to build a natural dialogue system and for deeper conversational analysis. The conversational agent might benefit in generating an appropriate response when considering both emotional states and dialogue acts in the utterances. In future work, we foresee the human in the loop for the annotation process along with a pool of automated neural annotators. Robust annotations can be achieved with very little human effort and supervision, for example, observing and correcting the final labels produced by ensemble output labels from the neural annotators. The human-annotator might also help to achieve segmented-utterance labelling of the dialogue acts. We also plan to use these datasets for conversational analysis to infer interactive behaviours of the emotional states with respect to dialogue acts. In our recent work, where we used dialogue acts to build a dialogue system for a social robot, we find this study and dataset very helpful. For example, we can extend our robotic conversational system to consider emotion as an added linguistic feature to produce natural interaction.",Acknowledgements,We would like to acknowledge funding from the European Union's Horizon 2020 research and innovation programme under the Marie Sklodowska Curie grant agreement No 642667 (SECURE).,,,,,,,,,,,,,,,,,,,,,,,,,What other relations were found in the datasets?,5937ebbf04f62d41b48cbc6b5c38fc309e5c2328,two,unfamiliar,no,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,,"We can see emotional dialogue act co-occurrences with respect to emotion labels in Figure FIGREF12 for both datasets. There are sets of three bars per dialogue act in the figure, the first and second bar represent emotion labels of IEMOCAP (IE) and MELD (ME), and the third bar is for MELD sentiment (MS) labels. MELD emotion and sentiment statistics are interesting as they are strongly correlated to each other. The bars contain the normalized number of utterances for emotion labels with respect to the total number of utterances for that particular dialogue act category. The statements without-opinion (sd) and with-opinion (sv) contain utterances with almost all emotions. Many neutral utterances are spanning over all the dialogue acts. Quotation (⌃q) dialogue acts, on the other hand, are mostly used with `Anger' and `Frustration' (in case of IEMOCAP), however, some utterances with `Joy' or `Sadness' as well (see examples in Table TABREF21). Action Directive (ad) dialogue act utterances, which are usually orders, frequently occur with `Anger' or `Frustration' although many with `Happy' emotion in case of the MELD dataset. Acknowledgements (b) are mostly with positive or neutral, however, Appreciation (ba) and Rhetorical (bh) backchannels often occur with a greater number in `Surprise', `Joy' and/or with `Excited' (in case of IEMOCAP). Questions (qh, qw, qy and qy⌃d) are mostly asked with emotions `Surprise', `Excited', `Frustration' or `Disgust' (in case of MELD) and many are neutral. No-answers (nn) are mostly `Sad' or `Frustrated' as compared to yes-answers (ny). Forward-functions such as Apology (fa) are mostly with `Sadness' whereas Thanking (ft) and Conventional-closing or -opening (fc or fp) are usually with `Joy' or `Excited'. FLOAT SELECTED: Figure 4: EDAs: Visualizing co-occurrence of utterances with respect to emotion states in the particular dialogue acts (only major and significant are shown here). IE: IEMOCAP, ME: MELD Emotion and MS: MELD Sentiment.","The statements without-opinion (sd) and with-opinion (sv) contain utterances with almost all emotions. Many neutral utterances are spanning over all the dialogue acts.

Quotation (⌃q) dialogue acts, on the other hand, are mostly used with `Anger' and `Frustration' (in case of IEMOCAP), however, some utterances with `Joy' or `Sadness' as well (see examples in Table TABREF21). Action Directive (ad) dialogue act utterances, which are usually orders, frequently occur with `Anger' or `Frustration' although many with `Happy' emotion in case of the MELD dataset. Acknowledgements (b) are mostly with positive or neutral, however, Appreciation (ba) and Rhetorical (bh) backchannels often occur with a greater number in `Surprise', `Joy' and/or with `Excited' (in case of IEMOCAP). Questions (qh, qw, qy and qy⌃d) are mostly asked with emotions `Surprise', `Excited', `Frustration' or `Disgust' (in case of MELD) and many are neutral. No-answers (nn) are mostly `Sad' or `Frustrated' as compared to yes-answers (ny). Forward-functions such as Apology (fa) are mostly with `Sadness' whereas Thanking (ft) and Conventional-closing or -opening (fc or fp) are usually with `Joy' or `Excited'.

 FLOAT SELECTED: Figure 4: EDAs: Visualizing co-occurrence of utterances with respect to emotion states in the particular dialogue acts (only major and significant are shown here). IE: IEMOCAP, ME: MELD Emotion and MS: MELD Sentiment.",706d31c3b62c8a0164277513b424f6bb322e2f69,2cfd959e433f290bb50b55722370f0d22fe090b7,,,,,,,,,How does the ensemble annotator extract the final label?,dcd6f18922ac5c00c22cef33c53ff5ae08b42298,two,unfamiliar,no,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,,"First preference is given to the labels that are perfectly matching in all the neural annotators. In Table TABREF11, we can see that both datasets have about 40% of exactly matching labels over all models (AM). Then priority is given to the context-based models to check if the label in all context models is matching perfectly. In case two out of three context models are correct, then it is being checked if that label is also produced by at least one of the non-context models. Then, we allow labels to rely on these at least two context models. As a result, about 47% of the labels are taken based on the context models (CM). When we see that none of the context models is producing the same results, then we rank the labels with their respective confidence values produced as a probability distribution using the $softmax$ function. The labels are sorted in descending order according to confidence values. Then we check if the first three (case when one context model and both non-context models produce the same label) or at least two labels are matching, then we allow to pick that one. There are about 3% in IEMOCAP and 5% in MELD (BM). Finally, when none the above conditions are fulfilled, we leave out the label with an unknown category. This unknown category of the dialogue act is labeled with `xx' in the final annotations, and they are about 7% in IEMOCAP and 11% in MELD (NM). The statistics of the EDAs is reported in Table TABREF13 for both datasets. Total utterances in MELD includes training, validation and test datasets.","First preference is given to the labels that are perfectly matching in all the neural annotators. In Table TABREF11, we can see that both datasets have about 40% of exactly matching labels over all models (AM). Then priority is given to the context-based models to check if the label in all context models is matching perfectly. In case two out of three context models are correct, then it is being checked if that label is also produced by at least one of the non-context models. Then, we allow labels to rely on these at least two context models. As a result, about 47% of the labels are taken based on the context models (CM). When we see that none of the context models is producing the same results, then we rank the labels with their respective confidence values produced as a probability distribution using the $softmax$ function. The labels are sorted in descending order according to confidence values. Then we check if the first three (case when one context model and both non-context models produce the same label) or at least two labels are matching, then we allow to pick that one. There are about 3% in IEMOCAP and 5% in MELD (BM).

Finally, when none the above conditions are fulfilled, we leave out the label with an unknown category. This unknown category of the dialogue act is labeled with `xx' in the final annotations, and they are about 7% in IEMOCAP and 11% in MELD (NM).",33b18d270e3871d77ad11e6c8fd0fbf35e20cdf3,2cfd959e433f290bb50b55722370f0d22fe090b7,How were dialogue act labels defined?,2965c86467d12b79abc16e1457d848cb6ca88973,two,unfamiliar,no,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,,"There have been many taxonomies for dialogue acts: speech acts BIBREF14 refer to the utterance, not only to present information but to the action at is performed. Speech acts were later modified into five classes (Assertive, Directive, Commissive, Expressive, Declarative) BIBREF15. There are many such standard taxonomies and schemes to annotate conversational data, and most of them follow the discourse compositionality. These schemes have proven their importance for discourse or conversational analysis BIBREF16. During the increased development of dialogue systems and discourse analysis, the standard taxonomy was introduced in recent decades, called Dialogue Act Markup in Several Layers (DAMSL) tag set. According to DAMSL, each DA has a forward-looking function (such as Statement, Info-request, Thanking) and a backwards-looking function (such as Accept, Reject, Answer) BIBREF17.","There have been many taxonomies for dialogue acts: speech acts BIBREF14 refer to the utterance, not only to present information but to the action at is performed. Speech acts were later modified into five classes (Assertive, Directive, Commissive, Expressive, Declarative) BIBREF15. There are many such standard taxonomies and schemes to annotate conversational data, and most of them follow the discourse compositionality. These schemes have proven their importance for discourse or conversational analysis BIBREF16. During the increased development of dialogue systems and discourse analysis, the standard taxonomy was introduced in recent decades, called Dialogue Act Markup in Several Layers (DAMSL) tag set. According to DAMSL, each DA has a forward-looking function (such as Statement, Info-request, Thanking) and a backwards-looking function (such as Accept, Reject, Answer) BIBREF17.",1aaaeb22e8d034e77a3081a514770a00556dcd95,2cfd959e433f290bb50b55722370f0d22fe090b7,,,,,,,,How many models were used?,b99948ac4810a7fe3477f6591b8cf211d6398e67,two,unfamiliar,no,,c1fbdd7a261021041f75fbe00a55b4c386ebbbb4,False,,,"In this work, we apply an automated neural ensemble annotation process for dialogue act labeling. Several neural models are trained with the Switchboard Dialogue Act (SwDA) Corpus BIBREF9, BIBREF10 and used for inferring dialogue acts on the emotion datasets. We ensemble five model output labels by checking majority occurrences (most of the model labels are the same) and ranking confidence values of the models. We have annotated two potential multi-modal conversation datasets for emotion recognition: IEMOCAP (Interactive Emotional dyadic MOtion CAPture database) BIBREF6 and MELD (Multimodal EmotionLines Dataset) BIBREF8. Figure FIGREF2, shows an example of dialogue acts with emotion and sentiment labels from the MELD dataset. We confirmed the reliability of annotations with inter-annotator metrics. We analysed the co-occurrences of the dialogue act and emotion labels and discovered a key relationship between them; certain dialogue acts of the utterances show significant and useful association with respective emotional states. For example, Accept/Agree dialogue act often occurs with the Joy emotion while Reject with Anger, Acknowledgements with Surprise, Thanking with Joy, and Apology with Sadness, etc. The detailed analysis of the emotional dialogue acts (EDAs) and annotated datasets are being made available at the SECURE EU Project website. We adopted the neural architectures based on Bothe et al. bothe2018discourse where two variants are: non-context model (classifying at utterance level) and context model (recognizing the dialogue act of the current utterance given a few preceding utterances). From conversational analysis using dialogue acts in Bothe et al. bothe2018interspeech, we learned that the preceding two utterances contribute significantly to recognizing the dialogue act of the current utterance. Hence, we adapt this setting for the context model and create a pool of annotators using recurrent neural networks (RNNs). RNNs can model the contextual information in the sequence of words of an utterance and in the sequence of utterances of a dialogue. Each word in an utterance is represented with a word embedding vector of dimension 1024. We use the word embedding vectors from pre-trained ELMo (Embeddings from Language Models) embeddings BIBREF22. We have a pool of five neural annotators as shown in Figure FIGREF6. Our online tool called Discourse-Wizard is available to practice automated dialogue act labeling. In this tool we use the same neural architectures but model-trained embeddings (while, in this work we use pre-trained ELMo embeddings, as they are better performant but computationally and size-wise expensive to be hosted in the online tool). The annotators are:","n this work, we apply an automated neural ensemble annotation process for dialogue act labeling. Several neural models are trained with the Switchboard Dialogue Act (SwDA) Corpus BIBREF9, BIBREF10 and used for inferring dialogue acts on the emotion datasets. We ensemble five model output labels by checking majority occurrences (most of the model labels are the same) and ranking confidence values of the models. We adopted the neural architectures based on Bothe et al. bothe2018discourse where two variants are: non-context model (classifying at utterance level) and context model (recognizing the dialogue act of the current utterance given a few preceding utterances).",058a263bde2c426f0df7b096a445571b1cca62b8,2cfd959e433f290bb50b55722370f0d22fe090b7,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2-Figure1-1.png,"Figure 1: Emotional Dialogue Acts: Example of a dialogue from MELD representing emotions and sentiment (rectangular boxes), in our work, we add dialogue acts (rounded boxes). Image source Poria et al. (2019).",3-Figure2-1.png,"Figure 2: Setting of the annotation process of the EDAs, above example utterances (with speaker identity) and emotion labels are from IEMOCAP database.",4-Figure3-1.png,Figure 3: Recurrent neural attention architecture with the utterance-level and context-based models.,4-Table2-1.png,Table 2: Number of utterances per DA in respective datasets. All values are in percentages (%) of the total number of utterances. IEMO is for IEMOCAP.,4-Table1-1.png,"Table 1: Annotations Statistics of EDAs - AM: All Absolute Match (in %), CM: Context-based Models Absolute Match (in %, matched all context models or at least two context models matched with one non-context model), BM: Based-on Confidence Ranking, and NM: No Match (in %) (these labeled as ‘xx’: determined in EDAs).",5-Figure4-1.png,"Figure 4: EDAs: Visualizing co-occurrence of utterances with respect to emotion states in the particular dialogue acts (only major and significant are shown here). IE: IEMOCAP, ME: MELD Emotion and MS: MELD Sentiment.",,,,,,,,,"First preference is given to the labels that are perfectly matching in all the neural annotators. In case two out of three context models are correct, then it is being checked if that label is also produced by at least one of the non-context models. When we see that none of the context models is producing the same results, then we rank the labels with their respective confidence values produced as a probability distribution using the $softmax$ function. The labels are sorted in descending order according to confidence values. Then we check if the first three (case when one context model and both non-context models produce the same label) or at least two labels are matching, then we allow to pick that one.  Finally, when none the above conditions are fulfilled, we leave out the label with an unknown category.",Dialogue Act Markup in Several Layers (DAMSL) tag set,5-Table3-1.png,"Table 3: Annotations Metrics of EDAs - α: Krippendorff’s Alpha coefficient, k: Fleiss’ Kappa score, and SCCM: Spearman Correlation between Context-based Models.",6-Table4-1.png,"Table 4: Examples of EDAs with annotation from the MELD dataset. Emotion and sentiment labels are given in the dataset, while EDAs are determined by our ensemble of models. P-DA: previous utterance dialogue act.",,,,,,,,"Quotation (⌃q) dialogue acts, on the other hand, are mostly used with `Anger' and `Frustration' Action Directive (ad) dialogue act utterances, which are usually orders, frequently occur with `Anger' or `Frustration' although many with `Happy' emotion in case of the MELD dataset Acknowledgements (b) are mostly with positive or neutral Appreciation (ba) and Rhetorical (bh) backchannels often occur with a greater number in `Surprise', `Joy' and/or with `Excited' (in case of IEMOCAP) Questions (qh, qw, qy and qy⌃d) are mostly asked with emotions `Surprise', `Excited', `Frustration' or `Disgust' (in case of MELD) and many are neutral No-answers (nn) are mostly `Sad' or `Frustrated' as compared to yes-answers (ny). Forward-functions such as Apology (fa) are mostly with `Sadness' whereas Thanking (ft) and Conventional-closing or -opening (fc or fp) are usually with `Joy' or `Excited'",,,,,,,,,five,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
